id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:47,Performance,load,load,47,// Check that the base address of flat scratch load/store in the form of `base +; // offset` is legal to be put in SGPR/VGPR (i.e. unsigned per hardware; // requirement). We always treat the first operand as the base address here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:249,Security,access,access,249,"// If the immediate offset is negative and within certain range, the base; // address cannot also be negative. If the base is also negative, the sum; // would be either negative or much larger than the valid range of scratch; // memory a thread can access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:249,Security,access,access,249,"// If the immediate offset is negative and within certain range, the base; // address cannot also be negative. If the base is also negative, the sum; // would be either negative or much larger than the valid range of scratch; // memory a thread can access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:328,Performance,perform,perform,328,"// Offsets in vaddr must be positive if range checking is enabled.; //; // The total computation of vaddr + soffset + offset must not overflow. If; // vaddr is negative, even if offset is 0 the sgpr offset add will end up; // overflowing.; //; // Prior to gfx9, MUBUF instructions with the vaddr offset enabled would; // always perform a range check. If a negative vaddr base index was used,; // this would fail the range check. The overall address computation would; // compute a valid address, but this doesn't happen due to the range; // check. For out-of-bounds MUBUF loads, a 0 is returned.; //; // Therefore it should be safe to fold any VGPR offset on gfx9 into the; // MUBUF vaddr, but not on older subtargets which can only do this if the; // sign bit is known 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:572,Performance,load,loads,572,"// Offsets in vaddr must be positive if range checking is enabled.; //; // The total computation of vaddr + soffset + offset must not overflow. If; // vaddr is negative, even if offset is 0 the sgpr offset add will end up; // overflowing.; //; // Prior to gfx9, MUBUF instructions with the vaddr offset enabled would; // always perform a range check. If a negative vaddr base index was used,; // this would fail the range check. The overall address computation would; // compute a valid address, but this doesn't happen due to the range; // check. For out-of-bounds MUBUF loads, a 0 is returned.; //; // Therefore it should be safe to fold any VGPR offset on gfx9 into the; // MUBUF vaddr, but not on older subtargets which can only do this if the; // sign bit is known 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:627,Safety,safe,safe,627,"// Offsets in vaddr must be positive if range checking is enabled.; //; // The total computation of vaddr + soffset + offset must not overflow. If; // vaddr is negative, even if offset is 0 the sgpr offset add will end up; // overflowing.; //; // Prior to gfx9, MUBUF instructions with the vaddr offset enabled would; // always perform a range check. If a negative vaddr base index was used,; // this would fail the range check. The overall address computation would; // compute a valid address, but this doesn't happen due to the range; // check. For out-of-bounds MUBUF loads, a 0 is returned.; //; // Therefore it should be safe to fold any VGPR offset on gfx9 into the; // MUBUF vaddr, but not on older subtargets which can only do this if the; // sign bit is known 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:10,Performance,load,load,10,"// Find a load or store from corresponding pattern root.; // Roots may be build_vector, bitconvert or their combinations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:154,Security,access,access,154,"// If the offset doesn't fit, put the low bits into the offset field and; // add the rest.; //; // For a FLAT instruction the hardware decides whether to access; // global/scratch/shared memory based on the high bits of vaddr,; // ignoring the offset field, so we have to ensure that when we add; // remainder to vaddr it still points into the same underlying object.; // The easiest way to do that is to make sure that we split the offset; // into two pieces that are both >= 0 or both <= 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:100,Usability,usab,usable,100,// TODO: Should this try to use a scalar add pseudo if the base address; // is uniform and saddr is usable?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:95,Performance,perform,perform,95,// We are adding a 64 bit SGPR and a constant. If constant bus limit; // is 1 we would need to perform 1 or 2 extra moves for each half of; // the constant and it is better to do a scalar add and then issue a; // single VALU instruction to materialize zero. Otherwise it is less; // instructions to perform VALU adds with immediates or inline literals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:299,Performance,perform,perform,299,// We are adding a 64 bit SGPR and a constant. If constant bus limit; // is 1 we would need to perform 1 or 2 extra moves for each half of; // the constant and it is better to do a scalar add and then issue a; // single VALU instruction to materialize zero. Otherwise it is less; // instructions to perform VALU adds with immediates or inline literals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:13,Modifiability,variab,variable,13,// Match the variable offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:61,Safety,avoid,avoid,61,// Materialize this into a scalar move for scalar address to avoid; // readfirstlane.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:63,Security,access,access,63,// Check whether the flat scratch SVS swizzle bug affects this access.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:40,Security,access,accesses,40,// The bug affects the swizzling of SVS accesses if there is any carry out; // from the two low order bits (i.e. from bit 1 into bit 2) when adding; // voffset to (soffset + inst_offset).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:156,Availability,avail,available,156,"// Match an immediate (if Offset is not null) or an SGPR (if SOffset is; // not null) offset. If Imm32Only is true, match only 32-bit immediate; // offsets available on CI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:8,Modifiability,extend,extend,8,// Zero-extend a 32-bit address.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:188,Availability,avail,available,188,"// Match a base and an immediate (if Offset is not null) or an SGPR (if; // SOffset is not null) or an immediate+SGPR offset. If Imm32Only is; // true, match only 32-bit immediate offsets available on CI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:76,Integrability,wrap,wraparound,76,"// A 32-bit (address + offset) should not cause unsigned 32-bit integer; // wraparound, because s_load instructions perform the addition in 64 bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:116,Performance,perform,perform,116,"// A 32-bit (address + offset) should not cause unsigned 32-bit integer; // wraparound, because s_load instructions perform the addition in 64 bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:16,Availability,mask,mask,16,"// ""(a srl b) & mask"" ---> ""BFE_U32 a, b, popcount(mask)""; // Predicate: isMask(mask)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:51,Availability,mask,mask,51,"// ""(a srl b) & mask"" ---> ""BFE_U32 a, b, popcount(mask)""; // Predicate: isMask(mask)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:80,Availability,mask,mask,80,"// ""(a srl b) & mask"" ---> ""BFE_U32 a, b, popcount(mask)""; // Predicate: isMask(mask)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:9,Availability,mask,mask,9,"// ""(a & mask) srl b)"" ---> ""BFE_U32 a, b, popcount(mask >> b)""; // Predicate: isMask(mask >> b)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:52,Availability,mask,mask,52,"// ""(a & mask) srl b)"" ---> ""BFE_U32 a, b, popcount(mask >> b)""; // Predicate: isMask(mask >> b)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:86,Availability,mask,mask,86,"// ""(a & mask) srl b)"" ---> ""BFE_U32 a, b, popcount(mask >> b)""; // Predicate: isMask(mask >> b)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:329,Availability,mask,mask,329,"// Special case for amdgcn.ballot:; // %Cond = i1 (and/or combination of i1 ISD::SETCCs); // %VCMP = i(WaveSize) AMDGPUISD::SETCC (ext %Cond), 0, setne/seteq; // =>; // Use i1 %Cond value instead of i(WaveSize) %VCMP.; // This is possible because divergent ISD::SETCC is selected as V_CMP and; // Cond becomes a i(WaveSize) full mask value.; // Note that ballot doesn't use SETEQ condition but its easy to support it; // here for completeness, so in this case Negate is set true on return.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:33,Testability,assert,assert,33,// TODO: make condition below an assert after fixing ballot bitwidth.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:198,Availability,mask,mask,198,"// This is the case that we are selecting to S_CBRANCH_VCCNZ. We have not; // analyzed what generates the vcc value, so we do not know whether vcc; // bits for disabled lanes are 0. Thus we need to mask out bits for; // disabled lanes.; //; // For the case that we select S_CBRANCH_SCC1 and it gets; // changed to S_CBRANCH_VCCNZ in SIFixSGPRCopies, SIFixSGPRCopies calls; // SIInstrInfo::moveToVALU which inserts the S_AND).; //; // We could add an analysis of what generates the vcc value here and omit; // the S_AND when is unnecessary. But it would be better to add a separate; // pass after SIFixSGPRCopies to do the unnecessary S_AND removal, so it; // catches both cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:12,Availability,error,error,12,// Let this error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:270,Usability,guid,guide,270,"// Don't worry if the offset ends up in a VGPR. Only one lane will have; // effect, so SIFixSGPRCopies will validly insert readfirstlane.; // The resource id offset is computed as (<isa opaque base> + M0[21:16] +; // offset field) % 64. Some versions of the programming guide omit the m0; // part, or claim it's from offset 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:16,Availability,error,error,16,// Emit default error,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:55,Integrability,depend,depending,55,"// Fold fsub [+-]0 into fneg. This may not have folded depending on the; // denormal mode, but we're implicitly canonicalizing in a source operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:79,Safety,avoid,avoid,79,// Really a scalar input. Just select from the low half of the register to; // avoid packing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.h:41,Availability,mask,masking,41,// Returns true if ISD::AND SDNode `N`'s masking of the shift amount operand's; // `ShAmtBits` bits is unneeded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:30,Performance,load,load,30,// Find a larger type to do a load / store of a vector with.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:60,Energy Efficiency,reduce,reduce,60,// Lower floating point store/load to integer store/load to reduce the number; // of patterns in tablegen.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:30,Performance,load,load,30,// Lower floating point store/load to integer store/load to reduce the number; // of patterns in tablegen.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:52,Performance,load,load,52,// Lower floating point store/load to integer store/load to reduce the number; // of patterns in tablegen.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:78,Availability,error,error,78,"// For R600, this is totally unsupported, just custom lower to produce an; // error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:28,Integrability,depend,dependencies,28,"// We want to find all load dependencies for long chains of stores to enable; // merging into very wide vectors. The problem is with vectors with > 4; // elements. MergeConsecutiveStores will attempt to merge these because x8/x16; // vectors are a legal type, even though we have to split the loads; // usually. When we can more precisely specify load legality per address; // space, we should be able to make FindBetterChain/MergeConsecutiveStores; // smarter so that they can figure out what to do in 2 iterations without all; // N > 4 stores on the same chain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:23,Performance,load,load,23,"// We want to find all load dependencies for long chains of stores to enable; // merging into very wide vectors. The problem is with vectors with > 4; // elements. MergeConsecutiveStores will attempt to merge these because x8/x16; // vectors are a legal type, even though we have to split the loads; // usually. When we can more precisely specify load legality per address; // space, we should be able to make FindBetterChain/MergeConsecutiveStores; // smarter so that they can figure out what to do in 2 iterations without all; // N > 4 stores on the same chain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:293,Performance,load,loads,293,"// We want to find all load dependencies for long chains of stores to enable; // merging into very wide vectors. The problem is with vectors with > 4; // elements. MergeConsecutiveStores will attempt to merge these because x8/x16; // vectors are a legal type, even though we have to split the loads; // usually. When we can more precisely specify load legality per address; // space, we should be able to make FindBetterChain/MergeConsecutiveStores; // smarter so that they can figure out what to do in 2 iterations without all; // N > 4 stores on the same chain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:347,Performance,load,load,347,"// We want to find all load dependencies for long chains of stores to enable; // merging into very wide vectors. The problem is with vectors with > 4; // elements. MergeConsecutiveStores will attempt to merge these because x8/x16; // vectors are a legal type, even though we have to split the loads; // usually. When we can more precisely specify load legality per address; // space, we should be able to make FindBetterChain/MergeConsecutiveStores; // smarter so that they can figure out what to do in 2 iterations without all; // N > 4 stores on the same chain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:55,Performance,perform,performFNegCombine,55,// TODO: Is there a benefit to checking the conditions performFNegCombine; // does? We don't for the other cases.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:262,Safety,avoid,avoid,262,"// Some users (such as 3-operand FMA/MAD) must use a VOP3 encoding, and thus; // it is truly free to use a source modifier in all cases. If there are; // multiple users but for each one will necessitate using VOP3, there will be; // a code size increase. Try to avoid increasing code size unless we know it; // will save on the instruction count.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:54,Testability,test,tests,54,// TODO: This may be worth removing. Check regression tests for diffs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:34,Performance,load,load,34,"// If we are reducing to a 32-bit load or a smaller multi-dword load,; // this is always better.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:64,Performance,load,load,64,"// If we are reducing to a 32-bit load or a smaller multi-dword load,; // this is always better.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:109,Deployability,Update,Update,109,// Do not shrink an aligned scalar load to sub-dword.; // Scalar engine cannot do sub-dword loads.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:35,Performance,load,load,35,// Do not shrink an aligned scalar load to sub-dword.; // Scalar engine cannot do sub-dword loads.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:92,Performance,load,loads,92,// Do not shrink an aligned scalar load to sub-dword.; // Scalar engine cannot do sub-dword loads.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:164,Performance,load,loads,164,// Do not shrink an aligned scalar load to sub-dword.; // Scalar engine cannot do sub-dword loads.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:329,Energy Efficiency,reduce,reduce,329,"// Don't produce extloads from sub 32-bit types. SI doesn't have scalar; // extloads, so doing one requires using a buffer_load. In cases where we; // still couldn't use a scalar load, using the wider load shouldn't really; // hurt anything.; // If the old size already had to be an extload, there's no harm in continuing; // to reduce the width.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:179,Performance,load,load,179,"// Don't produce extloads from sub 32-bit types. SI doesn't have scalar; // extloads, so doing one requires using a buffer_load. In cases where we; // still couldn't use a scalar load, using the wider load shouldn't really; // hurt anything.; // If the old size already had to be an extload, there's no harm in continuing; // to reduce the width.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:201,Performance,load,load,201,"// Don't produce extloads from sub 32-bit types. SI doesn't have scalar; // extloads, so doing one requires using a buffer_load. In cases where we; // still couldn't use a scalar load, using the wider load shouldn't really; // hurt anything.; // If the old size already had to be an extload, there's no harm in continuing; // to reduce the width.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:288,Safety,avoid,avoid,288,"// There are few operations which truly have vector input operands. Any vector; // operation is going to involve operations on each component, and a; // build_vector will be a copy per element, so it always makes sense to use a; // build_vector input in place of the extracted element to avoid a copy into a; // super register.; //; // We should probably only do this if all users are extracts only, but this; // should be the common case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:20,Security,access,accessing,20,// Truncate is just accessing a subregister.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:20,Security,access,accessing,20,// Truncate is just accessing a subregister.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:16,Performance,load,load,16,"// Any register load of a 64-bit value really requires 2 32-bit moves. For all; // practical purposes, the extra mov 0 to load a 64-bit is free. As used,; // this will enable reducing 64-bit operations the 32-bit, which is always; // good.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:122,Performance,load,load,122,"// Any register load of a 64-bit value really requires 2 32-bit moves. For all; // practical purposes, the extra mov 0 to load a 64-bit is free. As used,; // this will enable reducing 64-bit operations the 32-bit, which is always; // good.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:327,Energy Efficiency,reduce,reduce,327,"// There aren't really 64-bit registers, but pairs of 32-bit ones and only a; // limited number of native 64-bit operations. Shrinking an operation to fit; // in a single 32-bit register should always be helpful. As currently used,; // this is much less general than the name suggests, and is only used in; // places trying to reduce the sizes of loads. Shrinking loads to < 32-bits is; // not profitable, and may actually be harmful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:347,Performance,load,loads,347,"// There aren't really 64-bit registers, but pairs of 32-bit ones and only a; // limited number of native 64-bit operations. Shrinking an operation to fit; // in a single 32-bit register should always be helpful. As currently used,; // this is much less general than the name suggests, and is only used in; // places trying to reduce the sizes of loads. Shrinking loads to < 32-bits is; // not profitable, and may actually be harmful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:364,Performance,load,loads,364,"// There aren't really 64-bit registers, but pairs of 32-bit ones and only a; // limited number of native 64-bit operations. Shrinking an operation to fit; // in a single 32-bit register should always be helpful. As currently used,; // this is much less general than the name suggests, and is only used in; // places trying to reduce the sizes of loads. Shrinking loads to < 32-bits is; // not profitable, and may actually be harmful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:14,Modifiability,extend,extended,14,"// We have an extended type, like i24, so we should just use the; // register type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:14,Modifiability,extend,extended,14,"// We have an extended type, like i65.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:25,Testability,test,tests,25,"// FIXME: Fails for r600 tests; //assert(!isVarArg && Outs.empty() && OutVals.empty() &&; // ""wave terminate should not have return values"");",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:34,Testability,assert,assert,34,"// FIXME: Fails for r600 tests; //assert(!isVarArg && Outs.empty() && OutVals.empty() &&; // ""wave terminate should not have return values"");",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:134,Modifiability,extend,extended,134,"// Different parts of legalization seem to interpret which type of; // sign_extend_inreg is the one to check for custom lowering. The extended; // from type is what really matters, but some places check for custom; // lowering of the result type. This results in trying to use; // ReplaceNodeResults to sext_in_reg to an illegal type, so we'll just do; // nothing here and let the illegal result integer be handled normally.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:266,Availability,error,error,266,"// We currently don't have a way to correctly allocate LDS objects that; // aren't directly associated with a kernel. We do force inlining of; // functions that use local objects. However, if these dead functions are; // not eliminated, we don't want a compile time error. Just emit a warning; // and a trap, since there should be no callable path here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:46,Energy Efficiency,allocate,allocate,46,"// We currently don't have a way to correctly allocate LDS objects that; // aren't directly associated with a kernel. We do force inlining of; // functions that use local objects. However, if these dead functions are; // not eliminated, we don't want a compile time error. Just emit a warning; // and a trap, since there should be no callable path here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:178,Availability,error,errored,178,// TODO: We could emit code to handle the initialization somewhere.; // We ignore the initializer for now and legalize it to allow selection.; // The initializer will anyway get errored out during assembly emission.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:81,Safety,avoid,avoid,81,// Ordered. Assume ordered for undefined.; // Only do this after legalization to avoid interfering with other combines; // which might occur.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:3,Usability,Undo,Undo,3,"// Undo the combine foldFreeOpFromSelect does if it helps us match the; // fmin/fmax.; //; // select (fcmp olt (lhs, K)), (fneg lhs), -K; // -> fneg (fmin_legacy lhs, K); //; // TODO: Use getNegatedExpression",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:59,Energy Efficiency,power,power,59,"// Split a vector type into two parts. The first part is a power of two vector.; // The second part is whatever is left over, and is a scalar if it would; // otherwise be a 1-vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:39,Energy Efficiency,power,power,39,// This is the case that the vector is power of two so was evenly split.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:36,Performance,load,load,36,"// Widen from vec3 to vec4 when the load is at least 8-byte aligned; // or 16-byte fully dereferenceable. Otherwise, split the vector load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:134,Performance,load,load,134,"// Widen from vec3 to vec4 when the load is at least 8-byte aligned; // or 16-byte fully dereferenceable. Otherwise, split the vector load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:3,Deployability,Update,Update,3,// Update REM,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:3,Modifiability,Extend,Extend,3,// Extend back to 64-bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:3,Testability,Log,Log,3,// Log and multiply in f32 is good enough for f16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:19,Integrability,contract,contract,19,// It is unsafe to contract this fsub into the PH multiply.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:9,Safety,unsafe,unsafe,9,// It is unsafe to contract this fsub into the PH multiply.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:1246,Availability,avail,available,1246,"// The regular method converting a 64-bit integer to float roughly consists of; // 2 steps: normalization and rounding. In fact, after normalization, the; // conversion from a 64-bit integer to a float is essentially the same as the; // one from a 32-bit integer. The only difference is that it has more; // trailing bits to be rounded. To leverage the native 32-bit conversion, a; // 64-bit integer could be preprocessed and fit into a 32-bit integer then; // converted into the correct float number. The basic steps for the unsigned; // conversion are illustrated in the following pseudo code:; //; // f32 uitofp(i64 u) {; // i32 hi, lo = split(u);; // // Only count the leading zeros in hi as we have native support of the; // // conversion from i32 to f32. If hi is all 0s, the conversion is; // // reduced to a 32-bit one automatically.; // i32 shamt = clz(hi); // Return 32 if hi is all 0s.; // u <<= shamt;; // hi, lo = split(u);; // hi |= (lo != 0) ? 1 : 0; // Adjust rounding bit in hi based on lo.; // // convert it as a 32-bit integer and scale the result back.; // return uitofp(hi) * 2^(32 - shamt);; // }; //; // The signed one follows the same principle but uses 'ffbh_i32' to count its; // sign bits instead. If 'ffbh_i32' is not available, its absolute value is; // converted instead followed by negation based its sign bit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:803,Energy Efficiency,reduce,reduced,803,"// The regular method converting a 64-bit integer to float roughly consists of; // 2 steps: normalization and rounding. In fact, after normalization, the; // conversion from a 64-bit integer to a float is essentially the same as the; // one from a 32-bit integer. The only difference is that it has more; // trailing bits to be rounded. To leverage the native 32-bit conversion, a; // 64-bit integer could be preprocessed and fit into a 32-bit integer then; // converted into the correct float number. The basic steps for the unsigned; // conversion are illustrated in the following pseudo code:; //; // f32 uitofp(i64 u) {; // i32 hi, lo = split(u);; // // Only count the leading zeros in hi as we have native support of the; // // conversion from i32 to f32. If hi is all 0s, the conversion is; // // reduced to a 32-bit one automatically.; // i32 shamt = clz(hi); // Return 32 if hi is all 0s.; // u <<= shamt;; // hi, lo = split(u);; // hi |= (lo != 0) ? 1 : 0; // Adjust rounding bit in hi based on lo.; // // convert it as a 32-bit integer and scale the result back.; // return uitofp(hi) * 2^(32 - shamt);; // }; //; // The signed one follows the same principle but uses 'ffbh_i32' to count its; // sign bits instead. If 'ffbh_i32' is not available, its absolute value is; // converted instead followed by negation based its sign bit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:607,Energy Efficiency,reduce,reduce,607,"// We also need to consider the sign bit in Lo if Hi has just sign bits,; // i.e. Hi is 0 or -1. However, that only needs to take the MSB into; // account. That is, the maximal shift is; // - 32 if Lo and Hi have opposite signs;; // - 33 if Lo and Hi have the same sign.; //; // Or, MaxShAmt = 33 + OppositeSign, where; //; // OppositeSign is defined as ((Lo ^ Hi) >> 31), which is; // - -1 if Lo and Hi have opposite signs; and; // - 0 otherwise.; //; // All in all, ShAmt is calculated as; //; // umin(sffbh(Hi), 33 + (Lo^Hi)>>31) - 1.; //; // or; //; // umin(sffbh(Hi) - 1, 32 + (Lo^Hi)>>31).; //; // to reduce the critical path.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:175,Safety,avoid,avoid,175,"// Otherwise, align 'ShAmt' to the exponent part and add it into the exponent; // part directly to emulate the multiplication of 2^ShAmt. That 8-bit; // exponent is enough to avoid overflowing into the sign bit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:162,Safety,avoid,avoid,162,"// However, a 32-bit floating point number has only 23 bits mantissa and; // it's not enough to hold all the significant bits of `lof` if val is; // negative. To avoid the loss of precision, We need to take the absolute; // value after truncating and flip the result back based on the original; // signedness.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:49,Safety,unsafe,unsafe,49,// There is a generic expand for FP_TO_FP16 with unsafe fast math.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:96,Performance,optimiz,optimizations,96,//===----------------------------------------------------------------------===//; // Custom DAG optimizations; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:127,Performance,perform,perform,127,"// First try to simplify using SimplifyMultipleUseDemandedBits which allows; // the operands to have other uses, but will only perform simplifications that; // involve bypassing some nodes for this user.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:16,Usability,simpl,simplify,16,"// First try to simplify using SimplifyMultipleUseDemandedBits which allows; // the operands to have other uses, but will only perform simplifications that; // involve bypassing some nodes for this user.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:31,Usability,Simpl,SimplifyMultipleUseDemandedBits,31,"// First try to simplify using SimplifyMultipleUseDemandedBits which allows; // the operands to have other uses, but will only perform simplifications that; // involve bypassing some nodes for this user.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:135,Usability,simpl,simplifications,135,"// First try to simplify using SimplifyMultipleUseDemandedBits which allows; // the operands to have other uses, but will only perform simplifications that; // involve bypassing some nodes for this user.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:11,Usability,Simpl,SimplifyDemandedBits,11,// Now try SimplifyDemandedBits which can simplify the nodes used by our; // operands if this node is the only user.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:42,Usability,simpl,simplify,42,// Now try SimplifyDemandedBits which can simplify the nodes used by our; // operands if this node is the only user.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:11,Performance,load,load,11,// Replace load of an illegal type with a store of a bitcast to a friendlier; // type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:20,Performance,load,loads,20,"// Expand unaligned loads earlier than legalization. Due to visitation order; // problems during legalization, the emitted instructions to pack and unpack; // the bytes again are not eliminated in the case of an unaligned copy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:9,Testability,assert,assertzext,9,"// (vt2 (assertzext (truncate vt0:x), vt1)) ->; // (vt2 (truncate (assertzext vt0:x, vt1)))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:67,Testability,assert,assertzext,67,"// (vt2 (assertzext (truncate vt0:x), vt1)) ->; // (vt2 (truncate (assertzext vt0:x, vt1)))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:66,Performance,perform,perform,66,"/// Split the 64-bit value \p LHS into two 32-bit components, and perform the; /// binary operation \p Opc to it with the corresponding constant operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:79,Usability,simpl,simplify,79,// Re-visit the ands. It's possible we eliminated one of them and it could; // simplify the vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:27,Security,access,accessing,27,"// Equivalent of above for accessing the high element of a vector as an; // integer operation.; // trunc (srl (bitcast (build_vector x, y))), 16 -> trunc (bitcast y)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:47,Energy Efficiency,reduce,reduced,47,"// Partially shrink 64-bit shifts to 32-bit if reduced to 16-bit.; //; // i16 (trunc (srl i64:x, K)), K <= 16 ->; // i16 (trunc (srl (i32 (trunc x), K)))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:184,Safety,avoid,avoid,184,"// - For left shifts, do the transform as long as the shift; // amount is still legal for i32, so when ShiftAmt < 32 (<= 31); // - For right shift, do it if ShiftAmt <= (32 - Size) to avoid; // losing information stored in the high bits when truncating.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:50,Safety,avoid,avoid,50,"// We need to specifically handle i64 mul here to avoid unnecessary conversion; // instructions. If we only match on the legalized i64 mul expansion,; // SimplifyDemandedBits will be unable to remove them because there will be; // multiple uses due to the separate mul + mulh[su].",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:154,Usability,Simpl,SimplifyDemandedBits,154,"// We need to specifically handle i64 mul here to avoid unnecessary conversion; // instructions. If we only match on the legalized i64 mul expansion,; // SimplifyDemandedBits will be unable to remove them because there will be; // multiple uses due to the separate mul + mulh[su].",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:114,Safety,avoid,avoid,114,"// Don't generate 24-bit multiplies on values that are in SGPRs, since; // we only have a 32-bit scalar multiply (avoid values being moved to VGPRs; // unnecessarily). isDivergent() is used as an approximation of whether the; // value is in an SGPR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:3,Usability,Undo,Undo,3,"// Undo InstCombine canonicalize X * (Y + 1) -> X * Y + X to enable mad; // matching.; // mul x, (add y, 1) -> add (mul x, y), x",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:244,Safety,avoid,avoid,244,"// SimplifyDemandedBits has the annoying habit of turning useful zero_extends; // in the source into any_extends if the result of the mul is truncated. Since; // we can assume the high bits are whatever we want, use the underlying value; // to avoid the unknown high bits from interfering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:3,Usability,Simpl,SimplifyDemandedBits,3,"// SimplifyDemandedBits has the annoying habit of turning useful zero_extends; // in the source into any_extends if the result of the mul is truncated. Since; // we can assume the high bits are whatever we want, use the underlying value; // to avoid the unknown high bits from interfering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:244,Safety,avoid,avoid,244,"// SimplifyDemandedBits has the annoying habit of turning useful zero_extends; // in the source into any_extends if the result of the mul is truncated. Since; // we can assume the high bits are whatever we want, use the underlying value; // to avoid the unknown high bits from interfering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:3,Usability,Simpl,SimplifyDemandedBits,3,"// SimplifyDemandedBits has the annoying habit of turning useful zero_extends; // in the source into any_extends if the result of the mul is truncated. Since; // we can assume the high bits are whatever we want, use the underlying value; // to avoid the unknown high bits from interfering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:99,Modifiability,extend,extending,99,// Try to use two fast 24-bit multiplies (one for each half of the result); // instead of one slow extending multiply.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:291,Availability,avail,available,291,"// Don't generate 24-bit multiplies on values that are in SGPRs, since; // we only have a 32-bit scalar multiply (avoid values being moved to VGPRs; // unnecessarily). isDivergent() is used as an approximation of whether the; // value is in an SGPR.; // This doesn't apply if no s_mul_hi is available (since we'll end up with a; // valu op anyway)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:114,Safety,avoid,avoid,114,"// Don't generate 24-bit multiplies on values that are in SGPRs, since; // we only have a 32-bit scalar multiply (avoid values being moved to VGPRs; // unnecessarily). isDivergent() is used as an approximation of whether the; // value is in an SGPR.; // This doesn't apply if no s_mul_hi is available (since we'll end up with a; // valu op anyway)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:291,Availability,avail,available,291,"// Don't generate 24-bit multiplies on values that are in SGPRs, since; // we only have a 32-bit scalar multiply (avoid values being moved to VGPRs; // unnecessarily). isDivergent() is used as an approximation of whether the; // value is in an SGPR.; // This doesn't apply if no s_mul_hi is available (since we'll end up with a; // valu op anyway)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:114,Safety,avoid,avoid,114,"// Don't generate 24-bit multiplies on values that are in SGPRs, since; // we only have a 32-bit scalar multiply (avoid values being moved to VGPRs; // unnecessarily). isDivergent() is used as an approximation of whether the; // value is in an SGPR.; // This doesn't apply if no s_mul_hi is available (since we'll end up with a; // valu op anyway)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:49,Performance,Optimiz,Optimize,49,"// The native instructions return -1 on 0 input. Optimize out a select that; // produces -1 on 0.; //; // TODO: If zero is not undef, we could also do this if the output is compared; // against the bitwidth.; //; // TODO: Should probably combine against FFBH_U32 instead of ctlz directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:90,Availability,down,down,90,"// If one side is an fneg/fabs and the other is a constant, we can push the; // fneg/fabs down. If it's an fabs, the constant needs to be non-negative.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:67,Availability,down,down,67,"// Careful: if the neg can be folded up, don't try to pull it back down.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:68,Availability,down,down,68,"// If the input has multiple uses and we can either fold the negate down, or; // the other uses cannot, give up. This both prevents unprofitable; // transformations and infinite loops: we won't repeatedly try to fold around; // a negate that has no 'good' form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:48,Safety,avoid,avoid,48,"// Push casts through vector builds. This helps avoid emitting a large; // number of copies when materializing floating point vector constants.; //; // vNt1 bitcast (vNt0 (build_vector t0:x, t0:y)) =>; // vnt1 = build_vector (t1 (bitcast t0:x)), (t1 (bitcast t0:y))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:31,Modifiability,extend,extended,31,"// This is already sign / zero extended, so try to fold away extra BFEs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:186,Modifiability,extend,extended,186,"// This is a sign_extend_inreg. Replace it to take advantage of existing; // DAG Combines. If not eliminated, we will match back to BFE during; // selection.; // TODO: The sext_inreg of extended types ends, although we can could; // handle them in a single BFE.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:78,Usability,clear,clear,78,"// TODO: There is also f64 rsq instruction, but the documentation is less; // clear on its precision.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:23,Availability,error,error,23,"// Reciprocal, < 1 ulp error.; //; // This reciprocal approximation converges to < 0.5 ulp error with one; // newton rhapson performed with two fused multiple adds (FMAs).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:91,Availability,error,error,91,"// Reciprocal, < 1 ulp error.; //; // This reciprocal approximation converges to < 0.5 ulp error with one; // newton rhapson performed with two fused multiple adds (FMAs).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:125,Performance,perform,performed,125,"// Reciprocal, < 1 ulp error.; //; // This reciprocal approximation converges to < 0.5 ulp error with one; // newton rhapson performed with two fused multiple adds (FMAs).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:78,Usability,clear,clear,78,"// TODO: There is also f64 rcp instruction, but the documentation is less; // clear on its precision.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:47,Integrability,Interface,Interface,47,"//===-- AMDGPUISelLowering.h - AMDGPU Lowering Interface --------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition of the TargetLowering class that is common; /// to all AMD GPUs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:395,Integrability,Interface,Interface,395,"//===-- AMDGPUISelLowering.h - AMDGPU Lowering Interface --------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition of the TargetLowering class that is common; /// to all AMD GPUs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:140,Modifiability,extend,extending,140,/// \returns The minimum number of bits needed to store the value of \Op as an; /// unsigned integer. Truncating to this size and then zero-extending to the; /// original size will not change the value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:137,Modifiability,extend,extending,137,/// \returns The minimum number of bits needed to store the value of \Op as a; /// signed integer. Truncating to this size and then sign-extending to the; /// original size will not change the value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:60,Energy Efficiency,power,power,60,"/// Split a vector type into two parts. The first part is a power of two; /// vector. The second part is whatever is left over, and is a scalar if it; /// would otherwise be a 1-vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:19,Performance,load,load,19,/// Split a vector load into 2 loads of half the vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:31,Performance,load,loads,31,/// Split a vector load into 2 loads of half the vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:32,Performance,load,load,32,"/// Widen a suitably aligned v3 load. For all other cases, split the input; /// vector load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:87,Performance,load,load,87,"/// Widen a suitably aligned v3 load. For all other cases, split the input; /// vector load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:48,Availability,Mask,Mask,48,/// Determine which of the bits specified in \p Mask are known to be; /// either zero or one and return them in the \p KnownZero and \p KnownOne; /// bitsets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:56,Performance,load,loaded,56,"/// Similar to CreateLiveInRegister, except value maybe loaded from a stack; /// slot rather than passed in a register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:3,Availability,Mask,Masked,3,// Masked control flow nodes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:118,Security,access,access,118,// Convert a unswizzled wave uniform stack address to an address compatible; // with a vector offset for use in stack access.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:31,Availability,mask,mask,31,// This is SETCC with the full mask result which is used for a compare with a; // result bit per item in the wavefront.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:42,Availability,error,error,42,"// SIN_HW, COS_HW - f32 for SI, 1 ULP max error, valid from -100 pi to 100 pi.; // Denormals handled on some parts.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:33,Availability,error,error,33,"// RCP, RSQ - For f32, 1 ULP max error, no denormal handling.; // For f64, max error 2^29 ULP, handles denormals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h:79,Availability,error,error,79,"// RCP, RSQ - For f32, 1 ULP max error, no denormal handling.; // For f64, max error 2^29 ULP, handles denormals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:423,Performance,optimiz,optimizations,423,"//===-- AMDGPUCodeGenPrepare.cpp ------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass does misc. AMDGPU optimizations on IR *just* before instruction; /// selection.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:140,Modifiability,enhance,enhances,140,"// Scalar load widening needs running after load-store-vectorizer as that pass; // doesn't handle overlapping cases. In addition, this pass enhances the; // widening to handle cases where scalar sub-dword loads are naturally aligned; // only but not dword aligned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:10,Performance,load,load,10,"// Scalar load widening needs running after load-store-vectorizer as that pass; // doesn't handle overlapping cases. In addition, this pass enhances the; // widening to handle cases where scalar sub-dword loads are naturally aligned; // only but not dword aligned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:44,Performance,load,load-store-vectorizer,44,"// Scalar load widening needs running after load-store-vectorizer as that pass; // doesn't handle overlapping cases. In addition, this pass enhances the; // widening to handle cases where scalar sub-dword loads are naturally aligned; // only but not dword aligned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:205,Performance,load,loads,205,"// Scalar load widening needs running after load-store-vectorizer as that pass; // doesn't handle overlapping cases. In addition, this pass enhances the; // widening to handle cases where scalar sub-dword loads are naturally aligned; // only but not dword aligned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:19,Performance,load,loads,19,// Skip non-simple loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:12,Usability,simpl,simple,12,// Skip non-simple loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:25,Performance,load,loads,25,// Only handle sub-DWORD loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:8,Performance,load,load,8,// That load must be at least naturally aligned.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:39,Performance,load,load,39,"// It should be uniform, i.e. a scalar load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:16,Performance,load,load,16,// Skip if that load is already aligned on DWORD at least as it's handled in; // SDAG.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:55,Performance,perform,perform,55,"// If that base is not DWORD aligned, it's not safe to perform the following; // transforms.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:47,Safety,safe,safe,47,"// If that base is not DWORD aligned, it's not safe to perform the following; // transforms.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:14,Performance,load,load,14,// Hack until load/store selection patterns support any tuple of legal types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:44,Energy Efficiency,power,power,44,// Round the number of elements to the next power of two elements,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:40,Energy Efficiency,power,power,40,// Round the number of bits to the next power of two bits,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:107,Energy Efficiency,reduce,reduce,107,"// If we have a truncating store or an extending load with a data size larger; // than 32-bits, we need to reduce to a 32-bit type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:39,Modifiability,extend,extending,39,"// If we have a truncating store or an extending load with a data size larger; // than 32-bits, we need to reduce to a 32-bit type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:49,Performance,load,load,49,"// If we have a truncating store or an extending load with a data size larger; // than 32-bits, we need to reduce to a 32-bit type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:49,Modifiability,extend,extend,49,"// TODO: Should load to s16 be legal? Most loads extend to 32-bits, but we; // handle some operations by just promoting the register during; // selection. There are also d16 loads on GFX9+ which preserve the high bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:16,Performance,load,load,16,"// TODO: Should load to s16 be legal? Most loads extend to 32-bits, but we; // handle some operations by just promoting the register during; // selection. There are also d16 loads on GFX9+ which preserve the high bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:43,Performance,load,loads,43,"// TODO: Should load to s16 be legal? Most loads extend to 32-bits, but we; // handle some operations by just promoting the register during; // selection. There are also d16 loads on GFX9+ which preserve the high bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:174,Performance,load,loads,174,"// TODO: Should load to s16 be legal? Most loads extend to 32-bits, but we; // handle some operations by just promoting the register during; // selection. There are also d16 loads on GFX9+ which preserve the high bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:154,Integrability,depend,depending,154,"// Treat constant and global as identical. SMRD loads are sometimes usable for; // global loads (ideally constant address space should be eliminated); // depending on the context. Legality cannot be context dependent, but; // RegBankSelect can split the load as necessary depending on the pointer; // register bank/uniformity and if the memory is invariant or not written in a; // kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:207,Integrability,depend,dependent,207,"// Treat constant and global as identical. SMRD loads are sometimes usable for; // global loads (ideally constant address space should be eliminated); // depending on the context. Legality cannot be context dependent, but; // RegBankSelect can split the load as necessary depending on the pointer; // register bank/uniformity and if the memory is invariant or not written in a; // kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:272,Integrability,depend,depending,272,"// Treat constant and global as identical. SMRD loads are sometimes usable for; // global loads (ideally constant address space should be eliminated); // depending on the context. Legality cannot be context dependent, but; // RegBankSelect can split the load as necessary depending on the pointer; // register bank/uniformity and if the memory is invariant or not written in a; // kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:48,Performance,load,loads,48,"// Treat constant and global as identical. SMRD loads are sometimes usable for; // global loads (ideally constant address space should be eliminated); // depending on the context. Legality cannot be context dependent, but; // RegBankSelect can split the load as necessary depending on the pointer; // register bank/uniformity and if the memory is invariant or not written in a; // kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:90,Performance,load,loads,90,"// Treat constant and global as identical. SMRD loads are sometimes usable for; // global loads (ideally constant address space should be eliminated); // depending on the context. Legality cannot be context dependent, but; // RegBankSelect can split the load as necessary depending on the pointer; // register bank/uniformity and if the memory is invariant or not written in a; // kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:254,Performance,load,load,254,"// Treat constant and global as identical. SMRD loads are sometimes usable for; // global loads (ideally constant address space should be eliminated); // depending on the context. Legality cannot be context dependent, but; // RegBankSelect can split the load as necessary depending on the pointer; // register bank/uniformity and if the memory is invariant or not written in a; // kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:68,Usability,usab,usable,68,"// Treat constant and global as identical. SMRD loads are sometimes usable for; // global loads (ideally constant address space should be eliminated); // depending on the context. Legality cannot be context dependent, but; // RegBankSelect can split the load as necessary depending on the pointer; // register bank/uniformity and if the memory is invariant or not written in a; // kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:105,Integrability,depend,depending,105,// FIXME: Flat addresses may contextually need to be split to 32-bit parts; // if they may alias scratch depending on the subtarget. This needs to be; // moved to custom handling to use addressMayBeAccessedAsPrivate,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:17,Modifiability,extend,extending,17,// Do not handle extending vector loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:34,Performance,load,loads,34,// Do not handle extending vector loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:36,Performance,load,loads,36,"// TODO: We should be able to widen loads if the alignment is high enough, but; // we also need to modify the memory access size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:117,Security,access,access,117,"// TODO: We should be able to widen loads if the alignment is high enough, but; // we also need to modify the memory access size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:19,Performance,load,loads,19,// Accept widening loads based on alignment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:44,Availability,down,down,44,// These may contextually need to be broken down.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:139,Performance,load,loads,139,"// The current selector can't handle <6 x s16>, <8 x s16>, s96, s128 etc, so; // workaround this. Eventually it should ignore the type for loads and only care; // about the size. Return true in cases where we will workaround this for now by; // bitcasting.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:21,Performance,load,load,21,/// Return true if a load or store of the type should be lowered with a bitcast; /// to a different type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:45,Performance,load,loads,45,// Don't try to handle bitcasting vector ext loads for now.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:40,Performance,load,load,40,"/// Return true if we should legalize a load by widening an odd sized memory; /// access up to the alignment. Note this case when the memory access itself; /// changes, not the size of the result register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:82,Security,access,access,82,"/// Return true if we should legalize a load by widening an odd sized memory; /// access up to the alignment. Note this case when the memory access itself; /// changes, not the size of the result register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:141,Security,access,access,141,"/// Return true if we should legalize a load by widening an odd sized memory; /// access up to the alignment. Note this case when the memory access itself; /// changes, not the size of the result register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:116,Performance,load,load,116,"// If we have 96-bit memory operations, we shouldn't touch them. Note we may; // end up widening these for a scalar load during RegBankSelect, if we don't; // have 96-bit scalar loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:178,Performance,load,loads,178,"// If we have 96-bit memory operations, we shouldn't touch them. Note we may; // end up widening these for a scalar load during RegBankSelect, if we don't; // have 96-bit scalar loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:5,Performance,load,load,5,"// A load is known dereferenceable up to the alignment, so it's legal to widen; // to it.; //; // TODO: Could check dereferenceable for less aligned cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:55,Performance,load,load,55,// Do not widen if it would introduce a slow unaligned load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:27,Performance,load,load,27,"/// Mutates IR (typicaly a load instruction) to use a <4 x s32> as the initial; /// type of the operand `idx` and then to transform it to a `p8` via bitcasts; /// and inttoptr. In addition, handle vectors of p8. Returns the new type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:25,Integrability,depend,depends,25,// Whether this is legal depends on the floating point mode for the function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:3,Safety,Avoid,Avoid,3,"// Avoid scalarizing in cases that should be truly illegal. In unresolvable; // situations (like an invalid implicit use), we don't want to infinite loop; // in the legalizer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:25,Availability,avail,available,25,"// If no 16 bit instr is available, lower into different instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:21,Energy Efficiency,power,power-of-,21,// FIXME: Fixing non-power-of-2 before clamp is workaround for; // narrowScalar limitation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:21,Energy Efficiency,power,power-of-,21,// FIXME: Fixing non-power-of-2 before clamp is workaround for; // narrowScalar limitation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:105,Integrability,depend,depending,105,// Catch weird sized loads that don't evenly divide into the access sizes; // TODO: May be able to widen depending on alignment etc.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:21,Performance,load,loads,21,// Catch weird sized loads that don't evenly divide into the access sizes; // TODO: May be able to widen depending on alignment etc.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:61,Security,access,access,61,// Catch weird sized loads that don't evenly divide into the access sizes; // TODO: May be able to widen depending on alignment etc.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:60,Security,access,access,60,// TODO: Refine based on subtargets which support unaligned access or 128-bit; // LDS; // TODO: Unsupported flat for SI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:72,Performance,load,load,72,"// The custom pointers (fat pointers, buffer resources) don't work with load; // and store at this level. Fat pointers should have been lowered to; // intrinsics before the translation to MIR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:51,Performance,load,load,51,"// Address space 8 pointers are handled by a 4xs32 load, bitcast, and; // ptrtoint. This is needed to account for the fact that we can't have i128; // as a register class for SelectionDAG reasons.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:26,Performance,load,loads,26,// Widen suitably aligned loads by loading extra bytes. The standard; // legalization actions can't properly express widening memory operands.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:35,Performance,load,loading,35,// Widen suitably aligned loads by loading extra bytes. The standard; // legalization actions can't properly express widening memory operands.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:10,Performance,load,load,10,// FIXME: load/store narrowing should be moved to lower action,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:28,Energy Efficiency,power,power,28,// FIXME: Handle widened to power of 2 results better. This ends; // up scalarizing.; // FIXME: 3 element stores scalarized on SI; // Split if it's too large for the address space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:41,Modifiability,extend,extending,41,// FIXME: We could probably handle weird extending loads better.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:51,Performance,load,loads,51,// FIXME: We could probably handle weird extending loads better.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:20,Security,access,accesses,20,// FIXME: Unaligned accesses not lowered.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:89,Modifiability,flexible,flexible,89,"// TODO: Only the low 4/5/6 bits of the shift amount are observed, so we can; // be more flexible with the shift amount type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:172,Integrability,wrap,wrap,172,"// Address space 8 pointers are 128-bit wide values, but the logic; // below will try to bitcast them to 2N x s64, which will fail.; // Therefore, as an intermediate step, wrap extracts/insertions from a; // ptrtoint-ing the vector and scalar arguments (or inttoptring the; // extraction result) in order to produce a vector operation that can; // be handled by the logic below.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:61,Testability,log,logic,61,"// Address space 8 pointers are 128-bit wide values, but the logic; // below will try to bitcast them to 2N x s64, which will fail.; // Therefore, as an intermediate step, wrap extracts/insertions from a; // ptrtoint-ing the vector and scalar arguments (or inttoptring the; // extraction result) in order to produce a vector operation that can; // be handled by the logic below.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:366,Testability,log,logic,366,"// Address space 8 pointers are 128-bit wide values, but the logic; // below will try to bitcast them to 2N x s64, which will fail.; // Therefore, as an intermediate step, wrap extracts/insertions from a; // ptrtoint-ing the vector and scalar arguments (or inttoptring the; // extraction result) in order to produce a vector operation that can; // be handled by the logic below.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:36,Modifiability,variab,variable,36,"// It should only be necessary with variable indexes.; // As a last resort, lower to the stack",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:52,Energy Efficiency,power,power,52,// Clamp the little scalar to s8-s256 and make it a power of 2. It's not; // worth considering the multiples of 64 since 2*192 and 2*384 are not; // valid.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:17,Energy Efficiency,power,power,17,"// Pick the next power of 2, or a multiple of 64 over 128.; // Whichever is smaller.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:13,Energy Efficiency,reduce,reduce,13,"// Prefer to reduce vector widths for 16-bit vectors before lowering, to; // get more vector shift opportunities, since we'll get those when; // expanded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:126,Modifiability,extend,extend,126,// Prefer to promote to s32 before lowering if we don't have 16-bit; // shifts. This avoid a lot of intermediate truncate and extend operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:85,Safety,avoid,avoid,85,// Prefer to promote to s32 before lowering if we don't have 16-bit; // shifts. This avoid a lot of intermediate truncate and extend operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:3,Performance,Load,Load,3,// Load address,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:34,Performance,queue,queue,34,"// TODO: Avoid reloading from the queue ptr for each cast, or at least each; // vector element.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:9,Safety,Avoid,Avoid,9,"// TODO: Avoid reloading from the queue ptr for each cast, or at least each; // vector element.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:78,Safety,avoid,avoid,78,// TODO: Should we allow mismatched types but matching sizes in merges to; // avoid the ptrtoint?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:3,Modifiability,Extend,Extend,3,// Extend back to 64-bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:48,Testability,log,logic,48,// TODO: Copied from DAG implementation. Verify logic and document how this; // actually works.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:162,Safety,avoid,avoid,162,"// However, a 32-bit floating point number has only 23 bits mantissa and; // it's not enough to hold all the significant bits of `lof` if val is; // negative. To avoid the loss of precision, We need to take the absolute; // value after truncating and flip the result back based on the original; // signedness.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:116,Testability,log,logic,116,"// Other legalization maps vector<? x [type bigger than 64 bits]> via bitcasts; // but we can't go directly to that logic becasue you can't bitcast a vector; // of pointers to a vector of integers. Therefore, introduce an intermediate; // vector of integers using ptrtoint (and inttoptr on the output) in order to; // drive the legalization forward.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:116,Testability,log,logic,116,"// Other legalization maps vector<? x [type bigger than 64 bits]> via bitcasts; // but we can't go directly to that logic becasue you can't bitcast a vector; // of pointers to a vector of integers. Therefore, make the pointer vector; // into an equivalent vector of integers with ptrtoint, insert the ptrtoint'd; // new value, and then inttoptr the result vector back. This will then allow; // the rest of legalization to take over.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:481,Modifiability,variab,variable,481,"// In order to support pc-relative addressing, SI_PC_ADD_REL_OFFSET is lowered; // to the following code sequence:; //; // For constant address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol; // s_addc_u32 s1, s1, 0; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // a fixup or relocation is emitted to replace $symbol with a literal; // constant, which is a pc-relative offset from the encoding of the $symbol; // operand to the global variable.; //; // For global address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol@{gotpc}rel32@lo; // s_addc_u32 s1, s1, $symbol@{gotpc}rel32@hi; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // fixups or relocations are emitted to replace $symbol@*@lo and; // $symbol@*@hi with lower 32 bits and higher 32 bits of a literal constant,; // which is a 64-bit pc-relative offset from the encoding of the $symbol; // operand to the global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:963,Modifiability,variab,variable,963,"// In order to support pc-relative addressing, SI_PC_ADD_REL_OFFSET is lowered; // to the following code sequence:; //; // For constant address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol; // s_addc_u32 s1, s1, 0; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // a fixup or relocation is emitted to replace $symbol with a literal; // constant, which is a pc-relative offset from the encoding of the $symbol; // operand to the global variable.; //; // For global address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol@{gotpc}rel32@lo; // s_addc_u32 s1, s1, $symbol@{gotpc}rel32@hi; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // fixups or relocations are emitted to replace $symbol@*@lo and; // $symbol@*@hi with lower 32 bits and higher 32 bits of a literal constant,; // which is a 64-bit pc-relative offset from the encoding of the $symbol; // operand to the global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:41,Testability,stub,stub,41,// Emit a ABS32_LO / ABS32_HI relocation stub.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:266,Availability,error,error,266,"// We currently don't have a way to correctly allocate LDS objects that; // aren't directly associated with a kernel. We do force inlining of; // functions that use local objects. However, if these dead functions are; // not eliminated, we don't want a compile time error. Just emit a warning; // and a trap, since there should be no callable path here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:46,Energy Efficiency,allocate,allocate,46,"// We currently don't have a way to correctly allocate LDS objects that; // aren't directly associated with a kernel. We do force inlining of; // functions that use local objects. However, if these dead functions are; // not eliminated, we don't want a compile time error. Just emit a warning; // and a trap, since there should be no callable path here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:178,Availability,error,errored,178,// TODO: We could emit code to handle the initialization somewhere.; // We ignore the initializer for now and legalize it to allow selection.; // The initializer will anyway get errored out during assembly emission.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:208,Energy Efficiency,allocate,allocated,208,// HIP uses an unsized array `extern __shared__ T s[]` or similar; // zero-sized type in other languages to declare the dynamic shared; // memory which size is not known at the compile time. They will be; // allocated by the runtime and placed directly after the static; // allocated ones. They all share the same offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:274,Energy Efficiency,allocate,allocated,274,// HIP uses an unsized array `extern __shared__ T s[]` or similar; // zero-sized type in other languages to declare the dynamic shared; // memory which size is not known at the compile time. They will be; // allocated by the runtime and placed directly after the static; // allocated ones. They all share the same offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:13,Energy Efficiency,power,power-of-,13,// Widen non-power-of-2 loads to the alignment if needed,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:24,Performance,load,loads,24,// Widen non-power-of-2 loads to the alignment if needed,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:32,Modifiability,extend,extending,32,"// This was already the correct extending load result type, so just adjust; // the memory type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:42,Performance,load,load,42,"// This was already the correct extending load result type, so just adjust; // the memory type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:19,Integrability,contract,contract,19,// It is unsafe to contract this fsub into the PH multiply.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:9,Safety,unsafe,unsafe,9,// It is unsafe to contract this fsub into the PH multiply.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:152,Energy Efficiency,efficient,efficient,152,"// V_FRACT is buggy on SI, so the F32 version is never used and (x-floor(x)); // is used instead. However, SI doesn't have V_FLOOR_F64, so the most; // efficient way to implement it is using V_FRACT_F64. The workaround for the; // V_FRACT bug is:; // fract(x) = isnan(x) ? x : min(V_FRACT(x), 0.99999999999999999); //; // Convert floor(x) to (x - fract(x))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:107,Safety,avoid,avoid,107,// Give source modifier matching some assistance before obscuring a foldable; // pattern.; // TODO: We can avoid the neg on the fract? The input sign to fract; // shouldn't matter?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:204,Deployability,update,updated,204,"// Build a multiply-add chain to compute; //; // LocalAccum + (partial products at DstIndex); // + (opportunistic subset of CarryIn); //; // LocalAccum is an array of one or two 32-bit registers that are updated; // in-place. The incoming registers may be null.; //; // In some edge cases, carry-ins can be consumed ""for free"". In that case,; // the consumed carry bits are removed from CarryIn in-place.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:160,Deployability,integrat,integrate,160,"// Custom narrowing of wide multiplies using wide multiply-add instructions.; //; // TODO: If the multiply is followed by an addition, we should attempt to; // integrate it to make better use of V_MAD_U64_U32's multiply-add capabilities.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:160,Integrability,integrat,integrate,160,"// Custom narrowing of wide multiplies using wide multiply-add instructions.; //; // TODO: If the multiply is followed by an addition, we should attempt to; // integrate it to make better use of V_MAD_U64_U32's multiply-add capabilities.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:133,Integrability,depend,dependency,133,// Whether to use MAD_64_32 for partial products whose high half is; // discarded. This avoids some ADD instructions but risks false dependency; // stalls on some subtargets in some cases.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:88,Safety,avoid,avoids,88,// Whether to use MAD_64_32 for partial products whose high half is; // discarded. This avoids some ADD instructions but risks false dependency; // stalls on some subtargets in some cases.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:121,Safety,risk,risks,121,// Whether to use MAD_64_32 for partial products whose high half is; // discarded. This avoids some ADD instructions but risks false dependency; // stalls on some subtargets in some cases.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:9,Safety,Avoid,Avoid,9,// TODO: Avoid clearing the high bits if we know workitem id y/z are always; // 0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:15,Usability,clear,clearing,15,// TODO: Avoid clearing the high bits if we know workitem id y/z are always; // 0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:122,Availability,mask,mask,122,"// If GridZ is not programmed in an entry function then the hardware will set; // it to all zeros, so there is no need to mask the GridY value in the low; // order bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:81,Availability,mask,masking,81,// Don't bother inserting AssertZext for packed IDs since we're emitting the; // masking operations anyway.; //; // TODO: We could assert the top bit is 0 for the source copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:26,Testability,Assert,AssertZext,26,// Don't bother inserting AssertZext for packed IDs since we're emitting the; // masking operations anyway.; //; // TODO: We could assert the top bit is 0 for the source copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:131,Testability,assert,assert,131,// Don't bother inserting AssertZext for packed IDs since we're emitting the; // masking operations anyway.; //; // TODO: We could assert the top bit is 0 for the source copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:28,Performance,load,loaded,28,/// Legalize a value that's loaded from kernel arguments. This is only used by; /// legacy intrinsics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:112,Availability,error,error,112,"// v_rcp_f32 and v_rsq_f32 do not support denormals, and according to; // the CI documentation has a worst case error of 1 ulp.; // OpenCL requires <= 2.5 ulp for 1.0 / x, so it should always be OK to; // use it as long as we aren't trying to use denormals.; //; // v_rcp_f16 and v_rsq_f16 DO support denormals and 0.51ulp.; // 1 / x -> RCP(x)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:88,Usability,usab,usable,88,// Workaround a hardware bug on SI where the condition output from div_scale; // is not usable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:9,Availability,down,down,9,// Scale down the result.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:55,Availability,mask,mask,55,"/// To create a buffer resource from a 64-bit pointer, mask off the upper 32; /// bits of the pointer and replace them with the stride argument, then; /// merge_values everything together. In the common case of a raw buffer (the; /// stride component is 0), we can just AND off the upper half.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:353,Availability,down,down,353,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:206,Energy Efficiency,power,power,206,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:304,Performance,load,load,304,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer(imm)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer(imm)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:29,Performance,load,loads,29,"// Make addrspace 8 pointers loads into 4xs32 loads here, so the rest of the; // logic doesn't have to handle that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:46,Performance,load,loads,46,"// Make addrspace 8 pointers loads into 4xs32 loads here, so the rest of the; // logic doesn't have to handle that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:81,Testability,log,logic,81,"// Make addrspace 8 pointers loads into 4xs32 loads here, so the rest of the; // logic doesn't have to handle that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:42,Performance,load,loads,42,// TODO: Support TFE for typed and narrow loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer(imm)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:23,Performance,optimiz,optimization,23,// Check for _L to _LZ optimization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:90,Integrability,Depend,Depending,90,"/// Rewrite image intrinsics to use register layouts expected by the subtarget.; ///; /// Depending on the subtarget, load/store with 16-bit element data need to be; /// rewritten to use the low half of 32-bit registers, or directly use a packed; /// layout. 16-bit addresses should also sometimes be packed into 32-bit; /// registers.; ///; /// We don't want to directly select image instructions just yet, but also want; /// to exposes all register repacking to the legalizer/combiners. We also don't; /// want a selected instruction entering RegBankSelect. In order to avoid; /// defining a multitude of intermediate image instructions, directly hack on; /// the intrinsic's arguments. In cases like a16 addresses, this requires; /// padding now unnecessary arguments with $noreg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:4,Modifiability,Rewrite,Rewrite,4,"/// Rewrite image intrinsics to use register layouts expected by the subtarget.; ///; /// Depending on the subtarget, load/store with 16-bit element data need to be; /// rewritten to use the low half of 32-bit registers, or directly use a packed; /// layout. 16-bit addresses should also sometimes be packed into 32-bit; /// registers.; ///; /// We don't want to directly select image instructions just yet, but also want; /// to exposes all register repacking to the legalizer/combiners. We also don't; /// want a selected instruction entering RegBankSelect. In order to avoid; /// defining a multitude of intermediate image instructions, directly hack on; /// the intrinsic's arguments. In cases like a16 addresses, this requires; /// padding now unnecessary arguments with $noreg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:118,Performance,load,load,118,"/// Rewrite image intrinsics to use register layouts expected by the subtarget.; ///; /// Depending on the subtarget, load/store with 16-bit element data need to be; /// rewritten to use the low half of 32-bit registers, or directly use a packed; /// layout. 16-bit addresses should also sometimes be packed into 32-bit; /// registers.; ///; /// We don't want to directly select image instructions just yet, but also want; /// to exposes all register repacking to the legalizer/combiners. We also don't; /// want a selected instruction entering RegBankSelect. In order to avoid; /// defining a multitude of intermediate image instructions, directly hack on; /// the intrinsic's arguments. In cases like a16 addresses, this requires; /// padding now unnecessary arguments with $noreg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:572,Safety,avoid,avoid,572,"/// Rewrite image intrinsics to use register layouts expected by the subtarget.; ///; /// Depending on the subtarget, load/store with 16-bit element data need to be; /// rewritten to use the low half of 32-bit registers, or directly use a packed; /// layout. 16-bit addresses should also sometimes be packed into 32-bit; /// registers.; ///; /// We don't want to directly select image instructions just yet, but also want; /// to exposes all register repacking to the legalizer/combiners. We also don't; /// want a selected instruction entering RegBankSelect. In order to avoid; /// defining a multitude of intermediate image instructions, directly hack on; /// the intrinsic's arguments. In cases like a16 addresses, this requires; /// padding now unnecessary arguments with $noreg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:430,Security,expose,exposes,430,"/// Rewrite image intrinsics to use register layouts expected by the subtarget.; ///; /// Depending on the subtarget, load/store with 16-bit element data need to be; /// rewritten to use the low half of 32-bit registers, or directly use a packed; /// layout. 16-bit addresses should also sometimes be packed into 32-bit; /// registers.; ///; /// We don't want to directly select image instructions just yet, but also want; /// to exposes all register repacking to the legalizer/combiners. We also don't; /// want a selected instruction entering RegBankSelect. In order to avoid; /// defining a multitude of intermediate image instructions, directly hack on; /// the intrinsic's arguments. In cases like a16 addresses, this requires; /// padding now unnecessary arguments with $noreg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:34,Performance,load,load,34,"// If dmask is 0, this is a no-op load. This can be eliminated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:23,Availability,error,error,23,// Expecting to get an error flag since TFC is on - and dmask is 0 Force; // dmask to be at least 1 otherwise the instruction will fail,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:3,Modifiability,Rewrite,Rewrite,3,// Rewrite the addressing register layout before doing anything else.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:376,Performance,load,load,376,"// Image atomic instructions are using DMask to specify how many bits; // input/output data will have. 32-bits (s32, v2s16) or 64-bits (s64, v4s16).; // DMaskLanes for image atomic has default value '0'.; // We must be sure that atomic variants (especially packed) will not be; // truncated from v2s16 or v4s16 to s16 type.; //; // ChangeElementCount will be needed for image load where Ty is always scalar.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:47,Performance,load,load,47,"// The raw dword aligned data component of the load. The only legal cases; // where this matters should be when using the packed D16 format, for; // s16 -> <2 x s16>, and <3 x s16> -> <4 x s16>,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:33,Performance,load,loaded,33,// Register type to use for each loaded component. Will be S32 or V2S16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:212,Modifiability,Rewrite,Rewrite,212,"// In the IR, TFE is supposed to be used with a 2 element struct return; // type. The instruction really returns these two values in one contiguous; // register, with one additional dword beyond the loaded data. Rewrite the; // return type to use a single register result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:199,Performance,load,loaded,199,"// In the IR, TFE is supposed to be used with a 2 element struct return; // type. The instruction really returns these two values in one contiguous; // register, with one additional dword beyond the loaded data. Rewrite the; // return type to use a single register result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:3,Safety,Avoid,Avoid,3,// Avoid a build/concat_vector of 1 entry.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:155,Performance,load,load,155,"// For packed D16 results with TFE enabled, all the data components are; // S32. Cast back to the expected type.; //; // TODO: We don't really need to use load s32 elements. We would only need one; // cast for the TFE result if a multiple of v2s16 was used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:38,Performance,load,load,38,// The 8-bit and 16-bit scalar buffer load instructions have 32-bit; // destination register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:30,Performance,load,load,30,// Handle needing to s.buffer.load() a p8 value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:41,Performance,load,loads,41,"// If we don't have 96-bit result scalar loads, widening to 128-bit should; // always be legal. We may need to restore this to a 96-bit result if it turns; // out this needs to be converted to a vector load during RegBankSelect.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:202,Performance,load,load,202,"// If we don't have 96-bit result scalar loads, widening to 128-bit should; // always be legal. We may need to restore this to a 96-bit result if it turns; // out this needs to be converted to a vector load during RegBankSelect.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:3,Performance,Load,Load,3,// Load address,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:8,Performance,queue,queue,8,"// Pass queue pointer to trap handler as input, and insert trap instruction; // Reference: https://llvm.org/docs/AMDGPUUsage.html#trap-handler-abi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:14,Availability,error,error,14,// TODO: Emit error for hsa,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:431,Performance,optimiz,optimizations,431,"//===- AMDGPULibCalls.cpp -------------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This file does AMD library function optimizations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:15,Performance,optimiz,optimizations,15,/* Specialized optimizations */; // pow/powr/pown,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:36,Performance,optimiz,optimizations,36,// Data structures for table-driven optimizations.; // FuncTbl works for both f32 and f64 functions with 1 input argument,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:66,Safety,safe,safe,66,"// If we are doing PreLinkOpt, the function is external. So it is safe to; // use getOrInsertFunction() at this stage.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:33,Integrability,contract,contract,33,// TODO: Refine to approxFunc or contract,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:317,Energy Efficiency,power,power,317,"// Clang emits call of __read_pipe_2 or __read_pipe_4 for OpenCL read_pipe; // builtin, with appended type size and alignment arguments, where 2 or 4; // indicates the original number of arguments. The library has optimized version; // of __read_pipe_2/__read_pipe_4 when the type size and alignment has the same; // power of 2 value. This function transforms __read_pipe_2 to __read_pipe_2_N; // for such cases where N is the size in bytes of the type (N = 1, 2, 4, 8, ...,; // 128). The same for __read_pipe_4, write_pipe_2, and write_pipe_4.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:214,Performance,optimiz,optimized,214,"// Clang emits call of __read_pipe_2 or __read_pipe_4 for OpenCL read_pipe; // builtin, with appended type size and alignment arguments, where 2 or 4; // indicates the original number of arguments. The library has optimized version; // of __read_pipe_2/__read_pipe_4 when the type size and alignment has the same; // power of 2 value. This function transforms __read_pipe_2 to __read_pipe_2_N; // for such cases where N is the size in bytes of the type (N = 1, 2, 4, 8, ...,; // 128). The same for __read_pipe_4, write_pipe_2, and write_pipe_4.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:9,Safety,unsafe,unsafe-math,9,"// Under unsafe-math, evaluate calls if possible.; // According to Brian Sumner, we can do this for all f32 function calls; // using host's double function calls.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:15,Performance,optimiz,optimizations,15,// Specialized optimizations for each function call.; //; // TODO: Handle native functions,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:15,Performance,optimiz,optimizations,15,// Specialized optimizations for each function call,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:16,Performance,optimiz,optimization,16,// Table-Driven optimization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:15,Performance,optimiz,optimization,15,// Unsafe Math optimization; // Remember that ci_opr1 is set if opr1 is integral,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:3,Safety,Unsafe,Unsafe,3,// Unsafe Math optimization; // Remember that ci_opr1 is set if opr1 is integral,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:31,Integrability,wrap,wrappers,31,"// Some library calls are just wrappers around llvm intrinsics, but compiled; // conservatively. Preserve the flags from the original call site by; // substituting them with direct calls with all the flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:18,Energy Efficiency,allocate,allocates,18,"// The allocaInst allocates the memory in private address space. This need; // to be addrspacecasted to point to the address space of cos pointer type.; // In OpenCL 2.0 this is generic, while in 1.2 that is private.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:83,Performance,load,load,83,// TODO: Is it worth trying to preserve the location for the cos calls for the; // load?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:125,Availability,avail,available,125,"// Merge the sin and cos. For OpenCL 2.0, there may only be a generic pointer; // implementation. Prefer the private form if available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:40,Safety,avoid,avoid,40,// Leave the other dead instructions to avoid clobbering iterators.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:8,Safety,safe,safe,8,// It's safe to delete the original now.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:15,Testability,log,log,15,// acosh(x) == log(x + sqrt(x*x - 1)),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:15,Testability,log,log,15,// asinh(x) == log(x + sqrt(x*x + 1)),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:16,Testability,log,log,16,// atanh(x) == (log(x+1) - log(x-1))/2;,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:27,Testability,log,log,27,// atanh(x) == (log(x+1) - log(x-1))/2;,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp:1037,Usability,simpl,simple,1037,"// This table describes function formal argument type rules. The order of rules; // corresponds to the EFuncId enum at AMDGPULibFunc.h; //; // ""<func name>"", { <leads> }, { <param rules> }; // where:; // <leads> - list of integers that are one-based indexes of formal argument; // used to mangle a function name. Other argument types are derived from types; // of these 'leads'. The order of integers in this list correspond to the; // order in which these arguments are mangled in the EDG mangling scheme. The; // same order should be preserved for arguments in the AMDGPULibFunc structure; // when it is used for mangling. For example:; // { ""vstorea_half"", {3,1}, {E_ANY,EX_SIZET,E_ANY}},; // will be mangled in EDG scheme as vstorea_half_<3dparam>_<1stparam>; // When mangling from code use:; // AMDGPULibFunc insc;; // insc.param[0] = ... // describe 3rd parameter; // insc.param[1] = ... // describe 1rd parameter; //; // <param rules> - list of rules used to derive all of the function formal; // argument types. EX_ prefixed are simple types, other derived from the; // latest 'lead' argument type in the order of encoding from first to last.; // E_ANY - use prev lead type, E_CONSTPTR_ANY - make const pointer out of; // prev lead type, etc. see ParamIterator::getNextParam() for details.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp:39,Modifiability,extend,extended,39,// TBD - This switch may require to be extended for other intrinsics,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp:397,Modifiability,extend,extended,397,"// Itanium mangling ABI says:; // ""5.1.8. Compression; // ... Each non-terminal in the grammar for which <substitution> appears on the; // right-hand side is both a source of future substitutions and a candidate; // for being substituted. There are two exceptions that appear to be; // substitution candidates from the grammar, but are explicitly excluded:; // 1. <builtin-type> other than vendor extended types ...""; // For the purpose of functions the following productions make sense for the; // substitution:; // <type> ::= <builtin-type>; // ::= <class-enum-type>; // ::= <array-type>; // ::=<CV-qualifiers> <type>; // ::= P <type> # pointer-to; // ::= <substitution>; //; // Note that while types like images, samplers and events are by the ABI encoded; // using <class-enum-type> production rule they're not used for substitution; // because clang consider them as builtin types.; //; // DvNN_ type is GCC extension for vectors and is a subject for the; // substitution.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp:405,Performance,optimiz,optimize,405,"// Itanium mangling ABI 5.1.8. Compression:; // Logically, the substitutable components of a mangled name are considered; // left-to-right, components before the composite structure of which they; // are a part. If a component has been encountered before, it is substituted; // as described below. This decision is independent of whether its components; // have been substituted, so an implementation may optimize by considering; // large structures for substitution before their components. If a component; // has not been encountered before, its mangling is identified, and it is; // added to a dictionary of substitution candidates. No entity is added to; // the dictionary twice.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp:48,Testability,Log,Logically,48,"// Itanium mangling ABI 5.1.8. Compression:; // Logically, the substitutable components of a mangled name are considered; // left-to-right, components before the composite structure of which they; // are a part. If a component has been encountered before, it is substituted; // as described below. This decision is independent of whether its components; // have been substituted, so an implementation may optimize by considering; // large structures for substitution before their components. If a component; // has not been encountered before, its mangling is identified, and it is; // added to a dictionary of substitution candidates. No entity is added to; // the dictionary twice.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp:9,Security,Validat,Validate,9,// TODO: Validate types make sense,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.h:4,Integrability,Wrap,Wrapper,4,/// Wrapper class for AMDGPULIbFuncImpl,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.h:3,Security,Validat,Validate,3,// Validate the call type matches the expected libfunc type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibFunc.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:449,Performance,load,loads,449,"//===-- AMDGPULowerKernelArguments.cpp ------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass replaces accesses to kernel arguments with loads from; /// offsets from the kernarg base pointer.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:415,Security,access,accesses,415,"//===-- AMDGPULowerKernelArguments.cpp ------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass replaces accesses to kernel arguments with loads from; /// offsets from the kernarg base pointer.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:57,Availability,avail,available,57,// Returns the maximum number of user SGPRs that we have available to preload; // arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:33,Performance,load,loaded,33,// Check if this argument may be loaded into the same register as the; // previous argument.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:46,Integrability,depend,depend,46,"// If this is a dynamic alloca, the value may depend on the loaded kernargs,; // so loads will need to be inserted before it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:60,Performance,load,loaded,60,"// If this is a dynamic alloca, the value may depend on the loaded kernargs,; // so loads will need to be inserted before it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:84,Performance,load,loads,84,"// If this is a dynamic alloca, the value may depend on the loaded kernargs,; // so loads will need to be inserted before it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:89,Modifiability,rewrite,rewrite,89,"// If this is byval, the loads are already explicit in the function. We just; // need to rewrite the pointer values.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:25,Performance,load,loads,25,"// If this is byval, the loads are already explicit in the function. We just; // need to rewrite the pointer values.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:137,Integrability,wrap,wrap,137,// FIXME: Hack. We rely on AssertZext to be able to fold DS addressing; // modes on SI to know the high bits are 0 so pointer adds don't wrap. We; // can't represent this with range metadata because it's only allowed for; // integer types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:27,Testability,Assert,AssertZext,27,// FIXME: Hack. We rely on AssertZext to be able to fold DS addressing; // modes on SI to know the high bits are 0 so pointer adds don't wrap. We; // can't represent this with range metadata because it's only allowed for; // integer types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:202,Deployability,Update,Update,202,"// FIXME: Handle aggregate types; // Since we don't have sub-dword scalar loads, avoid doing an extload by; // loading earlier than the argument address, and extracting the relevant; // bits.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.; //; // Additionally widen any sub-dword load to i32 even if suitably aligned,; // so that CSE between different argument loads works easily.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:74,Performance,load,loads,74,"// FIXME: Handle aggregate types; // Since we don't have sub-dword scalar loads, avoid doing an extload by; // loading earlier than the argument address, and extracting the relevant; // bits.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.; //; // Additionally widen any sub-dword load to i32 even if suitably aligned,; // so that CSE between different argument loads works easily.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:111,Performance,load,loading,111,"// FIXME: Handle aggregate types; // Since we don't have sub-dword scalar loads, avoid doing an extload by; // loading earlier than the argument address, and extracting the relevant; // bits.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.; //; // Additionally widen any sub-dword load to i32 even if suitably aligned,; // so that CSE between different argument loads works easily.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:257,Performance,load,loads,257,"// FIXME: Handle aggregate types; // Since we don't have sub-dword scalar loads, avoid doing an extload by; // loading earlier than the argument address, and extracting the relevant; // bits.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.; //; // Additionally widen any sub-dword load to i32 even if suitably aligned,; // so that CSE between different argument loads works easily.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:305,Performance,load,load,305,"// FIXME: Handle aggregate types; // Since we don't have sub-dword scalar loads, avoid doing an extload by; // loading earlier than the argument address, and extracting the relevant; // bits.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.; //; // Additionally widen any sub-dword load to i32 even if suitably aligned,; // so that CSE between different argument loads works easily.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:386,Performance,load,loads,386,"// FIXME: Handle aggregate types; // Since we don't have sub-dword scalar loads, avoid doing an extload by; // loading earlier than the argument address, and extracting the relevant; // bits.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.; //; // Additionally widen any sub-dword load to i32 even if suitably aligned,; // so that CSE between different argument loads works easily.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:81,Safety,avoid,avoid,81,"// FIXME: Handle aggregate types; // Since we don't have sub-dword scalar loads, avoid doing an extload by; // loading earlier than the argument address, and extracting the relevant; // bits.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.; //; // Additionally widen any sub-dword load to i32 even if suitably aligned,; // so that CSE between different argument loads works easily.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:65,Performance,load,loads,65,// Use the hack that clang uses to avoid SelectionDAG ruining v3 loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:35,Safety,avoid,avoid,35,// Use the hack that clang uses to avoid SelectionDAG ruining v3 loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp:484,Performance,load,loads,484,"//===-- AMDGPULowerKernelAttributes.cpp ------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass does attempts to make use of reqd_work_group_size metadata; /// to eliminate loads from the dispatch packet and to constant fold OpenCL; /// get_local_size-like functions.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp:78,Performance,load,loaded,78,"// We expect to see several GEP users, casted to the appropriate type and; // loaded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp:3,Performance,Load,Load,3,// Load from ImplicitArgPtr/DispatchPtr?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp:3,Performance,Load,Load,3,// Load from GEP?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp:3,Performance,Load,Load,3,// Load from BCI?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp:23,Performance,load,loads,23,// TODO: Handle merged loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelAttributes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:1090,Availability,avail,available,1090,"ject, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more compli",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2680,Availability,error,error,2680," as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kern",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4230,Availability,avail,available,4230,"ted here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It wil",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:5255,Availability,error,error,5255,"e integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:7904,Availability,error,error,7904,"his pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:8707,Availability,error,error,8707,"re given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:10187,Availability,error,error,10187," it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts until after the LDS frame lowering, it should be; // possible to drop the name mapping and fold equivalent memory layouts.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:488,Energy Efficiency,allocate,allocated,488,"//===-- AMDGPULowerModuleLDSPass.cpp ------------------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global represe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:831,Energy Efficiency,allocate,allocate,831,"//===-- AMDGPULowerModuleLDSPass.cpp ------------------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global represe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:1011,Energy Efficiency,allocate,allocated,1011,"//===-- AMDGPULowerModuleLDSPass.cpp ------------------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global represe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2813,Energy Efficiency,allocate,allocated,2813,"by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycle",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:3031,Energy Efficiency,allocate,allocate,3031,"ol over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variable",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4417,Energy Efficiency,allocate,allocates,4417,"| No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4588,Energy Efficiency,allocate,allocate,4588," save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free fo",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:6942,Energy Efficiency,allocate,allocates,6942,"ctions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:7748,Energy Efficiency,allocate,allocated,7748,"dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // s",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:8353,Energy Efficiency,allocate,allocate,8353," kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that w",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:8551,Energy Efficiency,allocate,allocates,8551," LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // T",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:9465,Energy Efficiency,reduce,reduce,9465,"und is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:10139,Energy Efficiency,allocate,allocated,10139," it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts until after the LDS frame lowering, it should be; // possible to drop the name mapping and fold equivalent memory layouts.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:6188,Integrability,inject,injected,6188,"bove. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during th",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:580,Modifiability,variab,variables,580,"//===-- AMDGPULowerModuleLDSPass.cpp ------------------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global represe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:633,Modifiability,variab,variables,633,"//===-- AMDGPULowerModuleLDSPass.cpp ------------------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global represe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:811,Modifiability,variab,variables,811,"//===-- AMDGPULowerModuleLDSPass.cpp ------------------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global represe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:1164,Modifiability,variab,variable,1164,"ject, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more compli",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:1690,Modifiability,variab,variables,1690,"quivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annota",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:1904,Modifiability,variab,variables,1904,"his model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2565,Modifiability,variab,variables,2565," as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kern",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2787,Modifiability,variab,variables,2787,"by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycle",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2994,Modifiability,variab,variables,2994,"ol over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variable",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:3095,Modifiability,variab,variables,3095,"ol over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variable",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:3692,Modifiability,variab,variables,3692,"iagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:3910,Modifiability,variab,variables,3910," simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4027,Modifiability,variab,variables,4027," simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4038,Modifiability,Variab,Variables,4038,"el such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, a",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4431,Modifiability,variab,variables,4431,"| No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4517,Modifiability,variab,variable,4517,"| No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4605,Modifiability,variab,variable,4605," save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free fo",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4916,Modifiability,variab,variable,4916,"use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user c",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:4975,Modifiability,variab,variables,4975,"ct with a field for each of those; // LDS variables. Variables that are only used from kernels are excluded.; //; // The ""table"" lowering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:5080,Modifiability,variab,variable,5080,"owering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:5268,Modifiability,variab,variable,5268,"e integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:5518,Modifiability,variab,variable,5518,"ernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:5634,Modifiability,variab,variable,5634,"ernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:5655,Modifiability,variab,variables,5655," a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // gra",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:6132,Modifiability,Variab,Variables,6132," specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the ta",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:6764,Modifiability,variab,variables,6764," are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be all",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:6907,Modifiability,variab,variables,6907,"ctions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:7268,Modifiability,variab,variable,7268,"ad; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:7335,Modifiability,variab,variables,7335,"ad; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:7385,Modifiability,variab,variables,7385,"cted to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-addr",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:7520,Modifiability,variab,variables,7520,"ation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This require",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:7728,Modifiability,variab,variables,7728,"dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // s",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:7835,Modifiability,variab,variables,7835,"his pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:8215,Modifiability,variab,variables,8215,"otes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:8380,Modifiability,variab,variables,8380," kernel.; // Handling ConstantExpr during the pass complicated this significantly so now; // all ConstantExpr uses of LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that w",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:8533,Modifiability,variab,variables,8533," LDS variables are expanded to instructions. This; // may need amending when implementing non-undef initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // T",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:8636,Modifiability,variab,variable,8636,"def initialisers.; //; // Lowering is split between this IR pass and the back end. This pass chooses; // where given variables should be allocated and marks them with metadata,; // MD_absolute_symbol. The backend places the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:8875,Modifiability,variab,variable,8875,"the variables in coincidentally the; // same location and raises a fatal error if something has gone awry. This works; // in practice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts unt",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:10006,Modifiability,variab,variables,10006," it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts until after the LDS frame lowering, it should be; // possible to drop the name mapping and fold equivalent memory layouts.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:10121,Modifiability,variab,variable,10121," it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts until after the LDS frame lowering, it should be; // possible to drop the name mapping and fold equivalent memory layouts.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:10254,Modifiability,variab,variables,10254," it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts until after the LDS frame lowering, it should be; // possible to drop the name mapping and fold equivalent memory layouts.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:896,Performance,perform,performance,896,"//===-- AMDGPULowerModuleLDSPass.cpp ------------------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global represe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2437,Performance,perform,performance,2437,"se.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost |",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2234,Safety,avoid,avoid,2234,"se.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost |",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2482,Safety,avoid,avoid,2482," a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:9045,Safety,hazard,hazard,9045,"ges it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:10065,Safety,avoid,avoids,10065," it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts until after the LDS frame lowering, it should be; // possible to drop the name mapping and fold equivalent memory layouts.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:10088,Safety,hazard,hazard,10088," it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts until after the LDS frame lowering, it should be; // possible to drop the name mapping and fold equivalent memory layouts.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:644,Security,access,accessible,644,"//===-- AMDGPULowerModuleLDSPass.cpp ------------------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass eliminates local data store, LDS, uses from non-kernel functions.; // LDS is contiguous memory allocated per kernel execution.; //; // Background.; //; // The programming model is global variables, or equivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global represe",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:1424,Security,access,accessed,1424,"quivalently function local; // static variables, accessible from kernels or other functions. For uses from; // kernels this is straightforward - assign an integer to the kernel for the; // memory required by all the variables combined, allocate them within that.; // For uses from functions there are performance tradeoffs to choose between.; //; // This model means the GPU runtime can specify the amount of memory allocated.; // If this is more than the kernel assumed, the excess can be made available; // using a language specific feature, which IR represents as a variable with; // no initializer. This feature is referred to here as ""Dynamic LDS"" and is; // lowered slightly differently to the normal case.; //; // Consequences of this GPU feature:; // - memory is limited and exceeding it halts compilation; // - a global accessed by one kernel exists independent of other kernels; // - a global exists independent of simultaneous execution of the same kernel; // - the address of the global may be different from different kernels as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annota",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:5064,Security,access,accesses,5064,"owering implemented here has three components.; // First kernels are assigned a unique integer identifier which is available in; // functions it calls through the intrinsic amdgcn_lds_kernel_id. The integer; // is passed through a specific SGPR, thus works with indirect calls.; // Second, each kernel allocates LDS variables independent of other kernels and; // writes the addresses it chose for each variable into an array in consistent; // order. If the kernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:5527,Security,access,accessed,5527,"ernel does not allocate a given variable, it writes undef to; // the corresponding array location. These arrays are written to a constant; // table in the order matching the kernel unique integer identifier.; // Third, uses from non-kernel functions are replaced with a table lookup using; // the intrinsic function to find the address of the variable.; //; // ""Kernel"" lowering is only applicable for variables that are unambiguously; // reachable from exactly one kernel. For those cases, accesses to the variable; // can be lowered to ConstantExpr address of a struct instance specific to that; // one kernel. This is zero cost in space and in compute. It will raise a fatal; // error on any variable that might be reachable from multiple kernels and is; // thus most easily used as part of the hybrid lowering strategy.; //; // Hybrid lowering is a mixture of the above. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:6188,Security,inject,injected,6188,"bove. It uses the zero cost kernel; // lowering where it can. It lowers the variable accessed by the greatest; // number of kernels using the module strategy as that is free for the first; // variable. Any futher variables that can be lowered with the module strategy; // without incurring LDS memory overhead are. The remaining ones are lowered; // via table.; //; // Consequences; // - No heuristics or user controlled magic numbers, hybrid is the right choice; // - Kernels that don't use functions (or have had them all inlined) are not; // affected by any lowering for kernels that do.; // - Kernels that don't make indirect function calls are not affected by those; // that do.; // - Variables which are used by lots of kernels, e.g. those injected by a; // language runtime in most kernels, are expected to have no overhead; // - Implementations that instantiate templates per-kernel where those templates; // use LDS are expected to hit the ""Kernel"" lowering strategy; // - The runtime properties impose a cost in compiler implementation complexity; //; // Dynamic LDS implementation; // Dynamic LDS is lowered similarly to the ""table"" strategy above and uses the; // same intrinsic to identify which kernel is at the root of the dynamic call; // graph. This relies on the specified behaviour that all dynamic LDS variables; // alias one another, i.e. are at the same address, with respect to a given; // kernel. Therefore this pass creates new dynamic LDS variables for each kernel; // that allocates any dynamic LDS and builds a table of addresses out of those.; // The AMDGPUPromoteAlloca pass skips kernels that use dynamic LDS.; // The corresponding optimisation for ""kernel"" lowering where the table lookup; // is elided is not implemented.; //; //; // Implementation notes / limitations; // A single LDS global variable represents an instance per kernel that can reach; // said variables. This pass essentially specialises said variables per kernel.; // Handling ConstantExpr during th",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:8959,Testability,test,tests,8959,"ctice because the only pass between this one and the backend that; // changes LDS is PromoteAlloca and the changes it makes do not conflict.; //; // Addresses are written to constant global arrays based on the same metadata.; //; // The backend lowers LDS variables in the order of traversal of the function.; // This is at odds with the deterministic layout required. The workaround is to; // allocate the fixed-address variables immediately upon starting the function; // where they can be placed as intended. This requires a means of mapping from; // the function to the variables that it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probab",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:9526,Testability,test,test,9526,"at it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts until after the LDS frame lowering, it should be; // possible to drop the name mapping and fold equivalent memory layouts.; //; //===----------------------------------------------------------------------===",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:9652,Testability,test,test,9652," it allocates. For the module scope lds,; // this is via metadata indicating whether the variable is not required. If a; // pass deletes that metadata, a fatal error on disagreement with the absolute; // symbol metadata will occur. For kernel scope and dynamic, this is by _name_; // correspondence between the function and the variable. It requires the; // kernel to have a name (which is only a limitation for tests in practice) and; // for nothing to rename the corresponding symbols. This is a hazard if the pass; // is run multiple times during debugging. Alternative schemes considered all; // involve bespoke metadata.; //; // If the name correspondence can be replaced, multiple distinct kernels that; // have the same memory layout can map to the same kernel id (as the address; // itself is handled by the absolute symbol metadata) and that will allow more; // uses of the ""kernel"" style faster lowering and reduce the size of the lookup; // tables.; //; // There is a test that checks this does not fire for a graphics shader. This; // lowering is expected to work for graphics if the isKernel test is changed.; //; // The current markUsedByKernel is sufficient for PromoteAlloca but is elided; // before codegen. Replacing this with an equivalent intrinsic which lasts until; // shortly after the machine function lowering of LDS would help break the name; // mapping. The other part needed is probably to amend PromoteAlloca to embed; // the LDS variables it creates in the same struct created here. That avoids the; // current hazard where a PromoteAlloca LDS variable might be allocated before; // the kernel scope (and thus error on the address check). Given a new invariant; // that no LDS variables exist outside of the structs managed here, and an; // intrinsic that lasts until after the LDS frame lowering, it should be; // possible to drop the name mapping and fold equivalent memory layouts.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2662,Usability,simpl,simplification,2662," as they; // do not alias, which permits only allocating variables they use; // - if the address is allowed to differ, functions need help to find it; //; // Uses from kernels are implemented here by grouping them in a per-kernel; // struct instance. This duplicates the variables, accurately modelling their; // aliasing properties relative to a single global representation. It also; // permits control over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kern",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:2947,Usability,simpl,simplest,2947,"ol over alignment via padding.; //; // Uses from functions are more complicated and the primary purpose of this; // IR pass. Several different lowering are chosen between to meet requirements; // to avoid allocating any LDS where it is not necessary, as that impacts; // occupancy and may fail the compilation, while not imposing overhead on a; // feature whose primary advantage over global memory is performance. The basic; // design goal is to avoid one kernel imposing overhead on another.; //; // Implementation.; //; // LDS variables with constant annotation or non-undef initializer are passed; // through unchanged for simplification or error diagnostics in later passes.; // Non-undef initializers are not yet implemented for LDS.; //; // LDS variables that are always allocated at the same address can be found; // by lookup at that address. Otherwise runtime information/cost is required.; //; // The simplest strategy possible is to group all LDS variables in a single; // struct and allocate that struct in every kernel such that the original; // variables are always at the same address. LDS is however a limited resource; // so this strategy is unusable in practice. It is not implemented here.; //; // Strategy | Precise allocation | Zero runtime cost | General purpose |; // --------+--------------------+-------------------+-----------------+; // Module | No | Yes | Yes |; // Table | Yes | No | Yes |; // Kernel | Yes | Yes | No |; // Hybrid | Yes | Partial | Yes |; //; // ""Module"" spends LDS memory to save cycles. ""Table"" spends cycles and global; // memory to save LDS. ""Kernel"" is as fast as kernel allocation but only works; // for variables that are known reachable from a single kernel. ""Hybrid"" picks; // between all three. When forced to choose between LDS and cycles we minimise; // LDS use.; // The ""module"" lowering implemented here finds LDS variables which are used by; // non-kernel functions and creates a new struct with a field for each of those; // LDS variable",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:290,Testability,test,test,290,"// Some weirdness here. AMDGPU::isKernelCC does not call into; // AMDGPU::isKernel with the calling conv, it instead calls into; // isModuleEntryFunction which returns true for more calling conventions; // than AMDGPU::isKernel does. There's a FIXME on AMDGPU::isKernel.; // There's also a test that checks that the LDS lowering does not hit on; // a graphics shader, denoted amdgpu_ps, so stay with the limited case.; // Putting LDS in the name of the function to draw attention to this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:90,Modifiability,variab,variables,90,// The verifier rejects used lists containing an inttoptr of a constant; // so remove the variables from these lists before replaceAllUsesWith,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:668,Availability,robust,robust,668,"// The llvm.amdgcn.module.lds instance is implicitly used by all kernels; // that might call a function which accesses a field within it. This is; // presently approximated to 'all kernels' if there are any such functions; // in the module. This implicit use is redefined as an explicit use here so; // that later passes, specifically PromoteAlloca, account for the required; // memory without any knowledge of this transform.; // An operand bundle on llvm.donothing works because the call instruction; // survives until after the last pass that needs to account for LDS. It is; // better than inline asm as the latter survives until the end of codegen. A; // totally robust solution would be a function with the same semantics as; // llvm.donothing that takes a pointer to the instance and is lowered to a; // no-op after LDS is allocated, but that is not presently necessary.; // This intrinsic is eliminated shortly before instruction selection. It; // does not suffice to indicate to ISel that a given global which is not; // immediately used by the kernel must still be allocated by it. An; // equivalent target specific intrinsic which lasts until immediately after; // codegen would suffice for that, but one would still need to ensure that; // the variables are allocated in the anticpated order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:830,Energy Efficiency,allocate,allocated,830,"// The llvm.amdgcn.module.lds instance is implicitly used by all kernels; // that might call a function which accesses a field within it. This is; // presently approximated to 'all kernels' if there are any such functions; // in the module. This implicit use is redefined as an explicit use here so; // that later passes, specifically PromoteAlloca, account for the required; // memory without any knowledge of this transform.; // An operand bundle on llvm.donothing works because the call instruction; // survives until after the last pass that needs to account for LDS. It is; // better than inline asm as the latter survives until the end of codegen. A; // totally robust solution would be a function with the same semantics as; // llvm.donothing that takes a pointer to the instance and is lowered to a; // no-op after LDS is allocated, but that is not presently necessary.; // This intrinsic is eliminated shortly before instruction selection. It; // does not suffice to indicate to ISel that a given global which is not; // immediately used by the kernel must still be allocated by it. An; // equivalent target specific intrinsic which lasts until immediately after; // codegen would suffice for that, but one would still need to ensure that; // the variables are allocated in the anticpated order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:1075,Energy Efficiency,allocate,allocated,1075,"// The llvm.amdgcn.module.lds instance is implicitly used by all kernels; // that might call a function which accesses a field within it. This is; // presently approximated to 'all kernels' if there are any such functions; // in the module. This implicit use is redefined as an explicit use here so; // that later passes, specifically PromoteAlloca, account for the required; // memory without any knowledge of this transform.; // An operand bundle on llvm.donothing works because the call instruction; // survives until after the last pass that needs to account for LDS. It is; // better than inline asm as the latter survives until the end of codegen. A; // totally robust solution would be a function with the same semantics as; // llvm.donothing that takes a pointer to the instance and is lowered to a; // no-op after LDS is allocated, but that is not presently necessary.; // This intrinsic is eliminated shortly before instruction selection. It; // does not suffice to indicate to ISel that a given global which is not; // immediately used by the kernel must still be allocated by it. An; // equivalent target specific intrinsic which lasts until immediately after; // codegen would suffice for that, but one would still need to ensure that; // the variables are allocated in the anticpated order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:1270,Energy Efficiency,allocate,allocated,1270,"// The llvm.amdgcn.module.lds instance is implicitly used by all kernels; // that might call a function which accesses a field within it. This is; // presently approximated to 'all kernels' if there are any such functions; // in the module. This implicit use is redefined as an explicit use here so; // that later passes, specifically PromoteAlloca, account for the required; // memory without any knowledge of this transform.; // An operand bundle on llvm.donothing works because the call instruction; // survives until after the last pass that needs to account for LDS. It is; // better than inline asm as the latter survives until the end of codegen. A; // totally robust solution would be a function with the same semantics as; // llvm.donothing that takes a pointer to the instance and is lowered to a; // no-op after LDS is allocated, but that is not presently necessary.; // This intrinsic is eliminated shortly before instruction selection. It; // does not suffice to indicate to ISel that a given global which is not; // immediately used by the kernel must still be allocated by it. An; // equivalent target specific intrinsic which lasts until immediately after; // codegen would suffice for that, but one would still need to ensure that; // the variables are allocated in the anticpated order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:1256,Modifiability,variab,variables,1256,"// The llvm.amdgcn.module.lds instance is implicitly used by all kernels; // that might call a function which accesses a field within it. This is; // presently approximated to 'all kernels' if there are any such functions; // in the module. This implicit use is redefined as an explicit use here so; // that later passes, specifically PromoteAlloca, account for the required; // memory without any knowledge of this transform.; // An operand bundle on llvm.donothing works because the call instruction; // survives until after the last pass that needs to account for LDS. It is; // better than inline asm as the latter survives until the end of codegen. A; // totally robust solution would be a function with the same semantics as; // llvm.donothing that takes a pointer to the instance and is lowered to a; // no-op after LDS is allocated, but that is not presently necessary.; // This intrinsic is eliminated shortly before instruction selection. It; // does not suffice to indicate to ISel that a given global which is not; // immediately used by the kernel must still be allocated by it. An; // equivalent target specific intrinsic which lasts until immediately after; // codegen would suffice for that, but one would still need to ensure that; // the variables are allocated in the anticpated order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:110,Security,access,accesses,110,"// The llvm.amdgcn.module.lds instance is implicitly used by all kernels; // that might call a function which accesses a field within it. This is; // presently approximated to 'all kernels' if there are any such functions; // in the module. This implicit use is redefined as an explicit use here so; // that later passes, specifically PromoteAlloca, account for the required; // memory without any knowledge of this transform.; // An operand bundle on llvm.donothing works because the call instruction; // survives until after the last pass that needs to account for LDS. It is; // better than inline asm as the latter survives until the end of codegen. A; // totally robust solution would be a function with the same semantics as; // llvm.donothing that takes a pointer to the instance and is lowered to a; // no-op after LDS is allocated, but that is not presently necessary.; // This intrinsic is eliminated shortly before instruction selection. It; // does not suffice to indicate to ISel that a given global which is not; // immediately used by the kernel must still be allocated by it. An; // equivalent target specific intrinsic which lasts until immediately after; // codegen would suffice for that, but one would still need to ensure that; // the variables are allocated in the anticpated order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:220,Energy Efficiency,allocate,allocates,220,"// Constants are uniqued within LLVM. A ConstantExpr referring to a LDS; // global may have uses from multiple different functions as a result.; // This pass specialises LDS variables with respect to the kernel that; // allocates them.; // This is semantically equivalent to (the unimplemented as slow):; // for (auto &F : M.functions()); // for (auto &BB : F); // for (auto &I : BB); // for (Use &Op : I.operands()); // if (constantExprUsesLDS(Op)); // replaceConstantExprInFunction(I, Op);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:174,Modifiability,variab,variables,174,"// Constants are uniqued within LLVM. A ConstantExpr referring to a LDS; // global may have uses from multiple different functions as a result.; // This pass specialises LDS variables with respect to the kernel that; // allocates them.; // This is semantically equivalent to (the unimplemented as slow):; // for (auto &F : M.functions()); // for (auto &BB : F); // for (auto &I : BB); // for (Use &Op : I.operands()); // if (constantExprUsesLDS(Op)); // replaceConstantExprInFunction(I, Op);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:89,Modifiability,variab,variables,89,"// Get uses from the current function, excluding uses by called functions; // Two output variables to avoid walking the globals list twice",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:102,Safety,avoid,avoid,102,"// Get uses from the current function, excluding uses by called functions; // Two output variables to avoid walking the globals list twice",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:11,Modifiability,variab,variables,11,// Collect variables that are used by functions whose address has escaped,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:18,Modifiability,variab,variables,18,// Work out which variables are reachable through function calls,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:92,Modifiability,variab,variables,92,"// If the function makes any unknown call, assume the worst case that it can; // access all variables accessed by functions whose address escaped",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:81,Security,access,access,81,"// If the function makes any unknown call, assume the worst case that it can; // access all variables accessed by functions whose address escaped",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:102,Security,access,accessed,102,"// If the function makes any unknown call, assume the worst case that it can; // access all variables accessed by functions whose address escaped",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:43,Modifiability,variab,variables,43,// Direct implementation of collecting all variables reachable from each; // function,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:33,Modifiability,variab,variables,33,// direct_map_kernel lists which variables are used by the kernel; // find the variables which are used through a function call,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:79,Modifiability,variab,variables,79,// direct_map_kernel lists which variables are used by the kernel; // find the variables which are used through a function call,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:181,Modifiability,variab,variable,181,// remap from lds global to a constantexpr gep to where it has been moved to; // for each kernel; // an array with an element for each kernel containing where the corresponding; // variable was remapped to,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:165,Energy Efficiency,allocate,allocate,165,"// Create a ConstantArray containing the address of each Variable within the; // kernel corresponding to LDSVarsToConstantGEP, or poison if that kernel; // does not allocate it; // TODO: Drop the ptrtoint conversion",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:57,Modifiability,Variab,Variable,57,"// Create a ConstantArray containing the address of each Variable within the; // kernel corresponding to LDSVarsToConstantGEP, or poison if that kernel; // does not allocate it; // TODO: Drop the ptrtoint conversion",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:19,Modifiability,variab,variable,19,// Find the global variable with the most indirect uses from kernels,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:34,Modifiability,variab,variable,34,// Fewer users makes module scope variable less attractive,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:29,Modifiability,variab,variable,29,// Bigger makes module scope variable less attractive,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:5,Modifiability,variab,variable,5,// A variable reachable by only one kernel is best lowered with kernel; // strategy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:3,Security,Access,Accesses,3,// Accesses from a function use the amdgcn_lds_kernel_id intrinsic which; // lowers to a read from a live in register. Emit it once in the entry; // block to spare deduplicating it later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:14,Availability,error,error,14,// else fatal error earlier,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:58,Modifiability,variab,variable,58,// Each iteration of this loop assigns exactly one global variable to; // exactly one of the implementation strategies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:11,Modifiability,variab,variables,11,// All LDS variables accessed indirectly have now been partitioned into; // the distinct lowering strategies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:21,Security,access,accessed,21,// All LDS variables accessed indirectly have now been partitioned into; // the distinct lowering strategies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:209,Energy Efficiency,allocate,allocate,209,// Create a struct to hold the ModuleScopeVariables; // Replace all uses of those variables from non-kernel functions with the; // new struct instance Replace only the uses from kernel functions that will; // allocate this instance. That is a space optimisation - kernels that use a; // subset of the module scope struct and do not need to allocate it for; // indirect calls will only allocate the subset they use (they do so as part; // of the per-kernel lowering).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:340,Energy Efficiency,allocate,allocate,340,// Create a struct to hold the ModuleScopeVariables; // Replace all uses of those variables from non-kernel functions with the; // new struct instance Replace only the uses from kernel functions that will; // allocate this instance. That is a space optimisation - kernels that use a; // subset of the module scope struct and do not need to allocate it for; // indirect calls will only allocate the subset they use (they do so as part; // of the per-kernel lowering).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:385,Energy Efficiency,allocate,allocate,385,// Create a struct to hold the ModuleScopeVariables; // Replace all uses of those variables from non-kernel functions with the; // new struct instance Replace only the uses from kernel functions that will; // allocate this instance. That is a space optimisation - kernels that use a; // subset of the module scope struct and do not need to allocate it for; // indirect calls will only allocate the subset they use (they do so as part; // of the per-kernel lowering).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:82,Modifiability,variab,variables,82,// Create a struct to hold the ModuleScopeVariables; // Replace all uses of those variables from non-kernel functions with the; // new struct instance Replace only the uses from kernel functions that will; // allocate this instance. That is a space optimisation - kernels that use a; // subset of the module scope struct and do not need to allocate it for; // indirect calls will only allocate the subset they use (they do so as part; // of the per-kernel lowering).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:22,Energy Efficiency,allocate,allocated,22,// module.lds will be allocated at zero in any kernel that allocates it,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:59,Energy Efficiency,allocate,allocates,59,// module.lds will be allocated at zero in any kernel that allocates it,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:36,Modifiability,variab,variable,36,// Replace all uses of module scope variable from non-kernel functions,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:72,Energy Efficiency,allocate,allocate,72,"// Replace uses of module scope variable from kernel functions that; // allocate the module scope variable, otherwise leave them unchanged; // Record on each kernel whether the module scope global is used by it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:32,Modifiability,variab,variable,32,"// Replace uses of module scope variable from kernel functions that; // allocate the module scope variable, otherwise leave them unchanged; // Record on each kernel whether the module scope global is used by it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:98,Modifiability,variab,variable,98,"// Replace uses of module scope variable from kernel functions that; // allocate the module scope variable, otherwise leave them unchanged; // Record on each kernel whether the module scope global is used by it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:60,Modifiability,variab,variables,60,// Create a struct for each kernel for the non-module-scope variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:14,Modifiability,variab,variables,14,// Allocating variables that are used directly in this struct to get; // alignment aware allocation and predictable frame size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:104,Safety,predict,predictable,104,// Allocating variables that are used directly in this struct to get; // alignment aware allocation and predictable frame size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:14,Modifiability,variab,variables,14,// Allocating variables that are accessed indirectly so that a lookup of; // this struct instance can find them from nested functions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:33,Security,access,accessed,33,// Allocating variables that are accessed indirectly so that a lookup of; // this struct instance can find them from nested functions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:13,Energy Efficiency,allocate,allocated,13,"// Variables allocated in module lds must all resolve to that struct,; // not to the per-kernel instance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:3,Modifiability,Variab,Variables,3,"// Variables allocated in module lds must all resolve to that struct,; // not to the per-kernel instance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:331,Testability,test,test,331,"// The association between kernel function and LDS struct is done by; // symbol name, which only works if the function in question has a; // name This is not expected to be a problem in practice as kernels; // are called by name making anonymous ones (which are named by the; // backend) difficult to use. This does mean that llvm test cases need; // to name the kernels.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:147,Testability,test,test,147,"// If any indirect uses, create a direct use to ensure allocation; // TODO: Simpler to unconditionally mark used but that regresses; // codegen in test/CodeGen/AMDGPU/noclobber-barrier.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:76,Usability,Simpl,Simpler,76,"// If any indirect uses, create a direct use to ensure allocation; // TODO: Simpler to unconditionally mark used but that regresses; // codegen in test/CodeGen/AMDGPU/noclobber-barrier.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:3,Modifiability,Rewrite,Rewrite,3,// Rewrite uses within kernel to the new struct,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:191,Energy Efficiency,allocate,allocated,191,"// Create a dynamic lds variable with a name associated with the passed; // function that has the maximum alignment of any dynamic lds variable; // reachable from this kernel. Dynamic LDS is allocated after the static LDS; // allocation, possibly after alignment padding. The representative variable; // created here has the maximum alignment of any other dynamic variable; // reachable by that kernel. All dynamic LDS variables are allocated at the; // same address in each kernel in order to provide the documented aliasing; // semantics. Setting the alignment here allows this IR pass to accurately; // predict the exact constant at which it will be allocated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:433,Energy Efficiency,allocate,allocated,433,"// Create a dynamic lds variable with a name associated with the passed; // function that has the maximum alignment of any dynamic lds variable; // reachable from this kernel. Dynamic LDS is allocated after the static LDS; // allocation, possibly after alignment padding. The representative variable; // created here has the maximum alignment of any other dynamic variable; // reachable by that kernel. All dynamic LDS variables are allocated at the; // same address in each kernel in order to provide the documented aliasing; // semantics. Setting the alignment here allows this IR pass to accurately; // predict the exact constant at which it will be allocated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:653,Energy Efficiency,allocate,allocated,653,"// Create a dynamic lds variable with a name associated with the passed; // function that has the maximum alignment of any dynamic lds variable; // reachable from this kernel. Dynamic LDS is allocated after the static LDS; // allocation, possibly after alignment padding. The representative variable; // created here has the maximum alignment of any other dynamic variable; // reachable by that kernel. All dynamic LDS variables are allocated at the; // same address in each kernel in order to provide the documented aliasing; // semantics. Setting the alignment here allows this IR pass to accurately; // predict the exact constant at which it will be allocated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:24,Modifiability,variab,variable,24,"// Create a dynamic lds variable with a name associated with the passed; // function that has the maximum alignment of any dynamic lds variable; // reachable from this kernel. Dynamic LDS is allocated after the static LDS; // allocation, possibly after alignment padding. The representative variable; // created here has the maximum alignment of any other dynamic variable; // reachable by that kernel. All dynamic LDS variables are allocated at the; // same address in each kernel in order to provide the documented aliasing; // semantics. Setting the alignment here allows this IR pass to accurately; // predict the exact constant at which it will be allocated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:135,Modifiability,variab,variable,135,"// Create a dynamic lds variable with a name associated with the passed; // function that has the maximum alignment of any dynamic lds variable; // reachable from this kernel. Dynamic LDS is allocated after the static LDS; // allocation, possibly after alignment padding. The representative variable; // created here has the maximum alignment of any other dynamic variable; // reachable by that kernel. All dynamic LDS variables are allocated at the; // same address in each kernel in order to provide the documented aliasing; // semantics. Setting the alignment here allows this IR pass to accurately; // predict the exact constant at which it will be allocated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:291,Modifiability,variab,variable,291,"// Create a dynamic lds variable with a name associated with the passed; // function that has the maximum alignment of any dynamic lds variable; // reachable from this kernel. Dynamic LDS is allocated after the static LDS; // allocation, possibly after alignment padding. The representative variable; // created here has the maximum alignment of any other dynamic variable; // reachable by that kernel. All dynamic LDS variables are allocated at the; // same address in each kernel in order to provide the documented aliasing; // semantics. Setting the alignment here allows this IR pass to accurately; // predict the exact constant at which it will be allocated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:364,Modifiability,variab,variable,364,"// Create a dynamic lds variable with a name associated with the passed; // function that has the maximum alignment of any dynamic lds variable; // reachable from this kernel. Dynamic LDS is allocated after the static LDS; // allocation, possibly after alignment padding. The representative variable; // created here has the maximum alignment of any other dynamic variable; // reachable by that kernel. All dynamic LDS variables are allocated at the; // same address in each kernel in order to provide the documented aliasing; // semantics. Setting the alignment here allows this IR pass to accurately; // predict the exact constant at which it will be allocated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:419,Modifiability,variab,variables,419,"// Create a dynamic lds variable with a name associated with the passed; // function that has the maximum alignment of any dynamic lds variable; // reachable from this kernel. Dynamic LDS is allocated after the static LDS; // allocation, possibly after alignment padding. The representative variable; // created here has the maximum alignment of any other dynamic variable; // reachable by that kernel. All dynamic LDS variables are allocated at the; // same address in each kernel in order to provide the documented aliasing; // semantics. Setting the alignment here allows this IR pass to accurately; // predict the exact constant at which it will be allocated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:606,Safety,predict,predict,606,"// Create a dynamic lds variable with a name associated with the passed; // function that has the maximum alignment of any dynamic lds variable; // reachable from this kernel. Dynamic LDS is allocated after the static LDS; // allocation, possibly after alignment padding. The representative variable; // created here has the maximum alignment of any other dynamic variable; // reachable by that kernel. All dynamic LDS variables are allocated at the; // same address in each kernel in order to provide the documented aliasing; // semantics. Setting the alignment here allows this IR pass to accurately; // predict the exact constant at which it will be allocated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:21,Availability,down,down,21,"// todo: narrow this down; // For each kernel, what variables does it access directly or through; // callees",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:52,Modifiability,variab,variables,52,"// todo: narrow this down; // For each kernel, what variables does it access directly or through; // callees",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:70,Security,access,access,70,"// todo: narrow this down; // For each kernel, what variables does it access directly or through; // callees",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:12,Modifiability,variab,variable,12,"// For each variable accessed through callees, which kernels access it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:21,Security,access,accessed,21,"// For each variable accessed through callees, which kernels access it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:61,Security,access,access,61,"// For each variable accessed through callees, which kernels access it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:13,Modifiability,variab,variables,13,// Partition variables accessed indirectly into the different strategies,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:23,Security,access,accessed,23,// Partition variables accessed indirectly into the different strategies,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:132,Energy Efficiency,allocate,allocate,132,// If the kernel accesses a variable that is going to be stored in the; // module instance through a call then that kernel needs to allocate the; // module instance,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:28,Modifiability,variab,variable,28,// If the kernel accesses a variable that is going to be stored in the; // module instance through a call then that kernel needs to allocate the; // module instance,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:17,Security,access,accesses,17,// If the kernel accesses a variable that is going to be stored in the; // module instance through a call then that kernel needs to allocate the; // module instance,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:19,Security,access,accesses,19,// Lower zero cost accesses to the kernel instances just created,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:23,Security,access,access,23,// Only one kernel can access it,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:57,Security,access,accesses,57,// The order must be consistent between lookup table and accesses to; // lookup table,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:196,Security,access,access,196,// Strip amdgpu-no-lds-kernel-id from all functions reachable from the; // kernel. We may have inferred this wasn't used prior to the pass.; //; // TODO: We could filter out subgraphs that do not access LDS globals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:31,Energy Efficiency,allocate,allocated,31,// All kernel frames have been allocated. Calculate and record the; // addresses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:58,Energy Efficiency,allocate,allocated,58,// All three of these are optional. The first variable is allocated at; // zero. They are allocated by AMDGPUMachineFunction as one block.; // Layout:; //{; // module.lds; // alignment padding; // kernel instance; // alignment padding; // dynamic lds variables; //},MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:90,Energy Efficiency,allocate,allocated,90,// All three of these are optional. The first variable is allocated at; // zero. They are allocated by AMDGPUMachineFunction as one block.; // Layout:; //{; // module.lds; // alignment padding; // kernel instance; // alignment padding; // dynamic lds variables; //},MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:46,Modifiability,variab,variable,46,// All three of these are optional. The first variable is allocated at; // zero. They are allocated by AMDGPUMachineFunction as one block.; // Layout:; //{; // module.lds; // alignment padding; // kernel instance; // alignment padding; // dynamic lds variables; //},MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:251,Modifiability,variab,variables,251,// All three of these are optional. The first variable is allocated at; // zero. They are allocated by AMDGPUMachineFunction as one block.; // Layout:; //{; // module.lds; // alignment padding; // kernel instance; // alignment padding; // dynamic lds variables; //},MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:3,Energy Efficiency,Allocate,Allocated,3,"// Allocated at zero, recorded once on construction, not once per; // kernel",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:143,Modifiability,variab,variable,143,"// If there is dynamic allocation, the alignment needed is included in; // the static frame size. There may be no reference to the dynamic; // variable in the kernel itself, so without including it here, that; // alignment padding could be missed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:174,Availability,avail,available,174,"// Instead of explictly marking kernels that access dynamic variables; // using special case metadata, annotate with min-lds == max-lds, i.e.; // that there is no more space available for allocating more static; // LDS variables. That is the right condition to prevent allocating; // more variables which would collide with the addresses assigned to; // dynamic variables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:60,Modifiability,variab,variables,60,"// Instead of explictly marking kernels that access dynamic variables; // using special case metadata, annotate with min-lds == max-lds, i.e.; // that there is no more space available for allocating more static; // LDS variables. That is the right condition to prevent allocating; // more variables which would collide with the addresses assigned to; // dynamic variables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:219,Modifiability,variab,variables,219,"// Instead of explictly marking kernels that access dynamic variables; // using special case metadata, annotate with min-lds == max-lds, i.e.; // that there is no more space available for allocating more static; // LDS variables. That is the right condition to prevent allocating; // more variables which would collide with the addresses assigned to; // dynamic variables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:289,Modifiability,variab,variables,289,"// Instead of explictly marking kernels that access dynamic variables; // using special case metadata, annotate with min-lds == max-lds, i.e.; // that there is no more space available for allocating more static; // LDS variables. That is the right condition to prevent allocating; // more variables which would collide with the addresses assigned to; // dynamic variables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:362,Modifiability,variab,variables,362,"// Instead of explictly marking kernels that access dynamic variables; // using special case metadata, annotate with min-lds == max-lds, i.e.; // that there is no more space available for allocating more static; // LDS variables. That is the right condition to prevent allocating; // more variables which would collide with the addresses assigned to; // dynamic variables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:45,Security,access,access,45,"// Instead of explictly marking kernels that access dynamic variables; // using special case metadata, annotate with min-lds == max-lds, i.e.; // that there is no more space available for allocating more static; // LDS variables. That is the right condition to prevent allocating; // more variables which would collide with the addresses assigned to; // dynamic variables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:125,Security,access,access,125,// Increase the alignment of LDS globals if necessary to maximise the chance; // that we can use aligned LDS instructions to access them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:34,Modifiability,variab,variables,34,// Only changing alignment of LDS variables,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:30,Modifiability,variab,variable,30,"// cuda/hip extern __shared__ variable, leave alignment alone",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:38,Performance,load,load,38,// We might want to use a b96 or b128 load/store,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:30,Performance,load,load,30,// We might want to use a b64 load/store,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:30,Performance,load,load,30,// We might want to use a b32 load/store,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:30,Performance,load,load,30,// We might want to use a b16 load/store,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:81,Modifiability,variab,variables,81,// Create a struct instance containing LDSVarsToTransform and map from those; // variables to ConstantExprGEP; // Variables may be introduced to meet alignment requirements. No aliasing; // metadata is useful for these as they have no uses. Erased before return.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:114,Modifiability,Variab,Variables,114,// Create a struct instance containing LDSVarsToTransform and map from those; // variables to ConstantExprGEP; // Variables may be introduced to meet alignment requirements. No aliasing; // metadata is useful for these as they have no uses. Erased before return.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:38,Integrability,depend,depends,38,"// The order of fields in this struct depends on the order of; // varables in the argument which varies when changing how they; // are identified, leading to spurious test breakage.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:167,Testability,test,test,167,"// The order of fields in this struct depends on the order of; // varables in the argument which varies when changing how they; // are identified, leading to spurious test breakage.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:178,Energy Efficiency,allocate,allocated,178,"// A hack... we need to insert the aliasing info in a predictable order for; // lit tests. Would like to have them in a stable order already, ideally the; // same order they get allocated, which might mean an ordered set container",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:54,Safety,predict,predictable,54,"// A hack... we need to insert the aliasing info in a predictable order for; // lit tests. Would like to have them in a stable order already, ideally the; // same order they get allocated, which might mean an ordered set container",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:84,Testability,test,tests,84,"// A hack... we need to insert the aliasing info in a predictable order for; // lit tests. Would like to have them in a stable order already, ideally the; // same order they get allocated, which might mean an ordered set container",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:112,Energy Efficiency,allocate,allocated,112,// Replace uses of ith variable with a constantexpr to the corresponding; // field of the instance that will be allocated by AMDGPUMachineFunction,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp:23,Modifiability,variab,variable,23,// Replace uses of ith variable with a constantexpr to the corresponding; // field of the instance that will be allocated by AMDGPUMachineFunction,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerModuleLDSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp:3,Testability,Assert,Assertion,3,"// Assertion ensures we don't use the same SourceMBB for the; // sources, because we cannot have different registers with; // identical predecessors, but we can have the same register for; // multiple predecessors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp:48,Deployability,update,update,48,"// If we are replacing outside, we also need to update the LiveOuts",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp:12,Modifiability,rewrite,rewrite,12,// We don't rewrite defs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp:114,Usability,simpl,simply,114,"// If a region is just a sequence of regions (and the exit; // block in the case of the top level region), we can simply skip; // linearizing it, because it is already linear",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp:17,Modifiability,rewrite,rewrite,17,"// We should not rewrite the code, we should only pick up the single value; // that represents the shrunk PHI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp:73,Modifiability,rewrite,rewrite,73,"// If the register simply lives through the CodeBB, we don't have; // to rewrite anything since the register is not defined in this; // part of the code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp:19,Usability,simpl,simply,19,"// If the register simply lives through the CodeBB, we don't have; // to rewrite anything since the register is not defined in this; // part of the code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:24,Energy Efficiency,allocate,allocates,24,// Assume the attribute allocates before any known GDS globals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:20,Modifiability,variab,variables,20,"// The two separate variables are only profitable when the LDS module lowering; // pass is disabled. If graphics does not use dynamic LDS, this is never; // profitable. Leaving cleanup for a later change.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:94,Availability,error,error,94,"// Absolute address LDS variables that exist prior to the LDS lowering; // pass raise a fatal error in that pass. These failure modes are only; // reachable if that lowering pass is disabled or broken. If/when adding; // support for absolute addresses on user specified variables, the; // alignment check moves to the lowering pass and the frame calculation; // needs to take the user variables into consideration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:120,Availability,failure,failure,120,"// Absolute address LDS variables that exist prior to the LDS lowering; // pass raise a fatal error in that pass. These failure modes are only; // reachable if that lowering pass is disabled or broken. If/when adding; // support for absolute addresses on user specified variables, the; // alignment check moves to the lowering pass and the frame calculation; // needs to take the user variables into consideration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:24,Modifiability,variab,variables,24,"// Absolute address LDS variables that exist prior to the LDS lowering; // pass raise a fatal error in that pass. These failure modes are only; // reachable if that lowering pass is disabled or broken. If/when adding; // support for absolute addresses on user specified variables, the; // alignment check moves to the lowering pass and the frame calculation; // needs to take the user variables into consideration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:270,Modifiability,variab,variables,270,"// Absolute address LDS variables that exist prior to the LDS lowering; // pass raise a fatal error in that pass. These failure modes are only; // reachable if that lowering pass is disabled or broken. If/when adding; // support for absolute addresses on user specified variables, the; // alignment check moves to the lowering pass and the frame calculation; // needs to take the user variables into consideration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:385,Modifiability,variab,variables,385,"// Absolute address LDS variables that exist prior to the LDS lowering; // pass raise a fatal error in that pass. These failure modes are only; // reachable if that lowering pass is disabled or broken. If/when adding; // support for absolute addresses on user specified variables, the; // alignment check moves to the lowering pass and the frame calculation; // needs to take the user variables into consideration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:313,Availability,error,error,313,"// If this is a module entry function, we can also sanity check against; // the static frame. Strictly it would be better to check against the; // attribute, i.e. that the variable is within the always-allocated; // section, and not within some other non-absolute-address object; // allocated here, but the extra error detection is minimal and we would; // have to pass the Function around or cache the attribute value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:202,Energy Efficiency,allocate,allocated,202,"// If this is a module entry function, we can also sanity check against; // the static frame. Strictly it would be better to check against the; // attribute, i.e. that the variable is within the always-allocated; // section, and not within some other non-absolute-address object; // allocated here, but the extra error detection is minimal and we would; // have to pass the Function around or cache the attribute value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:283,Energy Efficiency,allocate,allocated,283,"// If this is a module entry function, we can also sanity check against; // the static frame. Strictly it would be better to check against the; // attribute, i.e. that the variable is within the always-allocated; // section, and not within some other non-absolute-address object; // allocated here, but the extra error detection is minimal and we would; // have to pass the Function around or cache the attribute value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:172,Modifiability,variab,variable,172,"// If this is a module entry function, we can also sanity check against; // the static frame. Strictly it would be better to check against the; // attribute, i.e. that the variable is within the always-allocated; // section, and not within some other non-absolute-address object; // allocated here, but the extra error detection is minimal and we would; // have to pass the Function around or cache the attribute value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:393,Performance,cache,cache,393,"// If this is a module entry function, we can also sanity check against; // the static frame. Strictly it would be better to check against the; // attribute, i.e. that the variable is within the always-allocated; // section, and not within some other non-absolute-address object; // allocated here, but the extra error detection is minimal and we would; // have to pass the Function around or cache the attribute value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:51,Safety,sanity check,sanity check,51,"// If this is a module entry function, we can also sanity check against; // the static frame. Strictly it would be better to check against the; // attribute, i.e. that the variable is within the always-allocated; // section, and not within some other non-absolute-address object; // allocated here, but the extra error detection is minimal and we would; // have to pass the Function around or cache the attribute value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:319,Safety,detect,detection,319,"// If this is a module entry function, we can also sanity check against; // the static frame. Strictly it would be better to check against the; // attribute, i.e. that the variable is within the always-allocated; // section, and not within some other non-absolute-address object; // allocated here, but the extra error detection is minimal and we would; // have to pass the Function around or cache the attribute value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:111,Energy Efficiency,allocate,allocated,111,"// If there is a dynamic LDS variable associated with this function F, every; // further dynamic LDS instance (allocated by calling setDynLDSAlign) must; // map to the same address. This holds because no LDS is allocated after the; // lowering pass if there are dynamic LDS variables present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:211,Energy Efficiency,allocate,allocated,211,"// If there is a dynamic LDS variable associated with this function F, every; // further dynamic LDS instance (allocated by calling setDynLDSAlign) must; // map to the same address. This holds because no LDS is allocated after the; // lowering pass if there are dynamic LDS variables present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:29,Modifiability,variab,variable,29,"// If there is a dynamic LDS variable associated with this function F, every; // further dynamic LDS instance (allocated by calling setDynLDSAlign) must; // map to the same address. This holds because no LDS is allocated after the; // lowering pass if there are dynamic LDS variables present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:274,Modifiability,variab,variables,274,"// If there is a dynamic LDS variable associated with this function F, every; // further dynamic LDS instance (allocated by calling setDynLDSAlign) must; // map to the same address. This holds because no LDS is allocated after the; // lowering pass if there are dynamic LDS variables present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h:3,Performance,Cache,Cache,3,// Cache for this.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h:3,Performance,Cache,Cache,3,// Cache for this.; /// Number of bytes in the LDS that are being used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h:31,Energy Efficiency,allocate,allocated,31,/// Number of bytes in the LDS allocated statically. This field is only used; /// in the instruction selector and not part of the machine function info.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h:236,Deployability,update,updated,236,"/// Align for dynamic shared memory if any. Dynamic shared memory is; /// allocated directly after the static one, i.e., LDSSize. Need to pad; /// LDSSize to ensure that dynamic one is aligned accordingly.; /// The maximal alignment is updated during IR translation or lowering; /// stages.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h:74,Energy Efficiency,allocate,allocated,74,"/// Align for dynamic shared memory if any. Dynamic shared memory is; /// allocated directly after the static one, i.e., LDSSize. Need to pad; /// LDSSize to ensure that dynamic one is aligned accordingly.; /// The maximal alignment is updated during IR translation or lowering; /// stages.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h:51,Performance,perform,performance,51,// Kernel may need limited waves per EU for better performance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:24,Integrability,synchroniz,synchronization,24,// All supported memory/synchronization scopes can be found here:; // http://llvm.org/docs/AMDGPUUsage.html#memory-scopes; /// Agent synchronization scope ID (cross address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:133,Integrability,synchroniz,synchronization,133,// All supported memory/synchronization scopes can be found here:; // http://llvm.org/docs/AMDGPUUsage.html#memory-scopes; /// Agent synchronization scope ID (cross address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:14,Integrability,synchroniz,synchronization,14,/// Workgroup synchronization scope ID (cross address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:14,Integrability,synchroniz,synchronization,14,/// Wavefront synchronization scope ID (cross address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:11,Integrability,synchroniz,synchronization,11,/// System synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:10,Integrability,synchroniz,synchronization,10,/// Agent synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:14,Integrability,synchroniz,synchronization,14,/// Workgroup synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:14,Integrability,synchroniz,synchronization,14,/// Wavefront synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:18,Integrability,synchroniz,synchronization,18,/// Single thread synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:21,Integrability,synchroniz,synchronization,21,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns \p SSID's inclusion ordering, or ""std::nullopt"" if \p SSID is not; /// supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:81,Integrability,synchroniz,synchronization,81,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns \p SSID's inclusion ordering, or ""std::nullopt"" if \p SSID is not; /// supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:129,Integrability,synchroniz,synchronization,129,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns \p SSID's inclusion ordering, or ""std::nullopt"" if \p SSID is not; /// supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:19,Integrability,synchroniz,synchronization,19,/// \returns Agent synchronization scope ID (cross address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:23,Integrability,synchroniz,synchronization,23,/// \returns Workgroup synchronization scope ID (cross address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:23,Integrability,synchroniz,synchronization,23,/// \returns Wavefront synchronization scope ID (cross address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:20,Integrability,synchroniz,synchronization,20,/// \returns System synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:19,Integrability,synchroniz,synchronization,19,/// \returns Agent synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:23,Integrability,synchroniz,synchronization,23,/// \returns Workgroup synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:23,Integrability,synchroniz,synchronization,23,/// \returns Wavefront synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:27,Integrability,synchroniz,synchronization,27,/// \returns Single thread synchronization scope ID (single address space).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:21,Integrability,synchroniz,synchronization,21,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns True if synchronization scope \p A is larger than or equal to; /// synchronization scope \p B, false if synchronization scope \p A is smaller; /// than synchronization scope \p B, or ""std::nullopt"" if either; /// synchronization scope \p A or \p B is not supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:81,Integrability,synchroniz,synchronization,81,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns True if synchronization scope \p A is larger than or equal to; /// synchronization scope \p B, false if synchronization scope \p A is smaller; /// than synchronization scope \p B, or ""std::nullopt"" if either; /// synchronization scope \p A or \p B is not supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:129,Integrability,synchroniz,synchronization,129,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns True if synchronization scope \p A is larger than or equal to; /// synchronization scope \p B, false if synchronization scope \p A is smaller; /// than synchronization scope \p B, or ""std::nullopt"" if either; /// synchronization scope \p A or \p B is not supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:184,Integrability,synchroniz,synchronization,184,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns True if synchronization scope \p A is larger than or equal to; /// synchronization scope \p B, false if synchronization scope \p A is smaller; /// than synchronization scope \p B, or ""std::nullopt"" if either; /// synchronization scope \p A or \p B is not supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:243,Integrability,synchroniz,synchronization,243,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns True if synchronization scope \p A is larger than or equal to; /// synchronization scope \p B, false if synchronization scope \p A is smaller; /// than synchronization scope \p B, or ""std::nullopt"" if either; /// synchronization scope \p A or \p B is not supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:280,Integrability,synchroniz,synchronization,280,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns True if synchronization scope \p A is larger than or equal to; /// synchronization scope \p B, false if synchronization scope \p A is smaller; /// than synchronization scope \p B, or ""std::nullopt"" if either; /// synchronization scope \p A or \p B is not supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:328,Integrability,synchroniz,synchronization,328,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns True if synchronization scope \p A is larger than or equal to; /// synchronization scope \p B, false if synchronization scope \p A is smaller; /// than synchronization scope \p B, or ""std::nullopt"" if either; /// synchronization scope \p A or \p B is not supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h:389,Integrability,synchroniz,synchronization,389,"/// In AMDGPU target synchronization scopes are inclusive, meaning a; /// larger synchronization scope is inclusive of a smaller synchronization; /// scope.; ///; /// \returns True if synchronization scope \p A is larger than or equal to; /// synchronization scope \p B, false if synchronization scope \p A is smaller; /// than synchronization scope \p B, or ""std::nullopt"" if either; /// synchronization scope \p A or \p B is not supported by the AMDGPU target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineModuleInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMacroFusion.cpp:446,Energy Efficiency,schedul,scheduling,446,"//===--- AMDGPUMacroFusion.cpp - AMDGPU Macro Fusion ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This file contains the AMDGPU implementation of the DAG scheduling; /// mutation to pair instructions back to back.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMacroFusion.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMacroFusion.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMacroFusion.cpp:102,Availability,avail,available,102,// Try to cluster defs of condition registers to their uses. This improves; // the chance VCC will be available which will allow shrinking to VOP2; // encodings.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMacroFusion.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMacroFusion.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMarkLastScratchLoad.cpp:396,Performance,load,load,396,"//===-- AMDGPUMarkLastScratchLoad.cpp -------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Mark scratch load/spill instructions which are guaranteed to be the last time; // this scratch slot is used so it can be evicted from caches.; //; // TODO: Handle general stack accesses not just spilling.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMarkLastScratchLoad.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMarkLastScratchLoad.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMarkLastScratchLoad.cpp:517,Performance,cache,caches,517,"//===-- AMDGPUMarkLastScratchLoad.cpp -------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Mark scratch load/spill instructions which are guaranteed to be the last time; // this scratch slot is used so it can be evicted from caches.; //; // TODO: Handle general stack accesses not just spilling.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMarkLastScratchLoad.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMarkLastScratchLoad.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMarkLastScratchLoad.cpp:560,Security,access,accesses,560,"//===-- AMDGPUMarkLastScratchLoad.cpp -------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Mark scratch load/spill instructions which are guaranteed to be the last time; // this scratch slot is used so it can be evicted from caches.; //; // TODO: Handle general stack accesses not just spilling.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMarkLastScratchLoad.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMarkLastScratchLoad.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp:41,Safety,avoid,avoid,41,// TODO: How to use branch immediate and avoid register+add?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp:17,Modifiability,variab,variables,17,// Intercept LDS variables with known addresses,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp:55,Testability,test,test,55,"// FIXME: Enable feature predicate checks once all the test pass.; // AMDGPU_MC::verifyInstructionPredicates(MI->getOpcode(),; // getSubtargetInfo().getFeatureBits());",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp:223,Integrability,depend,depend,223,"// Check getInstSizeInBytes on explicitly specified CPUs (it cannot; // work correctly for the generic CPU).; //; // The isPseudo check really shouldn't be here, but unfortunately there are; // some negative lit tests that depend on being able to continue through; // here even when pseudo instructions haven't been lowered.; //; // We also overestimate branch sizes with the offset bug.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp:212,Testability,test,tests,212,"// Check getInstSizeInBytes on explicitly specified CPUs (it cannot; // work correctly for the generic CPU).; //; // The isPseudo check really shouldn't be here, but unfortunately there are; // some negative lit tests that depend on being able to continue through; // here even when pseudo instructions haven't been lowered.; //; // We also overestimate branch sizes with the offset bug.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.h:83,Safety,safe,safe,83,// TargetMachine does not support llvm-style cast. Use C++-style cast.; // This is safe since TM is always of type AMDGPUTargetMachine or its; // derived class.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMCInstLower.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp:710,Energy Efficiency,allocate,allocates,710,"//===- AMDGPUOpenCLEnqueuedBlockLowering.cpp - Lower enqueued block -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file; // This post-linking pass replaces the function pointer of enqueued; // block kernel with a global variable (runtime handle) and adds; // ""runtime-handle"" attribute to the enqueued block kernel.; //; // In LLVM CodeGen the runtime-handle metadata will be translated to; // RuntimeHandle metadata in code object. Runtime allocates a global buffer; // for each kernel with RuntimeHandle metadata and saves the kernel address; // required for the AQL packet into the buffer. __enqueue_kernel function; // in device library knows that the invoke function pointer in the block; // literal is actually runtime handle and loads the kernel address from it; // and put it into AQL packet for dispatching.; //; // This cannot be done in FE since FE cannot create a unique global variable; // with external linkage across LLVM modules. The global variable with internal; // linkage does not work since optimization passes will try to replace loads; // of the global variable with its initialization value.; //; // It also identifies the kernels directly or indirectly enqueues kernels; // and adds ""calls-enqueue-kernel"" function attribute to them, which will; // be used to determine whether to emit runtime metadata for the kernel; // enqueue related hidden kernel arguments.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp:489,Modifiability,variab,variable,489,"//===- AMDGPUOpenCLEnqueuedBlockLowering.cpp - Lower enqueued block -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file; // This post-linking pass replaces the function pointer of enqueued; // block kernel with a global variable (runtime handle) and adds; // ""runtime-handle"" attribute to the enqueued block kernel.; //; // In LLVM CodeGen the runtime-handle metadata will be translated to; // RuntimeHandle metadata in code object. Runtime allocates a global buffer; // for each kernel with RuntimeHandle metadata and saves the kernel address; // required for the AQL packet into the buffer. __enqueue_kernel function; // in device library knows that the invoke function pointer in the block; // literal is actually runtime handle and loads the kernel address from it; // and put it into AQL packet for dispatching.; //; // This cannot be done in FE since FE cannot create a unique global variable; // with external linkage across LLVM modules. The global variable with internal; // linkage does not work since optimization passes will try to replace loads; // of the global variable with its initialization value.; //; // It also identifies the kernels directly or indirectly enqueues kernels; // and adds ""calls-enqueue-kernel"" function attribute to them, which will; // be used to determine whether to emit runtime metadata for the kernel; // enqueue related hidden kernel arguments.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp:1159,Modifiability,variab,variable,1159,"//===- AMDGPUOpenCLEnqueuedBlockLowering.cpp - Lower enqueued block -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file; // This post-linking pass replaces the function pointer of enqueued; // block kernel with a global variable (runtime handle) and adds; // ""runtime-handle"" attribute to the enqueued block kernel.; //; // In LLVM CodeGen the runtime-handle metadata will be translated to; // RuntimeHandle metadata in code object. Runtime allocates a global buffer; // for each kernel with RuntimeHandle metadata and saves the kernel address; // required for the AQL packet into the buffer. __enqueue_kernel function; // in device library knows that the invoke function pointer in the block; // literal is actually runtime handle and loads the kernel address from it; // and put it into AQL packet for dispatching.; //; // This cannot be done in FE since FE cannot create a unique global variable; // with external linkage across LLVM modules. The global variable with internal; // linkage does not work since optimization passes will try to replace loads; // of the global variable with its initialization value.; //; // It also identifies the kernels directly or indirectly enqueues kernels; // and adds ""calls-enqueue-kernel"" function attribute to them, which will; // be used to determine whether to emit runtime metadata for the kernel; // enqueue related hidden kernel arguments.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp:1226,Modifiability,variab,variable,1226,"//===- AMDGPUOpenCLEnqueuedBlockLowering.cpp - Lower enqueued block -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file; // This post-linking pass replaces the function pointer of enqueued; // block kernel with a global variable (runtime handle) and adds; // ""runtime-handle"" attribute to the enqueued block kernel.; //; // In LLVM CodeGen the runtime-handle metadata will be translated to; // RuntimeHandle metadata in code object. Runtime allocates a global buffer; // for each kernel with RuntimeHandle metadata and saves the kernel address; // required for the AQL packet into the buffer. __enqueue_kernel function; // in device library knows that the invoke function pointer in the block; // literal is actually runtime handle and loads the kernel address from it; // and put it into AQL packet for dispatching.; //; // This cannot be done in FE since FE cannot create a unique global variable; // with external linkage across LLVM modules. The global variable with internal; // linkage does not work since optimization passes will try to replace loads; // of the global variable with its initialization value.; //; // It also identifies the kernels directly or indirectly enqueues kernels; // and adds ""calls-enqueue-kernel"" function attribute to them, which will; // be used to determine whether to emit runtime metadata for the kernel; // enqueue related hidden kernel arguments.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp:1345,Modifiability,variab,variable,1345,"//===- AMDGPUOpenCLEnqueuedBlockLowering.cpp - Lower enqueued block -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file; // This post-linking pass replaces the function pointer of enqueued; // block kernel with a global variable (runtime handle) and adds; // ""runtime-handle"" attribute to the enqueued block kernel.; //; // In LLVM CodeGen the runtime-handle metadata will be translated to; // RuntimeHandle metadata in code object. Runtime allocates a global buffer; // for each kernel with RuntimeHandle metadata and saves the kernel address; // required for the AQL packet into the buffer. __enqueue_kernel function; // in device library knows that the invoke function pointer in the block; // literal is actually runtime handle and loads the kernel address from it; // and put it into AQL packet for dispatching.; //; // This cannot be done in FE since FE cannot create a unique global variable; // with external linkage across LLVM modules. The global variable with internal; // linkage does not work since optimization passes will try to replace loads; // of the global variable with its initialization value.; //; // It also identifies the kernels directly or indirectly enqueues kernels; // and adds ""calls-enqueue-kernel"" function attribute to them, which will; // be used to determine whether to emit runtime metadata for the kernel; // enqueue related hidden kernel arguments.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp:1005,Performance,load,loads,1005,"//===- AMDGPUOpenCLEnqueuedBlockLowering.cpp - Lower enqueued block -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file; // This post-linking pass replaces the function pointer of enqueued; // block kernel with a global variable (runtime handle) and adds; // ""runtime-handle"" attribute to the enqueued block kernel.; //; // In LLVM CodeGen the runtime-handle metadata will be translated to; // RuntimeHandle metadata in code object. Runtime allocates a global buffer; // for each kernel with RuntimeHandle metadata and saves the kernel address; // required for the AQL packet into the buffer. __enqueue_kernel function; // in device library knows that the invoke function pointer in the block; // literal is actually runtime handle and loads the kernel address from it; // and put it into AQL packet for dispatching.; //; // This cannot be done in FE since FE cannot create a unique global variable; // with external linkage across LLVM modules. The global variable with internal; // linkage does not work since optimization passes will try to replace loads; // of the global variable with its initialization value.; //; // It also identifies the kernels directly or indirectly enqueues kernels; // and adds ""calls-enqueue-kernel"" function attribute to them, which will; // be used to determine whether to emit runtime metadata for the kernel; // enqueue related hidden kernel arguments.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp:1281,Performance,optimiz,optimization,1281,"//===- AMDGPUOpenCLEnqueuedBlockLowering.cpp - Lower enqueued block -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file; // This post-linking pass replaces the function pointer of enqueued; // block kernel with a global variable (runtime handle) and adds; // ""runtime-handle"" attribute to the enqueued block kernel.; //; // In LLVM CodeGen the runtime-handle metadata will be translated to; // RuntimeHandle metadata in code object. Runtime allocates a global buffer; // for each kernel with RuntimeHandle metadata and saves the kernel address; // required for the AQL packet into the buffer. __enqueue_kernel function; // in device library knows that the invoke function pointer in the block; // literal is actually runtime handle and loads the kernel address from it; // and put it into AQL packet for dispatching.; //; // This cannot be done in FE since FE cannot create a unique global variable; // with external linkage across LLVM modules. The global variable with internal; // linkage does not work since optimization passes will try to replace loads; // of the global variable with its initialization value.; //; // It also identifies the kernels directly or indirectly enqueues kernels; // and adds ""calls-enqueue-kernel"" function attribute to them, which will; // be used to determine whether to emit runtime metadata for the kernel; // enqueue related hidden kernel arguments.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp:1321,Performance,load,loads,1321,"//===- AMDGPUOpenCLEnqueuedBlockLowering.cpp - Lower enqueued block -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file; // This post-linking pass replaces the function pointer of enqueued; // block kernel with a global variable (runtime handle) and adds; // ""runtime-handle"" attribute to the enqueued block kernel.; //; // In LLVM CodeGen the runtime-handle metadata will be translated to; // RuntimeHandle metadata in code object. Runtime allocates a global buffer; // for each kernel with RuntimeHandle metadata and saves the kernel address; // required for the AQL packet into the buffer. __enqueue_kernel function; // in device library knows that the invoke function pointer in the block; // literal is actually runtime handle and loads the kernel address from it; // and put it into AQL packet for dispatching.; //; // This cannot be done in FE since FE cannot create a unique global variable; // with external linkage across LLVM modules. The global variable with internal; // linkage does not work since optimization passes will try to replace loads; // of the global variable with its initialization value.; //; // It also identifies the kernels directly or indirectly enqueues kernels; // and adds ""calls-enqueue-kernel"" function attribute to them, which will; // be used to determine whether to emit runtime metadata for the kernel; // enqueue related hidden kernel arguments.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUOpenCLEnqueuedBlockLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:523,Energy Efficiency,reduce,reduce,523,"//===- AMDGPUPerfHintAnalysis.cpp - analysis of functions memory traffic --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// \brief Analyzes if a function potentially memory bound and if a kernel; /// kernel may benefit from limiting number of waves to reduce cache thrashing.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:530,Performance,cache,cache,530,"//===- AMDGPUPerfHintAnalysis.cpp - analysis of functions memory traffic --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// \brief Analyzes if a function potentially memory bound and if a kernel; /// kernel may benefit from limiting number of waves to reduce cache thrashing.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:15,Security,access,access,15,// Last memory access info,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:85,Security,access,access,85,"/// Check if the instruction is large stride.; /// The purpose is to identify memory access pattern like:; /// x = a[i];; /// y = a[i+1000];; /// z = a[i+2000];; /// In the above example, the second and third memory access will be marked; /// large stride memory access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:216,Security,access,access,216,"/// Check if the instruction is large stride.; /// The purpose is to identify memory access pattern like:; /// x = a[i];; /// y = a[i+1000];; /// z = a[i+2000];; /// In the above example, the second and third memory access will be marked; /// large stride memory access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:263,Security,access,access,263,"/// Check if the instruction is large stride.; /// The purpose is to identify memory access pattern like:; /// x = a[i];; /// y = a[i+1000];; /// z = a[i+2000];; /// In the above example, the second and third memory access will be marked; /// large stride memory access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:30,Performance,load,load,30,// Returns true if the global load `I` is used in its own basic block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:29,Performance,load,load,29,// TODO: Check if the global load and its user are close to each other; // instead (Or do this analysis in GCNSchedStrategy?).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:37,Performance,load,load,37,// Offset will likely be folded into load or store,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:21,Energy Efficiency,schedul,scheduling,21,// Reverting optimal scheduling in favour of occupancy with basic block(s); // having dense global memory access can potentially hurt performance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:134,Performance,perform,performance,134,// Reverting optimal scheduling in favour of occupancy with basic block(s); // having dense global memory access can potentially hurt performance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:106,Security,access,access,106,// Reverting optimal scheduling in favour of occupancy with basic block(s); // having dense global memory access can potentially hurt performance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp:34,Security,access,access,34,// Do not treat local-addr memory access as large stride.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h:523,Energy Efficiency,reduce,reduce,523,"//===- AMDGPUPerfHintAnalysis.h ---- analysis of memory traffic -*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// \brief Analyzes if a function potentially memory bound and if a kernel; /// kernel may benefit from limiting number of waves to reduce cache thrashing.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h:530,Performance,cache,cache,530,"//===- AMDGPUPerfHintAnalysis.h ---- analysis of memory traffic -*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// \brief Analyzes if a function potentially memory bound and if a kernel; /// kernel may benefit from limiting number of waves to reduce cache thrashing.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h:12,Security,access,access,12,// Indirect access memory instruction count,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h:71,Security,access,access,71,// Set if at least 1 basic block has relatively; // high global memory access,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPerfHintAnalysis.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:27,Performance,load,load,27,// Combine unsigned buffer load and signed extension instructions to generate; // signed buffer laod instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:77,Modifiability,extend,extended,77,// Find the s_mul_u64 instructions where the higher bits are either; // zero-extended or sign-extended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:94,Modifiability,extend,extended,94,// Find the s_mul_u64 instructions where the higher bits are either; // zero-extended or sign-extended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:99,Modifiability,extend,extended,99,// Replace the s_mul_u64 instructions with S_MUL_I64_I32_PSEUDO if the higher; // 33 bits are sign extended and with S_MUL_U64_U32_PSEUDO if the higher 32; // bits are zero extended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:173,Modifiability,extend,extended,173,// Replace the s_mul_u64 instructions with S_MUL_I64_I32_PSEUDO if the higher; // 33 bits are sign extended and with S_MUL_U64_U32_PSEUDO if the higher 32; // bits are zero extended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:9,Usability,Simpl,Simplify,9,// TODO: Simplify demanded bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:72,Performance,load,load,72,// Check if the first operand of the sign extension is a subword buffer load; // instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:3,Deployability,Update,Update,3,// Update the destination register of the load with the destination register; // of the sign extension.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:42,Performance,load,load,42,// Update the destination register of the load with the destination register; // of the sign extension.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPreLegalizerCombiner.cpp:198,Energy Efficiency,efficient,efficiently,198,"// We want to find a combination of instructions that; // gets generated when an i64 gets clamped to i16.; // The corresponding pattern is:; // G_MAX / G_MAX for i16 <= G_TRUNC i64.; // This can be efficiently written as following:; // v_cvt_pk_i16_i32 v0, v0, v1; // v_med3_i32 v0, Clamp_Min, v0, Clamp_Max",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPrintfRuntimeBinding.cpp:30,Modifiability,variab,variables,30,"// Instead of creating global variables, the printf format strings are; // extracted and passed as metadata. This avoids polluting llvm's symbol; // tables in this module. Metadata is going to be extracted by the backend; // passes and inserted into the OpenCL binary as appropriate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPrintfRuntimeBinding.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPrintfRuntimeBinding.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPrintfRuntimeBinding.cpp:114,Safety,avoid,avoids,114,"// Instead of creating global variables, the printf format strings are; // extracted and passed as metadata. This avoids polluting llvm's symbol; // tables in this module. Metadata is going to be extracted by the backend; // passes and inserted into the OpenCL binary as appropriate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPrintfRuntimeBinding.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPrintfRuntimeBinding.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:593,Deployability,pipeline,pipeline,593,"//===-- AMDGPUPromoteAlloca.cpp - Promote Allocas -------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Eliminates allocas by either converting them into vectors or by migrating; // them to local address space.; //; // Two passes are exposed by this file:; // - ""promote-alloca-to-vector"", which runs early in the pipeline and only; // promotes to vector. Promotion to vector is almost always profitable; // except when the alloca is too big and the promotion would result in; // very high register pressure.; // - ""promote-alloca"", which does both promotion to vector and LDS and runs; // much later in the pipeline. This runs after SROA because promoting to; // LDS is of course less profitable than getting rid of the alloca or; // vectorizing it, thus we only want to do it when the only alternative is; // lowering the alloca to stack.; //; // Note that both of them exist for the old and new PMs. The new PM passes are; // declared in AMDGPU.h and the legacy PM ones are declared here.s; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:887,Deployability,pipeline,pipeline,887,"//===-- AMDGPUPromoteAlloca.cpp - Promote Allocas -------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Eliminates allocas by either converting them into vectors or by migrating; // them to local address space.; //; // Two passes are exposed by this file:; // - ""promote-alloca-to-vector"", which runs early in the pipeline and only; // promotes to vector. Promotion to vector is almost always profitable; // except when the alloca is too big and the promotion would result in; // very high register pressure.; // - ""promote-alloca"", which does both promotion to vector and LDS and runs; // much later in the pipeline. This runs after SROA because promoting to; // LDS is of course less profitable than getting rid of the alloca or; // vectorizing it, thus we only want to do it when the only alternative is; // lowering the alloca to stack.; //; // Note that both of them exist for the old and new PMs. The new PM passes are; // declared in AMDGPU.h and the legacy PM ones are declared here.s; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:513,Security,expose,exposed,513,"//===-- AMDGPUPromoteAlloca.cpp - Promote Allocas -------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Eliminates allocas by either converting them into vectors or by migrating; // them to local address space.; //; // Two passes are exposed by this file:; // - ""promote-alloca-to-vector"", which runs early in the pipeline and only; // promotes to vector. Promotion to vector is almost always profitable; // except when the alloca is too big and the promotion would result in; // very high register pressure.; // - ""promote-alloca"", which does both promotion to vector and LDS and runs; // much later in the pipeline. This runs after SROA because promoting to; // LDS is of course less profitable than getting rid of the alloca or; // vectorizing it, thus we only want to do it when the only alternative is; // lowering the alloca to stack.; //; // Note that both of them exist for the old and new PMs. The new PM passes are; // declared in AMDGPU.h and the legacy PM ones are declared here.s; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:100,Availability,avail,available,100,// Move LDS uses from functions to kernels before promote alloca for accurate; // estimation of LDS available,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:173,Deployability,update,updated,173,"// NOTE: tryPromoteAllocaToVector removes the alloca, so Allocas contains; // dangling pointers. If we want to reuse it past this point, the loop above; // would need to be updated to remove successfully promoted allocas.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:397,Performance,cache,cache,397,"/// Promotes a single user of the alloca to a vector form.; ///; /// \param Inst Instruction to be promoted.; /// \param DL Module Data Layout.; /// \param VectorTy Vectorized Type.; /// \param VecStoreSize Size of \p VectorTy in bytes.; /// \param ElementSize Size of \p VectorTy element type in bytes.; /// \param TransferInfo MemTransferInst info map.; /// \param GEPVectorIdx GEP -> VectorIdx cache.; /// \param CurVal Current value of the vector (e.g. last stored value); /// \param[out] DeferredLoads \p Inst is added to this vector if it can't; /// be promoted now. This happens when promoting requires \p; /// CurVal, but \p CurVal is nullptr.; /// \return the stored value if \p Inst would have written to the alloca, or; /// nullptr otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:53,Performance,load,load,53,"// If the current value is not known, insert a dummy load and lower it on; // the second pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:3,Performance,Load,Loads,3,// Loads can only be lowered if the value is known.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:9,Performance,load,loading,9,// We're loading the full vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:3,Performance,Load,Loading,3,// Loading a subvector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:9,Performance,load,loading,9,// We're loading one element.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:42,Integrability,depend,depends,42,"// For stores, it's a bit trickier and it depends on whether we're storing; // the full vector or not. If we're storing the full vector, we don't need; // to know the current value. If this is a store of a single element, we; // need to know the value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:3,Security,Access,Access,3,"// Access as a vector type can work if the size of the access vector is a; // multiple of the size of the alloca's vector element type.; //; // Examples:; // - VecTy = <8 x float>, AccessTy = <4 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <2 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <3 x float> -> NOT OK; // - 3*32 is not a multiple of 64; //; // We could handle more complicated cases, but it'd make things a lot more; // complicated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:55,Security,access,access,55,"// Access as a vector type can work if the size of the access vector is a; // multiple of the size of the alloca's vector element type.; //; // Examples:; // - VecTy = <8 x float>, AccessTy = <4 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <2 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <3 x float> -> NOT OK; // - 3*32 is not a multiple of 64; //; // We could handle more complicated cases, but it'd make things a lot more; // complicated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:181,Security,Access,AccessTy,181,"// Access as a vector type can work if the size of the access vector is a; // multiple of the size of the alloca's vector element type.; //; // Examples:; // - VecTy = <8 x float>, AccessTy = <4 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <2 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <3 x float> -> NOT OK; // - 3*32 is not a multiple of 64; //; // We could handle more complicated cases, but it'd make things a lot more; // complicated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:238,Security,Access,AccessTy,238,"// Access as a vector type can work if the size of the access vector is a; // multiple of the size of the alloca's vector element type.; //; // Examples:; // - VecTy = <8 x float>, AccessTy = <4 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <2 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <3 x float> -> NOT OK; // - 3*32 is not a multiple of 64; //; // We could handle more complicated cases, but it'd make things a lot more; // complicated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:295,Security,Access,AccessTy,295,"// Access as a vector type can work if the size of the access vector is a; // multiple of the size of the alloca's vector element type.; //; // Examples:; // - VecTy = <8 x float>, AccessTy = <4 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <2 x float> -> OK; // - VecTy = <4 x double>, AccessTy = <3 x float> -> NOT OK; // - 3*32 is not a multiple of 64; //; // We could handle more complicated cases, but it'd make things a lot more; // complicated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:3,Usability,Clear,Clear,3,// Clear the block so we know it's been processed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:20,Availability,avail,available,20,// Use up to 1/4 of available register budget for vectorization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:31,Security,access,access,31,// Check that this is a simple access of a vector element.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:24,Usability,simpl,simple,24,// Check that this is a simple access of a vector element.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:18,Security,access,accessed,18,// Alloca already accessed as vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:24,Performance,load,loads,24,// Then handle deferred loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:57,Performance,load,loads,57,"// Delete all instructions. On the first pass, new dummy loads may have been; // added so we need to collect them too.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:772,Integrability,wrap,wrapper,772,"// We are indexing into this struct, and want to extract the workgroup_size_*; // fields.; //; // typedef struct hsa_kernel_dispatch_packet_s {; // uint16_t header;; // uint16_t setup;; // uint16_t workgroup_size_x ;; // uint16_t workgroup_size_y;; // uint16_t workgroup_size_z;; // uint16_t reserved0;; // uint32_t grid_size_x ;; // uint32_t grid_size_y ;; // uint32_t grid_size_z;; //; // uint32_t private_segment_size;; // uint32_t group_segment_size;; // uint64_t kernel_object;; //; // #ifdef HSA_LARGE_MODEL; // void *kernarg_address;; // #elif defined HSA_LITTLE_ENDIAN; // void *kernarg_address;; // uint32_t reserved1;; // #else; // uint32_t reserved1;; // void *kernarg_address;; // #endif; // uint64_t reserved2;; // hsa_signal_t completion_signal; // uint64_t wrapper; // } hsa_kernel_dispatch_packet_t; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:31,Performance,load,load,31,"// We could do a single 64-bit load here, but it's likely that the basic; // 32-bit and extract sequence is already present, and it is probably easier; // to CSE this. The loads should be mergeable later anyway.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:172,Performance,load,loads,172,"// We could do a single 64-bit load here, but it's likely that the basic; // 32-bit and extract sequence is already present, and it is probably easier; // to CSE this. The loads should be mergeable later anyway.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:38,Performance,Load,LoadZU,38,// Extract y component. Upper half of LoadZU should be zero already.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:15,Modifiability,rewrite,rewrite,15,// May need to rewrite constant operands.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:79,Energy Efficiency,allocate,allocated,79,"// HIP uses an extern unsized array in local address space for dynamically; // allocated shared memory. In that case, we have to disable the promotion.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:60,Energy Efficiency,reduce,reduce,60,"// Restrict local memory usage so that we don't drastically reduce occupancy,; // unless it is already significantly reduced.; // TODO: Have some sort of hint or other heuristics to guess occupancy based; // on other factors..",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:117,Energy Efficiency,reduce,reduced,117,"// Restrict local memory usage so that we don't drastically reduce occupancy,; // unless it is already significantly reduced.; // TODO: Have some sort of hint or other heuristics to guess occupancy based; // on other factors..",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:59,Availability,avail,available,59,// Program is possibly broken by using more local mem than available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:181,Availability,avail,available,181,// Don't promote the alloca to LDS for shader calling conventions as the work; // item ID intrinsics are not supported for these calling conventions.; // Furthermore not all LDS is available for some of the stages.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:57,Integrability,depend,depends,57,// FIXME: This computed padding is likely wrong since it depends on inverse; // usage order.; //; // FIXME: It is also possible that if we're allowed to use all of the memory; // could end up using more than the maximum due to alignment padding.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp:553,Performance,load,loads,553,"//===-- AMDGPUPromoteKernelArguments.cpp ----------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass recursively promotes generic pointer arguments of a kernel; /// into the global address space.; ///; /// The pass walks kernel's pointer arguments, then loads from them. If a loaded; /// value is a pointer and loaded pointer is unmodified in the kernel before the; /// load, then promote loaded pointer to global. Then recursively continue.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp:575,Performance,load,loaded,575,"//===-- AMDGPUPromoteKernelArguments.cpp ----------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass recursively promotes generic pointer arguments of a kernel; /// into the global address space.; ///; /// The pass walks kernel's pointer arguments, then loads from them. If a loaded; /// value is a pointer and loaded pointer is unmodified in the kernel before the; /// load, then promote loaded pointer to global. Then recursively continue.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp:610,Performance,load,loaded,610,"//===-- AMDGPUPromoteKernelArguments.cpp ----------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass recursively promotes generic pointer arguments of a kernel; /// into the global address space.; ///; /// The pass walks kernel's pointer arguments, then loads from them. If a loaded; /// value is a pointer and loaded pointer is unmodified in the kernel before the; /// load, then promote loaded pointer to global. Then recursively continue.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp:669,Performance,load,load,669,"//===-- AMDGPUPromoteKernelArguments.cpp ----------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass recursively promotes generic pointer arguments of a kernel; /// into the global address space.; ///; /// The pass walks kernel's pointer arguments, then loads from them. If a loaded; /// value is a pointer and loaded pointer is unmodified in the kernel before the; /// load, then promote loaded pointer to global. Then recursively continue.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp:688,Performance,load,loaded,688,"//===-- AMDGPUPromoteKernelArguments.cpp ----------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass recursively promotes generic pointer arguments of a kernel; /// into the global address space.; ///; /// The pass walks kernel's pointer arguments, then loads from them. If a loaded; /// value is a pointer and loaded pointer is unmodified in the kernel before the; /// load, then promote loaded pointer to global. Then recursively continue.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp:46,Integrability,depend,depend,46,"// If this is a dynamic alloca, the value may depend on the loaded kernargs,; // so loads will need to be inserted before it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp:60,Performance,load,loaded,60,"// If this is a dynamic alloca, the value may depend on the loaded kernargs,; // so loads will need to be inserted before it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp:84,Performance,load,loads,84,"// If this is a dynamic alloca, the value may depend on the loaded kernargs,; // so loads will need to be inserted before it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteKernelArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:24,Availability,avail,available,24,"// med3 for i16 is only available on gfx9+, and not available for v2i16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:52,Availability,avail,available,52,"// med3 for i16 is only available on gfx9+, and not available for v2i16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:24,Availability,avail,available,24,"// med3 for f16 is only available on gfx9+, and not available for v2f16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:52,Availability,avail,available,52,"// med3 for f16 is only available on gfx9+, and not available for v2f16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:18,Performance,perform,perform,18,"// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. fmed3(NaN, K0, K1) is equivalent to; // min(min(NaN, K0), K1). Safe to fold for min(max(Val, K0), K1) since inner; // nodes(max/min) have same behavior when one input is NaN and other isn't.; // Don't consider max(min(SNaN, K1), K0) since there is no isKnownNeverQNaN,; // also post-legalizer inputs to min/max are fcanonicalized (never SNaN).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:49,Safety,safe,safe,49,"// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. fmed3(NaN, K0, K1) is equivalent to; // min(min(NaN, K0), K1). Safe to fold for min(max(Val, K0), K1) since inner; // nodes(max/min) have same behavior when one input is NaN and other isn't.; // Don't consider max(min(SNaN, K1), K0) since there is no isKnownNeverQNaN,; // also post-legalizer inputs to min/max are fcanonicalized (never SNaN).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:250,Safety,Safe,Safe,250,"// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. fmed3(NaN, K0, K1) is equivalent to; // min(min(NaN, K0), K1). Safe to fold for min(max(Val, K0), K1) since inner; // nodes(max/min) have same behavior when one input is NaN and other isn't.; // Don't consider max(min(SNaN, K1), K0) since there is no isKnownNeverQNaN,; // also post-legalizer inputs to min/max are fcanonicalized (never SNaN).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:12,Availability,avail,available,12,"// Clamp is available on all types after regbankselect (f16, f32, f64, v2f16).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:18,Performance,perform,perform,18,"// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. Only min(max(QNaN, 0.0), 1.0) evaluates; // to 0.0 requires dx10_clamp = true.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:49,Safety,safe,safe,49,"// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. Only min(max(QNaN, 0.0), 1.0) evaluates; // to 0.0 requires dx10_clamp = true.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:18,Performance,perform,perform,18,// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. Requires dx10_clamp = true. Safe to fold; // when Val could be QNaN. If Val can also be SNaN third input should be 0.0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:49,Safety,safe,safe,49,// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. Requires dx10_clamp = true. Safe to fold; // when Val could be QNaN. If Val can also be SNaN third input should be 0.0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:215,Safety,Safe,Safe,215,// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. Requires dx10_clamp = true. Safe to fold; // when Val could be QNaN. If Val can also be SNaN third input should be 0.0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankSelect.cpp:15,Deployability,pipeline,pipeline,15,"// If the ISel pipeline failed, do not bother running that pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankSelect.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankSelect.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3121,Integrability,depend,dependent,3121,"gether, this means we need to adjust the; /// type of boolean operations to be regbank legal. All SALU booleans need to be; /// widened to 32-bits, and all VALU booleans need to be s1 values.; ///; /// A noteworthy exception to the s1-means-vcc rule is for legalization artifact; /// casts. G_TRUNC s1 results, and G_SEXT/G_ZEXT/G_ANYEXT sources are never vcc; /// bank. A non-boolean source (such as a truncate from a 1-bit load from; /// memory) will require a copy to the VCC bank which will require clearing the; /// high bits and inserting a compare.; ///; /// \par Constant bus restriction; ///; /// VALU instructions have a limitation known as the constant bus; /// restriction. Most VALU instructions can use SGPR operands, but may read at; /// most 1 SGPR or constant literal value (this to 2 in gfx10 for most; /// instructions). This is one unique SGPR, so the same SGPR may be used for; /// multiple operands. From a register bank perspective, any combination of; /// operands should be legal as an SGPR, but this is contextually dependent on; /// the SGPR operands all being the same register. There is therefore optimal to; /// choose the SGPR with the most uses to minimize the number of copies.; ///; /// We avoid trying to solve this problem in RegBankSelect. Any VALU G_*; /// operation should have its source operands all mapped to VGPRs (except for; /// VCC), inserting copies from any SGPR operands. This the most trivial legal; /// mapping. Anything beyond the simplest 1:1 instruction selection would be too; /// complicated to solve here. Every optimization pattern or instruction; /// selected to multiple outputs would have to enforce this rule, and there; /// would be additional complexity in tracking this rule for every G_*; /// operation. By forcing all inputs to VGPRs, it also simplifies the task of; /// picking the optimal operand combination from a post-isel optimization pass.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:2504,Performance,load,load,2504," in a 32 or 64-bit; /// register. These are represented with the VCC bank. During selection, we need; /// to be able to unambiguously go back from a register class to a register; /// bank. To distinguish whether an SGPR should use the SGPR or VCC register; /// bank, we need to know the use context type. An SGPR s1 value always means a; /// VCC bank value, otherwise it will be the SGPR bank. A scalar compare sets; /// SCC, which is a 1-bit unaddressable register. This will need to be copied to; /// a 32-bit virtual register. Taken together, this means we need to adjust the; /// type of boolean operations to be regbank legal. All SALU booleans need to be; /// widened to 32-bits, and all VALU booleans need to be s1 values.; ///; /// A noteworthy exception to the s1-means-vcc rule is for legalization artifact; /// casts. G_TRUNC s1 results, and G_SEXT/G_ZEXT/G_ANYEXT sources are never vcc; /// bank. A non-boolean source (such as a truncate from a 1-bit load from; /// memory) will require a copy to the VCC bank which will require clearing the; /// high bits and inserting a compare.; ///; /// \par Constant bus restriction; ///; /// VALU instructions have a limitation known as the constant bus; /// restriction. Most VALU instructions can use SGPR operands, but may read at; /// most 1 SGPR or constant literal value (this to 2 in gfx10 for most; /// instructions). This is one unique SGPR, so the same SGPR may be used for; /// multiple operands. From a register bank perspective, any combination of; /// operands should be legal as an SGPR, but this is contextually dependent on; /// the SGPR operands all being the same register. There is therefore optimal to; /// choose the SGPR with the most uses to minimize the number of copies.; ///; /// We avoid trying to solve this problem in RegBankSelect. Any VALU G_*; /// operation should have its source operands all mapped to VGPRs (except for; /// VCC), inserting copies from any SGPR operands. This the most trivial legal; /// mapping. ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3648,Performance,optimiz,optimization,3648,"gether, this means we need to adjust the; /// type of boolean operations to be regbank legal. All SALU booleans need to be; /// widened to 32-bits, and all VALU booleans need to be s1 values.; ///; /// A noteworthy exception to the s1-means-vcc rule is for legalization artifact; /// casts. G_TRUNC s1 results, and G_SEXT/G_ZEXT/G_ANYEXT sources are never vcc; /// bank. A non-boolean source (such as a truncate from a 1-bit load from; /// memory) will require a copy to the VCC bank which will require clearing the; /// high bits and inserting a compare.; ///; /// \par Constant bus restriction; ///; /// VALU instructions have a limitation known as the constant bus; /// restriction. Most VALU instructions can use SGPR operands, but may read at; /// most 1 SGPR or constant literal value (this to 2 in gfx10 for most; /// instructions). This is one unique SGPR, so the same SGPR may be used for; /// multiple operands. From a register bank perspective, any combination of; /// operands should be legal as an SGPR, but this is contextually dependent on; /// the SGPR operands all being the same register. There is therefore optimal to; /// choose the SGPR with the most uses to minimize the number of copies.; ///; /// We avoid trying to solve this problem in RegBankSelect. Any VALU G_*; /// operation should have its source operands all mapped to VGPRs (except for; /// VCC), inserting copies from any SGPR operands. This the most trivial legal; /// mapping. Anything beyond the simplest 1:1 instruction selection would be too; /// complicated to solve here. Every optimization pattern or instruction; /// selected to multiple outputs would have to enforce this rule, and there; /// would be additional complexity in tracking this rule for every G_*; /// operation. By forcing all inputs to VGPRs, it also simplifies the task of; /// picking the optimal operand combination from a post-isel optimization pass.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3974,Performance,optimiz,optimization,3974,"gether, this means we need to adjust the; /// type of boolean operations to be regbank legal. All SALU booleans need to be; /// widened to 32-bits, and all VALU booleans need to be s1 values.; ///; /// A noteworthy exception to the s1-means-vcc rule is for legalization artifact; /// casts. G_TRUNC s1 results, and G_SEXT/G_ZEXT/G_ANYEXT sources are never vcc; /// bank. A non-boolean source (such as a truncate from a 1-bit load from; /// memory) will require a copy to the VCC bank which will require clearing the; /// high bits and inserting a compare.; ///; /// \par Constant bus restriction; ///; /// VALU instructions have a limitation known as the constant bus; /// restriction. Most VALU instructions can use SGPR operands, but may read at; /// most 1 SGPR or constant literal value (this to 2 in gfx10 for most; /// instructions). This is one unique SGPR, so the same SGPR may be used for; /// multiple operands. From a register bank perspective, any combination of; /// operands should be legal as an SGPR, but this is contextually dependent on; /// the SGPR operands all being the same register. There is therefore optimal to; /// choose the SGPR with the most uses to minimize the number of copies.; ///; /// We avoid trying to solve this problem in RegBankSelect. Any VALU G_*; /// operation should have its source operands all mapped to VGPRs (except for; /// VCC), inserting copies from any SGPR operands. This the most trivial legal; /// mapping. Anything beyond the simplest 1:1 instruction selection would be too; /// complicated to solve here. Every optimization pattern or instruction; /// selected to multiple outputs would have to enforce this rule, and there; /// would be additional complexity in tracking this rule for every G_*; /// operation. By forcing all inputs to VGPRs, it also simplifies the task of; /// picking the optimal operand combination from a post-isel optimization pass.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3303,Safety,avoid,avoid,3303,"gether, this means we need to adjust the; /// type of boolean operations to be regbank legal. All SALU booleans need to be; /// widened to 32-bits, and all VALU booleans need to be s1 values.; ///; /// A noteworthy exception to the s1-means-vcc rule is for legalization artifact; /// casts. G_TRUNC s1 results, and G_SEXT/G_ZEXT/G_ANYEXT sources are never vcc; /// bank. A non-boolean source (such as a truncate from a 1-bit load from; /// memory) will require a copy to the VCC bank which will require clearing the; /// high bits and inserting a compare.; ///; /// \par Constant bus restriction; ///; /// VALU instructions have a limitation known as the constant bus; /// restriction. Most VALU instructions can use SGPR operands, but may read at; /// most 1 SGPR or constant literal value (this to 2 in gfx10 for most; /// instructions). This is one unique SGPR, so the same SGPR may be used for; /// multiple operands. From a register bank perspective, any combination of; /// operands should be legal as an SGPR, but this is contextually dependent on; /// the SGPR operands all being the same register. There is therefore optimal to; /// choose the SGPR with the most uses to minimize the number of copies.; ///; /// We avoid trying to solve this problem in RegBankSelect. Any VALU G_*; /// operation should have its source operands all mapped to VGPRs (except for; /// VCC), inserting copies from any SGPR operands. This the most trivial legal; /// mapping. Anything beyond the simplest 1:1 instruction selection would be too; /// complicated to solve here. Every optimization pattern or instruction; /// selected to multiple outputs would have to enforce this rule, and there; /// would be additional complexity in tracking this rule for every G_*; /// operation. By forcing all inputs to VGPRs, it also simplifies the task of; /// picking the optimal operand combination from a post-isel optimization pass.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:2582,Usability,clear,clearing,2582," in a 32 or 64-bit; /// register. These are represented with the VCC bank. During selection, we need; /// to be able to unambiguously go back from a register class to a register; /// bank. To distinguish whether an SGPR should use the SGPR or VCC register; /// bank, we need to know the use context type. An SGPR s1 value always means a; /// VCC bank value, otherwise it will be the SGPR bank. A scalar compare sets; /// SCC, which is a 1-bit unaddressable register. This will need to be copied to; /// a 32-bit virtual register. Taken together, this means we need to adjust the; /// type of boolean operations to be regbank legal. All SALU booleans need to be; /// widened to 32-bits, and all VALU booleans need to be s1 values.; ///; /// A noteworthy exception to the s1-means-vcc rule is for legalization artifact; /// casts. G_TRUNC s1 results, and G_SEXT/G_ZEXT/G_ANYEXT sources are never vcc; /// bank. A non-boolean source (such as a truncate from a 1-bit load from; /// memory) will require a copy to the VCC bank which will require clearing the; /// high bits and inserting a compare.; ///; /// \par Constant bus restriction; ///; /// VALU instructions have a limitation known as the constant bus; /// restriction. Most VALU instructions can use SGPR operands, but may read at; /// most 1 SGPR or constant literal value (this to 2 in gfx10 for most; /// instructions). This is one unique SGPR, so the same SGPR may be used for; /// multiple operands. From a register bank perspective, any combination of; /// operands should be legal as an SGPR, but this is contextually dependent on; /// the SGPR operands all being the same register. There is therefore optimal to; /// choose the SGPR with the most uses to minimize the number of copies.; ///; /// We avoid trying to solve this problem in RegBankSelect. Any VALU G_*; /// operation should have its source operands all mapped to VGPRs (except for; /// VCC), inserting copies from any SGPR operands. This the most trivial legal; /// mapping. ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3562,Usability,simpl,simplest,3562,"gether, this means we need to adjust the; /// type of boolean operations to be regbank legal. All SALU booleans need to be; /// widened to 32-bits, and all VALU booleans need to be s1 values.; ///; /// A noteworthy exception to the s1-means-vcc rule is for legalization artifact; /// casts. G_TRUNC s1 results, and G_SEXT/G_ZEXT/G_ANYEXT sources are never vcc; /// bank. A non-boolean source (such as a truncate from a 1-bit load from; /// memory) will require a copy to the VCC bank which will require clearing the; /// high bits and inserting a compare.; ///; /// \par Constant bus restriction; ///; /// VALU instructions have a limitation known as the constant bus; /// restriction. Most VALU instructions can use SGPR operands, but may read at; /// most 1 SGPR or constant literal value (this to 2 in gfx10 for most; /// instructions). This is one unique SGPR, so the same SGPR may be used for; /// multiple operands. From a register bank perspective, any combination of; /// operands should be legal as an SGPR, but this is contextually dependent on; /// the SGPR operands all being the same register. There is therefore optimal to; /// choose the SGPR with the most uses to minimize the number of copies.; ///; /// We avoid trying to solve this problem in RegBankSelect. Any VALU G_*; /// operation should have its source operands all mapped to VGPRs (except for; /// VCC), inserting copies from any SGPR operands. This the most trivial legal; /// mapping. Anything beyond the simplest 1:1 instruction selection would be too; /// complicated to solve here. Every optimization pattern or instruction; /// selected to multiple outputs would have to enforce this rule, and there; /// would be additional complexity in tracking this rule for every G_*; /// operation. By forcing all inputs to VGPRs, it also simplifies the task of; /// picking the optimal operand combination from a post-isel optimization pass.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3889,Usability,simpl,simplifies,3889,"gether, this means we need to adjust the; /// type of boolean operations to be regbank legal. All SALU booleans need to be; /// widened to 32-bits, and all VALU booleans need to be s1 values.; ///; /// A noteworthy exception to the s1-means-vcc rule is for legalization artifact; /// casts. G_TRUNC s1 results, and G_SEXT/G_ZEXT/G_ANYEXT sources are never vcc; /// bank. A non-boolean source (such as a truncate from a 1-bit load from; /// memory) will require a copy to the VCC bank which will require clearing the; /// high bits and inserting a compare.; ///; /// \par Constant bus restriction; ///; /// VALU instructions have a limitation known as the constant bus; /// restriction. Most VALU instructions can use SGPR operands, but may read at; /// most 1 SGPR or constant literal value (this to 2 in gfx10 for most; /// instructions). This is one unique SGPR, so the same SGPR may be used for; /// multiple operands. From a register bank perspective, any combination of; /// operands should be legal as an SGPR, but this is contextually dependent on; /// the SGPR operands all being the same register. There is therefore optimal to; /// choose the SGPR with the most uses to minimize the number of copies.; ///; /// We avoid trying to solve this problem in RegBankSelect. Any VALU G_*; /// operation should have its source operands all mapped to VGPRs (except for; /// VCC), inserting copies from any SGPR operands. This the most trivial legal; /// mapping. Anything beyond the simplest 1:1 instruction selection would be too; /// complicated to solve here. Every optimization pattern or instruction; /// selected to multiple outputs would have to enforce this rule, and there; /// would be additional complexity in tracking this rule for every G_*; /// operation. By forcing all inputs to VGPRs, it also simplifies the task of; /// picking the optimal operand combination from a post-isel optimization pass.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:44,Security,access,access,44,"// 32-bit extract of a 64-bit value is just access of a subregister, so free.; // TODO: Cost of 0 hits assert, though it's not clear it's what we really; // want.; // TODO: 32-bit insert to a 64-bit SGPR may incur a non-free copy due to SGPR; // alignment restrictions, but this probably isn't important.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:103,Testability,assert,assert,103,"// 32-bit extract of a 64-bit value is just access of a subregister, so free.; // TODO: Cost of 0 hits assert, though it's not clear it's what we really; // want.; // TODO: 32-bit insert to a 64-bit SGPR may incur a non-free copy due to SGPR; // alignment restrictions, but this probably isn't important.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:127,Usability,clear,clear,127,"// 32-bit extract of a 64-bit value is just access of a subregister, so free.; // TODO: Cost of 0 hits assert, though it's not clear it's what we really; // want.; // TODO: 32-bit insert to a 64-bit SGPR may incur a non-free copy due to SGPR; // alignment restrictions, but this probably isn't important.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:28,Performance,load,load,28,// Can't do a scalar atomic load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:20,Performance,load,loads,20,// Don't use scalar loads for volatile accesses to non-constant address; // spaces.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:39,Security,access,accesses,39,// Don't use scalar loads for volatile accesses to non-constant address; // spaces.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:61,Performance,load,load,61,"// Memory must be known constant, or not written before this load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:39,Performance,load,load,39,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:119,Performance,load,load,119,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:317,Performance,load,loads,317,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:339,Performance,load,load,339,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:274,Safety,detect,detecting,274,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:412,Availability,Mask,Mask,412,"/// Legalize instruction \p MI where operands in \p OpIndices must be SGPRs. If; /// any of the required SGPR operands are VGPRs, perform a waterfall loop to; /// execute the instruction for each unique combination of values in all lanes; /// in the wave. The block will be split such that rest of the instructions are; /// moved to a new block.; ///; /// Essentially performs this loop:; //; /// Save Execution Mask; /// For (Lane : Wavefront) {; /// Enable Lane, Disable all other lanes; /// SGPR = read SGPR value for current lane from VGPR; /// VGPRResult[Lane] = use_op SGPR; /// }; /// Restore Execution Mask; ///; /// There is additional complexity to try for compare values to identify the; /// unique values used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:610,Availability,Mask,Mask,610,"/// Legalize instruction \p MI where operands in \p OpIndices must be SGPRs. If; /// any of the required SGPR operands are VGPRs, perform a waterfall loop to; /// execute the instruction for each unique combination of values in all lanes; /// in the wave. The block will be split such that rest of the instructions are; /// moved to a new block.; ///; /// Essentially performs this loop:; //; /// Save Execution Mask; /// For (Lane : Wavefront) {; /// Enable Lane, Disable all other lanes; /// SGPR = read SGPR value for current lane from VGPR; /// VGPRResult[Lane] = use_op SGPR; /// }; /// Restore Execution Mask; ///; /// There is additional complexity to try for compare values to identify the; /// unique values used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:130,Performance,perform,perform,130,"/// Legalize instruction \p MI where operands in \p OpIndices must be SGPRs. If; /// any of the required SGPR operands are VGPRs, perform a waterfall loop to; /// execute the instruction for each unique combination of values in all lanes; /// in the wave. The block will be split such that rest of the instructions are; /// moved to a new block.; ///; /// Essentially performs this loop:; //; /// Save Execution Mask; /// For (Lane : Wavefront) {; /// Enable Lane, Disable all other lanes; /// SGPR = read SGPR value for current lane from VGPR; /// VGPRResult[Lane] = use_op SGPR; /// }; /// Restore Execution Mask; ///; /// There is additional complexity to try for compare values to identify the; /// unique values used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:368,Performance,perform,performs,368,"/// Legalize instruction \p MI where operands in \p OpIndices must be SGPRs. If; /// any of the required SGPR operands are VGPRs, perform a waterfall loop to; /// execute the instruction for each unique combination of values in all lanes; /// in the wave. The block will be split such that rest of the instructions are; /// moved to a new block.; ///; /// Essentially performs this loop:; //; /// Save Execution Mask; /// For (Lane : Wavefront) {; /// Enable Lane, Disable all other lanes; /// SGPR = read SGPR value for current lane from VGPR; /// VGPRResult[Lane] = use_op SGPR; /// }; /// Restore Execution Mask; ///; /// There is additional complexity to try for compare values to identify the; /// unique values used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:66,Availability,mask,mask,66,// Don't bother using generic instructions/registers for the exec mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Deployability,Update,Update,3,"// Update EXEC, save the original EXEC value to VCC.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Deployability,Update,Update,3,"// Update EXEC, switch all done bits to 0 and all todo bits to 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:17,Availability,mask,mask,17,// Save the EXEC mask before the loop.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:20,Availability,mask,mask,20,// Restore the EXEC mask after the loop.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:16,Safety,avoid,avoid,16,// Use a set to avoid extra readfirstlanes in the case where multiple operands; // are the same register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:87,Performance,load,loads,87,// There are some special cases that we need to look at for 32 bit and 96; // bit SGPR loads otherwise we have nothing to do.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:10,Performance,load,loads,10,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:163,Performance,load,loads,163,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:183,Performance,load,load,183,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:132,Security,access,access,132,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:210,Security,access,access,210,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:14,Modifiability,extend,extending,14,// This is an extending load from a sub-dword size. Widen the memory; // access size to 4 bytes and clear the extra high bits appropriately,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:24,Performance,load,load,24,// This is an extending load from a sub-dword size. Widen the memory; // access size to 4 bytes and clear the extra high bits appropriately,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:73,Security,access,access,73,// This is an extending load from a sub-dword size. Widen the memory; // access size to 4 bytes and clear the extra high bits appropriately,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:100,Usability,clear,clear,100,// This is an extending load from a sub-dword size. Widen the memory; // access size to 4 bytes and clear the extra high bits appropriately,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:8,Modifiability,extend,extend,8,// Must extend the sign bit into higher bits for a G_SEXTLOAD,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:8,Modifiability,extend,extend,8,// Must extend zero into higher bits with an AND for a G_ZEXTLOAD,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:55,Performance,load,loads,55,// We do not need to touch the higher bits for regular loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:25,Availability,avail,available,25,"// 96-bit loads are only available for vector loads. We need to split this; // into a 64-bit part, and 32 (unless we can widen to a 128-bit load).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:10,Performance,load,loads,10,"// 96-bit loads are only available for vector loads. We need to split this; // into a 64-bit part, and 32 (unless we can widen to a 128-bit load).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:46,Performance,load,loads,46,"// 96-bit loads are only available for vector loads. We need to split this; // into a 64-bit part, and 32 (unless we can widen to a 128-bit load).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:140,Performance,load,load,140,"// 96-bit loads are only available for vector loads. We need to split this; // into a 64-bit part, and 32 (unless we can widen to a 128-bit load).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:11,Performance,load,loads,11,// 128-bit loads are supported for all instruction types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:14,Modifiability,variab,variable,14,// Handle the variable sgpr + vgpr case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:16,Performance,load,loads,16,"// TODO: 96-bit loads were widened to 128-bit results. Shrink the result if we; // can, but we need to track an MMO for that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:56,Performance,load,load,56,"// If only the offset is divergent, emit a MUBUF buffer load instead. We can; // assume that the buffer is unswizzled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer(imm)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:84,Performance,load,load,84,"// TODO: If only the resource is a VGPR, it may be better to execute the; // scalar load in the waterfall loop if the resource is expected to frequently; // be dynamically uniform.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:38,Safety,avoid,avoid,38,// Remove the original instruction to avoid potentially confusing the; // waterfall loop logic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:89,Testability,log,logic,89,// Remove the original instruction to avoid potentially confusing the; // waterfall loop logic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:79,Integrability,Depend,Depending,79,"// Use the 32-bit bitfield extract instruction if the width is a constant.; // Depending on the width size, use either the low or high 32-bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:66,Modifiability,extend,extend,66,"// Use bitfield extract on the lower 32-bit source, and then sign-extend; // or clear the upper 32-bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:80,Usability,clear,clear,80,"// Use bitfield extract on the lower 32-bit source, and then sign-extend; // or clear the upper 32-bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:28,Usability,clear,clear,28,// Ensure the high bits are clear to insert the offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:50,Safety,avoid,avoid,50,// TODO: It might be worth using a pseudo here to avoid scc clobber and; // register class constraints.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:32,Modifiability,extend,extending,32,// Return a suitable opcode for extending the operands of Opc when widening.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:101,Modifiability,extend,extend,101,"// Emit a legalized extension from <2 x s16> to 2 32-bit components, avoiding; // any illegal vector extend or unmerge operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:69,Safety,avoid,avoiding,69,"// Emit a legalized extension from <2 x s16> to 2 32-bit components, avoiding; // any illegal vector extend or unmerge operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:353,Availability,down,down,353,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:206,Energy Efficiency,power,power,206,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:304,Performance,load,load,304,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:41,Integrability,depend,dependency,41,// Use a v_mov_b32 here to make the exec dependency explicit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:14,Modifiability,extend,extending,14,"/// Implement extending a 32-bit value to a 64-bit value. \p Lo32Reg is the; /// original 32-bit source value (to be inserted in the low part of the combined; /// 64-bit result), and \p Hi32Reg is the high half of the combined 64-bit; /// value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:34,Modifiability,extend,extended,34,// Replicate sign bit from 32-bit extended part.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Integrability,Depend,Depending,3,"// Depending on where the source registers came from, the generic code may; // have decided to split the inputs already or not. If not, we still need to; // extract the values.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:22,Availability,avail,available,22,"// 64-bit and is only available on the SALU, so split into 2 32-bit ops if; // there is a VGPR input.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Integrability,Depend,Depending,3,"// Depending on where the source registers came from, the generic code may; // have decided to split the inputs already or not. If not, we still need to; // extract the values.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:106,Availability,down,down,106,"// Special case for s_mul_u64. There is not a vector equivalent of; // s_mul_u64. Hence, we have to break down s_mul_u64 into 32-bit vector; // multiplications.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:156,Modifiability,extend,extended,156,"// This is a special case for s_mul_u64. We use; // G_AMDGPU_S_MUL_I64_I32 opcode to represent an s_mul_u64 operation; // where the 33 higher bits are sign-extended and; // G_AMDGPU_S_MUL_U64_U32 opcode to represent an s_mul_u64 operation; // where the 32 higher bits are zero-extended. In case scalar registers are; // selected, both opcodes are lowered as s_mul_u64. If the vector registers; // are selected, then G_AMDGPU_S_MUL_I64_I32 and; // G_AMDGPU_S_MUL_U64_U32 are lowered with a vector mad instruction.; // Insert basic copies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:277,Modifiability,extend,extended,277,"// This is a special case for s_mul_u64. We use; // G_AMDGPU_S_MUL_I64_I32 opcode to represent an s_mul_u64 operation; // where the 33 higher bits are sign-extended and; // G_AMDGPU_S_MUL_U64_U32 opcode to represent an s_mul_u64 operation; // where the 32 higher bits are zero-extended. In case scalar registers are; // selected, both opcodes are lowered as s_mul_u64. If the vector registers; // are selected, then G_AMDGPU_S_MUL_I64_I32 and; // G_AMDGPU_S_MUL_U64_U32 are lowered with a vector mad instruction.; // Insert basic copies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:14,Availability,repair,repair,14,// Nothing to repair,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Availability,Down,Downstream,3,"// Downstream users have expectations for the high bit behavior, so freeze; // incoming undefined bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Modifiability,Extend,Extend,3,// Extend in the low bits and propagate the sign bit to the high half.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:35,Modifiability,extend,extend,35,"// The low bits are unchanged, and extend in the high bits.; // No freeze required",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:27,Energy Efficiency,efficient,efficiently,27,"// We can narrow this more efficiently than Helper can by using ffbh/ffbl; // which return -1 when the input is zero:; // (ctlz_zero_undef hi:lo) -> (umin (ffbh hi), (add (ffbh lo), 32)); // (cttz_zero_undef hi:lo) -> (umin (add (ffbl hi), 32), (ffbl lo)); // (ffbh hi:lo) -> (umin (ffbh hi), (uaddsat (ffbh lo), 32)); // (ffbl hi:lo) -> (umin (uaddsat (ffbh hi), 32), (ffbh lo))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Modifiability,Extend,Extend,3,"// Extend to 32-bit, and then extend the low half.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:30,Modifiability,extend,extend,30,"// Extend to 32-bit, and then extend the low half.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:41,Integrability,depend,dependency,41,// Use a v_mov_b32 here to make the exec dependency explicit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:38,Safety,avoid,avoid,38,// Remove the original instruction to avoid potentially confusing the; // waterfall loop logic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:89,Testability,log,logic,89,// Remove the original instruction to avoid potentially confusing the; // waterfall loop logic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Availability,Mask,Mask,3,// Mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:68,Safety,safe,safe,68,"// This is only allowed to execute with 1 lane, so readfirstlane is safe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:56,Safety,safe,safe,56,"// Only the first lane is executes, so readfirstlane is safe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:56,Safety,safe,safe,56,"// Only the first lane is executes, so readfirstlane is safe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:16,Safety,avoid,avoid,16,// Use a set to avoid extra readfirstlanes in the case where multiple; // operands are the same register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:91,Performance,optimiz,optimized,91,// Also move the copy from the scratch rsrc descriptor into the loop; // to allow it to be optimized away.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:59,Performance,load,load,59,// We have a uniform instruction so we want to use an SMRD load,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:47,Performance,load,load,47,"// FIXME: Do we want to add a mapping for FLAT load, or should we just; // handle that during instruction selection?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:15,Testability,log,logic,15,"// The default logic bothers to analyze impossible alternative mappings. We; // want the most straightforward mapping, so just directly handle this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:133,Availability,mask,masking,133,// The default handling is broken and doesn't handle illegal SGPR->VGPR copies; // properly.; //; // TODO: There are additional exec masking dependencies to analyze.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:141,Integrability,depend,dependencies,141,// The default handling is broken and doesn't handle illegal SGPR->VGPR copies; // properly.; //; // TODO: There are additional exec masking dependencies to analyze.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:10,Modifiability,extend,extend,10,"// Scalar extend can use 64-bit BFE, but VGPRs require extending to; // 32-bits, and then to 64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:55,Modifiability,extend,extending,55,"// Scalar extend can use 64-bit BFE, but VGPRs require extending to; // 32-bits, and then to 64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:50,Availability,down,down,50,"// This is a weird case, because we need to break down the mapping based on; // the register bank of a different operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURemoveIncompatibleFunctions.cpp:52,Testability,test,testing,52,// Check the GPU isn't generic. Generic is used for testing only; // and we don't want this pass to interfere with it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURemoveIncompatibleFunctions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURemoveIncompatibleFunctions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp:101,Security,access,access,101,"// Even if FLAT_SCRATCH is implicitly used, it has no effect if flat; // instructions aren't used to access the scratch buffer. Inline assembly may; // need it though.; //; // If we only have implicit uses of flat_scr on flat instructions, it is not; // really needed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp:239,Availability,error,errored,239,"// Avoid crashing on undefined behavior with an illegal call to a; // kernel. If a callsite's calling convention doesn't match the; // function's, it's undefined behavior. If the callsite calling; // convention does match, that would have errored earlier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp:3,Safety,Avoid,Avoid,3,"// Avoid crashing on undefined behavior with an illegal call to a; // kernel. If a callsite's calling convention doesn't match the; // function's, it's undefined behavior. If the callsite calling; // convention does match, that would have errored earlier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.h:72,Integrability,depend,depending,72,// Total number of VGPRs is actually a combination of AGPR and VGPR; // depending on architecture - and some alignment constraints,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:1413,Modifiability,variab,variable,1413,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:736,Safety,avoid,avoid,736,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:687,Security,access,access,687,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:1522,Testability,stub,stub,1522,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:1673,Testability,stub,stub,1673,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:1383,Usability,simpl,simple,1383,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:109,Performance,optimiz,optimized,109,"// It is possible to see stores to the same argument multiple times,; // but we expect these would have been optimized out already.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:103,Testability,stub,stub,103,"// Move the body of the function into the new rewritten function, and replace; // this function with a stub.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:25,Testability,stub,stub,25,// The function is now a stub we want to inline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:947,Deployability,pipeline,pipeline,947,"//===- AMDGPURewriteUndefForPHI.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This file implements the idea to rewrite undef incoming operand for certain; // PHIs in structurized CFG. This pass only works on IR that has gone through; // StructurizedCFG pass, and this pass has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:1615,Deployability,pipeline,pipeline,1615,"g operand for certain; // PHIs in structurized CFG. This pass only works on IR that has gone through; // StructurizedCFG pass, and this pass has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef corre",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:1691,Energy Efficiency,allocate,allocates,1691," has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef correctly.; // %if; // | \; // | %then; // | /; // -> %header: %phi = phi [%uniform, %if], [%undef, %then], [%uniform2, %header]; // | |; // \---",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:1949,Energy Efficiency,allocate,allocated,1949," has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef correctly.; // %if; // | \; // | %then; // | /; // -> %header: %phi = phi [%uniform, %if], [%undef, %then], [%uniform2, %header]; // | |; // \---",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:2309,Integrability,depend,depend,2309," has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef correctly.; // %if; // | \; // | %then; // | /; // -> %header: %phi = phi [%uniform, %if], [%undef, %then], [%uniform2, %header]; // | |; // \---",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:412,Modifiability,rewrite,rewrite,412,"//===- AMDGPURewriteUndefForPHI.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This file implements the idea to rewrite undef incoming operand for certain; // PHIs in structurized CFG. This pass only works on IR that has gone through; // StructurizedCFG pass, and this pass has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:2410,Modifiability,rewrite,rewrite,2410," has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef correctly.; // %if; // | \; // | %then; // | /; // -> %header: %phi = phi [%uniform, %if], [%undef, %then], [%uniform2, %header]; // | |; // \---",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:3,Deployability,Update,Update,3,// Update DominateBB if necessary.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:194,Availability,avail,available,194,// We only replace the undef when DominateBB truly dominates all the; // other predecessors with undefined incoming value. Make sure DominateBB; // dominates BB so that UniqueDefinedIncoming is available in BB and; // afterwards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:64,Performance,load,load,64,"// Checks that for every predecessor Pred that can reach a VMEM load,; // none of Pred's successors can reach a VMEM load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:117,Performance,load,load,117,"// Checks that for every predecessor Pred that can reach a VMEM load,; // none of Pred's successors can reach a VMEM load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:13,Performance,load,loads,13,"// Find VMEM loads that may be executed before long-enough sequences of; // VALU instructions. We currently assume that backedges/loops, branch; // probabilities and other details can be ignored, so we essentially; // determine the largest number of VALU instructions along every; // possible path from the start of the function that may potentially be; // executed provided no backedge is ever taken.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:83,Performance,load,loads,83,// Lower the priority on edges where control leaves blocks from which; // the VMEM loads are reachable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:161,Usability,simpl,simplification,161,"// Where lowering the priority in predecessors is not possible, the; // block receiving control either was not part of a loop in the first; // place or the loop simplification/canonicalization pass should have; // already tried to split the edge and insert a preheader, and if for; // whatever reason it failed to do so, then this leaves us with the; // only option of lowering the priority within the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:128,Security,access,access,128,"// Targets must either support 64-bit offsets for MUBUF instructions, and/or; // support flat operations, otherwise they cannot access a 64-bit global; // address space",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:211,Security,access,access,211,"// Unless +-flat-for-global is specified, turn on FlatForGlobal for targets; // that do not support ADDR64 variants of MUBUF instructions. Such targets; // cannot use a 64 bit offset with a MUBUF instruction to access the global; // address space",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:128,Availability,avail,available,128,"// Unless +-flat-for-global is specified, use MUBUF instructions for global; // address space access if flat operations are not available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:94,Security,access,access,94,"// Unless +-flat-for-global is specified, use MUBUF instructions for global; // address space access if flat operations are not available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:55,Availability,down,down,55,// If reqd_work_group_size is present it narrows value down.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:12,Energy Efficiency,allocate,allocate,12,"// We don't allocate the segment if we know the implicit arguments weren't; // used, even if the ABI implies we need them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:72,Performance,load,loads,72,// Being able to dereference past the end is useful for emitting scalar loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:34,Energy Efficiency,schedul,scheduler,34,// Track register pressure so the scheduler can try to decrease; // pressure once register usage is above the threshold defined by; // SIRegisterInfo::getRegPressureSetLimit(),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:21,Availability,down,down,21,// Enabling both top down and bottom up scheduling seems to give us less; // register spills than just using one of these approaches on its own.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:40,Energy Efficiency,schedul,scheduling,40,// Enabling both top down and bottom up scheduling seems to give us less; // register spills than just using one of these approaches on its own.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:56,Energy Efficiency,Schedul,Scheduler,56,// Enabling ShouldTrackLaneMasks crashes the SI Machine Scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:114,Security,access,access,114,// In principle we do not need to reserve SGPR pair used for flat_scratch if; // we know flat instructions do not access the stack anywhere in the; // program. For now assume it's needed if we have flat instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:145,Energy Efficiency,Schedul,ScheduleDAGInstrs,145,"// Work around the fact that SIInstrInfo::fixImplicitOperands modifies; // implicit operands which come from the MCInstrDesc, which can fool; // ScheduleDAGInstrs::addPhysRegDataDeps into treating them as implicit; // pseudo operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:78,Availability,avail,available,78,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:246,Energy Efficiency,power,power,246,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:252,Energy Efficiency,consumption,consumption,252,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:60,Integrability,depend,dependency,60,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:22,Performance,latency,latency,22,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:275,Performance,throttle,throttle,275,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:97,Energy Efficiency,schedul,scheduled,97,// Find up to Lat independent scalar instructions as early as; // possible such that they can be scheduled after this MFMA.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:69,Safety,detect,detect,69,// FIXME: Should have analysis or something rather than attribute to detect; // calls.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:73,Safety,detect,detecting,73,// TODO: This could be refined a lot. The attribute is a poor way of; // detecting calls or stack objects that may require it before argument; // lowering.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.h:70,Performance,load,load,70,/// Creates value range metadata on an workitemid.* intrinsic call or load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:36,Testability,test,tests,36,// Option to disable vectorizer for tests.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:28,Performance,load,loads,28,// Option to control global loads scalarization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:20,Usability,simpl,simplifications,20,// Enable lib calls simplifications,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:24,Performance,optimiz,optimization,24,// Enable Mode register optimization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:25,Testability,test,tests,25,// Option is used in lit tests to prevent deadcoding of patterns inspected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:346,Security,access,accessed,346,"// 32-bit private, local, and region pointers. 64-bit global, constant and; // flat. 160-bit non-integral fat buffer pointers that include a 128-bit; // buffer descriptor and a 32-bit offset, which are indexed by 32-bit values; // (address space 7), and 128-bit non-integral buffer resourcees (address; // space 8) which cannot be non-trivilally accessed by LLVM memory operations; // like getelementptr.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:48,Deployability,pipeline,pipeline,48,// Add promote kernel arguments pass to the opt pipeline right before; // infer address spaces which is needed to do actual address space; // rewriting.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:44,Deployability,pipeline,pipeline,44,// Add infer address spaces pass to the opt pipeline after inlining; // but before SROA to increase SROA opportunities.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:101,Performance,optimiz,optimizations,101,"// This should run after inlining to have any chance of doing; // anything, and before other cleanup optimizations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:32,Performance,load,loaded,32,// It must be a generic pointer loaded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:25,Performance,load,loaded,25,"// For a generic pointer loaded from the constant memory, it could be assumed; // as a global pointer since the constant memory is only populated on the; // host side. As implied by the offload programming model, only global; // pointers could be referenced on the host side.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:97,Testability,log,logic,97,// Check the global pointer predication based on; // (!is_share(p) && !is_private(p)). Note that logic 'and' is commutative and; // the order of 'is_shared' and 'is_private' is not significant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:86,Integrability,depend,depend,86,// This needs to be done before we create a new subtarget since any; // creation will depend on the TM and the code generation flags on the; // function that reside in TargetOptions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:19,Security,expose,exposes,19,// ReassociateGEPs exposes more opportunities for SLSR. See; // the example in reassociate-geps-and-slsr.ll.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:35,Availability,redundant,redundant,35,"// NaryReassociate on GEPs creates redundant common expressions, so run; // EarlyCSE after it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:35,Safety,redund,redundant,35,"// NaryReassociate on GEPs creates redundant common expressions, so run; // EarlyCSE after it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:63,Modifiability,variab,variables,63,// Replace OpenCL enqueued block function pointers with global variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:14,Performance,optimiz,optimizer,14,// Run atomic optimizer before Atomic Expand,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:91,Integrability,depend,depends,91,// TODO: Move this right after structurizeCFG to avoid extra divergence; // analysis. This depends on stopping SIAnnotateControlFlow from making; // control flow modifications.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:49,Safety,avoid,avoid,49,// TODO: Move this right after structurizeCFG to avoid extra divergence; // analysis. This depends on stopping SIAnnotateControlFlow from making; // control flow modifications.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:13,Energy Efficiency,schedul,scheduler,13,// Allow the scheduler to run before SIWholeQuadMode inserts exec manipulation; // instructions that cause scheduling barriers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:107,Energy Efficiency,schedul,scheduling,107,// Allow the scheduler to run before SIWholeQuadMode inserts exec manipulation; // instructions that cause scheduling barriers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:28,Performance,optimiz,optimization,28,"// This is not an essential optimization and it has a noticeable impact on; // compilation time, so we only enable it from O2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:210,Availability,failure,failure,210,"// FIXME: when an instruction has a Killed operand, and the instruction is; // inside a bundle, seems only the BUNDLE instruction appears as the Kills of; // the register in LiveVariables, this would trigger a failure in verifier,; // we should fix it and enable the verifier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:10,Energy Efficiency,allocate,allocated,10,"// Commit allocated register changes. This is mostly necessary because too; // many things rely on the use lists of the physical registers, such as the; // verifier. This is only necessary with allocators which use LiveIntervals,; // since FastRegAlloc does the replacements itself.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:58,Energy Efficiency,schedul,scheduler,58,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:174,Energy Efficiency,schedul,scheduling,174,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:227,Energy Efficiency,schedul,scheduled,227,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:272,Energy Efficiency,schedul,schedule,272,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:7,Safety,hazard,hazard,7,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:113,Safety,hazard,hazards,113,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:393,Safety,hazard,hazard,393,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:23,Integrability,depend,dependent,23,// Fixup the subtarget dependent default value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:32,Availability,mask,mask,32,// Check and apply the optional mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h:53,Integrability,Interface,Interface,53,"//===-- AMDGPUTargetMachine.h - AMDGPU TargetMachine Interface --*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// The AMDGPU TargetMachine interface definition for hw codegen targets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h:420,Integrability,interface,interface,420,"//===-- AMDGPUTargetMachine.h - AMDGPU TargetMachine Interface --*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// The AMDGPU TargetMachine interface definition for hw codegen targets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h:189,Performance,optimiz,optimization,189,/// Check if a pass is enabled given \p Opt option. The option always; /// overrides defaults if explicitly used. Otherwise its default will; /// be used given that a pass shall work at an optimization \p Level; /// minimum.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetObjectFile.cpp:16,Security,access,access,16,// Set metadata access for the explicit section,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:71,Energy Efficiency,allocate,allocate,71,// If the amount of scratch memory to eliminate exceeds our ability to allocate; // it into registers we gain nothing by aggressively inlining functions for that; // heuristic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:75,Modifiability,variab,variable,75,"// Inhibit unroll for local memory if we have seen addressing not to; // a variable, most likely we will be unable to combine it.; // Do not unroll too deep inner loops for local memory to give a chance; // to unroll an outer loop for a more important reason.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:16,Integrability,depend,depends,16,// Check if GEP depends on a value defined by this loop itself.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:93,Integrability,depend,depend,93,"// The default assumption needs to be ecc is enabled, but no directly; // exposed operations depend on it, so it can be safely inlined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:120,Safety,safe,safely,120,"// The default assumption needs to be ecc is enabled, but no directly; // exposed operations depend on it, so it can be safely inlined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:74,Security,expose,exposed,74,"// The default assumption needs to be ecc is enabled, but no directly; // exposed operations depend on it, so it can be safely inlined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:358,Safety,avoid,avoid,358,"// NB: RCID is not an RCID. In fact it is 0 or 1 for scalar or vector; // registers. See getRegisterClassForType for the implementation.; // In this case vector registers are not vector in terms of; // VGPRs, but those which can hold multiple values.; // This is really the number of registers to fill when vectorizing /; // interleaving loops, so we lie to avoid trying to use all registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:106,Security,access,access,106,"// We allow vectorization of flat stores, even though we may need to decompose; // them later if they may access private memory. We don't have enough context; // here, and legalization can handle it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:57,Performance,load,loads,57,"// FIXME: Really we would like to issue multiple 128-bit loads and stores per; // iteration. Should we report a larger size and let it legalize?; //; // FIXME: Should we use narrower types for local/region, or account for when; // unaligned access is legal?; //; // FIXME: This could use fine tuning and microbenchmarks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:241,Security,access,access,241,"// FIXME: Really we would like to issue multiple 128-bit loads and stores per; // iteration. Should we report a larger size and let it legalize?; //; // FIXME: Should we use narrower types for local/region, or account for when; // unaligned access is legal?; //; // FIXME: This could use fine tuning and microbenchmarks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:177,Energy Efficiency,efficient,efficient,177,"// A (multi-)dword access at an address == 2 (mod 4) will be decomposed by the; // hardware into byte accesses. If you assume all alignments are equally; // probable, it's more efficient on average to use short accesses for this; // case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:19,Security,access,access,19,"// A (multi-)dword access at an address == 2 (mod 4) will be decomposed by the; // hardware into byte accesses. If you assume all alignments are equally; // probable, it's more efficient on average to use short accesses for this; // case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:102,Security,access,accesses,102,"// A (multi-)dword access at an address == 2 (mod 4) will be decomposed by the; // hardware into byte accesses. If you assume all alignments are equally; // probable, it's more efficient on average to use short accesses for this; // case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:211,Security,access,accesses,211,"// A (multi-)dword access at an address == 2 (mod 4) will be decomposed by the; // hardware into byte accesses. If you assume all alignments are equally; // probable, it's more efficient on average to use short accesses for this; // case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:41,Security,access,accesses,41,"// Global memory works best with 16-byte accesses. Private memory will also; // hit this, although they'll be decomposed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:40,Integrability,contract,contract,40,// Estimate all types may be fused with contract/unsafe flags,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:49,Safety,unsafe,unsafe,49,// Estimate all types may be fused with contract/unsafe flags,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:35,Safety,unsafe,unsafe,35,"// TODO: This is more complicated, unsafe flags etc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:8,Safety,unsafe,unsafe,8,// Fast unsafe fdiv lowering:; // f32 rcp; // f32 fmul,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:264,Safety,avoid,avoided,264,"// Extracts are just reads of a subregister, so are free. Inserts are; // considered free because we don't want to have any cost for scalarizing; // operations, and we don't have to copy into a different register class.; // Dynamic indexing isn't free and is best avoided.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:3,Performance,Load,Loads,3,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:101,Performance,load,load,101,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:188,Performance,load,loads,188,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:238,Performance,load,loads,238,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:505,Energy Efficiency,power,power,505,"// In most cases TID / wavefrontsize is uniform.; //; // However, if a kernel has uneven dimesions we can have a value of; // workitem-id-x divided by the wavefrontsize non-uniform. For example; // dimensions (65, 2) will have workitems with address (64, 0) and (0, 1); // packed into a same wave which gives 1 and 0 after the division by 64; // respectively.; //; // FIXME: limit it to 1D kernels only, although that shall be possible; // to perform this optimization is the size of the X dimension is a power; // of 2, we just do not currently have infrastructure to query it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:443,Performance,perform,perform,443,"// In most cases TID / wavefrontsize is uniform.; //; // However, if a kernel has uneven dimesions we can have a value of; // workitem-id-x divided by the wavefrontsize non-uniform. For example; // dimensions (65, 2) will have workitems with address (64, 0) and (0, 1); // packed into a same wave which gives 1 and 0 after the division by 64; // respectively.; //; // FIXME: limit it to 1D kernels only, although that shall be possible; // to perform this optimization is the size of the X dimension is a power; // of 2, we just do not currently have infrastructure to query it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:456,Performance,optimiz,optimization,456,"// In most cases TID / wavefrontsize is uniform.; //; // However, if a kernel has uneven dimesions we can have a value of; // workitem-id-x divided by the wavefrontsize non-uniform. For example; // dimensions (65, 2) will have workitems with address (64, 0) and (0, 1); // packed into a same wave which gives 1 and 0 after the division by 64; // respectively.; //; // FIXME: limit it to 1D kernels only, although that shall be possible; // to perform this optimization is the size of the X dimension is a power; // of 2, we just do not currently have infrastructure to query it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:80,Availability,mask,masking,80,// All valid 64-bit to 32-bit casts work by chopping off the high; // bits. Any masking only clearing the low bits will also apply in the new; // address space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:93,Usability,clear,clearing,93,// All valid 64-bit to 32-bit casts work by chopping off the high; // bits. Any masking only clearing the low bits will also apply in the new; // address space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:45,Security,access,access,45,"// With op_sel VOP3P instructions freely can access the low half or high; // half of a register, so any swizzle is free.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:256,Integrability,depend,dependencies,256,// The cost of passing function arguments through the stack:; // 1 instruction to put a function argument on the stack in the caller.; // 1 instruction to take a function argument from the stack in callee.; // 1 instruction is explicitly take care of data dependencies in callee; // function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:85,Performance,optimiz,optimized,85,"// If we have a pointer to a private array passed into a function; // it will not be optimized out, leaving scratch usage.; // This function calculates the total size in bytes of the memory that would; // end in scratch if the call was not inlined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:73,Performance,optimiz,optimized,73,"// Below the cutoff, assume that the private memory objects would be; // optimized",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:70,Integrability,depend,depending,70,"// Above the cutoff, we give a cost to each private memory object; // depending its size. If the array can be optimized by SROA this cost is not; // added to the total-cost in the inliner cost analysis.; //; // We choose the total cost of the alloca such that their sum cancels the; // bonus given in the threshold (ArgAllocaCost).; //; // Cost_Alloca_0 + ... + Cost_Alloca_N == ArgAllocaCost; //; // Awkwardly, the ArgAllocaCost bonus is multiplied by threshold-multiplier,; // the single-bb bonus and the vector-bonus.; //; // We compensate the first two multipliers, by repeating logic from the; // inliner-cost in here. The vector-bonus is 0 on AMDGPU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:110,Performance,optimiz,optimized,110,"// Above the cutoff, we give a cost to each private memory object; // depending its size. If the array can be optimized by SROA this cost is not; // added to the total-cost in the inliner cost analysis.; //; // We choose the total cost of the alloca such that their sum cancels the; // bonus given in the threshold (ArgAllocaCost).; //; // Cost_Alloca_0 + ... + Cost_Alloca_N == ArgAllocaCost; //; // Awkwardly, the ArgAllocaCost bonus is multiplied by threshold-multiplier,; // the single-bb bonus and the vector-bonus.; //; // We compensate the first two multipliers, by repeating logic from the; // inliner-cost in here. The vector-bonus is 0 on AMDGPU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:583,Testability,log,logic,583,"// Above the cutoff, we give a cost to each private memory object; // depending its size. If the array can be optimized by SROA this cost is not; // added to the total-cost in the inliner cost analysis.; //; // We choose the total cost of the alloca such that their sum cancels the; // bonus given in the threshold (ArgAllocaCost).; //; // Cost_Alloca_0 + ... + Cost_Alloca_N == ArgAllocaCost; //; // Awkwardly, the ArgAllocaCost bonus is multiplied by threshold-multiplier,; // the single-bb bonus and the vector-bonus.; //; // We compensate the first two multipliers, by repeating logic from the; // inliner-cost in here. The vector-bonus is 0 on AMDGPU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:11,Performance,load,load,11,// Maximum load or store can handle 8 dwords for scalar and 4 for; // vector ALU. Let's assume anything above 8 dwords is expensive; // even if legal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h:9,Performance,cache,cache,9,/// Data cache line size for LoopDataPrefetch pass. Has no use before GFX12.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h:22,Performance,load,load,22,/// How much before a load we should place the prefetch instruction.; /// This is currently measured in number of IR instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp:98,Energy Efficiency,efficient,efficient,98,/// \returns true if \p BB is reachable through only uniform branches.; /// XXX - Is there a more efficient way to find this?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp:28,Usability,simpl,simplifycfg,28,// FIXME: add PDT here once simplifycfg is ready.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:109,Integrability,depend,dependencies,109,"//---------------------------------------------------------------------------//; // AMD Kernel Code, and its dependencies //; //---------------------------------------------------------------------------//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:31,Availability,mask,mask,31,// Sets val bits for specified mask in specified dst packed instance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:27,Availability,mask,mask,27,// Gets bits for specified mask from specified src packed instance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:126,Availability,mask,mask,126,"/// Every amd_*_code_t has the following properties, which are composed of; /// a number of bit fields. Every bit field has a mask (AMD_CODE_PROPERTY_*),; /// bit width (AMD_CODE_PROPERTY_*_WIDTH, and bit shift amount; /// (AMD_CODE_PROPERTY_*_SHIFT) for convenient access. Unused bits must be 0.; ///; /// (Note that bit fields cannot be used as their layout is; /// implementation defined in the C standard and so cannot be used to; /// specify an ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:266,Security,access,access,266,"/// Every amd_*_code_t has the following properties, which are composed of; /// a number of bit fields. Every bit field has a mask (AMD_CODE_PROPERTY_*),; /// bit width (AMD_CODE_PROPERTY_*_WIDTH, and bit shift amount; /// (AMD_CODE_PROPERTY_*_SHIFT) for convenient access. Unused bits must be 0.; ///; /// (Note that bit fields cannot be used as their layout is; /// implementation defined in the C standard and so cannot be used to; /// specify an ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1050,Modifiability,config,configures,1050,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:429,Performance,load,load,429,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:602,Performance,load,loaded,602,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:649,Performance,load,loads,649,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:805,Performance,load,load,805,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:210,Security,access,accessed,210,/// This is a bit set indicating which control directives have been; /// specified. If the value is 0 then there are no control directives specified; /// and the rest of the fields can be ignored. The bits are accessed using the; /// hsa_ext_control_directives_present_mask_t. Any control directive that is not; /// enabled in this bit set must have the value of all 0s.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:246,Performance,perform,performance,246,"/// If enableBreakExceptions is not enabled then must be 0, otherwise must be; /// non-0 and specifies the set of HSAIL exceptions that must have the BREAK; /// policy enabled. If this set is not empty then the generated code may have; /// lower performance than if the set is empty. If the kernel being finalized; /// has any enablebreakexceptions control directives, then the values specified; /// by this argument are unioned with the values in these control; /// directives. If any of the functions the kernel calls have an; /// enablebreakexceptions control directive, then they must be equal or a; /// subset of, this union.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:248,Performance,perform,performance,248,"/// If enableDetectExceptions is not enabled then must be 0, otherwise must be; /// non-0 and specifies the set of HSAIL exceptions that must have the DETECT; /// policy enabled. If this set is not empty then the generated code may have; /// lower performance than if the set is empty. However, an implementation; /// should endeavour to make the performance impact small. If the kernel being; /// finalized has any enabledetectexceptions control directives, then the; /// values specified by this argument are unioned with the values in these; /// control directives. If any of the functions the kernel calls have an; /// enabledetectexceptions control directive, then they must be equal or a; /// subset of, this union.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:347,Performance,perform,performance,347,"/// If enableDetectExceptions is not enabled then must be 0, otherwise must be; /// non-0 and specifies the set of HSAIL exceptions that must have the DETECT; /// policy enabled. If this set is not empty then the generated code may have; /// lower performance than if the set is empty. However, an implementation; /// should endeavour to make the performance impact small. If the kernel being; /// finalized has any enabledetectexceptions control directives, then the; /// values specified by this argument are unioned with the values in these; /// control directives. If any of the functions the kernel calls have an; /// enabledetectexceptions control directive, then they must be equal or a; /// subset of, this union.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:151,Safety,DETECT,DETECT,151,"/// If enableDetectExceptions is not enabled then must be 0, otherwise must be; /// non-0 and specifies the set of HSAIL exceptions that must have the DETECT; /// policy enabled. If this set is not empty then the generated code may have; /// lower performance than if the set is empty. However, an implementation; /// should endeavour to make the performance impact small. If the kernel being; /// finalized has any enabledetectexceptions control directives, then the; /// values specified by this argument are unioned with the values in these; /// control directives. If any of the functions the kernel calls have an; /// enabledetectexceptions control directive, then they must be equal or a; /// subset of, this union.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:110,Energy Efficiency,allocate,allocated,110,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:241,Energy Efficiency,allocate,allocated,241,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:983,Energy Efficiency,allocate,allocated,983,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:648,Modifiability,variab,variables,648,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:797,Safety,detect,detect,797,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:631,Energy Efficiency,allocate,allocated,631,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:740,Energy Efficiency,allocate,allocated,740,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:963,Energy Efficiency,allocate,allocate,963,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:790,Performance,perform,performance,790,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1027,Performance,perform,performance,1027,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:640,Performance,optimiz,optimize,640,"/// If requiredWorkgroupSize is not enabled then all elements for Dim3 must be; /// 0, and the produced code can be dispatched with any legal work-group range; /// consistent with the dispatch dimensions. Otherwise, the code produced must; /// always be dispatched with the specified work-group range. No element of the; /// specified range must be 0. It must be consistent with required_dimensions; /// and max_flat_workgroup_size. If the kernel being finalized, or any of the; /// functions it calls, has a requiredworkgroupsize control directive, then the; /// values must be the same. Specifying a value can allow the finalizer to; /// optimize work-group id operations, and if the number of work-items in the; /// work-group is less than the WAVESIZE then barrier operations can be; /// optimized to just a memory fence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:792,Performance,optimiz,optimized,792,"/// If requiredWorkgroupSize is not enabled then all elements for Dim3 must be; /// 0, and the produced code can be dispatched with any legal work-group range; /// consistent with the dispatch dimensions. Otherwise, the code produced must; /// always be dispatched with the specified work-group range. No element of the; /// specified range must be 0. It must be consistent with required_dimensions; /// and max_flat_workgroup_size. If the kernel being finalized, or any of the; /// functions it calls, has a requiredworkgroupsize control directive, then the; /// values must be the same. Specifying a value can allow the finalizer to; /// optimize work-group id operations, and if the number of work-items in the; /// work-group is less than the WAVESIZE then barrier operations can be; /// optimized to just a memory fence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:445,Performance,optimiz,optimize,445,"/// If requiredDim is not enabled then must be 0 and the produced kernel code; /// can be dispatched with 1, 2 or 3 dimensions. If enabled then the value is; /// 1..3 and the code produced must only be dispatched with a dimension that; /// matches. Other values are illegal. If the kernel being finalized, or any of; /// the functions it calls, has a requireddimsize control directive, then the; /// values must be the same. This can be used to optimize the code generated to; /// compute the absolute and flat work-group and work-item id, and the dim; /// HSAIL operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4521,Energy Efficiency,allocate,allocated,4521,"):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((grid",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1836,Performance,Queue,Queue,1836,"n SGPR; /// number.; ///; /// The initial SGPRs comprise up to 16 User SRGPs that are set up by CP and; /// apply to all waves of the grid. It is possible to specify more than 16 User; /// SGPRs using the enable_sgpr_* bit fields, in which case only the first 16; /// are actually initialized. These are then immediately followed by the System; /// SGPRs that are set up by ADC/SPI and can have different values for each wave; /// of the grid dispatch.; ///; /// SGPR register initial state is defined as follows:; ///; /// Private Segment Buffer (enable_sgpr_private_segment_buffer):; /// Number of User SGPR registers: 4. V# that can be used, together with; /// Scratch Wave Offset as an offset, to access the Private/Spill/Arg; /// segments using a segment address. It must be set as follows:; /// - Base address: of the scratch memory area used by the dispatch. It; /// does not include the scratch wave offset. It will be the per process; /// SH_HIDDEN_PRIVATE_BASE_VMID plus any offset from this dispatch (for; /// example there may be a per pipe offset, or per AQL Queue offset).; /// - Stride + data_format: Element Size * Index Stride (???); /// - Cache swizzle: ???; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1921,Performance,Cache,Cache,1921,"e to specify more than 16 User; /// SGPRs using the enable_sgpr_* bit fields, in which case only the first 16; /// are actually initialized. These are then immediately followed by the System; /// SGPRs that are set up by ADC/SPI and can have different values for each wave; /// of the grid dispatch.; ///; /// SGPR register initial state is defined as follows:; ///; /// Private Segment Buffer (enable_sgpr_private_segment_buffer):; /// Number of User SGPR registers: 4. V# that can be used, together with; /// Scratch Wave Offset as an offset, to access the Private/Spill/Arg; /// segments using a segment address. It must be set as follows:; /// - Base address: of the scratch memory area used by the dispatch. It; /// does not include the scratch wave offset. It will be the per process; /// SH_HIDDEN_PRIVATE_BASE_VMID plus any offset from this dispatch (for; /// example there may be a per pipe offset, or per AQL Queue offset).; /// - Stride + data_format: Element Size * Index Stride (???); /// - Cache swizzle: ???; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueu",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:2818,Performance,Queue,Queue,2818,".; /// - Stride + data_format: Element Size * Index Stride (???); /// - Cache swizzle: ???; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the ke",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:2939,Performance,queue,queue,2939,"?; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instr",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:2978,Performance,queue,queued,2978,"?; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instr",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:3213,Performance,load,load,3213,"kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:3233,Performance,load,loading,3233,"kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4061,Performance,load,loaded,4061,ss of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the ,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4354,Performance,load,load,4354," ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X di",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4374,Performance,load,loading,4374," ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X di",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4942,Performance,load,load,4942,"nstructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.x+workgroupSize.x-1)/workgroupSize.x).; ///; /// Grid Work-Group Count Y (enable_sgpr_grid_workgroup_count_y):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the Y dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.y+workgroupSize.y-1)/workgroupSize.y).; ///; /// Only initialized if <16 previous SGPRs initialized.; ///; /// Grid Wor",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4962,Performance,load,loading,4962,"nstructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.x+workgroupSize.x-1)/workgroupSize.x).; ///; /// Grid Work-Group Count Y (enable_sgpr_grid_workgroup_count_y):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the Y dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.y+workgroupSize.y-1)/workgroupSize.y).; ///; /// Only initialized if <16 previous SGPRs initialized.; ///; /// Grid Wor",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:10066,Performance,load,loads,10066,"ront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - ATC=1; /// - MTYPE set to support memory coherence specified in; /// amd_kernel_code_t.globalMemoryCoherence; ///; /// When the Global Buffer is used to access the Kernarg segment, must add the; /// dispatch packet kernArgPtr to a kernarg segment address before using this V#.; /// Alternatively scalar loads can be used if the kernarg offset is uniform, as; /// the kernarg segment is constant for the duration of the kernel execution.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:3226,Safety,avoid,avoids,3226,"kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4367,Safety,avoid,avoids,4367," ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X di",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4955,Safety,avoid,avoids,4955,"nstructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.x+workgroupSize.x-1)/workgroupSize.x).; ///; /// Grid Work-Group Count Y (enable_sgpr_grid_workgroup_count_y):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the Y dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.y+workgroupSize.y-1)/workgroupSize.y).; ///; /// Only initialized if <16 previous SGPRs initialized.; ///; /// Grid Wor",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:8803,Safety,avoid,avoid,8803,"led register is VGPR1 etc.; disabled registers do not have an VGPR; /// number.; ///; /// VGPR register initial state is defined as follows:; ///; /// Work-Item Id X (always initialized):; /// Number of registers: 1. 32 bit work item id in X dimension of work-group; /// for wavefront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Y dimension of work-group; /// for wavefront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - AT",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1465,Security,access,access,1465," /// which ones are actually setup in the amd_kernel_code_t object using the; /// enable_sgpr_* bit fields. The register numbers used for enabled registers; /// are dense starting at SGPR0: the first enabled register is SGPR0, the next; /// enabled register is SGPR1 etc.; disabled registers do not have an SGPR; /// number.; ///; /// The initial SGPRs comprise up to 16 User SRGPs that are set up by CP and; /// apply to all waves of the grid. It is possible to specify more than 16 User; /// SGPRs using the enable_sgpr_* bit fields, in which case only the first 16; /// are actually initialized. These are then immediately followed by the System; /// SGPRs that are set up by ADC/SPI and can have different values for each wave; /// of the grid dispatch.; ///; /// SGPR register initial state is defined as follows:; ///; /// Private Segment Buffer (enable_sgpr_private_segment_buffer):; /// Number of User SGPR registers: 4. V# that can be used, together with; /// Scratch Wave Offset as an offset, to access the Private/Spill/Arg; /// segments using a segment address. It must be set as follows:; /// - Base address: of the scratch memory area used by the dispatch. It; /// does not include the scratch wave offset. It will be the per process; /// SH_HIDDEN_PRIVATE_BASE_VMID plus any offset from this dispatch (for; /// example there may be a per pipe offset, or per AQL Queue offset).; /// - Stride + data_format: Element Size * Index Stride (???); /// - Cache swizzle: ???; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add t",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:9237,Security,access,accessed,9237,"ront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - ATC=1; /// - MTYPE set to support memory coherence specified in; /// amd_kernel_code_t.globalMemoryCoherence; ///; /// When the Global Buffer is used to access the Kernarg segment, must add the; /// dispatch packet kernArgPtr to a kernarg segment address before using this V#.; /// Alternatively scalar loads can be used if the kernarg offset is uniform, as; /// the kernarg segment is constant for the duration of the kernel execution.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:9368,Security,access,access,9368,"ront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - ATC=1; /// - MTYPE set to support memory coherence specified in; /// amd_kernel_code_t.globalMemoryCoherence; ///; /// When the Global Buffer is used to access the Kernarg segment, must add the; /// dispatch packet kernArgPtr to a kernarg segment address before using this V#.; /// Alternatively scalar loads can be used if the kernarg offset is uniform, as; /// the kernarg segment is constant for the duration of the kernel execution.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:9916,Security,access,access,9916,"ront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - ATC=1; /// - MTYPE set to support memory coherence specified in; /// amd_kernel_code_t.globalMemoryCoherence; ///; /// When the Global Buffer is used to access the Kernarg segment, must add the; /// dispatch packet kernArgPtr to a kernarg segment address before using this V#.; /// Alternatively scalar loads can be used if the kernarg offset is uniform, as; /// the kernarg segment is constant for the duration of the kernel execution.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:444,Energy Efficiency,allocate,allocate,444,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:228,Performance,cache,cache,228,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:403,Performance,cache,cache,403,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:607,Performance,cache,cache,607,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:613,Performance,perform,performance,613,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:213,Availability,avail,available,213,/// Range of bytes to consider prefetching expressed as an offset; /// and size. The offset is from the start (possibly negative) of; /// amd_kernel_code_t object. Set both to 0 if no prefetch; /// information is available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:117,Energy Efficiency,allocate,allocated,117,/// The amount of group segment memory required by a work-group in; /// bytes. This does not include any dynamically allocated group; /// segment memory that may be added when the kernel is; /// dispatched.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:120,Energy Efficiency,allocate,allocate,120,/// Number of fbarrier's used in the kernel and all functions it; /// calls. If the implementation uses group memory to allocate the; /// fbarriers then that amount must already be included in the; /// workgroup_group_segment_byte_size total.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:116,Energy Efficiency,power,power,116,/// The maximum byte alignment of variables used by the kernel in; /// the specified memory segment. Expressed as a power of two. Must; /// be at least HSA_POWERTWO_16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:34,Modifiability,variab,variables,34,/// The maximum byte alignment of variables used by the kernel in; /// the specified memory segment. Expressed as a power of two. Must; /// be at least HSA_POWERTWO_16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:34,Energy Efficiency,power,power,34,"/// Wavefront size expressed as a power of two. Must be a power of 2; /// in range 1..64 inclusive. Used to support runtime query that; /// obtains wavefront size, which may be used by application to; /// allocated dynamic group memory and set the dispatch work-group; /// size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:58,Energy Efficiency,power,power,58,"/// Wavefront size expressed as a power of two. Must be a power of 2; /// in range 1..64 inclusive. Used to support runtime query that; /// obtains wavefront size, which may be used by application to; /// allocated dynamic group memory and set the dispatch work-group; /// size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:205,Energy Efficiency,allocate,allocated,205,"/// Wavefront size expressed as a power of two. Must be a power of 2; /// in range 1..64 inclusive. Used to support runtime query that; /// obtains wavefront size, which may be used by application to; /// allocated dynamic group memory and set the dispatch work-group; /// size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNCreateVOPD.cpp:605,Energy Efficiency,schedul,scheduler,605,"//===- GCNCreateVOPD.cpp - Create VOPD Instructions ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Combine VALU pairs into VOPD instructions; /// Only works on wave32; /// Has register requirements, we reject creating VOPD if the requirements are; /// not met.; /// shouldCombineVOPD mutator in postRA machine scheduler puts candidate; /// instructions for VOPD back-to-back; ///; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNCreateVOPD.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNCreateVOPD.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:31,Performance,optimiz,optimization,31,"//=======- GCNDPPCombine.cpp - optimization for DPP instructions ---==========//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // The pass combines V_MOV_B32_dpp instruction with its VALU uses as a DPP src0; // operand. If any of the use instruction cannot be combined with the mov the; // whole sequence is reverted.; //; // $old = ...; // $dpp_value = V_MOV_B32_dpp $old, $vgpr_to_be_read_from_other_lane,; // dpp_controls..., $row_mask, $bank_mask, $bound_ctrl; // $res = VALU $dpp_value [, src1]; //; // to; //; // $res = VALU_DPP $combined_old, $vgpr_to_be_read_from_other_lane, [src1,]; // dpp_controls..., $row_mask, $bank_mask, $combined_bound_ctrl; //; // Combining rules :; //; // if $row_mask and $bank_mask are fully enabled (0xF) and; // $bound_ctrl==DPP_BOUND_ZERO or $old==0; // -> $combined_old = undef,; // $combined_bound_ctrl = DPP_BOUND_ZERO; //; // if the VALU op is binary and; // $bound_ctrl==DPP_BOUND_OFF and; // $old==identity value (immediate) for the VALU op; // -> $combined_old = src1,; // $combined_bound_ctrl = DPP_BOUND_OFF; //; // Otherwise cancel.; //; // The mov_dpp instruction should reside in the same BB as all its uses; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:47,Safety,avoid,avoid,47,// Do not shrink True16 instructions pre-RA to avoid the restriction in; // register allocation from only being able to use 128 VGPRs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:22,Availability,Mask,Mask,22,"// Prior checks cover Mask with VOPC condition, but not on purpose",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:3,Security,Validat,Validate,3,// Validate OP_SEL has to be set to all 0 and OP_SEL_HI has to be set to; // all 1.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:96,Testability,assert,assert,96,// OldOpndValue is either undef (IMPLICIT_DEF) or immediate or something else; // We could use: assert(!OldOpndValue || OldOpndValue->isImm()); // but the third option is used to distinguish undef from non-immediate; // to reuse IMPLICIT_DEF instruction later,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:427,Energy Efficiency,schedul,scheduling,427,"//===-- GCNHazardRecognizers.cpp - GCN Hazard Recognizer Impls ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:39,Safety,Hazard,Hazard,39,"//===-- GCNHazardRecognizers.cpp - GCN Hazard Recognizer Impls ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:404,Safety,hazard,hazard,404,"//===-- GCNHazardRecognizers.cpp - GCN Hazard Recognizer Impls ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:85,Safety,Hazard,Hazard,85,//===----------------------------------------------------------------------===//; // Hazard Recognizer Implementation; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:84,Energy Efficiency,schedul,scheduler,84,"// If we are not in ""HazardRecognizerMode"" and therefore not being run from; // the scheduler, track possible stalls from hazards but don't insert noops.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:21,Safety,Hazard,HazardRecognizerMode,21,"// If we are not in ""HazardRecognizerMode"" and therefore not being run from; // the scheduler, track possible stalls from hazards but don't insert noops.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:122,Safety,hazard,hazards,122,"// If we are not in ""HazardRecognizerMode"" and therefore not being run from; // the scheduler, track possible stalls from hazards but don't insert noops.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:36,Safety,hazard,hazards,36,// Check bundled MachineInstr's for hazards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:12,Energy Efficiency,schedul,scheduler,12,"// When the scheduler detects a stall, it will call AdvanceCycle() without; // emitting any instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:22,Safety,detect,detects,22,"// When the scheduler detects a stall, it will call AdvanceCycle() without; // emitting any instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:16,Safety,hazard,hazard,16,// Search for a hazard in a block and its predecessors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:151,Safety,hazard,hazard,151,// Returns a minimum wait states since \p I walking all predecessors.; // Only scans until \p IsExpired does not return true.; // Can only be run in a hazard recognizer mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:91,Safety,Hazard,Hazard,91,//===----------------------------------------------------------------------===//; // No-op Hazard Detection; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:98,Safety,Detect,Detection,98,//===----------------------------------------------------------------------===//; // No-op Hazard Detection; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:35,Performance,load,loads,35,"// We need to make sure not to put loads and stores in the same clause if they; // use the same address. For now, just start a new clause whenever we see a; // store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:109,Safety,hazard,hazard,109,"// If the set of defs and uses intersect then we cannot add this instruction; // to the clause, so we have a hazard.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:13,Safety,hazard,hazard,13,// This SMRD hazard only affects SI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:33,Usability,undo,undocumented,33,"// This fixes what appears to be undocumented hardware behavior in SI where; // s_mov writing a descriptor and s_buffer_load_dword reading the descriptor; // needs some number of nops in between. We don't know how many we need, but; // let's use 4. This wasn't discovered before probably because the only; // case when this happens is when we expand a 64-bit pointer into a full; // descriptor and use s_buffer_load_dword instead of s_load_dword, which was; // probably never encountered in the closed-source land.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:15,Safety,hazard,hazard,15,// There is no hazard if the instruction does not use vector regs; // (like wbinvl1),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:37,Safety,hazard,hazard,37,// For MUBUF/MTBUF instructions this hazard only exists if the; // instruction is not using a register in the soffset field.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:30,Safety,hazard,hazard,30,"// MIMG instructions create a hazard if they don't use a 256-bit T# and; // the store size is greater than 8 bytes and they have more than two bits; // of their dmask set.; // All our MIMG definitions use a 256-bit T#, so we can skip checking for them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:27,Safety,hazard,hazard,27,// Helper to check for the hazard where VMEM instructions that store more than; // 8 bytes can have there store data over written by the next instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:23,Safety,hazard,hazard,23,// This checks for the hazard where VMEM instructions that store more than; // 8 bytes can have there store data over written by the next instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:173,Integrability,rout,routines,173,"// This checks for hazards associated with inline asm statements.; // Since inline asms can contain just about anything, we use this; // to call/leverage other check*Hazard routines. Note that; // this function doesn't attempt to address all possible inline asm; // hazards (good luck), but is a collection of what has been; // problematic thus far.; // see checkVALUHazards()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:19,Safety,hazard,hazards,19,"// This checks for hazards associated with inline asm statements.; // Since inline asms can contain just about anything, we use this; // to call/leverage other check*Hazard routines. Note that; // this function doesn't attempt to address all possible inline asm; // hazards (good luck), but is a collection of what has been; // problematic thus far.; // see checkVALUHazards()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:166,Safety,Hazard,Hazard,166,"// This checks for hazards associated with inline asm statements.; // Since inline asms can contain just about anything, we use this; // to call/leverage other check*Hazard routines. Note that; // this function doesn't attempt to address all possible inline asm; // hazards (good luck), but is a collection of what has been; // problematic thus far.; // see checkVALUHazards()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:266,Safety,hazard,hazards,266,"// This checks for hazards associated with inline asm statements.; // Since inline asms can contain just about anything, we use this; // to call/leverage other check*Hazard routines. Note that; // this function doesn't attempt to address all possible inline asm; // hazards (good luck), but is a collection of what has been; // problematic thus far.; // see checkVALUHazards()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:77,Availability,alive,alive,77,"// V_NOP will be discarded by SQ.; // Use V_MOV_B32 v?, v?. Register must be alive so use src0 of V_PERMLANE*; // which is always a VGPR and available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:141,Availability,avail,available,141,"// V_NOP will be discarded by SQ.; // Use V_MOV_B32 v?, v?. Register must be alive so use src0 of V_PERMLANE*; // which is always a VGPR and available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// These instructions cannot not mitigate the hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:52,Safety,hazard,hazard,52,// Reducing lgkmcnt count to 0 always mitigates the hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:41,Safety,hazard,hazard,41,// SOPP instructions cannot mitigate the hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:169,Integrability,depend,dependent,169,"// At this point the SALU can be assumed to mitigate the hazard; // because either:; // (a) it is independent of the at risk SMEM (breaking chain),; // or; // (b) it is dependent on the SMEM, in which case an appropriate; // s_waitcnt lgkmcnt _must_ exist between it and the at risk; // SMEM instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:57,Safety,hazard,hazard,57,"// At this point the SALU can be assumed to mitigate the hazard; // because either:; // (a) it is independent of the at risk SMEM (breaking chain),; // or; // (b) it is dependent on the SMEM, in which case an appropriate; // s_waitcnt lgkmcnt _must_ exist between it and the at risk; // SMEM instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:120,Safety,risk,risk,120,"// At this point the SALU can be assumed to mitigate the hazard; // because either:; // (a) it is independent of the at risk SMEM (breaking chain),; // or; // (b) it is dependent on the SMEM, in which case an appropriate; // s_waitcnt lgkmcnt _must_ exist between it and the at risk; // SMEM instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:278,Safety,risk,risk,278,"// At this point the SALU can be assumed to mitigate the hazard; // because either:; // (a) it is independent of the at risk SMEM (breaking chain),; // or; // (b) it is dependent on the SMEM, in which case an appropriate; // s_waitcnt lgkmcnt _must_ exist between it and the at risk; // SMEM instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:44,Safety,hazard,hazard,44,// Check if the necessary condition for the hazard is met: both LDS and VMEM; // instructions need to appear in the same function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// Instructions which cause va_vdst==0 expire hazard,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:22,Safety,hazard,hazard,22,// TODO: On GFX12 the hazard should expire on S_WAIT_LOADCNT/SAMPLECNT/BVHCNT; // according to the type of VMEM instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:53,Safety,detect,detection,53,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:25,Testability,test,testing,25,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// Instructions which cause va_vdst==0 expire hazard,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:53,Safety,detect,detection,53,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:25,Testability,test,testing,25,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// Instructions which cause va_vdst==0 expire hazard,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:3,Safety,Hazard,Hazard,3,// Hazard is observed - insert a wait on va_dst counter to ensure hazard is; // avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:66,Safety,hazard,hazard,66,// Hazard is observed - insert a wait on va_dst counter to ensure hazard is; // avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:80,Safety,avoid,avoided,80,// Hazard is observed - insert a wait on va_dst counter to ensure hazard is; // avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:26,Safety,hazard,hazard,26,// Exception: there is no hazard if the wmma instructions are of the same; // type and there is no input modifier on src2 of the current instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:44,Safety,hazard,hazard,44,// Insert V_SWAP_B32 instruction(s) and run hazard recognizer on them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:102,Safety,hazard,hazard,102,// Instructions emitted after the current instruction will be processed by the; // parent loop of the hazard recognizer in a natural way.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:14,Safety,hazard,hazard,14,"// Re-running hazard recognizer on the modified instruction is not necessary,; // inserted V_SWAP_B32 has already both read and write new registers so; // hazards related to these register has already been handled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:155,Safety,hazard,hazards,155,"// Re-running hazard recognizer on the modified instruction is not necessary,; // inserted V_SWAP_B32 has already both read and write new registers so; // hazards related to these register has already been handled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:13,Deployability,update,update,13,"// We do not update liveness, so verifier may see it as undef.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:57,Performance,perform,performance,57,// Pad neighboring MFMA with noops for better inter-wave performance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:23,Safety,hazard,hazards,23,// On gfx90a+ relevant hazards are checked in checkMAIVALUHazards(),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:8,Safety,hazard,hazard,8,// Only hazard if register is defined by a VALU and a DGEMM is found after; // after the def.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:26,Safety,hazard,hazard,26,// Workaround for HW data hazard bug observed only in GFX90A. When there; // is a DGEMM instruction in-between a VALU and a VMEM instruction it; // causes the SQ to incorrectly not insert two wait states between the two; // instructions needed to avoid data hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:247,Safety,avoid,avoid,247,// Workaround for HW data hazard bug observed only in GFX90A. When there; // is a DGEMM instruction in-between a VALU and a VMEM instruction it; // causes the SQ to incorrectly not insert two wait states between the two; // instructions needed to avoid data hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:258,Safety,hazard,hazard,258,// Workaround for HW data hazard bug observed only in GFX90A. When there; // is a DGEMM instruction in-between a VALU and a VMEM instruction it; // causes the SQ to incorrectly not insert two wait states between the two; // instructions needed to avoid data hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:72,Availability,mask,mask,72,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:7,Safety,hazard,hazard,7,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:132,Safety,hazard,hazard,132,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:278,Safety,hazard,hazard,278,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:318,Safety,avoid,avoid,318,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:32,Availability,mask,mask,32,// These implicitly read VCC as mask source.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:14,Availability,mask,mask,14,// Only check mask register overlaps.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:41,Safety,hazard,hazard,41,// s_waitcnt_depctr sa_sdst(0) mitigates hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:58,Safety,Hazard,HazardReg,58,// VALU access to any SGPR or literal constant other than HazardReg; // mitigates hazard. No need to check HazardReg here as this will; // only be called when !IsHazardFn.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:82,Safety,hazard,hazard,82,// VALU access to any SGPR or literal constant other than HazardReg; // mitigates hazard. No need to check HazardReg here as this will; // only be called when !IsHazardFn.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:107,Safety,Hazard,HazardReg,107,// VALU access to any SGPR or literal constant other than HazardReg; // mitigates hazard. No need to check HazardReg here as this will; // only be called when !IsHazardFn.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:8,Security,access,access,8,// VALU access to any SGPR or literal constant other than HazardReg; // mitigates hazard. No need to check HazardReg here as this will; // only be called when !IsHazardFn.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:13,Safety,hazard,hazard,13,// Check for hazard,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:3,Deployability,Update,Update,3,// Update offsets of any references in the bundle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:424,Energy Efficiency,schedul,scheduling,424,"//===-- GCNHazardRecognizers.h - GCN Hazard Recognizers ---------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:37,Safety,Hazard,Hazard,37,"//===-- GCNHazardRecognizers.h - GCN Hazard Recognizers ---------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:401,Safety,hazard,hazard,401,"//===-- GCNHazardRecognizers.h - GCN Hazard Recognizers ---------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:37,Energy Efficiency,schedul,scheduler,37,// Distinguish if we are called from scheduler or hazard recognizer,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:50,Safety,hazard,hazard,50,// Distinguish if we are called from scheduler or hazard recognizer,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:8,Modifiability,variab,variable,8,"// This variable stores the instruction that has been emitted this cycle. It; // will be added to EmittedInstrs, when AdvanceCycle() or RecedeCycle() is; // called.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:48,Safety,hazard,hazards,48,// Advance over a MachineInstr bundle. Look for hazards in the bundled; // instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:39,Safety,hazard,hazard,39,// Run on an individual instruction in hazard recognizer mode. This can be; // used on a newly inserted instruction before returning from PreEmitNoops.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:12,Performance,latency,latency,12,"/// Pad the latency between neighboring MFMA instructions with s_nops. The; /// percentage of wait states to fill with s_nops is specified by the command; /// line option '-amdgpu-mfma-padding-ratio'.; ///; /// For example, with '-amdgpu-mfma-padding-ratio=100':; ///; /// 2 pass MFMA instructions have a latency of 2 wait states. Therefore, a; /// 'S_NOP 1' will be added between sequential MFMA instructions.; ///; /// V_MFMA_F32_4X4X1F32; /// V_MFMA_F32_4X4X1F32; ///-->; /// V_MFMA_F32_4X4X1F32; /// S_NOP 1; /// V_MFMA_F32_4X4X1F32",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:305,Performance,latency,latency,305,"/// Pad the latency between neighboring MFMA instructions with s_nops. The; /// percentage of wait states to fill with s_nops is specified by the command; /// line option '-amdgpu-mfma-padding-ratio'.; ///; /// For example, with '-amdgpu-mfma-padding-ratio=100':; ///; /// 2 pass MFMA instructions have a latency of 2 wait states. Therefore, a; /// 'S_NOP 1' will be added between sequential MFMA instructions.; ///; /// V_MFMA_F32_4X4X1F32; /// V_MFMA_F32_4X4X1F32; ///-->; /// V_MFMA_F32_4X4X1F32; /// S_NOP 1; /// V_MFMA_F32_4X4X1F32",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:27,Energy Efficiency,schedul,scheduler,27,/// CurCycle - The current scheduler state corresponds to this cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:41,Availability,down,down,41,"// Lower priority means schedule further down. For bottom-up scheduling, lower; // priority SUs are scheduled before higher priority SUs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:24,Energy Efficiency,schedul,schedule,24,"// Lower priority means schedule further down. For bottom-up scheduling, lower; // priority SUs are scheduled before higher priority SUs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:61,Energy Efficiency,schedul,scheduling,61,"// Lower priority means schedule further down. For bottom-up scheduling, lower; // priority SUs are scheduled before higher priority SUs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:100,Energy Efficiency,schedul,scheduled,100,"// Lower priority means schedule further down. For bottom-up scheduling, lower; // priority SUs are scheduled before higher priority SUs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:212,Energy Efficiency,schedul,scheduled,212,"// If SU does not have a register use, i.e. it doesn't produce a value; // that would be consumed (e.g. store), then it terminates a chain of; // computation. Give it a large SethiUllman number so it will be; // scheduled right before its predecessors that it doesn't lengthen; // their live ranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:39,Energy Efficiency,schedul,schedule,39,"// If SU does not have a register def, schedule it close to its uses; // because it does not lengthen any live ranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:30,Energy Efficiency,schedul,scheduled,30,/// closestSucc - Returns the scheduled cycle of the successor which is; /// closest to the current cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:126,Integrability,depend,dependencies,126,"/// calcMaxScratches - Returns an cost estimate of the worse case requirement; /// for scratch registers, i.e. number of data dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:90,Performance,latency,latency-based,90,"// Return -1 if left has higher priority, 1 if right has higher priority.; // Return 0 if latency-based priority is equivalent.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:3,Energy Efficiency,Schedul,Scheduling,3,// Scheduling an instruction that uses a VReg whose postincrement has not yet; // been scheduled will induce a copy. Model this as an extra cycle of latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:87,Energy Efficiency,schedul,scheduled,87,// Scheduling an instruction that uses a VReg whose postincrement has not yet; // been scheduled will induce a copy. Model this as an extra cycle of latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:149,Performance,latency,latency,149,// Scheduling an instruction that uses a VReg whose postincrement has not yet; // been scheduled will induce a copy. Model this as an extra cycle of latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:21,Energy Efficiency,schedul,scheduling,21,"// If either node is scheduling for latency, sort them by height/depth; // and latency.; // If neither instruction stalls (!LStall && !RStall) and HazardRecognizer; // is enabled, grouping instructions by cycle, then its height is already; // covered so only its depth matters. We also reach this point if both stall; // but have the same height.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:36,Performance,latency,latency,36,"// If either node is scheduling for latency, sort them by height/depth; // and latency.; // If neither instruction stalls (!LStall && !RStall) and HazardRecognizer; // is enabled, grouping instructions by cycle, then its height is already; // covered so only its depth matters. We also reach this point if both stall; // but have the same height.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:79,Performance,latency,latency,79,"// If either node is scheduling for latency, sort them by height/depth; // and latency.; // If neither instruction stalls (!LStall && !RStall) and HazardRecognizer; // is enabled, grouping instructions by cycle, then its height is already; // covered so only its depth matters. We also reach this point if both stall; // but have the same height.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:147,Safety,Hazard,HazardRecognizer,147,"// If either node is scheduling for latency, sort them by height/depth; // and latency.; // If neither instruction stalls (!LStall && !RStall) and HazardRecognizer; // is enabled, grouping instructions by cycle, then its height is already; // covered so only its depth matters. We also reach this point if both stall; // but have the same height.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:62,Availability,down,down,62,// Prioritize by Sethi-Ulmann number and push CopyToReg nodes down.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:7,Energy Efficiency,schedul,schedule,7,"// Try schedule def + use closer when Sethi-Ullman numbers are the same.; // e.g.; // t1 = op t2, c1; // t3 = op t4, c2; //; // and the following instructions are both ready.; // t2 = op c3; // t4 = op c4; //; // Then schedule t2 = op first.; // i.e.; // t4 = op c4; // t2 = op c3; // t1 = op t2, c1; // t3 = op t4, c2; //; // This creates more short live intervals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:218,Energy Efficiency,schedul,schedule,218,"// Try schedule def + use closer when Sethi-Ullman numbers are the same.; // e.g.; // t1 = op t2, c1; // t3 = op t4, c2; //; // and the following instructions are both ready.; // t2 = op c3; // t4 = op c4; //; // Then schedule t2 = op first.; // i.e.; // t4 = op c4; // t2 = op c3; // t1 = op t2, c1; // t3 = op t4, c2; //; // This creates more short live intervals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:52,Energy Efficiency,schedul,scheduled,52,// How many registers becomes live when the node is scheduled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:98,Availability,avail,available,98,"// Check to see if any of the pending instructions are ready to issue. If; // so, add them to the available queue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:108,Performance,queue,queue,108,"// Check to see if any of the pending instructions are ready to issue. If; // so, add them to the available queue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:13,Energy Efficiency,schedul,scheduler,13,/// Move the scheduler state forward by the specified number of Cycles.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:8,Security,access,accessors,8,// shim accessors for different order containers,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:2,Testability,assert,assert,2,//assert(Rgn.End == Sch.RegionEnd);,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:95,Energy Efficiency,schedul,schedule,95,// DAG SUnits are stored using original region's order; // so just use SUnits as the restoring schedule,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:10,Testability,stub,stub,10,// just a stub to make base class happy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:3,Energy Efficiency,schedul,scheduleRegions,3,"// scheduleRegions walks bottom to top, so its likely we just get next; // instruction to track",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:40,Energy Efficiency,schedul,schedule,40,// returns max pressure for a tentative schedule,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:56,Energy Efficiency,schedul,schedule,56,// R.End points to the boundary instruction but the; // schedule doesn't include it,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:10,Energy Efficiency,schedul,schedule,10,// Detach schedule from SUnits and interleave it with debug values.; // Returned schedule becomes independent of DAG state.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:81,Energy Efficiency,schedul,schedule,81,// Detach schedule from SUnits and interleave it with debug values.; // Returned schedule becomes independent of DAG state.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:27,Energy Efficiency,schedul,scheduler,27,"// minimal required region scheduler, works for ranges of SUnits*,; // SUnits or MachineIntrs*",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:32,Deployability,update,update,32,// Reset read - undef flags and update them later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,// Schedule consisting of MachineInstr* is considered 'detached'; // and already interleaved with debug values,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:78,Testability,assert,assert,78,"// Unfortunately placeDebugValues incorrectly modifies RegionEnd, restore; // assert(R.End == RegionEnd);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:161,Energy Efficiency,schedul,scheduler,161,///////////////////////////////////////////////////////////////////////////////; // Legacy MaxOccupancy Strategy; // Tries to increase occupancy applying minreg scheduler for a sequence of; // most demanding regions. Obtained schedules are saved as BestSchedule for a; // region.; // TargetOcc is the best achievable occupancy for a kernel.; // Returns better occupancy on success or current occupancy on fail.; // BestSchedules aren't deleted on fail.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:226,Energy Efficiency,schedul,schedules,226,///////////////////////////////////////////////////////////////////////////////; // Legacy MaxOccupancy Strategy; // Tries to increase occupancy applying minreg scheduler for a sequence of; // most demanding regions. Obtained schedules are saved as BestSchedule for a; // region.; // TargetOcc is the best achievable occupancy for a kernel.; // Returns better occupancy on success or current occupancy on fail.; // BestSchedules aren't deleted on fail.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:9,Testability,assert,assert,9,// TODO: assert Regions are sorted descending by pressure,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:43,Energy Efficiency,schedul,scheduling,43,// This is really weird but for some magic scheduling regions twice; // gives performance improvement,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:78,Performance,perform,performance,78,// This is really weird but for some magic scheduling regions twice; // gives performance improvement,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:63,Energy Efficiency,schedul,scheduling,63,// running first pass with TargetOccupancy = 0 mimics previous scheduling; // approach and is a performance magic,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:96,Performance,perform,performance,96,// running first pass with TargetOccupancy = 0 mimics previous scheduling; // approach and is a performance magic,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:88,Energy Efficiency,schedul,scheduler,88,///////////////////////////////////////////////////////////////////////////////; // ILP scheduler port,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:37,Energy Efficiency,Schedul,Scheduler,37,"//===- GCNIterativeScheduler.h - GCN Scheduler ------------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines the class GCNIterativeScheduler, which uses an iterative; /// approach to find a best schedule for GCN architecture. It basically makes; /// use of various lightweight schedules, scores them, chooses best one based on; /// their scores, and finally implements the chosen one.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:500,Energy Efficiency,schedul,schedule,500,"//===- GCNIterativeScheduler.h - GCN Scheduler ------------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines the class GCNIterativeScheduler, which uses an iterative; /// approach to find a best schedule for GCN architecture. It basically makes; /// use of various lightweight schedules, scores them, chooses best one based on; /// their scores, and finally implements the chosen one.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:582,Energy Efficiency,schedul,schedules,582,"//===- GCNIterativeScheduler.h - GCN Scheduler ------------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines the class GCNIterativeScheduler, which uses an iterative; /// approach to find a best schedule for GCN architecture. It basically makes; /// use of various lightweight schedules, scores them, chooses best one based on; /// their scores, and finally implements the chosen one.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:141,Energy Efficiency,schedul,schedule,141,// Fields except for BestSchedule are supposed to reflect current IR state; // `const` fields are to emphasize they shouldn't change for any schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:8,Energy Efficiency,schedul,schedule,8,// best schedule for the region so far (not scheduled yet),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:44,Energy Efficiency,schedul,scheduled,44,// best schedule for the region so far (not scheduled yet),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp:505,Energy Efficiency,schedul,scheduler,505,"//===- GCNMinRegStrategy.cpp ----------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines and implements the class GCNMinRegScheduler, which; /// implements an experimental, simple scheduler whose main goal is to learn; /// ways about consuming less possible registers for a region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp:498,Usability,simpl,simple,498,"//===- GCNMinRegStrategy.cpp ----------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines and implements the class GCNMinRegScheduler, which; /// implements an experimental, simple scheduler whose main goal is to learn; /// ways about consuming less possible registers for a region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp:537,Usability,learn,learn,537,"//===- GCNMinRegStrategy.cpp ----------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines and implements the class GCNMinRegScheduler, which; /// implements an experimental, simple scheduler whose main goal is to learn; /// ways about consuming less possible registers for a region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp:9,Performance,queue,queue,9,// Ready queue,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:59,Performance,optimiz,optimize,59,// NSA with non-sequential address which we can try; // to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:135,Modifiability,extend,extend,135,"// TODO: address the below limitation to handle GFX11 BVH instructions; // Bail if address is not a VGPR32. That should be possible to extend the; // optimization to work with subregs of a wider register tuples, but the; // logic to find free registers will be much more complicated with much; // less chances for success. That seems reasonable to assume that in most; // cases a tuple is used because a vector variable contains different; // parts of an address and it is either already consecutive or cannot; // be reassigned if not. If needed it is better to rely on register; // coalescer to process such address tuples.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:411,Modifiability,variab,variable,411,"// TODO: address the below limitation to handle GFX11 BVH instructions; // Bail if address is not a VGPR32. That should be possible to extend the; // optimization to work with subregs of a wider register tuples, but the; // logic to find free registers will be much more complicated with much; // less chances for success. That seems reasonable to assume that in most; // cases a tuple is used because a vector variable contains different; // parts of an address and it is either already consecutive or cannot; // be reassigned if not. If needed it is better to rely on register; // coalescer to process such address tuples.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:150,Performance,optimiz,optimization,150,"// TODO: address the below limitation to handle GFX11 BVH instructions; // Bail if address is not a VGPR32. That should be possible to extend the; // optimization to work with subregs of a wider register tuples, but the; // logic to find free registers will be much more complicated with much; // less chances for success. That seems reasonable to assume that in most; // cases a tuple is used because a vector variable contains different; // parts of an address and it is either already consecutive or cannot; // be reassigned if not. If needed it is better to rely on register; // coalescer to process such address tuples.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:224,Testability,log,logic,224,"// TODO: address the below limitation to handle GFX11 BVH instructions; // Bail if address is not a VGPR32. That should be possible to extend the; // optimization to work with subregs of a wider register tuples, but the; // logic to find free registers will be much more complicated with much; // less chances for success. That seems reasonable to assume that in most; // cases a tuple is used because a vector variable contains different; // parts of an address and it is either already consecutive or cannot; // be reassigned if not. If needed it is better to rely on register; // coalescer to process such address tuples.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp:28,Availability,avail,available,28,"// For now, reserve highest available SGPR pair. After RA,; // shift down to a lower unused pair of SGPRs; // If all registers are used, then findUnusedRegister will return; // AMDGPU::NoRegister.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp:69,Availability,down,down,69,"// For now, reserve highest available SGPR pair. After RA,; // shift down to a lower unused pair of SGPRs; // If all registers are used, then findUnusedRegister will return; // AMDGPU::NoRegister.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp:159,Safety,avoid,avoid,159,"// Some subtargets cannot do an AGPR to AGPR copy directly, and need an; // intermdiate temporary VGPR register. Try to find the defining; // accvgpr_write to avoid temporary registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp:61,Deployability,update,update,61,"// Reg uses were changed, collect unique set of registers to update; // live intervals at the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp:52,Deployability,update,updated,52,"// For AGPR reg, check if live intervals need to be updated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:158,Availability,mask,mask,158,// We don't rely on read-undef flag because in case of tentative schedule; // tracking it isn't set correctly yet. This works correctly however since; // use mask has been tracked before using LIS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:65,Energy Efficiency,schedul,schedule,65,// We don't rely on read-undef flag because in case of tentative schedule; // tracking it isn't set correctly yet. This works correctly however since; // use mask has been tracked before using LIS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:38,Deployability,update,updated,38,// For a tentative schedule LIS isn't updated yet but livemask should; // remain the same on any schedule. Subreg defs can be reordered but they; // all must dominate uses anyway.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:19,Energy Efficiency,schedul,schedule,19,// For a tentative schedule LIS isn't updated yet but livemask should; // remain the same on any schedule. Subreg defs can be reordered but they; // all must dominate uses anyway.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:97,Energy Efficiency,schedul,schedule,97,// For a tentative schedule LIS isn't updated yet but livemask should; // remain the same on any schedule. Subreg defs can be reordered but they; // all must dominate uses anyway.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:3,Deployability,Update,Update,3,// Update MaxPressure with defs pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:13,Availability,alive,alive,13,// Make uses alive.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:3,Deployability,Update,Update,3,// Update MaxPressure with uses plus early-clobber defs pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:28,Availability,mask,mask,28,// Remove dead registers or mask bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:24,Availability,mask,mask,24,// Add new registers or mask bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:103,Availability,Mask,Mask,103,"// Return lanemask of Reg's subregs that are live-through at [Begin, End] and; // are fully covered by Mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.h:26,Usability,clear,clear,26,// Return MaxPressure and clear it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:746,Modifiability,rewrite,rewrites,746,"//===-------------- GCNRewritePartialRegUses.cpp --------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; /// RenameIndependentSubregs pass leaves large partially used super registers,; /// for example:; /// undef %0.sub4:VReg_1024 = ...; /// %0.sub5:VReg_1024 = ...; /// %0.sub6:VReg_1024 = ...; /// %0.sub7:VReg_1024 = ...; /// use %0.sub4_sub5_sub6_sub7; /// use %0.sub6_sub7; ///; /// GCNRewritePartialRegUses goes right after RenameIndependentSubregs and; /// rewrites such partially used super registers with registers of minimal size:; /// undef %0.sub0:VReg_128 = ...; /// %0.sub1:VReg_128 = ...; /// %0.sub2:VReg_128 = ...; /// %0.sub3:VReg_128 = ...; /// use %0.sub0_sub1_sub2_sub3; /// use %0.sub2_sub3; ///; /// This allows to avoid subreg lanemasks tracking during register pressure; /// calculation and creates more possibilities for the code unaware of lanemasks; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:1020,Safety,avoid,avoid,1020,"//===-------------- GCNRewritePartialRegUses.cpp --------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; /// RenameIndependentSubregs pass leaves large partially used super registers,; /// for example:; /// undef %0.sub4:VReg_1024 = ...; /// %0.sub5:VReg_1024 = ...; /// %0.sub6:VReg_1024 = ...; /// %0.sub7:VReg_1024 = ...; /// use %0.sub4_sub5_sub6_sub7; /// use %0.sub6_sub7; ///; /// GCNRewritePartialRegUses goes right after RenameIndependentSubregs and; /// rewrites such partially used super registers with registers of minimal size:; /// undef %0.sub0:VReg_128 = ...; /// %0.sub1:VReg_128 = ...; /// %0.sub2:VReg_128 = ...; /// %0.sub3:VReg_128 = ...; /// use %0.sub0_sub1_sub2_sub3; /// use %0.sub2_sub3; ///; /// This allows to avoid subreg lanemasks tracking during register pressure; /// calculation and creates more possibilities for the code unaware of lanemasks; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Modifiability,Rewrite,Rewrite,4,/// Rewrite partially used register Reg by shifting all its subregisters to; /// the right and replacing the original register with a register of minimal; /// size. Return true if the change has been made.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Deployability,Update,Update,4,/// Update live intervals after rewriting OldReg to NewReg with SubRegs map; /// describing OldSubReg -> NewSubReg mapping.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:119,Performance,cache,cached,119,"/// Find subreg index with a given Offset and Size, return 0 if there is no; /// such subregister index. The result is cached in SubRegs data-member.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Performance,Cache,Cache,4,"/// Cache for getSubReg method: {Offset, Size} -> SubReg index.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:15,Availability,mask,mask,15,/// Return bit mask that contains all register classes that are projected into; /// RC by SubRegIdx. The result is cached in SuperRegMasks data-member.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:115,Performance,cache,cached,115,/// Return bit mask that contains all register classes that are projected into; /// RC by SubRegIdx. The result is cached in SuperRegMasks data-member.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Performance,Cache,Cache,4,"/// Cache for getSuperRegClassMask method: { RC, SubRegIdx } -> Class bitmask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:122,Performance,cache,cached,122,/// Return bitmask containing all allocatable register classes with registers; /// aligned at AlignNumBits. The result is cached in; /// AllocatableAndAlignedRegClassMasks data-member.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Performance,Cache,Cache,4,/// Cache for getAllocatableAndAlignedRegClassMask method:; /// AlignNumBits -> Class bitmask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:35,Energy Efficiency,Schedul,Scheduler,35,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:539,Energy Efficiency,schedul,scheduling,539,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:636,Energy Efficiency,schedul,schedule,636,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:682,Energy Efficiency,schedul,scheduling,682,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:803,Energy Efficiency,schedul,scheduling,803,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:924,Energy Efficiency,schedul,scheduling,924,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:184,Energy Efficiency,schedul,scheduler,184,"// Set the initial TargetOccupnacy to the maximum occupancy that we can; // achieve for this function. This effectively sets a lower bound on the; // 'Critical' register limits in the scheduler.; // Allow for lower occupancy targets if kernel is wave limited or memory; // bound, and using the relaxed occupancy feature.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:12,Availability,error,error,12,// Subtract error margin and bias from register limits and avoid overflow.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:59,Safety,avoid,avoid,59,// Subtract error margin and bias from register limits and avoid overflow.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:32,Energy Efficiency,schedul,scheduling,32,"// FIXME: I think for bottom up scheduling, the register pressure is cached; // and can be retrieved by DAG->getPressureDif(SU).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:69,Performance,cache,cached,69,"// FIXME: I think for bottom up scheduling, the register pressure is cached; // and can be retrieved by DAG->getPressureDif(SU).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:108,Energy Efficiency,schedul,scheduler,108,"// If two instructions increase the pressure of different register sets; // by the same amount, the generic scheduler will prefer to schedule the; // instruction that increases the set with the least amount of registers,; // which in our case would be SGPRs. This is rarely what we want, so; // when we report excess/critical register pressure, we do it either; // only for VGPRs or only for SGPRs.; // FIXME: Better heuristics to determine whether to prefer SGPRs or VGPRs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:133,Energy Efficiency,schedul,schedule,133,"// If two instructions increase the pressure of different register sets; // by the same amount, the generic scheduler will prefer to schedule the; // instruction that increases the set with the least amount of registers,; // which in our case would be SGPRs. This is rarely what we want, so; // when we report excess/critical register pressure, we do it either; // only for VGPRs or only for SGPRs.; // FIXME: Better heuristics to determine whether to prefer SGPRs or VGPRs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:274,Deployability,update,update,274,// FIXME: We have to enter REG-EXCESS before we reach the actual threshold; // to increase the likelihood we don't go over the limits. We should improve; // the analysis to look through dependencies to find the path with the least; // register pressure.; // We only need to update the RPDelta for instructions that increase register; // pressure. Instructions that decrease or keep reg pressure the same will be; // marked as RegExcess in tryCandidate() when they are compared with; // instructions that increase the register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:186,Integrability,depend,dependencies,186,// FIXME: We have to enter REG-EXCESS before we reach the actual threshold; // to increase the likelihood we don't go over the limits. We should improve; // the analysis to look through dependencies to find the path with the least; // register pressure.; // We only need to update the RPDelta for instructions that increase register; // pressure. Instructions that decrease or keep reg pressure the same will be; // marked as RegExcess in tryCandidate() when they are compared with; // instructions that increase the register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:90,Energy Efficiency,reduce,reduce,90,"// Register pressure is considered 'CRITICAL' if it is approaching a value; // that would reduce the wave occupancy for the execution unit. When; // register pressure is 'CRITICAL', increasing SGPR and VGPR pressure both; // has the same cost, so we don't need to prefer one over the other.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Energy Efficiency,Schedul,Schedule,3,"// Schedule as far as possible in the direction of no choice. This is most; // efficient, but also provides the best heuristics for CriticalPSets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:79,Energy Efficiency,efficient,efficient,79,"// Schedule as far as possible in the direction of no choice. This is most; // efficient, but also provides the best heuristics for CriticalPSets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:15,Availability,down,down,15,"// Set the top-down policy based on the state of the current top zone and; // the instructions outside the zone, including the bottom zone.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:56,Energy Efficiency,schedul,scheduled,56,// See if BotCand is still valid (because we previously scheduled from Top).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Safety,Avoid,Avoid,3,// Avoid spilling by exceeding the register limit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:27,Energy Efficiency,consumption,consumption,27,// Avoid critical resource consumption and balance the schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:55,Energy Efficiency,schedul,schedule,55,// Avoid critical resource consumption and balance the schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Safety,Avoid,Avoid,3,// Avoid critical resource consumption and balance the schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:26,Energy Efficiency,reduce,reduce,26,// Unconditionally try to reduce latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:33,Performance,latency,latency,33,// Unconditionally try to reduce latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:46,Availability,down,downstream,46,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:94,Energy Efficiency,reduce,reduce,94,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:291,Energy Efficiency,schedul,scheduler,291,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:70,Performance,optimiz,optimizations,70,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:191,Performance,Optimiz,Optimizations,191,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:225,Performance,load,loads,225,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:323,Performance,load,loads,323,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:53,Energy Efficiency,schedul,scheduled,53,// Avoid increasing the max critical pressure in the scheduled region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Safety,Avoid,Avoid,3,// Avoid increasing the max critical pressure in the scheduled region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Safety,Avoid,Avoid,3,// Avoid increasing the max pressure of the entire region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:15,Energy Efficiency,schedul,scheduling,15,// Collect all scheduling regions. The actual scheduling is performed in; // GCNScheduleDAGMILive::finalizeSchedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:46,Energy Efficiency,schedul,scheduling,46,// Collect all scheduling regions. The actual scheduling is performed in; // GCNScheduleDAGMILive::finalizeSchedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:60,Performance,perform,performed,60,// Collect all scheduling regions. The actual scheduling is performed in; // GCNScheduleDAGMILive::finalizeSchedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:181,Energy Efficiency,schedul,scheduling,181,"// If the block has the only successor then live-ins of that successor are; // live-outs of the current block. We can reuse calculated live set if the; // successor will be sent to scheduling past current block.; // However, due to the bug in LiveInterval analysis it may happen that two; // predecessors of the same successor block have different lane bitmasks for; // a live-out register. Workaround that by sticking to one-to-one relationship; // i.e. one predecessor with one successor block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Energy Efficiency,Schedul,Scheduler,3,// Scheduler sends regions from the end of the block upwards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:16,Energy Efficiency,schedul,scheduling,16,// Start actual scheduling here. This function is called by the base; // MachineScheduler after all regions have been recorded by; // GCNScheduleDAGMILive::schedule().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:156,Energy Efficiency,schedul,schedule,156,// Start actual scheduling here. This function is called by the base; // MachineScheduler after all regions have been recorded by; // GCNScheduleDAGMILive::schedule().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:13,Energy Efficiency,schedul,scheduling,13,// Setup for scheduling the region and check whether it should be skipped.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:22,Energy Efficiency,reduce,reduce,22,// Aggressivly try to reduce register pressure in the unclustered high RP; // stage. Temporarily increase occupancy target in the region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:132,Energy Efficiency,schedul,scheduled,132,// Don't bother trying to improve ILP in lower RP regions if occupancy has not; // been dropped. All regions will have already been scheduled with the ideal; // occupancy targets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:36,Performance,cache,cached,36,// FIXME: This pass will invalidate cached MBBLiveIns for regions; // inbetween the defs and region we sinked the def to. Cached pressure; // for regions where a def is sinked from will also be invalidated. Will; // need to be fixed if there is another pass after this pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:122,Performance,Cache,Cached,122,// FIXME: This pass will invalidate cached MBBLiveIns for regions; // inbetween the defs and region we sinked the def to. Cached pressure; // for regions where a def is sinked from will also be invalidated. Will; // need to be fixed if there is another pass after this pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:14,Energy Efficiency,schedul,scheduling,14,// Skip empty scheduling regions (0 or 1 schedulable instructions).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:41,Energy Efficiency,schedul,schedulable,41,// Skip empty scheduling regions (0 or 1 schedulable instructions).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:42,Energy Efficiency,schedul,scheduling,42,// Save original instruction order before scheduling for possible revert.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:111,Testability,test,testing,111,"// We may need to reschedule this region if it wasn't rescheduled in the last; // stage, or if we found it was testing critical register pressure limits in; // the unclustered reschedule stage. The later is because we may not have been; // able to raise the min occupancy in the previous stage so the region may be; // overly constrained even if it was already rescheduled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:87,Energy Efficiency,schedul,schedule,87,// Get real RP for the region if it hasn't be calculated before. After the; // initial schedule stage real RP will be collected after scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:134,Energy Efficiency,schedul,scheduling,134,// Get real RP for the region if it hasn't be calculated before. After the; // initial schedule stage real RP will be collected after scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:10,Energy Efficiency,schedul,scheduling,10,// Revert scheduling if we have dropped occupancy or there is some other; // reason that the original schedule is better.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:102,Energy Efficiency,schedul,schedule,102,// Revert scheduling if we have dropped occupancy or there is some other; // reason that the original schedule is better.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:24,Energy Efficiency,schedul,scheduling,24,// Check the results of scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:83,Energy Efficiency,schedul,scheduled,83,"// We may not be able to keep the current target occupancy because of the just; // scheduled region. We might still be able to revert scheduling if the; // occupancy before was higher, or if the current schedule has register; // pressure higher than the excess limits which could lead to more spilling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:134,Energy Efficiency,schedul,scheduling,134,"// We may not be able to keep the current target occupancy because of the just; // scheduled region. We might still be able to revert scheduling if the; // occupancy before was higher, or if the current schedule has register; // pressure higher than the excess limits which could lead to more spilling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:203,Energy Efficiency,schedul,schedule,203,"// We may not be able to keep the current target occupancy because of the just; // scheduled region. We might still be able to revert scheduling if the; // occupancy before was higher, or if the current schedule has register; // pressure higher than the excess limits which could lead to more spilling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:27,Energy Efficiency,schedul,schedule,27,// Revert if this region's schedule would cause a drop in occupancy or; // spilling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:16,Energy Efficiency,reduce,reduced,16,"// If RP is not reduced in the unclustered reschedule stage, revert to the; // old schedule.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:83,Energy Efficiency,schedul,schedule,83,"// If RP is not reduced in the unclustered reschedule stage, revert to the; // old schedule.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:27,Energy Efficiency,schedul,schedule,27,// Do not attempt to relax schedule even more if we are already spilling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:30,Deployability,update,update,30,// Reset read-undef flags and update them later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:19,Energy Efficiency,schedul,schedule,19,"// After reverting schedule, debug instrs will now be at the end of the block; // and RegionEnd will point to the first debug instr. Increment RegionEnd; // pass debug instrs to the actual end of the scheduling region.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:200,Energy Efficiency,schedul,scheduling,200,"// After reverting schedule, debug instrs will now be at the end of the block; // and RegionEnd will point to the first debug instr. Increment RegionEnd; // pass debug instrs to the actual end of the scheduling region.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:59,Safety,avoid,avoid,59,// Collect regions with rematerializable reg as live-in to avoid; // searching later when updating RP.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:30,Modifiability,variab,variables,30,// Temporary copies of cached variables we will be modifying and replacing if; // sinking succeeds.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:23,Performance,cache,cached,23,// Temporary copies of cached variables we will be modifying and replacing if; // sinking succeeds.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:68,Deployability,update,updated,68,// Make copies of register pressure and live-ins cache that will be updated; // as we rematerialize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:49,Performance,cache,cache,49,// Make copies of register pressure and live-ins cache that will be updated; // as we rematerialize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Deployability,Update,Update,3,// Update region boundaries in scheduling region we sinked from since we; // may sink an instruction that was at the beginning or end of its region,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:31,Energy Efficiency,schedul,scheduling,31,// Update region boundaries in scheduling region we sinked from since we; // may sink an instruction that was at the beginning or end of its region,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Deployability,Update,Update,3,// Update region boundaries in region we sinked to.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:15,Deployability,update,update,15,// FIXME: Also update cached pressure for where the def was sinked from.; // Update RP for all regions that has this reg as a live-in and remove; // the reg from all regions as a live-in.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:77,Deployability,Update,Update,77,// FIXME: Also update cached pressure for where the def was sinked from.; // Update RP for all regions that has this reg as a live-in and remove; // the reg from all regions as a live-in.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:22,Performance,cache,cached,22,// FIXME: Also update cached pressure for where the def was sinked from.; // Update RP for all regions that has this reg as a live-in and remove; // the reg from all regions as a live-in.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:77,Usability,Undo,Undo,77,// Occupancy was not improved for all regions that were at MinOccupancy.; // Undo sinking and remove newly rematerialized instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:20,Deployability,update,update,20,// Remove OldMI and update LIS,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Deployability,Update,Update,3,"// Update live-ins, register pressure, and regions caches.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:51,Performance,cache,caches,51,"// Update live-ins, register pressure, and regions caches.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:172,Energy Efficiency,schedul,scheduling,172,"// When removing, we will have to check both beginning and ending of the region.; // When inserting, we will only have to check if we are inserting NewMI in front; // of a scheduling region and do not need to check the ending since we will only; // ever be inserting before an already existing MI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:33,Energy Efficiency,Schedul,Scheduler,33,"//===-- GCNSchedStrategy.h - GCN Scheduler Strategy -*- C++ -*-------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:22,Energy Efficiency,schedul,scheduler,22,/// This is a minimal scheduler strategy. The main difference between this; /// and the GenericScheduler is that GCNSchedStrategy uses different; /// heuristics to determine excess/critical pressure sets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:3,Energy Efficiency,Schedul,Scheduling,3,// Scheduling stages for this strategy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:3,Energy Efficiency,schedul,schedule,3,// schedule() have seen register pressure over the critical limits and had to; // track register pressure for actual scheduling heuristics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:117,Energy Efficiency,schedul,scheduling,117,// schedule() have seen register pressure over the critical limits and had to; // track register pressure for actual scheduling heuristics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:3,Energy Efficiency,Schedul,Schedule,3,// Schedule known to have excess register pressure. Be more conservative in; // increasing ILP and preserving VGPRs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:6,Availability,error,error,6,// An error margin is necessary because of poor performance of the generic RP; // tracker and can be adjusted up for tuning heuristics to try and more; // aggressively reduce register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:168,Energy Efficiency,reduce,reduce,168,// An error margin is necessary because of poor performance of the generic RP; // tracker and can be adjusted up for tuning heuristics to try and more; // aggressively reduce register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:48,Performance,perform,performance,48,// An error margin is necessary because of poor performance of the generic RP; // tracker and can be adjusted up for tuning heuristics to try and more; // aggressively reduce register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:21,Energy Efficiency,schedul,scheduling,21,/// The goal of this scheduling strategy is to maximize kernel occupancy (i.e.; /// maximum number of waves per simd).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:21,Energy Efficiency,schedul,scheduling,21,/// The goal of this scheduling strategy is to maximize ILP for a single wave; /// (i.e. latency hiding).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:89,Performance,latency,latency,89,/// The goal of this scheduling strategy is to maximize ILP for a single wave; /// (i.e. latency hiding).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:49,Energy Efficiency,schedul,scheduling,49,// Occupancy target at the beginning of function scheduling cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:34,Energy Efficiency,schedul,scheduled,34,"// Records if a region is not yet scheduled, or schedule has been reverted,; // or we generally desire to reschedule it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:48,Energy Efficiency,schedul,schedule,48,"// Records if a region is not yet scheduled, or schedule has been reverted,; // or we generally desire to reschedule it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:18,Performance,cache,cache,18,// Region live-in cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:19,Performance,cache,cache,19,// Region pressure cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:33,Performance,cache,cache,33,// Temporary basic block live-in cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:15,Performance,cache,cache,15,// Compute and cache live-ins and pressure for all regions in block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:3,Deployability,Update,Update,3,// Update region boundaries when removing MI or inserting NewMI before MI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:37,Energy Efficiency,schedul,scheduling,37,// GCNSchedStrategy applies multiple scheduling stages to a function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:27,Energy Efficiency,schedul,scheduled,27,// The current block being scheduled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:52,Energy Efficiency,schedul,scheduling,52,// Record the original order of instructions before scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:13,Energy Efficiency,schedul,scheduling,13,// RP before scheduling the current region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:12,Energy Efficiency,schedul,scheduling,12,// RP after scheduling the current region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:26,Energy Efficiency,schedul,scheduling,26,// Initialize state for a scheduling stage. Returns false if the current stage; // should be skipped.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:36,Energy Efficiency,schedul,scheduling,36,// Finalize state after finishing a scheduling pass on the function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:13,Energy Efficiency,schedul,scheduling,13,// Setup for scheduling a region. Returns false if the current region should; // be skipped.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:19,Energy Efficiency,schedul,scheduling,19,// Check result of scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:22,Energy Efficiency,schedul,schedule,22,// computes the given schedule virtual execution time in clocks,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:19,Energy Efficiency,schedul,scheduling,19,// Returns true if scheduling should be reverted.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:27,Energy Efficiency,schedul,schedule,27,// Returns true if the new schedule may result in more spilling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:21,Energy Efficiency,schedul,scheduling,21,// Attempt to revert scheduling for this region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:18,Energy Efficiency,schedul,scheduling,18,// Retry function scheduling if we found resulting occupancy and it is; // lower than used for other scheduling passes. This will give more freedom; // to schedule low register pressure blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:101,Energy Efficiency,schedul,scheduling,101,// Retry function scheduling if we found resulting occupancy and it is; // lower than used for other scheduling passes. This will give more freedom; // to schedule low register pressure blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:155,Energy Efficiency,schedul,schedule,155,// Retry function scheduling if we found resulting occupancy and it is; // lower than used for other scheduling passes. This will give more freedom; // to schedule low register pressure blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:119,Energy Efficiency,reduce,reduce,119,"// Each region at MinOccupancy will have their own list of trivially; // rematerializable instructions we can remat to reduce RP. The list maps an; // instruction to the position we should remat before, usually the MI using; // the rematerializable instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:32,Energy Efficiency,reduce,reduce,32,// TODO: Should also attempt to reduce RP of SGPRs and AGPRs; // Attempt to reduce RP of VGPR by sinking trivially rematerializable; // instructions. Returns true if we were able to sink instruction(s).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:76,Energy Efficiency,reduce,reduce,76,// TODO: Should also attempt to reduce RP of SGPRs and AGPRs; // Attempt to reduce RP of VGPR by sinking trivially rematerializable; // instructions. Returns true if we were able to sink instruction(s).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:130,Integrability,wrap,wrap,130,"/// True if the offset field of DS instructions works as expected. On SI, the; /// offset uses a 16-bit adder and does not always wrap properly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:39,Usability,usab,usable,39,/// Condition output from div_scale is usable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:15,Safety,hazard,hazard,15,/// Extra wait hazard is needed in some cases before; /// s_cbranch_vccnz/s_cbranch_vccz.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:28,Deployability,update,update,28,/// Writes to VCC_LO/VCC_HI update the VCCZ flag.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:14,Safety,hazard,hazard,14,/// Number of hazard wait states for s_setreg_b32/s_setreg_imm32_b32.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:42,Performance,perform,perform,42,"/// \returns If MUBUF instructions always perform range checking, even for; /// buffer resources used for private memory access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:121,Security,access,access,121,"/// \returns If MUBUF instructions always perform range checking, even for; /// buffer resources used for private memory access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:138,Testability,test,testing,138,"// True if the hardware rewinds and replays GWS operations if a wave is; // preempted.; //; // If this is false, a GWS operation requires testing if a nack set the; // MEM_VIOL bit, and repeating if so.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:14,Energy Efficiency,allocate,allocated,14,"// Scratch is allocated in 256 dword per wave blocks for the entire; // wavefront. When viewed from the perspective of an arbitrary workitem, this; // is 4-byte aligned.; //; // Only 4-byte alignment is really needed to access anything. Transformations; // on the pointer value itself may rely on the alignment / known low bits of; // the pointer. Set this to something above the minimum to avoid needing; // dynamic realignment in common cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:391,Safety,avoid,avoid,391,"// Scratch is allocated in 256 dword per wave blocks for the entire; // wavefront. When viewed from the perspective of an arbitrary workitem, this; // is 4-byte aligned.; //; // Only 4-byte alignment is really needed to access anything. Transformations; // on the pointer value itself may rely on the alignment / known low bits of; // the pointer. Set this to something above the minimum to avoid needing; // dynamic realignment in common cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:220,Security,access,access,220,"// Scratch is allocated in 256 dword per wave blocks for the entire; // wavefront. When viewed from the perspective of an arbitrary workitem, this; // is 4-byte aligned.; //; // Only 4-byte alignment is really needed to access anything. Transformations; // on the pointer value itself may rely on the alignment / known low bits of; // the pointer. Set this to something above the minimum to avoid needing; // dynamic realignment in common cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:10,Integrability,wrap,wrappers,10,// static wrappers,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:51,Performance,load,load,51,// \returns true if the subtarget supports DWORDX3 load/store instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:54,Energy Efficiency,allocate,allocated,54,// Shift amount of a 64 bit shift cannot be a highest allocated register; // if also at the end of the allocation block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:17,Safety,hazard,hazard,17,// Has one cycle hazard on transcendental instruction feeding a; // non transcendental VALU.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:17,Safety,hazard,hazard,17,// Has one cycle hazard on a VALU instruction partially writing dst with; // a shift of result bits feeding another VALU instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:37,Performance,LOAD,LOADcnt,37,"/// \returns true if the target uses LOADcnt/SAMPLEcnt/BVHcnt, DScnt/KMcnt; /// and STOREcnt rather than VMcnt, LGKMcnt and VScnt respectively.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:37,Modifiability,extend,extends,37,// \returns true if S_GETPC_B64 zero-extends the result from 48 bits instead; // of sign-extending.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:89,Modifiability,extend,extending,89,// \returns true if S_GETPC_B64 zero-extends the result from 48 bits instead; // of sign-extending.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:62,Energy Efficiency,schedul,scheduler,62,// \returns true if it's beneficial on this subtarget for the scheduler to; // cluster stores as well as loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:105,Performance,load,loads,105,// \returns true if it's beneficial on this subtarget for the scheduler to; // cluster stores as well as loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:40,Safety,hazard,hazard,40,"// \returns true if the subtarget has a hazard requiring an ""s_nop 0""; // instruction before ""s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)"".",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:56,Integrability,message,message,56,// Currently all targets that support the dealloc VGPRs message also require; // the nop.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:414,Energy Efficiency,schedul,scheduling,414,"//===- GCNVOPDUtils.cpp - GCN VOPD Utils ------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This file contains the AMDGPU DAG scheduling; /// mutation to pair VOPD instructions back to back. It also contains; // subroutines useful in the creation of VOPD instructions; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:15,Integrability,depend,dependent,15,// Cannot pair dependent instructions,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:72,Performance,cache,cache,72,// On GFX12 if both OpX and OpY are V_MOV_B32 then OPY uses SRC2 source-cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:61,Energy Efficiency,schedul,scheduled,61,"/// Check if the instr pair, FirstMI and SecondMI, should be scheduled; /// together. Given SecondMI, when FirstMI is unspecified, then check if; /// SecondMI may be part of a fused pair at all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:4,Energy Efficiency,Adapt,Adapts,4,/// Adapts design from MacroFusion; /// Puts valid candidate instructions back-to-back so they can easily; /// be turned into VOPD instructions; /// Greedily pairs instruction candidates. O(n^2) algorithm.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:4,Modifiability,Adapt,Adapts,4,/// Adapts design from MacroFusion; /// Puts valid candidate instructions back-to-back so they can easily; /// be turned into VOPD instructions; /// Greedily pairs instruction candidates. O(n^2) algorithm.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.h:412,Energy Efficiency,schedul,scheduling,412,"//===- GCNVOPDUtils.h - GCN VOPD Utils ------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This file contains the AMDGPU DAG scheduling; /// mutation to pair VOPD instructions back to back. It also contains; // subroutines useful in the creation of VOPD instructions; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600AsmPrinter.cpp:25,Performance,cache,cacheline,25,// Functions needs to be cacheline (256B) aligned.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp:377,Energy Efficiency,allocate,allocate,377,"// We are being conservative here. We only require this work-around if; // CurrentSubEntries > 3 &&; // (CurrentSubEntries % 4 == 3 || CurrentSubEntries % 4 == 0); //; // We have to be conservative, because we don't know for certain that; // our stack allocation algorithm for Evergreen/NI is correct. Applying this; // work-around when CurrentSubEntries > 3 allows us to over-allocate stack; // resources without any problems.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp:117,Energy Efficiency,allocate,allocate,117,"// Some documentation says that this is not necessary on Evergreen,; // but experimentation has show that we need to allocate 1 extra; // sub-entry for the first non-WQM push.; // +1 For the push operation.; // +1 Extra space required.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate new literal reg,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Defines.h:85,Modifiability,Config,Config,85,//===----------------------------------------------------------------------===//; // Config register definitions; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Defines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Defines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:517,Security,access,access,517,"//===-- R600EmitClauseMarkers.cpp - Emit CF_ALU ---------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Add CF_ALU. R600 Alu instructions are grouped in clause which can hold; /// 128 Alu instructions ; these instructions can access up to 4 prefetched; /// 4 lines of 16 registers from constant buffers. Such ALU clauses are; /// initiated by CF_ALU instructions.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:280,Energy Efficiency,schedul,scheduler,280,"// TODO: Is this true? kill flag appears to work OK below; // Register kill flags have been cleared by the time we get to this; // pass, but it is safe to assume that all uses of this register; // occur in the same basic block as its definition, because; // it is illegal for the scheduler to schedule them in; // different blocks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:293,Energy Efficiency,schedul,schedule,293,"// TODO: Is this true? kill flag appears to work OK below; // Register kill flags have been cleared by the time we get to this; // pass, but it is safe to assume that all uses of this register; // occur in the same basic block as its definition, because; // it is illegal for the scheduler to schedule them in; // different blocks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:147,Safety,safe,safe,147,"// TODO: Is this true? kill flag appears to work OK below; // Register kill flags have been cleared by the time we get to this; // pass, but it is safe to assume that all uses of this register; // occur in the same basic block as its definition, because; // it is illegal for the scheduler to schedule them in; // different blocks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:92,Usability,clear,cleared,92,"// TODO: Is this true? kill flag appears to work OK below; // Register kill flags have been cleared by the time we get to this; // pass, but it is safe to assume that all uses of this register; // occur in the same basic block as its definition, because; // it is illegal for the scheduler to schedule them in; // different blocks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:84,Safety,safe,safe,84,"// We don't use the ADDR field until R600ControlFlowFinalizer pass, where; // it is safe to assume it is 0. However if we always put 0 here, the ifcvt; // pass may assume that identical ALU clause starter at the beginning of a; // true and false branch can be factorized which is not the case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:145,Availability,mask,masked,145,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:186,Availability,mask,masked,186,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:227,Availability,mask,masked,227,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:374,Availability,mask,masked,374,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:421,Availability,mask,masked,421,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:468,Availability,mask,masked,468,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:3,Availability,Mask,Mask,3,// Mask the write if the original instruction does not write to; // the current Channel.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600FrameLowering.cpp:37,Energy Efficiency,allocate,allocated,37,/// \returns The number of registers allocated for \p FI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp:25,Performance,queue,queue,25,// The value from output queue A (denoted by register OQAP) can; // only be fetched during the first cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp:39,Availability,avail,available,39,// R600::BRANCH* instructions are only available after isel and are not; // handled,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp:3,Modifiability,Variab,Variable,3,// Variable sized objects are not supported,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp:73,Energy Efficiency,schedul,scheduling,73,"//XXX: The r600g finalizer expects this to be 1, once we've moved the; //scheduling to the backend, we can change the default to 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:48,Integrability,Interface,Interface,48,"//===-- R600InstrInfo.h - R600 Instruction Info Interface -------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for R600InstrInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:395,Integrability,Interface,Interface,395,"//===-- R600InstrInfo.h - R600 Instruction Info Interface -------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for R600InstrInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:272,Availability,avail,available,272,"/// Given the order VEC_012 < VEC_021 < VEC_120 < VEC_102 < VEC_201 < VEC_210; /// returns true and the first (in lexical order) BankSwizzle affectation; /// starting from the one already provided in the Instruction Group MIs that; /// fits Read Port limitations in BS if available. Otherwise returns false; /// and undefined content in BS.; /// isLastAluTrans should be set if the last Alu of MIs will be executed on; /// Trans ALU. In this case, ValidTSwizzle returns the BankSwizzle value to; /// apply to the last instruction.; /// PV holds GPR to PV registers in the Instruction Group MIs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:34,Security,access,access,34,/// An instruction group can only access 2 channel pair (either [XY] or [ZW]); /// from KCache bank on R700+. This function check if MI set in input meet; /// this limitations,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:38,Security,access,accessed,38,/// Reserve the registers that may be accessed using indirect addressing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:183,Performance,load,loads,183,"/// Calculate the ""Indirect Address"" for the given \p RegIndex and; /// \p Channel; ///; /// We model indirect addressing using a virtual address space that can be; /// accessed with loads and stores. The ""Indirect Address"" is the memory; /// address in this virtual address space that maps to the given \p RegIndex; /// and \p Channel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:169,Security,access,accessed,169,"/// Calculate the ""Indirect Address"" for the given \p RegIndex and; /// \p Channel; ///; /// We model indirect addressing using a virtual address space that can be; /// accessed with loads and stores. The ""Indirect Address"" is the memory; /// address in this virtual address space that maps to the given \p RegIndex; /// and \p Channel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:47,Performance,load,loading,47,"/// \returns The register class to be used for loading and storing values; /// from an ""Indirect Address"" .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:54,Security,access,accessed,54,/// \returns the smallest register index that will be accessed by an indirect; /// read or write or -1 if indirect addressing is not used by this program.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:53,Security,access,accessed,53,/// \returns the largest register index that will be accessed by an indirect; /// read or write or -1 if indirect addressing is not used by this program.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:97,Performance,perform,performs,97,/// Build instruction(s) for an indirect register write.; ///; /// \returns The instruction that performs the indirect register write,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:96,Performance,perform,performs,96,/// Build instruction(s) for an indirect register read.; ///; /// \returns The instruction that performs the indirect register read,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:178,Safety,avoid,avoid,178,/// buildDefaultInstruction - This function returns a MachineInstr with all; /// the instruction modifiers initialized to their default values. You can; /// use this function to avoid manually specifying each instruction modifier; /// operand when building a new instruction.; ///; /// \returns a MachineInstr with all the instruction modifiers initialized; /// to their default values.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:4,Usability,Clear,Clear,4,/// Clear the specified flag on the instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp:247,Energy Efficiency,schedul,scheduler,247,// BUILD_VECTOR was lowered into an IMPLICIT_DEF + 4 INSERT_SUBREG; // that adds a 128 bits reg copy when going through TwoAddressInstructions; // pass. We want to avoid 128 bits copies as much as possible because they; // can't be bundled by our scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp:164,Safety,avoid,avoid,164,// BUILD_VECTOR was lowered into an IMPLICIT_DEF + 4 INSERT_SUBREG; // that adds a 128 bits reg copy when going through TwoAddressInstructions; // pass. We want to avoid 128 bits copies as much as possible because they; // can't be bundled by our scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp:91,Energy Efficiency,schedul,scheduling,91,"/// This pass converts a legalized DAG into a R600-specific; // DAG, ready for instruction scheduling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:12,Performance,load,loads,12,// Legalize loads and stores to the private address space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:66,Performance,load,loads,66,// Workaround for LegalizeDAG asserting on expansion of i1 vector loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:30,Testability,assert,asserting,30,// Workaround for LegalizeDAG asserting on expansion of i1 vector loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:30,Testability,assert,asserting,30,// Workaround for LegalizeDAG asserting on expansion of i1 vector stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:112,Safety,avoid,avoid,112,// We don't have 64-bit shifts. Thus we need either SHX i64 or SHX_PARTS i32; // to be Legal/Custom in order to avoid library calls.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:3,Modifiability,Extend,Extend,3,// Extend sign.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:3,Performance,Load,Load,3,// Load dword; // TODO: can we be smarter about machine pointer info?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:3,Availability,Mask,Mask,3,// Mask the value to the right type,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:13,Availability,mask,mask,13,// Shift the mask in place,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:14,Availability,mask,mask,14,// Invert the mask. NOTE: if we had native ROL instructions we could; // use inverted mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:86,Availability,mask,mask,86,// Invert the mask. NOTE: if we had native ROL instructions we could; // use inverted mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:57,Integrability,depend,depend,57,"// If we are part of expanded vector, make our neighbors depend on this store",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:34,Integrability,depend,depend,34,// Make all other vector elements depend on this store,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:85,Integrability,depend,dependencies,85,// It is beneficial to create MSKOR here instead of combiner to avoid; // artificial dependencies introduced by RMW,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:64,Safety,avoid,avoid,64,// It is beneficial to create MSKOR here instead of combiner to avoid; // artificial dependencies introduced by RMW,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:11,Availability,mask,mask,11,// Put the mask in correct place,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:3,Performance,Load,Load,3,// Load dword; // TODO: can we be smarter about machine pointer info?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:35,Performance,load,load,35,// This is still used for explicit load from addrspace(8),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:87,Performance,load,load,87,"//TODO: Does this even work?; // non-constant ptr can't be folded, keeps it as a v4f32 load",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:348,Modifiability,extend,extended,348,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:138,Performance,LOAD,LOAD,138,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:178,Performance,load,loads,178,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:257,Performance,load,loads,257,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:409,Performance,load,loads,409,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:7,Performance,load,load,7,// Get load source type if scalarized.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:6,Availability,mask,mask,6,"// We mask write here to teach later passes that the ith element of this; // vector is undef. Thus we can use it to reduce 128 bits reg usage,; // break false dependencies and additionally make assembly easier to read.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:116,Energy Efficiency,reduce,reduce,116,"// We mask write here to teach later passes that the ith element of this; // vector is undef. Thus we can use it to reduce 128 bits reg usage,; // break false dependencies and additionally make assembly easier to read.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:159,Integrability,depend,dependencies,159,"// We mask write here to teach later passes that the ith element of this; // vector is undef. Thus we can use it to reduce 128 bits reg usage,; // break false dependencies and additionally make assembly easier to read.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:24,Performance,load,loads,24,//TODO: Support smaller loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:96,Performance,Optimiz,Optimizations,96,//===----------------------------------------------------------------------===//; // Custom DAG Optimizations; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:14,Performance,optimiz,optimizations,14,// Try common optimizations,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h:47,Integrability,Interface,Interface,47,"//===-- R600ISelLowering.h - R600 DAG Lowering Interface -*- C++ -*--------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 DAG Lowering interface definition; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h:413,Integrability,interface,interface,413,"//===-- R600ISelLowering.h - R600 DAG Lowering Interface -*- C++ -*--------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 DAG Lowering interface definition; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h:280,Performance,perform,performing,280,"// R600 has ""custom"" lowering for truncating stores despite not supporting; // those instructions. If we allow that custom lowering in the DAG combiner; // then all truncates are merged into truncating stores, giving worse code; // generation. This hook prevents the DAG combiner performing that combine.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h:164,Performance,load,load,164,/// Each OpenCL kernel has nine implicit parameters that are stored in the; /// first nine dwords of a Vertex Buffer. These implicit parameters are; /// lowered to load instructions which retrieve the values from the Vertex; /// Buffer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:4,Performance,Perform,Perform,4,/// Perform the CFG structurization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:4,Performance,Perform,Perform,4,/// Perform the CFG preparation; /// This step will remove every unconditionnal/dead jump instructions and make; /// sure all loops have an exit block,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:40,Availability,failure,failures,40,// FIXME: This pass causes verification failures.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:170,Integrability,rout,routine,170,/// MachineBasicBlock::ReplaceUsesOfBlockWith doesn't serve the purpose; /// because the AMDGPU instruction is not recognized as terminator fix this; /// and retire this routine,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:10,Usability,Simpl,Simplify,10,// FIXME: Simplify,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:166,Integrability,interface,interface,166,"// TODO to fix up jump table so later phase won't be confused. if; // (jumpTableInfo->isEmpty() == false) { need to clean the jump table, but; // there isn't such an interface yet. alternatively, replace all the other; // blocks in the jump table with the entryBlk //}",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:34,Testability,test,test,34,//Use the worse block ordering to test the algorithm.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:8,Integrability,wrap,wrap,8,// Misc wrap up to maintain the consistency of the Function representation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:25,Deployability,release,release,25,"// Detach retired Block, release memory.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:9,Usability,Simpl,Simplify,9,// TODO: Simplify,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:8,Availability,down,down,8,// walk down the postDomTree,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:39,Performance,optimiz,optimize,39,"// XXX: We have an opportunity here to optimize the ""branch into if"" case; // here. Branch into if looks like this:; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true; // \ /; // done; //; // The diamond_head block begins the ""if"" and the diamond_true block; // is the block being ""branched into"".; //; // If MigrateTrue is true, then TrueBB is the block being ""branched into""; // and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:551,Performance,optimiz,optimization,551,"// XXX: We have an opportunity here to optimize the ""branch into if"" case; // here. Branch into if looks like this:; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true; // \ /; // done; //; // The diamond_head block begins the ""if"" and the diamond_true block; // is the block being ""branched into"".; //; // If MigrateTrue is true, then TrueBB is the block being ""branched into""; // and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:1387,Performance,optimiz,optimization,1387,"and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false diamond_true; // \ /; // done; //; // Without this optimization, we are forced to duplicate the diamond_true; // block and we will end up with a CFG like this:; //; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true diamond_true (duplicate); // \ / |; // done --------------------|; //; // Duplicating diamond_true can be very costly especially if it has a; // lot of instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:1599,Performance,optimiz,optimization,1599,"and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false diamond_true; // \ /; // done; //; // Without this optimization, we are forced to duplicate the diamond_true; // block and we will end up with a CFG like this:; //; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true diamond_true (duplicate); // \ / |; // done --------------------|; //; // Duplicating diamond_true can be very costly especially if it has a; // lot of instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:2052,Performance,optimiz,optimization,2052,"and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false diamond_true; // \ /; // done; //; // Without this optimization, we are forced to duplicate the diamond_true; // block and we will end up with a CFG like this:; //; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true diamond_true (duplicate); // \ / |; // done --------------------|; //; // Duplicating diamond_true can be very costly especially if it has a; // lot of instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:24,Safety,avoid,avoid,24,"//insert R600::ENDIF to avoid special case ""input landBlk == NULL""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:96,Availability,failure,failure,96,"// XXX: We are running this after RA, so creating virtual registers will; // cause an assertion failure in the PostRA scheduling pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:118,Energy Efficiency,schedul,scheduling,118,"// XXX: We are running this after RA, so creating virtual registers will; // cause an assertion failure in the PostRA scheduling pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:86,Testability,assert,assertion,86,"// XXX: We are running this after RA, so creating virtual registers will; // cause an assertion failure in the PostRA scheduling pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:3,Deployability,update,update,3,// update landBlk,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:30,Safety,safe,safely,30,//now branchInst can be erase safely,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:40,Energy Efficiency,Schedul,Scheduler,40,"//===-- R600MachineScheduler.cpp - R600 Scheduler Interface -*- C++ -*-----===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:408,Energy Efficiency,Schedul,Scheduler,408,"//===-- R600MachineScheduler.cpp - R600 Scheduler Interface -*- C++ -*-----===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:50,Integrability,Interface,Interface,50,"//===-- R600MachineScheduler.cpp - R600 Scheduler Interface -*- C++ -*-----===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:418,Integrability,interface,interface,418,"//===-- R600MachineScheduler.cpp - R600 Scheduler Interface -*- C++ -*-----===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:95,Usability,Guid,Guide,95,// We use the heuristic provided by AMD Accelerated Parallel Processing; // OpenCL Programming Guide :; // The approx. number of WF that allows TEX inst to hide ALU inst is :; // 500 (cycles for TEX) / (AluFetchRatio * 8 (cycles for ALU)),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:247,Availability,Avail,Available,247,"// We assume the local GPR requirements to be ""dominated"" by the requirement; // of the TEX clause (which consumes 128 bits regs) ; ALU inst before and; // after TEX are indeed likely to consume or generate values from/for the; // TEX clause.; // Available[IDFetch].size() * 2 : GPRs required in the Fetch clause; // We assume that fetch instructions are either TnXYZW = TEX TnXYZW (need; // one GPR) or TmXYZW = TnXYZW (need 2 GPR).; // (TODO : use RegisterPressure); // If we are going too use too many GPR, we flush Fetch instruction to lower; // register pressure on 128 bits regs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:37,Energy Efficiency,schedul,schedule,37,"// There is no export clause, we can schedule one as soon as its ready",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:48,Energy Efficiency,schedul,scheduling,48,"// MI will become a KILL, don't considers it in scheduling",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:13,Energy Efficiency,schedul,scheduling,13,// Bottom up scheduling : predX must comes first,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:28,Availability,avail,available,28,"// If there is a T_XYZW alu available, use it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h:38,Energy Efficiency,Schedul,Scheduler,38,"//===-- R600MachineScheduler.h - R600 Scheduler Interface -*- C++ -*-------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h:408,Energy Efficiency,Schedul,Scheduler,408,"//===-- R600MachineScheduler.h - R600 Scheduler Interface -*- C++ -*-------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h:48,Integrability,Interface,Interface,48,"//===-- R600MachineScheduler.h - R600 Scheduler Interface -*- C++ -*-------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h:418,Integrability,interface,interface,418,"//===-- R600MachineScheduler.h - R600 Scheduler Interface -*- C++ -*-------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp:1200,Security,access,access,1200,"//===- R600OpenCLImageTypeLoweringPass.cpp ------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass resolves calls to OpenCL image attribute, image resource ID and; /// sampler resource ID getter functions.; ///; /// Image attributes (size and format) are expected to be passed to the kernel; /// as kernel arguments immediately following the image argument itself,; /// therefore this pass adds image size and format arguments to the kernel; /// functions in the module. The kernel functions with image arguments are; /// re-created using the new signature. The new arguments are added to the; /// kernel metadata with kernel_arg_type set to ""image_size"" or ""image_format"".; /// Note: this pass may invalidate pointers to functions.; ///; /// Resource IDs of read-only images, write-only images and samplers are; /// defined to be their index among the kernel arguments of the same; /// type and access qualifier.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp:3,Security,Validat,Validation,3,// Validation checks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp:1023,Energy Efficiency,reduce,reduce,1023,"//===- R600MergeVectorRegisters.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass merges inputs of swizzeable instructions into vector sharing; /// common data and/or have enough undef subreg using swizzle abilities.; ///; /// For instance let's consider the following pseudo code :; /// %5 = REG_SEQ %1, sub0, %2, sub1, %3, sub2, undef, sub3; /// ...; /// %7 = REG_SEQ %1, sub0, %3, sub1, undef, sub2, %4, sub3; /// (swizzable Inst) %7, SwizzleMask : sub0, sub1, sub2, sub3; ///; /// is turned into :; /// %5 = REG_SEQ %1, sub0, %2, sub1, %3, sub2, undef, sub3; /// ...; /// %7 = INSERT_SUBREG %4, sub3; /// (swizzable Inst) %7, SwizzleMask : sub0, sub2, sub1, sub3; ///; /// This allow regalloc to reduce register pressure for vector registers and; /// to reduce MOV count.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp:1081,Energy Efficiency,reduce,reduce,1081,"//===- R600MergeVectorRegisters.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass merges inputs of swizzeable instructions into vector sharing; /// common data and/or have enough undef subreg using swizzle abilities.; ///; /// For instance let's consider the following pseudo code :; /// %5 = REG_SEQ %1, sub0, %2, sub1, %3, sub2, undef, sub3; /// ...; /// %7 = REG_SEQ %1, sub0, %3, sub1, undef, sub2, %4, sub3; /// (swizzable Inst) %7, SwizzleMask : sub0, sub1, sub2, sub3; ///; /// is turned into :; /// %5 = REG_SEQ %1, sub0, %2, sub1, %3, sub2, undef, sub3; /// ...; /// %7 = INSERT_SUBREG %4, sub3; /// (swizzable Inst) %7, SwizzleMask : sub0, sub2, sub1, sub3; ///; /// This allow regalloc to reduce register pressure for vector registers and; /// to reduce MOV count.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp:3,Deployability,Update,Update,3,// Update RSI,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:53,Integrability,depend,dependency,53,// isLegalToPruneDependencies - Is it legal to prune dependency between SUI; // and SUJ.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:105,Integrability,depend,dependence,105,"//; // Loop over all basic blocks and remove KILL pseudo-instructions; // These instructions confuse the dependence analysis. Consider:; // D0 = ... (Insn 0); // R0 = KILL R0, D0 (Insn 1); // R0 = ... (Insn 2); // Here, Insn 1 will result in the dependence graph not emitting an output; // dependence between Insn 0 and Insn 2. This can lead to incorrect; // packetization; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:246,Integrability,depend,dependence,246,"//; // Loop over all basic blocks and remove KILL pseudo-instructions; // These instructions confuse the dependence analysis. Consider:; // D0 = ... (Insn 0); // R0 = KILL R0, D0 (Insn 1); // R0 = ... (Insn 2); // Here, Insn 1 will result in the dependence graph not emitting an output; // dependence between Insn 0 and Insn 2. This can lead to incorrect; // packetization; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:290,Integrability,depend,dependence,290,"//; // Loop over all basic blocks and remove KILL pseudo-instructions; // These instructions confuse the dependence analysis. Consider:; // D0 = ... (Insn 0); // R0 = KILL R0, D0 (Insn 1); // R0 = ... (Insn 2); // Here, Insn 1 will result in the dependence graph not emitting an output; // dependence between Insn 0 and Insn 2. This can lead to incorrect; // packetization; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:8,Energy Efficiency,schedul,scheduling,8,// Find scheduling regions and schedule / packetize each region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:31,Energy Efficiency,schedul,schedule,31,// Find scheduling regions and schedule / packetize each region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:14,Energy Efficiency,schedul,scheduling,14,// Skip empty scheduling regions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h:48,Integrability,Interface,Interface,48,"//===-- R600RegisterInfo.h - R600 Register Info Interface ------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for R600RegisterInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h:395,Integrability,Interface,Interface,395,"//===-- R600RegisterInfo.h - R600 Register Info Interface ------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for R600RegisterInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.cpp:86,Integrability,depend,depend,86,// This needs to be done before we create a new subtarget since any; // creation will depend on the TM and the code generation flags on the; // function that reside in TargetOptions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h:51,Integrability,Interface,Interface,51,"//===-- R600TargetMachine.h - AMDGPU TargetMachine Interface ----*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// The AMDGPU TargetMachine interface definition for hw codegen targets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h:420,Integrability,interface,interface,420,"//===-- R600TargetMachine.h - AMDGPU TargetMachine Interface ----*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// The AMDGPU TargetMachine interface definition for hw codegen targets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp:106,Security,access,access,106,"// We allow vectorization of flat stores, even though we may need to decompose; // them later if they may access private memory. We don't have enough context; // here, and legalization can handle it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp:264,Safety,avoid,avoided,264,"// Extracts are just reads of a subregister, so are free. Inserts are; // considered free because we don't want to have any cost for scalarizing; // operations, and we don't have to copy into a different register class.; // Dynamic indexing isn't free and is best avoided.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:20,Security,access,accesses,20,// FLAT instruction accesses FLAT_GLBL segment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:20,Security,access,accesses,20,// FLAT instruction accesses FLAT_SCRATCH segment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:35,Availability,mask,mask,35,// v_cmp_class_* etc. use a 10-bit mask for what operation is checked.; // The result is true if any of these tests are true.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:110,Testability,test,tests,110,// v_cmp_class_* etc. use a 10-bit mask for what operation is checked.; // The result is true if any of these tests are true.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:31,Availability,mask,masks,31,// Input operand modifiers bit-masks; // NEG and SEXT share same bit-mask because they can't be set simultaneously.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:69,Availability,mask,mask,69,// Input operand modifiers bit-masks; // NEG and SEXT share same bit-mask because they can't be set simultaneously.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:16,Modifiability,extend,extend,16,// Integer sign-extend modifier,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:26,Availability,mask,mask,26,// VOP3 dst op_sel (share mask with OP_SEL_1),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:20,Performance,cache,cache,20,// Below are GFX12+ cache policy bits; // Temporal hint,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:20,Performance,load,load,20,// unused value for load insts; // Bits of TH for atomics,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:3,Integrability,Message,Message,3,"// Message ID, width(4) [3:0].",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:2157,Performance,perform,perform,2157,"---------------------===//; //; /// \file; /// Copies from VGPR to SGPR registers are illegal and the register coalescer; /// will sometimes generate these illegal copies in situations like this:; ///; /// Register Class <vsrc> is the union of <vgpr> and <sgpr>; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// %1 <vsrc> = COPY %0 <sgpr>; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <vsrc> = COPY %2 <vgpr>; /// BB2:; /// %4 <vsrc> = PHI %1 <vsrc>, <%bb.0>, %3 <vrsc>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <vsrc>; ///; ///; /// The coalescer will begin at BB0 and eliminate its copy, then the resulting; /// code will look like this:; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <vsrc> = COPY %2 <vgpr>; /// BB2:; /// %4 <sgpr> = PHI %0 <sgpr>, <%bb.0>, %3 <vsrc>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <sgpr>; ///; /// Now that the result of the PHI instruction is an SGPR, the register; /// allocator is now forced to constrain the register class of %3 to; /// <sgpr> so we end up with final code like this:; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <sgpr> = COPY %2 <vgpr>; /// BB2:; /// %4 <sgpr> = PHI %0 <sgpr>, <%bb.0>, %3 <sgpr>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <sgpr>; ///; /// Now this code contains an illegal copy from a VGPR to an SGPR.; ///; /// In order to avoid this problem, this pass searches for PHI instructions; /// which define a <vsrc> register and constrains its definition class to; /// <vgpr> if the user of the PHI's definition register is a vector instruction.; /// If the PHI's definition class is constrained to <vgpr> then the coalescer; /// will be unable to perform the COPY removal from the above example which; /// ultimately led to the creation of an illegal COPY.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:1838,Safety,avoid,avoid,1838,"---------------------===//; //; /// \file; /// Copies from VGPR to SGPR registers are illegal and the register coalescer; /// will sometimes generate these illegal copies in situations like this:; ///; /// Register Class <vsrc> is the union of <vgpr> and <sgpr>; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// %1 <vsrc> = COPY %0 <sgpr>; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <vsrc> = COPY %2 <vgpr>; /// BB2:; /// %4 <vsrc> = PHI %1 <vsrc>, <%bb.0>, %3 <vrsc>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <vsrc>; ///; ///; /// The coalescer will begin at BB0 and eliminate its copy, then the resulting; /// code will look like this:; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <vsrc> = COPY %2 <vgpr>; /// BB2:; /// %4 <sgpr> = PHI %0 <sgpr>, <%bb.0>, %3 <vsrc>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <sgpr>; ///; /// Now that the result of the PHI instruction is an SGPR, the register; /// allocator is now forced to constrain the register class of %3 to; /// <sgpr> so we end up with final code like this:; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <sgpr> = COPY %2 <vgpr>; /// BB2:; /// %4 <sgpr> = PHI %0 <sgpr>, <%bb.0>, %3 <sgpr>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <sgpr>; ///; /// Now this code contains an illegal copy from a VGPR to an SGPR.; ///; /// In order to avoid this problem, this pass searches for PHI instructions; /// which define a <vsrc> register and constrains its definition class to; /// <vgpr> if the user of the PHI's definition register is a vector instruction.; /// If the PHI's definition class is constrained to <vgpr> then the coalescer; /// will be unable to perform the COPY removal from the above example which; /// ultimately led to the creation of an illegal COPY.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:250,Security,expose,exposes,250,"// Distribute an SGPR->VGPR copy of a REG_SEQUENCE into a VGPR REG_SEQUENCE.; //; // SGPRx = ...; // SGPRy = REG_SEQUENCE SGPRx, sub0 ...; // VGPRz = COPY SGPRy; //; // ==>; //; // VGPRx = COPY SGPRx; // VGPRz = REG_SEQUENCE VGPRx, sub0; //; // This exposes immediate folding opportunities when materializing 64-bit; // immediates.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:10,Energy Efficiency,schedul,schedule,10,// Try to schedule SGPR initializations as early as possible in the MBB.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:55,Security,access,access,55,// Some architectures allow more than one constant bus access without; // SGPR restriction,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:96,Performance,perform,perform,96,// Check for trivially easy constant prop into one of the operands; // If this is the case then perform the operation now to resolve SGPR; // issue. If we don't do that here we will always insert a mov to m0; // that can't be resolved in later operand folding pass,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:20,Testability,test,tests,20,// HACK to make MIR tests with no uses happy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:105,Safety,avoid,avoid,105,// TODO: Add heuristic that the frame index might not fit in the addressing mode; // immediate offset to avoid materializing in loops.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:123,Safety,avoid,avoid,123,"// If the literal can be inlined as-is, apply it and short-circuit the; // tests below. The main motivation for this is to avoid unintuitive; // uses of opsel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:75,Testability,test,tests,75,"// If the literal can be inlined as-is, apply it and short-circuit the; // tests below. The main motivation for this is to avoid unintuitive; // uses of opsel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:216,Deployability,pipeline,pipeline,216,"// Replace integer addition by subtraction and vice versa if it allows; // folding the immediate to an inline constant.; //; // We should only ever get here for SrcIdx == 1 due to canonicalization; // earlier in the pipeline, but we double-check here to be safe / fully; // general.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:257,Safety,safe,safe,257,"// Replace integer addition by subtraction and vice versa if it allows; // folding the immediate to an inline constant.; //; // We should only ever get here for SrcIdx == 1 due to canonicalization; // earlier in the pipeline, but we double-check here to be safe / fully; // general.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:38,Safety,avoid,avoid,38,"// Keep the old instruction around to avoid breaking iterators, but; // replace it with a dummy instruction to remove uses.; //; // FIXME: We should not invert how this pass looks at operands to avoid; // this. Should track set of foldable movs instead of looking for uses; // when looking at a use.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:195,Safety,avoid,avoid,195,"// Keep the old instruction around to avoid breaking iterators, but; // replace it with a dummy instruction to remove uses.; //; // FIXME: We should not invert how this pass looks at operands to avoid; // this. Should track set of foldable movs instead of looking for uses; // when looking at a use.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:113,Safety,risk,risk,113,"// If we are already folding into another operand of MI, then; // we can't commute the instruction, otherwise we risk making the; // other fold illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:139,Safety,avoid,avoided,139,"// One of operands might be an Imm operand, and OpNo may refer to it after; // the call of commuteInstruction() below. Such situations are avoided; // here explicitly as OpNo must be a register operand to be a candidate; // for memory folding.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:209,Safety,avoid,avoid,209,// Special case for s_fmac_f32 if we are trying to fold into Src0 or Src1.; // By changing into fmamk we can untie Src2.; // If folding for Src0 happens first and it is identical operand to Src1 we; // should avoid transforming into fmamk which requires commuting as it would; // cause folding into Src1 to fail later on due to wrong OpNo used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:31,Security,access,access,31,// Verify that this is a stack access.; // FIXME: Should probably use stack pseudos before frame lowering.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:81,Safety,safe,safe,81,"// A frame index will resolve to a positive constant, so it should always be; // safe to fold the addressing mode, even pre-GFX9.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:157,Availability,redundant,redundant,157,// Don't fold into a copy to a physical register with the same class. Doing; // so would interfere with the register coalescer's logic which would avoid; // redundant initializations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:147,Safety,avoid,avoid,147,// Don't fold into a copy to a physical register with the same class. Doing; // so would interfere with the register coalescer's logic which would avoid; // redundant initializations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:157,Safety,redund,redundant,157,// Don't fold into a copy to a physical register with the same class. Doing; // so would interfere with the register coalescer's logic which would avoid; // redundant initializations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:129,Testability,log,logic,129,// Don't fold into a copy to a physical register with the same class. Doing; // so would interfere with the register coalescer's logic which would avoid; // redundant initializations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:226,Safety,avoid,avoid,226,"// That is very tricky to store a value into an AGPR. v_accvgpr_write_b32; // can only accept VGPR or inline immediate. Recreate a reg_sequence with; // its initializers right here, so we will rematerialize immediates and; // avoid copies via different reg classes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:53,Safety,avoid,avoid,53,"// Direct copy from SGPR to AGPR is not possible. To avoid creation; // of exploded copies SGPR->VGPR->AGPR in the copyPhysReg() later,; // create a copy here and track if we already have such a copy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:10,Usability,simpl,simplify,10,// Try to simplify operations with a constant that may appear after instruction; // selection.; // TODO: See if a frame index with a fixed offset can fold.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:37,Usability,simpl,simpler,37,// Try to fold an instruction into a simpler one,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:3,Usability,Clear,Clear,3,// Clear kill flags.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:103,Availability,redundant,redundant,103,// FIXME: Probably shouldn't bother trying to fold if not an; // SGPR. PeepholeOptimizer can eliminate redundant VGPR->VGPR; // copies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:103,Safety,redund,redundant,103,// FIXME: Probably shouldn't bother trying to fold if not an; // SGPR. PeepholeOptimizer can eliminate redundant VGPR->VGPR; // copies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:19,Usability,simpl,simple,19,"// Specially track simple redefs of m0 to the same value in a block, so we; // can erase the later ones.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:128,Modifiability,flexible,flexible,128,"// Use of output modifiers forces VOP3 encoding for a VOP2 mac/fmac; // instruction, so we might as well convert it to the more flexible VOP3-only; // mad/fma form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:128,Modifiability,flexible,flexible,128,"// Use of output modifiers forces VOP3 encoding for a VOP2 mac/fmac; // instruction, so we might as well convert it to the more flexible VOP3-only; // mad/fma form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:3,Modifiability,Rewrite,Rewrite,3,// Rewrite the PHI's incoming values to ARC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:27,Performance,load,load,27,// Attempt to convert VGPR load to an AGPR load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:43,Performance,load,load,43,// Attempt to convert VGPR load to an AGPR load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:466,Performance,cache,caches,466,"// tryFoldPhiAGPR will aggressively try to create AGPR PHIs.; // For GFX90A and later, this is pretty much always a good thing, but for GFX908; // there's cases where it can create a lot more AGPR-AGPR copies, which are; // expensive on this architecture due to the lack of V_ACCVGPR_MOV.; //; // This function looks at all AGPR PHIs in a basic block and collects their; // operands. Then, it checks for register that are used more than once across; // all PHIs and caches them in a VGPR. This prevents ExpandPostRAPseudo from; // having to create one VGPR temporary per use, which can get very messy if; // these PHIs come from a broken-up large PHI (e.g. 32 AGPR phis, one per vector; // element).; //; // Example; // a:; // %in:agpr_256 = COPY %foo:vgpr_256; // c:; // %x:agpr_32 = ..; // b:; // %0:areg = PHI %in.sub0:agpr_32, %a, %x, %c; // %1:areg = PHI %in.sub0:agpr_32, %a, %y, %c; // %2:areg = PHI %in.sub0:agpr_32, %a, %z, %c; // =>; // a:; // %in:agpr_256 = COPY %foo:vgpr_256; // %tmp:vgpr_32 = V_ACCVGPR_READ_B32_e64 %in.sub0:agpr_32; // %tmp_agpr:agpr_32 = COPY %tmp; // c:; // %x:agpr_32 = ..; // b:; // %0:areg = PHI %tmp_agpr, %a, %x, %c; // %1:areg = PHI %tmp_agpr, %a, %y, %c; // %2:areg = PHI %tmp_agpr, %a, %z, %c",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:60,Performance,cache,cache,60,"// For all (Reg, SubReg) pair that are used more than once, cache the value in; // a VGPR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:400,Modifiability,extend,extends,400,"//===-- SIFormMemoryClauses.cpp -------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass extends the live ranges of registers used as pointers in; /// sequences of adjacent SMEM and VMEM instructions if XNACK is enabled. A; /// load that would overwrite a pointer would require breaking the soft clause.; /// Artificially extend the live ranges of the pointer operands by adding; /// implicit-def early-clobber operands throughout the soft clause.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:633,Modifiability,extend,extend,633,"//===-- SIFormMemoryClauses.cpp -------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass extends the live ranges of registers used as pointers in; /// sequences of adjacent SMEM and VMEM instructions if XNACK is enabled. A; /// load that would overwrite a pointer would require breaking the soft clause.; /// Artificially extend the live ranges of the pointer operands by adding; /// implicit-def early-clobber operands throughout the soft clause.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:539,Performance,load,load,539,"//===-- SIFormMemoryClauses.cpp -------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass extends the live ranges of registers used as pointers in; /// sequences of adjacent SMEM and VMEM instructions if XNACK is enabled. A; /// load that would overwrite a pointer would require breaking the soft clause.; /// Artificially extend the live ranges of the pointer operands by adding; /// implicit-def early-clobber operands throughout the soft clause.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:16,Performance,load,load,16,"// If this is a load instruction where the result has been coalesced with an operand, then we cannot clause it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:105,Availability,alive,alive,105,// NB: skip advanceBeforeNext() call. Since all defs will be marked; // early-clobber they will all stay alive at least to the end of the; // clause. Therefor we should not decrease pressure even if load; // pointer becomes dead and could otherwise be reused for destination.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:199,Performance,load,load,199,// NB: skip advanceBeforeNext() call. Since all defs will be marked; // early-clobber they will all stay alive at least to the end of the; // clause. Therefor we should not decrease pressure even if load; // pointer becomes dead and could otherwise be reused for destination.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:56,Availability,mask,masks,56,// Collect register defs and uses along with their lane masks and states.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:179,Deployability,update,updated,179,"// Check register def/use conflicts, occupancy limits and collect def/use maps.; // Return true if instruction can be bundled with previous. If it cannot; // def/use maps are not updated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:5,Performance,load,load,5,// A load from pointer which was loaded inside the same bundle is an; // impossible clause because we will need to write and read the same; // register inside. In this case processRegUses will return false.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:33,Performance,load,loaded,33,// A load from pointer which was loaded inside the same bundle is an; // impossible clause because we will need to write and read the same; // register inside. In this case processRegUses will return false.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:43,Modifiability,extend,extend,43,// Collect the register operands we should extend the live ranges of.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:19,Modifiability,extend,extend,19,"// We only want to extend the live ranges of used registers. If they; // already have existing uses beyond the bundle, we don't need the kill.; //; // It's possible all of the use registers were already live past the; // bundle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:76,Availability,avail,available,76,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:114,Availability,failure,failure,114,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:292,Availability,redundant,redundant,292,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:250,Energy Efficiency,reduce,reduce,250,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:161,Modifiability,Rewrite,Rewrite,161,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:286,Safety,avoid,avoid,286,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:292,Safety,redund,redundant,292,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:195,Integrability,wrap,wrapping,195,"// Find a scratch register that we can use in the prologue. We avoid using; // callee-save registers since they may appear to be free when this is called; // from canUseAsPrologue (during shrink wrapping), but then no longer be free; // when this is called from emitPrologue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:63,Safety,avoid,avoid,63,"// Find a scratch register that we can use in the prologue. We avoid using; // callee-save registers since they may appear to be free when this is called; // from canUseAsPrologue (during shrink wrapping), but then no longer be free; // when this is called from emitPrologue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:303,Safety,detect,detect,303,"// We don't need this if we only have spills since there is no user facing; // scratch.; // TODO: If we know we don't have flat instructions earlier, we can omit; // this from the input registers.; //; // TODO: We only need to know if we access scratch space through a flat; // pointer. Because we only detect if flat instructions are used at all,; // this will be used more often than necessary on VI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:238,Security,access,access,238,"// We don't need this if we only have spills since there is no user facing; // scratch.; // TODO: If we know we don't have flat instructions earlier, we can omit; // this from the input registers.; //; // TODO: We only need to know if we access scratch space through a flat; // pointer. Because we only detect if flat instructions are used at all,; // this will be used more often than necessary on VI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:22,Performance,load,load,22,// Find unused reg to load flat scratch init into,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:3,Availability,Mask,Mask,3,// Mask the offset in [47:0] of the descriptor,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:9,Availability,down,down,9,// Shift down registers reserved for the scratch RSRC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:53,Availability,down,down,53,// We reserved the last registers for this. Shift it down to the end of those; // which were actually used.; //; // FIXME: It might be safer to use a pseudoregister before replacement.; // FIXME: We should be able to eliminate unused input registers. We only; // cannot do this for the resources required for scratch access. For now we; // skip over user SGPRs and may leave unused holes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:135,Safety,safe,safer,135,// We reserved the last registers for this. Shift it down to the end of those; // which were actually used.; //; // FIXME: It might be safer to use a pseudoregister before replacement.; // FIXME: We should be able to eliminate unused input registers. We only; // cannot do this for the resources required for scratch access. For now we; // skip over user SGPRs and may leave unused holes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:317,Security,access,access,317,// We reserved the last registers for this. Shift it down to the end of those; // which were actually used.; //; // FIXME: It might be safer to use a pseudoregister before replacement.; // FIXME: We should be able to eliminate unused input registers. We only; // cannot do this for the resources required for scratch access. For now we; // skip over user SGPRs and may leave unused holes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:249,Energy Efficiency,allocate,allocateSystemSGPRs,249,"// We found the SRSRC first because it needs four registers and has an; // alignment requirement. If the SRSRC that we found is clobbering with; // the scratch wave offset, which may be in a fixed SGPR or a free SGPR; // chosen by SITargetLowering::allocateSystemSGPRs, COPY the scratch; // wave offset to a free SGPR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:78,Deployability,update,update,78,"// Add the scratch wave offset into the scratch RSRC.; //; // We only want to update the first 48 bits, which is the base address; // pointer, without touching the adjacent 16 bits of flags. We know this add; // cannot carry-out from bit 47, otherwise the scratch allocation would be; // impossible to fit in the 48-bit global address space.; //; // TODO: Evaluate if it is better to just construct an SRD using the flat; // scratch init and some constants rather than update the one we are passed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:469,Deployability,update,update,469,"// Add the scratch wave offset into the scratch RSRC.; //; // We only want to update the first 48 bits, which is the base address; // pointer, without touching the adjacent 16 bits of flags. We know this add; // cannot carry-out from bit 47, otherwise the scratch allocation would be; // impossible to fit in the 48-bit global address space.; //; // TODO: Evaluate if it is better to just construct an SRD using the flat; // scratch init and some constants rather than update the one we are passed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:66,Safety,avoid,avoids,66,// Copy FP to the scratch register now and emit the CFI entry. It avoids; // the extra FP copy needed in the other two cases when FP is spilled to; // memory or to a VGPR lane.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:151,Energy Efficiency,allocate,allocated,151,"// If we need a base pointer, set it up here. It's whatever the value of; // the stack pointer is at this point. Any variable size objects will be; // allocated after this, so we can still use the base pointer to reference; // the incoming arguments.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:117,Modifiability,variab,variable,117,"// If we need a base pointer, set it up here. It's whatever the value of; // the stack pointer is at this point. Any variable size objects will be; // allocated after this, so we can still use the base pointer to reference; // the incoming arguments.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,"// Allocate spill slots for WWM reserved VGPRs.; // For chain functions, we only need to do this if we have calls to; // llvm.amdgcn.cs.chain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:125,Deployability,update,update,125,"// FIXME: The dead frame indices are replaced with a null register from; // the debug value instructions. We should instead, update it with the; // correct register value. But not sure the register value alone is",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:31,Energy Efficiency,allocate,allocated,31,"// At this point we've already allocated all spilled SGPRs to VGPRs if we; // can. Any remaining SGPR spills will go to memory, so move them back to the; // default stack.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:37,Availability,redundant,redundant,37,"// FIXME: The other checks should be redundant with allStackObjectsAreDead,; // but currently hasNonSpillStackObjects is set only from source; // allocas. Stack temps produced from legalization are not counted currently.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:37,Safety,redund,redundant,37,"// FIXME: The other checks should be redundant with allStackObjectsAreDead,; // but currently hasNonSpillStackObjects is set only from source; // allocas. Stack temps produced from legalization are not counted currently.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:48,Availability,avail,available,48,"// On gfx908, we had initially reserved highest available VGPR for AGPR; // copy. Now since we are done with RA, check if there exist an unused VGPR; // which is lower than the eariler reserved VGPR before RA. If one exist,; // use it for AGPR copy instead of one reserved before RA.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:130,Safety,avoid,avoid,130,// Reserve this newly identified VGPR (for AGPR copy); // reserved registers should already be frozen at this point; // so we can avoid calling MRI.freezeReservedRegs and just use; // MRI.reserveReg,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:36,Availability,avail,available,36,"// We initally reserved the highest available SGPR pair for long branches; // now, after RA, we shift down to a lower unused one if one exists",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:102,Availability,down,down,102,"// We initally reserved the highest available SGPR pair for long branches; // now, after RA, we shift down to a lower unused one if one exists",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:151,Availability,down,down,151,"// If LongBranchReservedReg is null then we didn't find a long branch; // and never reserved a register to begin with so there is nothing to; // shift down. Then if UnusedLowSGPR is null, there isn't available lower; // register to use so just keep the original one we set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:200,Availability,avail,available,200,"// If LongBranchReservedReg is null then we didn't find a long branch; // and never reserved a register to begin with so there is nothing to; // shift down. Then if UnusedLowSGPR is null, there isn't available lower; // register to use so just keep the original one we set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:138,Safety,predict,predict,138,"// hasFP only knows about stack objects that already exist. We're now; // determining the stack slots that will be created, so we have to predict; // them. Stack objects force FP usage with calls.; //; // Note a new VGPR CSR may be introduced if one is used for the spill, but we; // don't want to report it here.; //; // FIXME: Is this really hasReservedCallFrame?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:360,Deployability,pipeline,pipeline,360,// WRITELANE instructions used for SGPR spills can overwrite the inactive; // lanes of VGPRs and callee must spill and restore them even if they are; // marked Caller-saved.; // TODO: Handle this elsewhere at an early point. Walking through all MBBs; // here would be a bad heuristic. A better way should be by calling; // allocateWWMSpill during the regalloc pipeline whenever a physical; // register is allocated for the intended virtual registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:323,Energy Efficiency,allocate,allocateWWMSpill,323,// WRITELANE instructions used for SGPR spills can overwrite the inactive; // lanes of VGPRs and callee must spill and restore them even if they are; // marked Caller-saved.; // TODO: Handle this elsewhere at an early point. Walking through all MBBs; // here would be a bad heuristic. A better way should be by calling; // allocateWWMSpill during the regalloc pipeline whenever a physical; // register is allocated for the intended virtual registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:405,Energy Efficiency,allocate,allocated,405,// WRITELANE instructions used for SGPR spills can overwrite the inactive; // lanes of VGPRs and callee must spill and restore them even if they are; // marked Caller-saved.; // TODO: Handle this elsewhere at an early point. Walking through all MBBs; // here would be a bad heuristic. A better way should be by calling; // allocateWWMSpill during the regalloc pipeline whenever a physical; // register is allocated for the intended virtual registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:109,Performance,load,loads,109,// Do not save AGPRs prior to GFX90A because there was no easy way to do so.; // In gfx908 there was do AGPR loads and stores and thus spilling also; // require a temporary VGPR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:267,Energy Efficiency,allocate,allocate,267,"// We have to anticipate introducing CSR VGPR spills or spill of caller; // save VGPR reserved for SGPR spills as we now always create stack entry; // for it, if we don't have any stack objects already, since we require a FP; // if there is a call and stack. We will allocate a VGPR for SGPR spills if; // there are any SGPR spills. Whether they are CSR spills or otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:43,Energy Efficiency,allocate,allocated,43,"// We need the emergency stack slots to be allocated in range of the; // MUBUF/flat scratch immediate offset from the base register, so assign these; // first at the incoming SP position.; //; // TODO: We could try sorting the objects to find a hole in the first bytes; // rather than allocating as close to possible. This could save a lot of space; // on frames with alignment requirements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:25,Energy Efficiency,reduce,reduced,25,"// This is essentially a reduced version of hasFP for entry functions. Since the; // stack pointer is known 0 on entry to kernels, we never really need an FP; // register. We may need to initialize the stack pointer depending on the frame; // properties, which logically overlaps many of the cases where an ordinary; // function would require an FP.; // Also used for chain functions. While not technically entry functions, chain; // functions may need to set up a stack pointer in some situations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:216,Integrability,depend,depending,216,"// This is essentially a reduced version of hasFP for entry functions. Since the; // stack pointer is known 0 on entry to kernels, we never really need an FP; // register. We may need to initialize the stack pointer depending on the frame; // properties, which logically overlaps many of the cases where an ordinary; // function would require an FP.; // Also used for chain functions. While not technically entry functions, chain; // functions may need to set up a stack pointer in some situations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:261,Testability,log,logically,261,"// This is essentially a reduced version of hasFP for entry functions. Since the; // stack pointer is known 0 on entry to kernels, we never really need an FP; // register. We may need to initialize the stack pointer depending on the frame; // properties, which logically overlaps many of the cases where an ordinary; // function would require an FP.; // Also used for chain functions. While not technically entry functions, chain; // functions may need to set up a stack pointer in some situations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:101,Modifiability,variab,variable,101,"// We still need to initialize the SP if we're doing anything weird that; // references the SP, like variable sized stack objects.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:801,Energy Efficiency,schedul,scheduler,801,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:465,Performance,load,load,465,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:492,Performance,cache,cache,492,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:564,Safety,detect,detected,564,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:670,Safety,detect,detection,670,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:1048,Testability,log,logically,1048,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:33,Security,access,access,33,// Common:; // Instructions that access LDS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:81,Integrability,message,message,81,"// Instructions that are not allowed in a hard clause: SALU, export, branch,; // message, GDS, s_waitcnt and anything else not mentioned above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:160,Safety,safe,safe,160,"// Don't form VALU clauses. It's not clear what benefit they give, if any.; // In practice s_nop is the only internal instruction we're likely to see.; // It's safe to treat the rest as illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:37,Usability,clear,clear,37,"// Don't form VALU clauses. It's not clear what benefit they give, if any.; // In practice s_nop is the only internal instruction we're likely to see.; // It's safe to treat the rest as illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:134,Energy Efficiency,schedul,scheduler,134,"// Note that we lie to shouldClusterMemOps about the size of the; // cluster. When shouldClusterMemOps is called from the machine; // scheduler it limits the size of the cluster to avoid increasing; // register pressure too much, but this pass runs after register; // allocation so there is no need for that kind of limit.; // We also lie about the Offset and OffsetIsScalable parameters,; // as they aren't used in the SIInstrInfo implementation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:181,Safety,avoid,avoid,181,"// Note that we lie to shouldClusterMemOps about the size of the; // cluster. When shouldClusterMemOps is called from the machine; // scheduler it limits the size of the cluster to avoid increasing; // register pressure too much, but this pass runs after register; // allocation so there is no need for that kind of limit.; // We also lie about the Offset and OffsetIsScalable parameters,; // as they aren't used in the SIInstrInfo implementation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:3,Modifiability,Extend,Extend,3,// Extend the current clause.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:575,Security,access,access,575,"//===- SIInsertWaitcnts.cpp - Insert Wait Instructions --------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert wait instructions for memory reads and writes.; ///; /// Memory reads and writes are issued asynchronously, so we need to insert; /// S_WAITCNT instructions when we want to access any of their results or; /// overwrite any register that's used asynchronously.; ///; /// TODO: This pass currently keeps one timeline per hardware counter. A more; /// finely-grained approach that keeps one timeline per event type could; /// sometimes get away with generating weaker s_waitcnt instructions. For; /// example, when both SMEM and LDS are in flight and we need to wait for; /// the i-th-last LDS instruction, then an lgkmcnt(i) is actually sufficient,; /// but the pass will currently generate a conservative lgkmcnt(0) because; /// multiple event types are in flight.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:8,Integrability,message,message,8,// send message,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:308,Performance,load,load,308,// The mapping is:; // 0 .. SQ_MAX_PGM_VGPRS-1 real VGPRs; // SQ_MAX_PGM_VGPRS .. NUM_ALL_VGPRS-1 extra VGPR-like slots; // NUM_ALL_VGPRS .. NUM_ALL_VGPRS+SQ_MAX_PGM_SGPRS-1 real SGPRs; // We reserve a fixed number of VGPR slots in the scoring tables for; // special tokens like SCMEM_LDS (needed for buffer load to LDS).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:234,Deployability,update,updated,234,// Reserved slots for DS.; // Artificial register slots to track LDS writes into specific LDS locations; // if a location is known. When slots are exhausted or location is; // unknown use the first slot. The first slot is also always updated in; // addition to known location's slot to properly generate waits if dependent; // instruction's location is unknown.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:313,Integrability,depend,dependent,313,// Reserved slots for DS.; // Artificial register slots to track LDS writes into specific LDS locations; // if a location is known. When slots are exhausted or location is; // unknown use the first slot. The first slot is also always updated in; // addition to known location's slot to properly generate waits if dependent; // instruction's location is unknown.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:56,Availability,mask,masks,56,// Mapping from event to counter according to the table masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:97,Energy Efficiency,efficient,efficient,97,// wait_cnt scores for every vgpr.; // Keep track of the VgprUB and SgprUB to make merge at join efficient.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:22,Testability,log,logic,22,"// This abstracts the logic for generating and updating S_WAIT* instructions; // away from the analysis that determines where they are needed. This was; // done because the set of counters and instructions for waiting on them; // underwent a major shift with gfx12, sufficiently so that having this; // abstraction allows the main analysis logic to be simpler than it would; // otherwise have had to become.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:340,Testability,log,logic,340,"// This abstracts the logic for generating and updating S_WAIT* instructions; // away from the analysis that determines where they are needed. This was; // done because the set of counters and instructions for waiting on them; // underwent a major shift with gfx12, sufficiently so that having this; // abstraction allows the main analysis logic to be simpler than it would; // otherwise have had to become.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:352,Usability,simpl,simpler,352,"// This abstracts the logic for generating and updating S_WAIT* instructions; // away from the analysis that determines where they are needed. This was; // done because the set of counters and instructions for waiting on them; // underwent a major shift with gfx12, sufficiently so that having this; // abstraction allows the main analysis logic to be simpler than it would; // otherwise have had to become.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:117,Deployability,update,updated,117,"// Edits an existing sequence of wait count instructions according; // to an incoming Waitcnt value, which is itself updated to reflect; // any new wait count instructions which may need to be generated by; // WaitcntGenerator::createNewWaitcnt(). It will return true if any edits; // were made.; //; // This editing will usually be merely updated operands, but it may also; // delete instructions if the incoming Wait value indicates they are not; // needed. It may also remove existing instructions for which a wait; // is needed if it can be determined that it is better to generate new; // instructions later, as can happen on gfx12.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:340,Deployability,update,updated,340,"// Edits an existing sequence of wait count instructions according; // to an incoming Waitcnt value, which is itself updated to reflect; // any new wait count instructions which may need to be generated by; // WaitcntGenerator::createNewWaitcnt(). It will return true if any edits; // were made.; //; // This editing will usually be merely updated operands, but it may also; // delete instructions if the incoming Wait value indicates they are not; // needed. It may also remove existing instructions for which a wait; // is needed if it can be determined that it is better to generate new; // instructions later, as can happen on gfx12.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:27,Availability,mask,masks,27,// Returns an array of bit masks which can be used to map values in; // WaitEventType to corresponding counter values in InstCounterType.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:75,Integrability,message,message,75,// S_ENDPGM instructions before which we should insert a DEALLOC_VGPRS; // message.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:13,Security,access,access,13,// Maps VMEM access types to their corresponding WaitEventType.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:11,Performance,load,loads,11,"// LDS DMA loads are also stores, but on the LDS side. On the VMEM side; // these should use VM_CNT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:37,Security,access,access,37,// FLAT and SCRATCH instructions may access scratch. Other VMEM; // instructions do not.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:40,Deployability,update,update,40,// PendingEvents and ScoreUB need to be update regardless if this event; // changes the score of a register or not.; // Examples including vm_cnt when buffer-store or lgkm_cnt when send-message.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:186,Integrability,message,message,186,// PendingEvents and ScoreUB need to be update regardless if this event; // changes the score of a register or not.; // Examples including vm_cnt when buffer-store or lgkm_cnt when send-message.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:112,Deployability,patch,patching,112,"// For export the destination registers are really temps that; // can be used as the actual source after export patching, so; // we need to treat them like sources and set the EXP_CNT; // score.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:3,Deployability,update,updateVMCntOnly,3,"// updateVMCntOnly should only leave us with VGPRs; // MUBUF, MTBUF, MIMG, FlatGlobal, and FlatScratch only have VGPR/AGPR; // defs. That's required for a sane index into `VgprMemTypes` below",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:100,Performance,load,load,100,// MUBUF and FLAT LDS DMA operations need a wait on vmcnt before LDS; // written can be accessed. A load from LDS to VMEM does not need a wait.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:88,Security,access,accessed,88,// MUBUF and FLAT LDS DMA operations need a wait on vmcnt before LDS; // written can be accessed. A load from LDS to VMEM does not need a wait.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:175,Availability,avail,available,175,// Alias scope information gives a way to definitely identify an; // original memory object and practically produced in the module LDS; // lowering pass. If there is no scope available we will not be able; // to disambiguate LDS aliasing as after the module lowering all LDS; // is squashed into a single big object. Do not attempt to use one of; // the limited LDSDMAStores for something we will not be able to use; // anyway.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:51,Availability,redundant,redundant,51,"/// Simplify the waitcnt, in the sense of removing redundant counts, and return; /// whether a waitcnt instruction is needed at all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:51,Safety,redund,redundant,51,"/// Simplify the waitcnt, in the sense of removing redundant counts, and return; /// whether a waitcnt instruction is needed at all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:4,Usability,Simpl,Simplify,4,"/// Simplify the waitcnt, in the sense of removing redundant counts, and return; /// whether a waitcnt instruction is needed at all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:211,Availability,redundant,redundant,211,"// The number of outstanding events for this type, T, can be calculated; // as (UB - LB). If the current Count is greater than or equal to the number; // of outstanding events, then the wait for this counter is redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:211,Safety,redund,redundant,211,"// The number of outstanding events for this type, T, can be calculated; // as (UB - LB). If the current Count is greater than or equal to the number; // of outstanding events, then the wait for this counter is redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:35,Safety,avoid,avoid,35,// If a counter has been maxed out avoid overflow by waiting for; // MAX(CounterType) - 1 instead.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:3,Deployability,Update,Update,3,"// Update required wait count. If this is a soft waitcnt (= it was added; // by an earlier pass), it may be entirely removed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:3,Deployability,Update,Update,3,"// Update required wait count. If this is a soft waitcnt (= it was added; // by an earlier pass), it may be entirely removed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:353,Availability,redundant,redundant,353,"// Only keep an S_WAIT_LOADCNT_DSCNT if both counters actually need; // to be waited for. Otherwise, let the instruction be deleted so; // the appropriate single counter wait instruction can be inserted; // instead, when new S_WAIT_*CNT instructions are inserted by; // createNewWaitcnt(). As a side effect, resetting the wait counts will; // cause any redundant S_WAIT_LOADCNT or S_WAIT_DSCNT to be removed by; // the loop below that deals with single counter instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:353,Safety,redund,redundant,353,"// Only keep an S_WAIT_LOADCNT_DSCNT if both counters actually need; // to be waited for. Otherwise, let the instruction be deleted so; // the appropriate single counter wait instruction can be inserted; // instead, when new S_WAIT_*CNT instructions are inserted by; // createNewWaitcnt(). As a side effect, resetting the wait counts will; // cause any redundant S_WAIT_LOADCNT or S_WAIT_DSCNT to be removed by; // the loop below that deals with single counter instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:44,Performance,LOAD,LOADcnt,44,"// If it's known that both DScnt and either LOADcnt or STOREcnt (but not; // both) need to be waited for, ensure that there are no existing; // individual wait count instructions for these.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:276,Security,access,access,276,"/// Generate s_waitcnt instruction to be placed before cur_Inst.; /// Instructions of a given type are returned in order,; /// but instructions of different types can complete out of order.; /// We rely on this in-order completion; /// and simply assign a score to the memory access instructions.; /// We keep track of the active ""score bracket"" to determine; /// if an access of a memory read requires an s_waitcnt; /// and if so what the value of each counter is.; /// The ""score bracket"" is bound by the lower bound and upper bound; /// scores (*_score_LB and *_score_ub respectively).; /// If FlushVmCnt is true, that means that we want to generate a s_waitcnt to; /// flush the vmcnt counter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:370,Security,access,access,370,"/// Generate s_waitcnt instruction to be placed before cur_Inst.; /// Instructions of a given type are returned in order,; /// but instructions of different types can complete out of order.; /// We rely on this in-order completion; /// and simply assign a score to the memory access instructions.; /// We keep track of the active ""score bracket"" to determine; /// if an access of a memory read requires an s_waitcnt; /// and if so what the value of each counter is.; /// The ""score bracket"" is bound by the lower bound and upper bound; /// scores (*_score_LB and *_score_ub respectively).; /// If FlushVmCnt is true, that means that we want to generate a s_waitcnt to; /// flush the vmcnt counter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:240,Usability,simpl,simply,240,"/// Generate s_waitcnt instruction to be placed before cur_Inst.; /// Instructions of a given type are returned in order,; /// but instructions of different types can complete out of order.; /// We rely on this in-order completion; /// and simply assign a score to the memory access instructions.; /// We keep track of the active ""score bracket"" to determine; /// if an access of a memory read requires an s_waitcnt; /// and if so what the value of each counter is.; /// The ""score bracket"" is bound by the lower bound and upper bound; /// scores (*_score_LB and *_score_ub respectively).; /// If FlushVmCnt is true, that means that we want to generate a s_waitcnt to; /// flush the vmcnt counter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:124,Testability,test,tests,124,"// FIXME: This should have already been handled by the memory legalizer.; // Removing this currently doesn't affect any lit tests, but we need to; // verify that nothing was relying on this. The number of buffer invalidates; // being handled here should not be expanded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:148,Integrability,rout,routines,148,// All waits must be resolved at call return.; // NOTE: this could be improved with knowledge of all call sites or; // with knowledge of the called routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:156,Deployability,release,release,156,"// Identify S_ENDPGM instructions which may have to wait for outstanding VMEM; // stores. In this case it can be useful to send a message to explicitly; // release all VGPRs before the stores have completed, but it is only safe to; // do this if:; // * there are no outstanding scratch stores; // * we are not in Dynamic VGPR mode",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:130,Integrability,message,message,130,"// Identify S_ENDPGM instructions which may have to wait for outstanding VMEM; // stores. In this case it can be useful to send a message to explicitly; // release all VGPRs before the stores have completed, but it is only safe to; // do this if:; // * there are no outstanding scratch stores; // * we are not in Dynamic VGPR mode",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:223,Safety,safe,safe,223,"// Identify S_ENDPGM instructions which may have to wait for outstanding VMEM; // stores. In this case it can be useful to send a message to explicitly; // release all VGPRs before the stores have completed, but it is only safe to; // do this if:; // * there are no outstanding scratch stores; // * we are not in Dynamic VGPR mode",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:33,Testability,log,logic,33,// TODO: the following blocks of logic when we have fence.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:47,Performance,load,load,47,// LDS may have to wait for VMcnt after buffer load to LDS,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:50,Availability,mask,mask,50,// Export & GDS instructions do not read the EXEC mask until after the export; // is granted (which can occur well after the instruction is issued).; // The shader program must flush all EXP operations on the export-count; // before overwriting the EXEC mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:254,Availability,mask,mask,254,// Export & GDS instructions do not read the EXEC mask until after the export; // is granted (which can occur well after the instruction is issued).; // The shader program must flush all EXP operations on the export-count; // before overwriting the EXEC mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:184,Integrability,depend,dependency,184,// The function is going to insert a wait on everything in its prolog.; // This still needs to be careful if the call target is a load (e.g. a GOT; // load). We also need to check WAW dependency with saved PC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:130,Performance,load,load,130,// The function is going to insert a wait on everything in its prolog.; // This still needs to be careful if the call target is a load (e.g. a GOT; // load). We also need to check WAW dependency with saved PC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:151,Performance,load,load,151,// The function is going to insert a wait on everything in its prolog.; // This still needs to be careful if the call target is a load (e.g. a GOT; // load). We also need to check WAW dependency with saved PC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:304,Performance,load,load,304,"// FIXME: Should not be relying on memoperands.; // Look at the source operands of every instruction to see if; // any of them results from a previous memory operation that affects; // its current usage. If so, an s_waitcnt instruction needs to be; // emitted.; // If the source operand was defined by a load, add the s_waitcnt; // instruction.; //; // Two cases are handled for destination operands:; // 1) If the destination operand was defined by a load, add the s_waitcnt; // instruction to guarantee the right WAW order.; // 2) If a destination operand that was used by a recent export/store ins,; // add s_waitcnt on exp_cnt to guarantee the WAR order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:452,Performance,load,load,452,"// FIXME: Should not be relying on memoperands.; // Look at the source operands of every instruction to see if; // any of them results from a previous memory operation that affects; // its current usage. If so, an s_waitcnt instruction needs to be; // emitted.; // If the source operand was defined by a load, add the s_waitcnt; // instruction.; //; // Two cases are handled for destination operands:; // 1) If the destination operand was defined by a load, add the s_waitcnt; // instruction to guarantee the right WAW order.; // 2) If a destination operand that was used by a recent export/store ins,; // add s_waitcnt on exp_cnt to guarantee the WAR order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:26,Performance,load,load,26,// No need to wait before load from VMEM to LDS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:88,Energy Efficiency,schedul,scheduler,88,"// TODO: Remove this work-around, enable the assert for Bug 457939; // after fixing the scheduler. Also, the Shader Compiler code is; // independent of target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:45,Testability,assert,assert,45,"// TODO: Remove this work-around, enable the assert for Bug 457939; // after fixing the scheduler. Also, the Shader Compiler code is; // independent of target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:30,Performance,LOAD,LOADcnt,30,"// Add a waitcnt to flush the LOADcnt, SAMPLEcnt and BVHcnt counters at the; // end of the given block if needed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:88,Availability,redundant,redundant,88,// Try to merge the required wait with preexisting waitcnt instructions.; // Also erase redundant waitcnt.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:88,Safety,redund,redundant,88,// Try to merge the required wait with preexisting waitcnt instructions.; // Also erase redundant waitcnt.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:89,Security,access,access,89,// If there are no memory operands then conservatively assume the flat; // operation may access VMEM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:300,Usability,simpl,simply,300,"// See if any memory operand specifies an address space that involves VMEM.; // Flat operations only supported FLAT, LOCAL (LDS), or address spaces; // involving VMEM such as GLOBAL, CONSTANT, PRIVATE (SCRATCH), etc. The REGION; // (GDS) address space is not supported by flat operations. Therefore, simply; // return true unless only the LDS address space is found.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:89,Security,access,access,89,// If there are no memory operands then conservatively assume the flat; // operation may access LDS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:31,Security,access,access,31,// SCRATCH instructions always access scratch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:29,Security,access,access,29,// GLOBAL instructions never access scratch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:89,Security,access,access,89,// If there are no memory operands then conservatively assume the flat; // operation may access scratch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:81,Deployability,update,update,81,"// Now look at the instruction opcode. If it is a memory access; // instruction, update the upper-bound of the appropriate counter's; // bracket and the destination operand scores.; // TODO: Use the (TSFlags & SIInstrFlags::DS_CNT) property everywhere.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:57,Security,access,access,57,"// Now look at the instruction opcode. If it is a memory access; // instruction, update the upper-bound of the appropriate counter's; // bracket and the destination operand scores.; // TODO: Use the (TSFlags & SIInstrFlags::DS_CNT) property everywhere.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:32,Security,access,access,32,// A Flat memory operation must access at least one address space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:183,Integrability,depend,dependency,183,"// This is a flat memory operation that access both VMEM and LDS, so note it; // - it will require that both the VM and LGKM be flushed to zero if it is; // pending when a VM or LGKM dependency occurs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:40,Security,access,access,40,"// This is a flat memory operation that access both VMEM and LDS, so note it; // - it will require that both the VM and LGKM be flushed to zero if it is; // pending when a VM or LGKM dependency occurs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:111,Performance,load,load,111,// vccz could be incorrect at a basic block boundary if a predecessor wrote; // to vcc and then issued an smem load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:49,Deployability,update,update,49,"// Up to gfx9, writes to vcc_lo and vcc_hi don't update vccz.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:96,Safety,detect,detect,96,"// There is a hardware bug on CI/SI where SMRD instruction may corrupt; // vccz bit, so when we detect that an instruction may read from a; // corrupt vccz bit, we need to:; // 1. Insert s_waitcnt lgkm(0) to wait for all outstanding SMRD; // operations to complete.; // 2. Restore the correct value of vccz by writing the current value; // of vcc back to vcc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:31,Performance,load,loads,31,"// No need to handle invariant loads when avoiding WAR conflicts, as; // there cannot be a vector store to the same memory location.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:42,Safety,avoid,avoiding,42,"// No need to handle invariant loads when avoiding WAR conflicts, as; // there cannot be a vector store to the same memory location.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:50,Energy Efficiency,schedul,scheduler,50,// TODO: Remove this work-around after fixing the scheduler and enable the; // assert above.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:79,Testability,assert,assert,79,// TODO: Remove this work-around after fixing the scheduler and enable the; // assert above.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:80,Deployability,update,updated,80,"// Restore the vccz bit. Any time a value is written to vcc, the vcc; // bit is updated, so we can restore the bit by reading the value of; // vcc and then writing it back to the register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:194,Performance,load,load,194,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:261,Performance,load,loaded,261,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:364,Performance,load,load,364,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:381,Performance,load,loaded,381,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:483,Performance,load,loaded,483,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:33,Performance,load,loaded,33,"// If we find a register that is loaded inside the loop, 1. and 2.; // are invalidated and we can exit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:87,Performance,load,loaded,87,"// If at least one of Op's registers is in the score brackets, the; // value is likely loaded outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:8,Performance,load,load,8,// VMem load vgpr def,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:33,Performance,load,loaded,33,"// If we find a register that is loaded inside the loop, 1. and 2.; // are invalidated and we can exit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:212,Energy Efficiency,schedul,schedule,212,// Wait for any outstanding memory operations that the input registers may; // depend on. We can't track them and it's better to do the wait after the; // costly call sequence.; // TODO: Could insert earlier and schedule more liberally with operations; // that only use caller preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:79,Integrability,depend,depend,79,// Wait for any outstanding memory operations that the input registers may; // depend on. We can't track them and it's better to do the wait after the; // costly call sequence.; // TODO: Could insert earlier and schedule more liberally with operations; // that only use caller preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:34,Performance,cache,cache,34,"// If scalar writes are used, the cache must be flushed or else the next; // wave to reuse the same scratch memory can be clobbered.; //; // Insert s_dcache_wb at wave termination points if there were any scalar; // stores, and only if the cache hasn't already been flushed. This could; // be improved by looking across blocks for flushes in postdominating; // blocks from the stores but an explicitly requested flush is probably; // very rare.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:240,Performance,cache,cache,240,"// If scalar writes are used, the cache must be flushed or else the next; // wave to reuse the same scratch memory can be clobbered.; //; // Insert s_dcache_wb at wave termination points if there were any scalar; // stores, and only if the cache hasn't already been flushed. This could; // be improved by looking across blocks for flushes in postdominating; // blocks from the stores but an explicitly requested flush is probably; // very rare.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:23,Integrability,message,messages,23,// Insert DEALLOC_VGPR messages before previously identified S_ENDPGM; // instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:149,Testability,test,tests,149,// Must be at least 4 to be able to branch over minimum unconditional branch; // code. This is only for making it possible to write reasonably small tests for; // long branches.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:390,Safety,safe,safe,390,"// Normally VALU use of exec would block the rematerialization, but that; // is OK in this case to have an implicit exec read as all VALU do.; // We really want all of the generic logic for this except for this.; // Another potential implicit use is mode register. The core logic of; // the RA will not attempt rematerialization if mode is set anywhere; // in the function, otherwise it is safe since mode is not changed.; // There is difference to generic method which does not allow; // rematerialization if there are virtual register uses. We allow this,; // therefore this method includes SOP instructions as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:180,Testability,log,logic,180,"// Normally VALU use of exec would block the rematerialization, but that; // is OK in this case to have an implicit exec read as all VALU do.; // We really want all of the generic logic for this except for this.; // Another potential implicit use is mode register. The core logic of; // the RA will not attempt rematerialization if mode is set anywhere; // in the function, otherwise it is safe since mode is not changed.; // There is difference to generic method which does not allow; // rematerialization if there are virtual register uses. We allow this,; // therefore this method includes SOP instructions as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:274,Testability,log,logic,274,"// Normally VALU use of exec would block the rematerialization, but that; // is OK in this case to have an implicit exec read as all VALU do.; // We really want all of the generic logic for this except for this.; // Another potential implicit use is mode register. The core logic of; // the RA will not attempt rematerialization if mode is set anywhere; // in the function, otherwise it is safe since mode is not changed.; // There is difference to generic method which does not allow; // rematerialization if there are virtual register uses. We allow this,; // therefore this method includes SOP instructions as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:59,Integrability,depend,depends,59,// Returns true if the scalar result of a VALU instruction depends on exec.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:42,Availability,mask,masked,42,// Ignore comparisons which are only used masked with exec.; // This allows some hoisting/sinking of VALU comparisons.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:34,Availability,mask,mask,34,// Allow sinking if MI edits lane mask (divergent i1 in sgpr).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:31,Performance,load,loads,31,// Make sure both are actually loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:48,Performance,load,load,48,// A mayLoad instruction without a def is not a load. Likely a prefetch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:36,Usability,simpl,simplicity,36,// Skip read2 / write2 variants for simplicity.; // TODO: We should report true if the used offsets are adjacent (excluded; // st64 versions).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:32,Performance,load,loads,32,"// XXX - be careful of dataless loads; // getNamedOperandIdx returns the index for MachineInstrs. Since they; // include the output in the operand list, but SDNodes don't, we need to; // subtract the index by one.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:17,Performance,cache,cache,17,// Skip time and cache invalidation instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:23,Security,access,access,23,// MUBUF and MTBUF can access the same addresses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:90,Performance,load,load,90,// The 2 offset instructions use offset0 and offset1 instead. We can treat; // these as a load with a single offset if the 2 offsets are consecutive.; // We will use this for some partially aligned loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:198,Performance,load,loads,198,// The 2 offset instructions use offset0 and offset1 instead. We can treat; // these as a load with a single offset if the 2 offsets are consecutive.; // We will use this for some partially aligned loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:139,Security,access,access,139,"// Only examine the first ""base"" operand of each instruction, on the; // assumption that it represents the real base address of the memory access.; // Other operands are typically offsets or indices from this base address.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:80,Performance,load,loaded,80,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:207,Performance,perform,performance,207,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:335,Performance,load,loads,335,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:377,Performance,load,loads,377,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:457,Performance,Load,LoadSize,457,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:481,Performance,Load,LoadSize,481,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:534,Performance,Load,LoadSize,534,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:587,Performance,Load,LoadSize,587,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:642,Performance,Load,LoadSize,642,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:691,Performance,Load,LoadSize,691,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:15,Safety,avoid,avoid,15,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:292,Safety,avoid,avoids,292,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:351,Safety,avoid,avoids,351,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:313,Energy Efficiency,schedul,schedule,313,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:63,Performance,load,load,63,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:95,Performance,load,loads,95,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:245,Performance,Load,Loads,245,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:416,Performance,load,loads,416,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:89,Energy Efficiency,schedul,schedule,89,"// If we have less than 16 loads in a row, and the offsets are within 64; // bytes, then schedule together.; // A cacheline is 64 bytes (for global memory).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:27,Performance,load,loads,27,"// If we have less than 16 loads in a row, and the offsets are within 64; // bytes, then schedule together.; // A cacheline is 64 bytes (for global memory).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:114,Performance,cache,cacheline,114,"// If we have less than 16 loads in a row, and the offsets are within 64; // bytes, then schedule together.; // A cacheline is 64 bytes (for global memory).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:47,Safety,avoid,avoid,47,"// First try to find defining accvgpr_write to avoid temporary registers.; // In the case of copies of overlapping AGPRs, we conservatively do not; // reuse previous accvgpr_writes. Otherwise, we may incorrectly pick up; // an accvgpr_write used for this same copy due to implicit-defs",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:100,Safety,safe,safe,100,// Check that register source operand is not clobbered before MI.; // Immediate operands are always safe to propagate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:33,Energy Efficiency,allocate,allocated,33,// Registers in the sequence are allocated contiguously so we can just; // use register number to pick one of three round-robin temps.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:44,Safety,avoid,avoid,44,// FIXME: Pass should maintain scavenger to avoid scan through the block on; // every AGPR spill.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:81,Safety,hazard,hazard,81,"// SI_RETURN_TO_EPILOG is a fallthrough to code outside of the function. The; // hazard, even if one exist, won't really be visible. Should we handle it?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:26,Performance,optimiz,optimize,26,// FIXME: We may possibly optimize the COPY once we find ways to make LLVM; // optimizations (mainly Register Coalescer) aware of WWM register liveness.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:79,Performance,optimiz,optimizations,79,// FIXME: We may possibly optimize the COPY once we find ways to make LLVM; // optimizations (mainly Register Coalescer) aware of WWM register liveness.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:80,Energy Efficiency,schedul,scheduler,80,// Create a bundle so these instructions won't be re-ordered by the; // post-RA scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:139,Modifiability,variab,variable,139,"// What we want here is an offset from the value returned by s_getpc (which; // is the address of the s_add_u32 instruction) to the global variable, but; // since the encoding of $symbol starts 4 bytes after the start of the; // s_add_u32 instruction, we end up with an offset that is 4 bytes too; // small. This requires us to add 4 to the global variable offset in order; // to compute the correct address. Similarly for the s_addc_u32 instruction,; // the encoding of $symbol starts 12 bytes after the start of the s_add_u32; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:348,Modifiability,variab,variable,348,"// What we want here is an offset from the value returned by s_getpc (which; // is the address of the s_add_u32 instruction) to the global variable, but; // since the encoding of $symbol starts 4 bytes after the start of the; // s_add_u32 instruction, we end up with an offset that is 4 bytes too; // small. This requires us to add 4 to the global variable offset in order; // to compute the correct address. Similarly for the s_addc_u32 instruction,; // the encoding of $symbol starts 12 bytes after the start of the s_add_u32; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:38,Modifiability,extend,extend,38,"// Fix up hardware that does not sign-extend the 48-bit PC value by; // inserting: s_sext_i32_i16 reghi, reghi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:38,Modifiability,extend,extend,38,"// Fix up hardware that does not sign-extend the 48-bit PC value by; // inserting: s_sext_i32_i16 dsthi, dsthi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:59,Deployability,update,updated,59,"// Use a smaller load with the desired size, possibly with updated offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:17,Performance,load,load,17,"// Use a smaller load with the desired size, possibly with updated offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:65,Safety,avoid,avoid,65,// If we've previously reserved a register for long branches; // avoid running the scavenger and just use those registers,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:25,Availability,avail,available,25,"// 64-bit select is only available for SALU.; // TODO: Split 96-bit into 64-bit and 32-bit, not 3x 32-bit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:56,Modifiability,rewrite,rewrite,56,"// V_FMAMK_F16_t16 takes VGPR_32_Lo128 operands, so the rewrite; // would also require restricting their register classes. For now; // just bail out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:56,Modifiability,rewrite,rewrite,56,"// V_FMAAK_F16_t16 takes VGPR_32_Lo128 operands, so the rewrite; // would also require restricting their register classes. For now; // just bail out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:241,Security,access,accesses,241,"// TODO: Should we check the address space from the MachineMemOperand? That; // would allow us to distinguish objects we know don't alias based on the; // underlying address space, even if it was lowered to a different one,; // e.g. private accesses lowered to use MUBUF instructions on a scratch; // buffer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:22,Safety,safe,safe,22,// It's not generally safe to move VALU instructions across these since it will; // start using the register as a base index rather than directly.; // XXX - Why isn't hasSideEffects sufficient for these?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:323,Energy Efficiency,schedul,scheduled,323,// Skipping the check for SP writes in the base implementation. The reason it; // was added was apparently due to compile time concerns.; //; // TODO: Do we really want this barrier? It triggers unnecessary hazard nops; // but is probably avoidable.; // Copied from base implementation.; // Terminators and labels can't be scheduled around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:207,Safety,hazard,hazard,207,// Skipping the check for SP writes in the base implementation. The reason it; // was added was apparently due to compile time concerns.; //; // TODO: Do we really want this barrier? It triggers unnecessary hazard nops; // but is probably avoidable.; // Copied from base implementation.; // Terminators and labels can't be scheduled around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:239,Safety,avoid,avoidable,239,// Skipping the check for SP writes in the base implementation. The reason it; // was added was apparently due to compile time concerns.; //; // TODO: Do we really want this barrier? It triggers unnecessary hazard nops; // but is probably avoidable.; // Copied from base implementation.; // Terminators and labels can't be scheduled around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:140,Energy Efficiency,schedul,scheduling,140,"// Target-independent instructions do not have an implicit-use of EXEC, even; // when they operate on VGPRs. Treating EXEC modifications as scheduling; // boundaries prevents incorrect movements of such instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:108,Availability,mask,mask,108,"// These instructions cause shader I/O that may cause hardware lockups; // when executed with an empty EXEC mask.; //; // Note: exp with VM = DONE = 0 is automatically skipped by hardware when; // EXEC = 0, but checking for that case here seems not worth it; // given the typical code patterns.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:224,Safety,avoid,avoid,224,"// These are like SALU instructions in terms of effects, so it's questionable; // whether we should return true for those.; //; // However, executing them with EXEC = 0 causes them to operate on undefined; // data, which we avoid by returning true here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:40,Availability,mask,mask,40,// This likely will be a condition code mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:21,Safety,safe,safely,21,// Implicit uses may safely overlap true operands,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:83,Energy Efficiency,schedul,scheduler,83,// Allow additional implicit operands. This allows a fixup done by the post; // RA scheduler where the main implicit operand is killed and implicit-defs; // are added for sub-registers that remain live after this instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:19,Security,access,access,19,// FIXME: This can access off the end of the operands() array.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:26,Modifiability,extend,extended,26,"// FIXME: We can use sign extended 64-bit literals, but only for signed; // operands. At the moment we do not know if an operand is signed.; // Such operand will be encoded as its low 32 bits and then either; // correctly sign extended or incorrectly zero extended by HW.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:227,Modifiability,extend,extended,227,"// FIXME: We can use sign extended 64-bit literals, but only for signed; // operands. At the moment we do not know if an operand is signed.; // Such operand will be encoded as its low 32 bits and then either; // correctly sign extended or incorrectly zero extended by HW.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:256,Modifiability,extend,extended,256,"// FIXME: We can use sign extended 64-bit literals, but only for signed; // operands. At the moment we do not know if an operand is signed.; // Such operand will be encoded as its low 32 bits and then either; // correctly sign extended or incorrectly zero extended by HW.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:139,Performance,load,loads,139,"// If the pointer is store in VGPRs, then we need to move them to; // SGPRs using v_readfirstlane. This is safe because we only select; // loads with uniform pointers to SMRD instruction so we know the; // pointer value is uniform.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:107,Safety,safe,safe,107,"// If the pointer is store in VGPRs, then we need to move them to; // SGPRs using v_readfirstlane. This is safe because we only select; // loads with uniform pointers to SMRD instruction so we know the; // pointer value is uniform.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Usability,Clear,Clear,3,// Clear use list from the old vaddr holding a zero register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update the use list with the pointer we have just moved from vaddr to; // saddr position. Otherwise new vaddr will be missing from the use list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:81,Testability,assert,asserts,81,"// removeOperand doesn't try to fixup tied operand indexes at it goes, so; // it asserts. Untie the operands for now and retie them afterwards.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:49,Integrability,wrap,wrapped,49,"// Emit the actual waterfall loop, executing the wrapped instruction for each; // unique value of \p ScalarOps across all lanes. In the best case we execute 1; // iteration, in the worst case we execute 64 (once per lane).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update ScalarOp operand to use the SGPR ScalarOp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update ScalarOp operand to use the SGPR ScalarOp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,"// Update EXEC to matching lanes, saving original to SaveExec.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,"// Update EXEC, switch all done bits to 0 and all todo bits to 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:17,Availability,mask,mask,17,// Save the EXEC mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,"// Update dominators. We know that MBB immediately dominates LoopBB, that; // LoopBB immediately dominates BodyBB, and BodyBB immediately dominates; // RemainderBB. RemainderBB immediately dominates all of the successors; // transferred to it from MBB that MBB used to properly dominate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:20,Availability,mask,mask,20,// Restore the EXEC mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update all the operands so they have the same type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Safety,Avoid,Avoid,3,// Avoid creating no-op copies with the same src and dst reg class. These; // confuse some of the machine passes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update all the operands so they are VGPR register classes. These may; // not be the same register class because REG_SEQUENCE supports mixing; // subregister index types e.g. sub0_sub1 + sub2 + sub3,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:157,Security,access,access,157,"// Legalize MIMG/VIMAGE/VSAMPLE and MUBUF/MTBUF for shaders.; //; // Shaders only generate MUBUF/MTBUF instructions via intrinsics or via; // scratch memory access. In both cases, the legalization never involves; // conversion to the addr64 form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:70,Safety,avoid,avoid,70,"// Legalize a VGPR Rsrc; //; // If the instruction is _ADDR64, we can avoid a waterfall by extracting; // the base pointer from the VGPR Rsrc, adding it to the VAddr, then using; // a zero-value SRsrc.; //; // If the instruction is _OFFSET (both idxen and offen disabled), and we; // support ADDR64 instructions, we can convert to ADDR64 and do the same as; // above.; //; // Otherwise we are on non-ADDR64 hardware, and/or we have; // idxen/offen/bothen and we fall back to a waterfall loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:18,Performance,load,load,18,// Regular buffer load / store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:82,Modifiability,extend,extended,82,// This is a special case of s_mul_u64 where all the operands are either; // zero extended or sign extended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:99,Modifiability,extend,extended,99,// This is a special case of s_mul_u64 where all the operands are either; // zero extended or sign extended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Usability,Clear,Clear,3,// Clear unused bits of vcc,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update the destination register class.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:216,Availability,mask,mask,216,"// SCC def is not a copy; // Insert a trivial select instead of creating a copy, because a copy from; // SCC would semantically mean just copying a single bit, but we may need; // the result to be a vector condition mask that needs preserving.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:97,Performance,perform,perform,97,"// Using the identity !(x ^ y) == (!x ^ y) == (x ^ !y), we can; // invert either source and then perform the XOR. If either source is a; // scalar register, then we can leave the inversion on the scalar unit to; // achieve a better distribution of scalar and vector instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:115,Performance,perform,performance,115,"// Set MTYPE = 2 (MTYPE_UC = uncached). GFX9 doesn't have this.; // BTW, it disables TC L2 and therefore decreases performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:72,Usability,Clear,Clear,72,"// If TID_ENABLE is set, DATA_FORMAT specifies stride bits [14:17].; // Clear them unless we want a huge stride.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:32,Energy Efficiency,schedul,scheduler,32,/// This is used by the post-RA scheduler (SchedulePostRAList.cpp). The; /// post-RA version of misched uses CreateTargetMIHazardRecognizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:43,Energy Efficiency,Schedul,SchedulePostRAList,43,/// This is used by the post-RA scheduler (SchedulePostRAList.cpp). The; /// post-RA version of misched uses CreateTargetMIHazardRecognizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:16,Safety,hazard,hazard,16,/// This is the hazard recognizer used at -O0 by the PostRAHazardRecognizer; /// pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:31,Energy Efficiency,schedul,scheduling,31,// Called during:; // - pre-RA scheduling and post-RA scheduling,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:54,Energy Efficiency,schedul,scheduling,54,// Called during:; // - pre-RA scheduling and post-RA scheduling,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:101,Energy Efficiency,schedul,scheduling,101,// Borrowed from Arm Target; // We would like to restrict this hazard recognizer to only; // post-RA scheduling; we can tell that we're post-RA because we don't; // track VRegLiveness.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:63,Safety,hazard,hazard,63,// Borrowed from Arm Target; // We would like to restrict this hazard recognizer to only; // post-RA scheduling; we can tell that we're post-RA because we don't; // track VRegLiveness.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:372,Availability,mask,mask,372,"// We need to handle instructions which may be inserted during register; // allocation to handle the prolog. The initial prolog instruction may have; // been separated from the start of the block by spills and copies inserted; // needed by the prolog. However, the insertions for scalar registers can; // always be placed at the BB top as they are independent of the exec mask; // value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:6,Availability,avail,available,6,"// If available, prefer to use vcc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:54,Performance,load,loads,54,"// Try to keep the same value in SOffset for adjacent loads, so that; // the corresponding register contents can be re-used.; //; // Load values with all low-bits (except for alignment bits) set into; // SOffset, so that a larger range of values can be covered using; // s_movk_i32.; //; // Atomic operations fail to work correctly when individual address; // components are unaligned, even if their sum is aligned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:133,Performance,Load,Load,133,"// Try to keep the same value in SOffset for adjacent loads, so that; // the corresponding register contents can be re-used.; //; // Load values with all low-bits (except for alignment bits) set into; // SOffset, so that a larger range of values can be covered using; // s_movk_i32.; //; // Atomic operations fail to work correctly when individual address; // components are unaligned, even if their sum is aligned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Integrability,Depend,Depending,3,"// Depending on the used address space and instructions, some immediate offsets; // are allowed and some are not.; // Pre-GFX12, flat instruction offsets can only be non-negative, global and; // scratch instruction offsets can also be negative. On GFX12, offsets can be; // negative for all variants.; //; // There are several bugs related to these offsets:; // On gfx10.1, flat instructions that go into the global address space cannot; // use an offset.; //; // For scratch instructions, the address can be either an SGPR or a VGPR.; // The following offsets can be used, depending on the architecture (x means; // cannot be used):; // +----------------------------+------+------+; // | Address-Mode | SGPR | VGPR |; // +----------------------------+------+------+; // | gfx9 | | |; // | negative, 4-aligned offset | x | ok |; // | negative, unaligned offset | x | ok |; // +----------------------------+------+------+; // | gfx10 | | |; // | negative, 4-aligned offset | ok | ok |; // | negative, unaligned offset | ok | x |; // +----------------------------+------+------+; // | gfx10.3 | | |; // | negative, 4-aligned offset | ok | ok |; // | negative, unaligned offset | ok | ok |; // +----------------------------+------+------+; //; // This function ignores the addressing mode, so if an offset cannot be used in; // one addressing mode, it is considered illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:574,Integrability,depend,depending,574,"// Depending on the used address space and instructions, some immediate offsets; // are allowed and some are not.; // Pre-GFX12, flat instruction offsets can only be non-negative, global and; // scratch instruction offsets can also be negative. On GFX12, offsets can be; // negative for all variants.; //; // There are several bugs related to these offsets:; // On gfx10.1, flat instructions that go into the global address space cannot; // use an offset.; //; // For scratch instructions, the address can be either an SGPR or a VGPR.; // The following offsets can be used, depending on the architecture (x means; // cannot be used):; // +----------------------------+------+------+; // | Address-Mode | SGPR | VGPR |; // +----------------------------+------+------+; // | gfx9 | | |; // | negative, 4-aligned offset | x | ok |; // | negative, unaligned offset | x | ok |; // +----------------------------+------+------+; // | gfx10 | | |; // | negative, 4-aligned offset | ok | ok |; // | negative, unaligned offset | ok | x |; // +----------------------------+------+------+; // | gfx10.3 | | |; // | negative, 4-aligned offset | ok | ok |; // | negative, unaligned offset | ok | ok |; // +----------------------------+------+------+; //; // This function ignores the addressing mode, so if an offset cannot be used in; // one addressing mode, it is considered illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:28,Energy Efficiency,power,power,28,// Use signed division by a power of two to truncate towards 0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:140,Safety,risk,risky,140,// These opcodes use indirect register addressing so; // they need special handling by codegen (currently missing).; // Therefore it is too risky to allow these opcodes; // to be selected by dpp combiner or sdwa peepholer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:22,Availability,mask,masks,22,// We don't check reg masks here as they're used only on calls:; // 1. EXEC is only considered const within one BB; // 2. Call should be a terminator instruction if present in a BB,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Performance,Load,Loads,3,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:101,Performance,load,load,101,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:188,Performance,load,loads,188,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:238,Performance,load,loads,238,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Performance,Load,Loads,3,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:101,Performance,load,load,101,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:43,Integrability,Interface,Interface,43,"//===- SIInstrInfo.h - SI Instruction Info Interface ------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for SIInstrInfo.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:395,Integrability,Interface,Interface,395,"//===- SIInstrInfo.h - SI Instruction Info Interface ------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for SIInstrInfo.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:30,Performance,load,load,30,/// Mark the MMO of a uniform load if there are no potentially clobbering stores; /// on any path from the start of an entry function to this load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:142,Performance,load,load,142,/// Mark the MMO of a uniform load if there are no potentially clobbering stores; /// on any path from the start of an entry function to this load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:22,Performance,load,load,22,/// Mark the MMO of a load as the last use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:39,Security,access,accesses,39,"// Is a FLAT encoded instruction which accesses a specific segment,; // i.e. global_* or scratch_*.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:54,Integrability,depend,depend,54,"/// Returns true if the instruction could potentially depend on the value of; /// exec. If false, exec dependencies may safely be ignored.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:103,Integrability,depend,dependencies,103,"/// Returns true if the instruction could potentially depend on the value of; /// exec. If false, exec dependencies may safely be ignored.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:120,Safety,safe,safely,120,"/// Returns true if the instruction could potentially depend on the value of; /// exec. If false, exec dependencies may safely be ignored.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:123,Security,validat,validate,123,/// Check if \p MO would be a valid operand for the given operand; /// definition \p OpInfo. Note this does not attempt to validate constant bus; /// restrictions (e.g. literal constant usage).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:152,Deployability,update,updated,152,"/// Legalize all operands in this instruction. This function may create new; /// instructions and control-flow around \p MI. If present, \p MDT is; /// updated.; /// \returns A new basic block that contains \p MI if new blocks were created.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:213,Deployability,update,updated,213,"/// Replace the instructions opcode with the equivalent VALU; /// opcode. This function will also move the users of MachineInstruntions; /// in the \p WorkList to the VALU if necessary. If present, \p MDT is; /// updated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:118,Modifiability,extend,extend,118,"// The boolean content concept here is too inflexible. Compares only ever; // really produce a 1-bit result. Any copy/extend from these will turn into a; // select, and zext/1 or sext/-1 are equally cheap. Arbitrarily choose 0/1, as; // it's what most targets use.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Performance,LOAD,LOAD,19,// We only support LOAD/STORE and vector manipulation ops for vectors; // with > 4 elements.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:239,Performance,load,load,239,"// TODO: For dynamic 64-bit vector inserts/extracts, should emit a pseudo that; // is expanded to avoid having two separate loops in case the index is a VGPR.; // Most operations are naturally 32-bit vector operations. We only support; // load and store of i64 vectors, so promote v2i64 vector operations to v4i32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:98,Safety,avoid,avoid,98,"// TODO: For dynamic 64-bit vector inserts/extracts, should emit a pseudo that; // is expanded to avoid having two separate loops in case the index is a VGPR.; // Most operations are naturally 32-bit vector operations. We only support; // load and store of i64 vectors, so promote v2i64 vector operations to v4i32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid stack access for these.; // TODO: Generalize to more vector types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:15,Security,access,access,15,// Avoid stack access for these.; // TODO: Generalize to more vector types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:27,Availability,failure,failure,27,"// We can't return success/failure, only the old value,; // let LLVM add the comparison",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:324,Energy Efficiency,reduce,reduce,324,"// We only really have 32-bit BFE instructions (and 16-bit on VI).; //; // On SI+ there are 64-bit BFEs, but they are scalar only and there isn't any; // effort to match them now. We want this to be false for i64 cases when the; // extraction isn't restricted to the upper or lower half. Ideally we would; // have some pass reduce 64-bit extracts to 32-bit if possible. Extracts that; // span the midpoint are probably relatively rare, so don't worry about them; // for now.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:69,Safety,avoid,avoiding,69,"// These are really only legal for ieee_mode functions. We should be avoiding; // them for functions that don't have ieee_mode enabled, so just say they are; // legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:9,Performance,Load,Load,9,// F16 - Load/Store Actions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Performance,Load,Load,10,// BF16 - Load/Store Actions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:37,Safety,avoid,avoids,37,"// This isn't really legal, but this avoids the legalizer unrolling it (and; // allows matching fneg (fabs x) patterns)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:171,Usability,clear,clear,171,"/// Map address space 7 to MVT::v5i32 because that's its in-memory; /// representation. This return value is vector-typed because there is no; /// MVT::i160 and it is not clear if one can be added. While this could; /// cause issues during codegen, these address space 7 pointers will be; /// rewritten away by then. Therefore, we can return MVT::v5i32 in order; /// to allow pre-codegen passes that query TargetTransformInfo, often for cost; /// modeling, to work.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:115,Security,access,access,115,"// We conservatively set the memory operand of a buffer intrinsic to the; // base resource pointer, so that we can access alias information about; // those pointers. Cases like ""this points at the same value; // but with a different offset"" are handled in; // areMemAccessesTriviallyDisjoint.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:46,Performance,load,loaded,46,"// If this isn't a gather, we may have excess loaded elements in the; // IR type. Check the dmask for the real number of elements loaded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:130,Performance,load,loaded,130,"// If this isn't a gather, we may have excess loaded elements in the; // IR type. Check the dmask for the real number of elements loaded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Security,access,access,23,"// This is an abstract access, but we need to specify a type and size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Security,access,access,23,"// This is an abstract access, but we need to specify a type and size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:481,Safety,risk,risky,481,"// Assume the we will use FLAT for all global memory accesses; // on VI.; // FIXME: This assumption is currently wrong. On VI we still use; // MUBUF instructions for the r + i addressing mode. As currently; // implemented, the MUBUF instructions only work on buffer < 4GB.; // It may be possible to support > 4GB buffers with MUBUF instructions,; // by setting the stride value in the resource descriptor which would; // increase the size limit to (stride * 4GB). However, this is risky,; // because it has never been validated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:53,Security,access,accesses,53,"// Assume the we will use FLAT for all global memory accesses; // on VI.; // FIXME: This assumption is currently wrong. On VI we still use; // MUBUF instructions for the r + i addressing mode. As currently; // implemented, the MUBUF instructions only work on buffer < 4GB.; // It may be possible to support > 4GB buffers with MUBUF instructions,; // by setting the stride value in the resource descriptor which would; // increase the size limit to (stride * 4GB). However, this is risky,; // because it has never been validated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:518,Security,validat,validated,518,"// Assume the we will use FLAT for all global memory accesses; // on VI.; // FIXME: This assumption is currently wrong. On VI we still use; // MUBUF instructions for the r + i addressing mode. As currently; // implemented, the MUBUF instructions only work on buffer < 4GB.; // It may be possible to support > 4GB buffers with MUBUF instructions,; // by setting the stride value in the resource descriptor which would; // increase the size limit to (stride * 4GB). However, this is risky,; // because it has never been validated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:162,Integrability,Depend,Depending,162,"// MUBUF / MTBUF instructions have a 12-bit unsigned byte offset, and; // additionally can do r + r + i with addr64. 32-bit has more addressing; // mode options. Depending on the resource constant, it can also do; // (i64 r0) + (i32 r1) * (i14 i).; //; // Private arrays end up using a scratch buffer most of the time, so also; // assume those use MUBUF instructions. Scratch loads / stores are currently; // implemented as mubuf instructions with offen bit set, so slightly; // different than the normal addr64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:376,Performance,load,loads,376,"// MUBUF / MTBUF instructions have a 12-bit unsigned byte offset, and; // additionally can do r + r + i with addr64. 32-bit has more addressing; // mode options. Depending on the resource constant, it can also do; // (i64 r0) + (i32 r1) * (i14 i).; //; // Private arrays end up using a scratch buffer most of the time, so also; // assume those use MUBUF instructions. Scratch loads / stores are currently; // implemented as mubuf instructions with offen bit set, so slightly; // different than the normal addr64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:20,Integrability,depend,depending,20,"// r + i or just i, depending on HasBaseReg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:203,Deployability,Update,Update,203,"// There are no SMRD extloads, so if we have to do a small type access we; // will use a MUBUF load.; // FIXME?: We also need to do this if unaligned, but we don't know the; // alignment here.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:95,Performance,load,load,95,"// There are no SMRD extloads, so if we have to do a small type access we; // will use a MUBUF load.; // FIXME?: We also need to do this if unaligned, but we don't know the; // alignment here.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:258,Performance,load,loads,258,"// There are no SMRD extloads, so if we have to do a small type access we; // will use a MUBUF load.; // FIXME?: We also need to do this if unaligned, but we don't know the; // alignment here.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:64,Security,access,access,64,"// There are no SMRD extloads, so if we have to do a small type access we; // will use a MUBUF load.; // FIXME?: We also need to do this if unaligned, but we don't know the; // alignment here.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:20,Integrability,depend,depending,20,"// r + i or just i, depending on HasBaseReg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:133,Security,access,access,133,"// Basic, single offset DS instructions allow a 16-bit unsigned immediate; // field.; // XXX - If doing a 4-byte aligned 8-byte type access, we effectively have; // an 8-bit dword offset but we don't know the alignment here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:20,Integrability,depend,depending,20,"// r + i or just i, depending on HasBaseReg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:86,Security,access,access,86,"// Either, the alignment requirements are ""enabled"", or there is an; // unaligned LDS access related hardware bug though alignment requirements; // are ""disabled"". In either case, we need to check for proper alignment; // requirements.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:219,Performance,load,loads,219,"// SI has a hardware bug in the LDS / GDS bounds checking: if the base; // address is negative, then the instruction is incorrectly treated as; // out-of-bounds even if base + offsets is in bounds. Split vectorized; // loads here to avoid emitting ds_read2_b32. We may re-combine the; // load later in the SILoadStoreOptimizer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:288,Performance,load,load,288,"// SI has a hardware bug in the LDS / GDS bounds checking: if the base; // address is negative, then the instruction is incorrectly treated as; // out-of-bounds even if base + offsets is in bounds. Split vectorized; // loads here to avoid emitting ds_read2_b32. We may re-combine the; // load later in the SILoadStoreOptimizer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:233,Safety,avoid,avoid,233,"// SI has a hardware bug in the LDS / GDS bounds checking: if the base; // address is negative, then the instruction is incorrectly treated as; // out-of-bounds even if base + offsets is in bounds. Split vectorized; // loads here to avoid emitting ds_read2_b32. We may re-combine the; // load later in the SILoadStoreOptimizer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Security,access,accessing,10,"// 8 byte accessing via ds_read/write_b64 require 8-byte alignment, but we; // can do a 4 byte aligned, 8 byte access in a single operation using; // ds_read2/write2_b32 with adjacent offsets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:111,Security,access,access,111,"// 8 byte accessing via ds_read/write_b64 require 8-byte alignment, but we; // can do a 4 byte aligned, 8 byte access in a single operation using; // ds_read2/write2_b32 with adjacent offsets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:85,Integrability,depend,depending,85,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:528,Performance,load,load,528,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:744,Performance,load,load,744,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:824,Performance,load,load,824,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:844,Performance,load,load,844,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:668,Security,access,access,668,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:770,Usability,simpl,simply,770,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:11,Security,access,accessing,11,// 12 byte accessing via ds_read/write_b96 require 16-byte alignment on; // gfx8 and older.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:123,Performance,load,load,123,"// Naturally aligned access is fastest. However, also report it is Fast; // if memory is aligned less than DWORD. A narrow load or store will be; // be equally slow as a single ds_read_b96/ds_write_b96, but there will; // be more of them, so overall we will pay less penalty issuing a single; // instruction.; // See comment on the values above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:21,Security,access,access,21,"// Naturally aligned access is fastest. However, also report it is Fast; // if memory is aligned less than DWORD. A narrow load or store will be; // be equally slow as a single ds_read_b96/ds_write_b96, but there will; // be more of them, so overall we will pay less penalty issuing a single; // instruction.; // See comment on the values above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:11,Security,access,accessing,11,"// 16 byte accessing via ds_read/write_b128 require 16-byte alignment on; // gfx8 and older, but we can do a 8 byte aligned, 16 byte access in a; // single operation using ds_read2/write2_b64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:133,Security,access,access,133,"// 16 byte accessing via ds_read/write_b128 require 16-byte alignment on; // gfx8 and older, but we can do a 8 byte aligned, 16 byte access in a; // single operation using ds_read2/write2_b64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:123,Performance,load,load,123,"// Naturally aligned access is fastest. However, also report it is Fast; // if memory is aligned less than DWORD. A narrow load or store will be; // be equally slow as a single ds_read_b128/ds_write_b128, but there; // will be more of them, so overall we will pay less penalty issuing a; // single instruction.; // See comment on the values above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:21,Security,access,access,21,"// Naturally aligned access is fastest. However, also report it is Fast; // if memory is aligned less than DWORD. A narrow load or store will be; // be equally slow as a single ds_read_b128/ds_write_b128, but there; // will be more of them, so overall we will pay less penalty issuing a; // single instruction.; // See comment on the values above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:140,Security,access,access,140,"// See comment on the values above.; // Note that we have a single-dword or sub-dword here, so if underaligned; // it is a slowest possible access, hence returned value is 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:83,Security,access,access,83,"// FIXME: We have to be conservative here and assume that flat operations; // will access scratch. If we had access to the IR function, then we; // could determine if any private memory was used in the function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:109,Security,access,access,109,"// FIXME: We have to be conservative here and assume that flat operations; // will access scratch. If we had access to the IR function, then we; // could determine if any private memory was used in the function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:62,Performance,perform,perform,62,"// So long as they are correct, wide global memory operations perform better; // than multiple smaller memory ops -- even when misaligned",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:174,Security,access,accesses,174,// FIXME: Should account for address space here.; // The default fallback uses the private pointer size as a guess for a type to; // use. Make sure we switch these to 64-bit accesses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Usability,simpl,simple,30,// Flat -> private/local is a simple truncate.; // Flat -> global is no-op,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Usability,Simpl,SimplifySetCC,3,// SimplifySetCC uses this function to determine whether or not it should; // create setcc with i1 operands. We don't have instructions for i1 setcc.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:36,Performance,load,loading,36,"// Try to avoid using an extload by loading earlier than the argument address,; // and extracting the relevant bits. The load should hopefully be merged with; // the previous argument.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:121,Performance,load,load,121,"// Try to avoid using an extload by loading earlier than the argument address,; // and extracting the relevant bits. The load should hopefully be merged with; // the previous argument.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Safety,avoid,avoid,10,"// Try to avoid using an extload by loading earlier than the argument address,; // and extracting the relevant bits. The load should hopefully be merged with; // the previous argument.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Performance,load,load,10,// Create load nodes to retrieve arguments from the stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:44,Testability,assert,assert,44,"// For NON_EXTLOAD, generic code in getLoad assert(ValVT == MemVT)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:122,Availability,mask,mask,122,"// If GridZ is not programmed in an entry function then the hardware will set; // it to all zeros, so there is no need to mask the GridY value in the low; // order bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:80,Energy Efficiency,allocate,allocated,80,"// It's possible for a kernarg intrinsic call to appear in a kernel with; // no allocated segment, in which case we do not add the user sgpr; // argument, so just return null.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Safety,safe,safely,10,// We can safely skip PS inputs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate special inputs passed in VGPRs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:131,Availability,Mask,Mask,131,"// Try to allocate a VGPR at the end of the argument list, or if no argument; // VGPRs are left allocating a stack slot.; // If \p Mask is is given it indicates bitfield position in the register.; // If \p Arg is given use it with new ]p Mask instead of allocating new.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:238,Availability,Mask,Mask,238,"// Try to allocate a VGPR at the end of the argument list, or if no argument; // VGPRs are left allocating a stack slot.; // If \p Mask is is given it indicates bitfield position in the register.; // If \p Arg is given use it with new ]p Mask instead of allocating new.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Energy Efficiency,allocate,allocate,10,"// Try to allocate a VGPR at the end of the argument list, or if no argument; // VGPRs are left allocating a stack slot.; // If \p Mask is is given it indicates bitfield position in the register.; // If \p Arg is given use it with new ]p Mask instead of allocating new.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:49,Energy Efficiency,allocate,allocate,49,"// If this has a fixed position, we still should allocate the register in the; // CCInfo state. Technically we could get away with this for values passed; // outside of the normal argument range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:4,Energy Efficiency,Allocate,Allocate,4,/// Allocate implicit function VGPR arguments at the end of allocated user; /// arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:60,Energy Efficiency,allocate,allocated,60,/// Allocate implicit function VGPR arguments at the end of allocated user; /// arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:4,Energy Efficiency,Allocate,Allocate,4,/// Allocate implicit function VGPR arguments in fixed registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate special inputs passed in user SGPRs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate pre-loaded kernel arguemtns. Arguments to be preloading must be; // sequential starting from the first argument.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:16,Performance,load,loaded,16,// Allocate pre-loaded kernel arguemtns. Arguments to be preloading must be; // sequential starting from the first argument.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Energy Efficiency,allocate,allocate,10,// Always allocate this last since it is a synthetic preload.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate special input registers that are initialized per-wave.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:24,Security,access,access,24,"// For now assume stack access is needed in any callee functions, so we need; // the scratch registers to pass in.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:234,Energy Efficiency,allocate,allocated,234,"// We tentatively reserve the last registers (skipping the last registers; // which may contain VCC, FLAT_SCR, and XNACK). After register allocation,; // we'll replace these with the ones immediately after those which were; // really allocated. In the prologue copies will be inserted from the; // argument to these reserved registers.; // Without HSA, relocations are used for the scratch pointer and the; // buffer resource setup is always inserted in the prologue. Scratch wave; // offset is still in an input SGPR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:655,Safety,avoid,avoid,655,"// For entry functions we have to set up the stack pointer if we use it,; // whereas non-entry functions get this ""for free"". This means there is no; // intrinsic advantage to using S32 over S34 in cases where we do not have; // calls but do need a frame pointer (i.e. if we are requested to have one; // because frame pointer elimination is disabled). To keep things simple we; // only ever use S32 as the call ABI stack pointer, and so using it does not; // imply we need a separate frame pointer.; //; // Try to use s32 as the SP, but move it if it would interfere with input; // arguments. This won't work with calls though.; //; // FIXME: Move SP to avoid any possible inputs, or find a way to spill input; // registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:368,Usability,simpl,simple,368,"// For entry functions we have to set up the stack pointer if we use it,; // whereas non-entry functions get this ""for free"". This means there is no; // intrinsic advantage to using S32 over S34 in cases where we do not have; // calls but do need a frame pointer (i.e. if we are requested to have one; // because frame pointer elimination is disabled). To keep things simple we; // only ever use S32 as the call ABI stack pointer, and so using it does not; // imply we need a separate frame pointer.; //; // Try to use s32 as the SP, but move it if it would interfere with input; // arguments. This won't work with calls though.; //; // FIXME: Move SP to avoid any possible inputs, or find a way to spill input; // registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:171,Modifiability,variab,variable,171,"// hasFP should be accurate for entry functions even before the frame is; // finalized, because it does not rely on the known stack size, only; // properties like whether variable sized objects are present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:25,Energy Efficiency,allocate,allocateSpecialInputSGPRs,25,// FIXME: Sink this into allocateSpecialInputSGPRs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:91,Testability,assert,assert,91,"// If this is an 8 or 16-bit value, it is really passed promoted; // to 32 bits. Insert an assert[sz]ext to capture this, then; // truncate to the right size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Deployability,update,updated,23,// Operand #0 = Chain (updated below); // Copy the result values into the output registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,// Update chain and glue.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:44,Integrability,depend,depending,44,// Add code to pass special inputs required depending on used features separate; // from the explicit user arguments present in the IR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:101,Energy Efficiency,allocate,allocate,101,"// We may have proven the input wasn't needed, although the ABI is; // requiring it. We just need to allocate the register appropriately.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:125,Usability,simpl,simple,125,"// For a divergent call target, we need to do a waterfall loop over the; // possible callees which precludes us from using a simple jump.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:21,Energy Efficiency,allocate,allocate,21,"// With a fixed ABI, allocate fixed registers before user arguments.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:98,Availability,avail,available,98,"// Since we're not changing the ABI to make this a tail call, the memory; // operands are already available in the caller's incoming argument space.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:58,Performance,load,loads,58,"// Walk the register/memloc assignments, inserting copies/loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:78,Performance,load,loaded,78,"// Make sure any stack arguments overlapping with where we're storing; // are loaded before this eventual operation. Otherwise they'll be; // clobbered.; // FIXME: Why is this really necessary? This seems to just result in a; // lot of code to copy the stack and write them back to the same; // locations, which are supposed to be immutable?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:9,Availability,redundant,redundant,9,"// Add a redundant copy of the callee global which will not be legalized, as; // we need direct access to the callee later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:9,Safety,redund,redundant,9,"// Add a redundant copy of the callee global which will not be legalized, as; // we need direct access to the callee later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:96,Security,access,access,96,"// Add a redundant copy of the callee global which will not be legalized, as; // we need direct access to the callee later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:148,Energy Efficiency,consumption,consumption,148,"// Each tail call may have to adjust the stack by a different amount, so; // this information must travel along with the operation for eventual; // consumption by emitEpilogue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:18,Availability,mask,mask,18,// Add a register mask operand representing the call-preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:26,Availability,error,error,26,// Defer to cannot select error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:27,Safety,avoid,avoid,27,// TODO: We could possibly avoid a 64-bit shift and use a simpler table if we; // knew only one mode was demanded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:58,Usability,simpl,simpler,58,// TODO: We could possibly avoid a 64-bit shift and use a simpler table if we; // knew only one mode was demanded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:93,Modifiability,extend,extended,93,"// There's a gap in the 4-bit encoded table and actual enum values, so offset; // if it's an extended value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Usability,Clear,Clear,3,// Clear TRAP_STS.MEM_VIOL,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,Load,Load,3,// Load and check TRAP_STS.MEM_VIOL,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,"// Update EXEC, save the original EXEC value to VCC.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,"// Update EXEC, switch all done bits to 0 and all todo bits to 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:181,Availability,alive,alive,181,"// This has slightly sub-optimal regalloc when the source vector is killed by; // the read. The register allocator does not understand that the kill is; // per-workitem, so is kept alive for the whole loop so we end up not re-using a; // subregister from it, using 1 more VGPR than necessary. This was saved when; // this was expanded after register allocation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:17,Availability,mask,mask,17,// Save the EXEC mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:148,Performance,optimiz,optimization,148,"// TODO: Look at the uses to avoid the copy. This may require rescheduling; // to avoid interfering with other uses, so probably requires a new; // optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:29,Safety,avoid,avoid,29,"// TODO: Look at the uses to avoid the copy. This may require rescheduling; // to avoid interfering with other uses, so probably requires a new; // optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:82,Safety,avoid,avoid,82,"// TODO: Look at the uses to avoid the copy. This may require rescheduling; // to avoid interfering with other uses, so probably requires a new; // optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:24,Integrability,depend,depend,24,// Reduction operations depend on whether the input operand is SGPR or VGPR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:71,Energy Efficiency,Reduce,Reduced,71,// These operations with a uniform value i.e. SGPR are idempotent.; // Reduced value will be same as given sgpr.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:189,Energy Efficiency,reduce,reduce,189,"// TODO: Implement DPP Strategy and switch based on immediate strategy; // operand. For now, for all the cases (default, Iterative and DPP we use; // iterative approach by default.); // To reduce the VGPR using iterative approach, we need to iterate; // over all the active lanes. Lowering consists of ComputeLoop,; // which iterate over only active lanes. We use copy of EXEC register; // as induction variable and every active lane modifies it using bitset0; // so that we will get the next active lane for next iteration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:403,Modifiability,variab,variable,403,"// TODO: Implement DPP Strategy and switch based on immediate strategy; // operand. For now, for all the cases (default, Iterative and DPP we use; // iterative approach by default.); // To reduce the VGPR using iterative approach, we need to iterate; // over all the active lanes. Lowering consists of ComputeLoop,; // which iterate over only active lanes. We use copy of EXEC register; // as induction variable and every active lane modifies it using bitset0; // so that we will get the next active lane for next iteration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:38,Modifiability,variab,variable,38,"// Create initail values of induction variable from Exec, Accumulator and; // insert branch instr to newly created ComputeBlockk",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,Perform,Perform,3,// Perform the computations,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Performance,optimiz,optimize,10,"// Try to optimize cases that only set the denormal mode or rounding mode.; //; // If the s_setreg_b32 fully sets all of the bits in the rounding mode or; // denormal mode to a constant, we can use s_round_mode or s_denorm_mode; // instead.; //; // FIXME: This could be predicates on the immediate, but tablegen doesn't; // allow you to have a no side effect instruction in the output of a; // sideeffecting pattern.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:190,Performance,perform,perform,190,"// This currently forces unfolding various combinations of fsub into fma with; // free fneg'd operands. As long as we have fast FMA (controlled by; // isFMAFasterThanFMulAndFAdd), we should perform these.; // When fma is quarter rate, for f64 where add / sub are at best half rate,; // most of these combines appear to be cycle neutral but save on instruction; // count / code size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:41,Integrability,depend,depends,41,"// Answering this is somewhat tricky and depends on the specific device which; // have different rates for fma or all f64 operations.; //; // v_fma_f64 and v_mul_f64 always take the same number of cycles as each other; // regardless of which device (although the number of cycles differs between; // devices), so it is always profitable for f64.; //; // v_fma_f32 takes 4 or 16 cycles depending on the device, so it is profitable; // only on full rate devices. Normally, we should prefer selecting v_mad_f32; // which we can always do even without fused FP ops since it returns the same; // result as the separate operations and since it is always full; // rate. Therefore, we lie and report that it is not faster for f32. v_mad_f32; // however does not support denormals, so we do report fma as faster if we have; // a fast fma device and require denormals.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:385,Integrability,depend,depending,385,"// Answering this is somewhat tricky and depends on the specific device which; // have different rates for fma or all f64 operations.; //; // v_fma_f64 and v_mul_f64 always take the same number of cycles as each other; // regardless of which device (although the number of cycles differs between; // devices), so it is always profitable for f64.; //; // v_fma_f32 takes 4 or 16 cycles depending on the device, so it is profitable; // only on full rate devices. Normally, we should prefer selecting v_mad_f32; // which we can always do even without fused FP ops since it returns the same; // result as the separate operations and since it is always full; // rate. Therefore, we lie and report that it is not faster for f32. v_mad_f32; // however does not support denormals, so we do report fma as faster if we have; // a fast fma device and require denormals.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:17,Availability,avail,available,17,// If mad is not available this depends only on if f32 fma is full rate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:32,Integrability,depend,depends,32,// If mad is not available this depends only on if f32 fma is full rate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:95,Performance,load,loads,95,"// Used for D16: Casts the result of an instruction into the right vector,; // packs values if loads return unpacked values.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:32,Performance,load,loads,32,// TODO: Support non-format TFE loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Performance,load,load,30,"// Lower llvm.amdgcn.s.buffer.load.(i8, u8) intrinsics. First, we generate; // s_buffer_load_u8 for signed and unsigned load instructions. Next, DAG; // combiner tries to merge the s_buffer_load_u8 with a sext instruction; // (performSignExtendInRegCombine()) and it replaces s_buffer_load_u8 with; // s_buffer_load_i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:120,Performance,load,load,120,"// Lower llvm.amdgcn.s.buffer.load.(i8, u8) intrinsics. First, we generate; // s_buffer_load_u8 for signed and unsigned load instructions. Next, DAG; // combiner tries to merge the s_buffer_load_u8 with a sext instruction; // (performSignExtendInRegCombine()) and it replaces s_buffer_load_u8 with; // s_buffer_load_i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:227,Performance,perform,performSignExtendInRegCombine,227,"// Lower llvm.amdgcn.s.buffer.load.(i8, u8) intrinsics. First, we generate; // s_buffer_load_u8 for signed and unsigned load instructions. Next, DAG; // combiner tries to merge the s_buffer_load_u8 with a sext instruction; // (performSignExtendInRegCombine()) and it replaces s_buffer_load_u8 with; // s_buffer_load_i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:17,Safety,avoid,avoid,17,// FIXME: Either avoid relying on address space here or change the default; // address space for functions to avoid the explicit check.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:110,Safety,avoid,avoid,110,// FIXME: Either avoid relying on address space here or change the default; // address space for functions to avoid the explicit check.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Testability,Assert,Assert,10,"// FIXME: Assert during selection that this is only selected for; // ieee_mode. Currently a combine can produce the ieee version for non-ieee; // mode functions, but this happens to be OK since it's only done in cases; // where there is known no sNaN.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:326,Energy Efficiency,efficient,efficient,326,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:585,Modifiability,extend,extended,585,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:599,Modifiability,extend,extended,599,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:762,Modifiability,extend,extended,762,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:779,Modifiability,extend,extended,779,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:912,Modifiability,extend,extended,912,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1002,Modifiability,extend,extended,1002,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:138,Modifiability,extend,extended,138,"// If all the operands are zero-enteted to 32-bits, then we replace s_mul_u64; // with s_mul_u64_u32_pseudo. If all the operands are sign-extended to; // 32-bits, then we replace s_mul_u64 with s_mul_i64_i32_pseudo.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Performance,Queue,QueuePtr,30,"// For code object version 5, QueuePtr is passed through implicit kernarg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:71,Performance,queue,queue-ptr,71,"// We probably are in a function incorrectly marked with; // amdgpu-no-queue-ptr. This is undefined. We don't want to delete the; // trap, so just use a null pointer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:345,Usability,simpl,simply,345,"// Note: this feature (register) is broken. When used as a 32-bit operand,; // it returns a wrong value (all zeroes?). The real value is in the upper 32; // bits.; //; // To work around the issue, directly emit a 64 bit mov from this register; // then extract the high bits. Note that this shouldn't even result in a; // shift being emitted and simply become a pair of registers (e.g.):; // s_mov_b64 s[6:7], src_shared_base; // v_mov_b32_e32 v1, s7; //; // FIXME: It would be more natural to emit a CopyFromReg here, but then copy; // coalescing would kick in and it would think it's okay to use the ""HI""; // subregister directly (instead of extracting the HI 32 bits) which is an; // artificial (unusable) register.; // Register TableGen definitions would need an overhaul to get rid of the; // artificial ""HI"" aperture registers and prevent this kind of issue from; // happening.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:71,Performance,queue,queue-ptr,71,// We probably are in a function incorrectly marked with; // amdgpu-no-queue-ptr. This is undefined.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:132,Availability,avail,available,132,"// TODO: Use custom target PseudoSourceValue.; // TODO: We should use the value from the IR intrinsic call, but it might not; // be available and how do we get it?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:57,Performance,load,loads,57,"// TODO: Search through arithmetic, handle arguments and loads; // marked nonnull.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:266,Performance,optimiz,optimized,266,"// This lowers an INSERT_SUBVECTOR by extracting the individual elements from; // the small vector and inserting them into the big vector. That is better than; // the default expansion of doing it via a stack slot. Even though the use of; // the stack slot would be optimized away afterwards, the stack slot itself; // remains.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:113,Safety,avoid,avoid,113,"// Static indexing does not lower to stack access, and hence there is no need; // for special custom lowering to avoid stack access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:43,Security,access,access,43,"// Static indexing does not lower to stack access, and hence there is no need; // for special custom lowering to avoid stack access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:125,Security,access,access,125,"// Static indexing does not lower to stack access, and hence there is no need; // for special custom lowering to avoid stack access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,"// Avoid stack access for dynamic indexing by custom lowering to; // v_bfi_b32 (v_bfm_b32 16, (shl idx, 16)), val, vec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:15,Security,access,access,15,"// Avoid stack access for dynamic indexing by custom lowering to; // v_bfi_b32 (v_bfm_b32 16, (shl idx, 16)), val, vec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:62,Availability,mask,mask,62,// Convert vector index to bit-index and get the required bit mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Availability,Mask,Mask,6,// 2. Mask off all other indicies except the required index within (1).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Availability,Mask,Mask,6,// 3. Mask off the required index within the target vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Performance,optimiz,optimizations,23,// Make sure we do any optimizations that will make it easier to fold; // source modifiers before obscuring it with bit operations.; // XXX - Why doesn't this get called when vector_shuffle is expanded?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:356,Safety,Avoid,Avoid,356,"// vector_shuffle <0,1,6,7> lhs, rhs; // -> concat_vectors (extract_subvector lhs, 0), (extract_subvector rhs, 2); //; // vector_shuffle <6,7,2,3> lhs, rhs; // -> concat_vectors (extract_subvector rhs, 2), (extract_subvector lhs, 2); //; // vector_shuffle <6,7,0,1> lhs, rhs; // -> concat_vectors (extract_subvector rhs, 2), (extract_subvector lhs, 0); // Avoid scalarizing when both halves are reading from consecutive elements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid adding defined bits with the zero_extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:489,Modifiability,variab,variable,489,"// In order to support pc-relative addressing, the PC_ADD_REL_OFFSET SDNode is; // lowered to the following code sequence:; //; // For constant address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol; // s_addc_u32 s1, s1, 0; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // a fixup or relocation is emitted to replace $symbol with a literal; // constant, which is a pc-relative offset from the encoding of the $symbol; // operand to the global variable.; //; // For global address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol@{gotpc}rel32@lo; // s_addc_u32 s1, s1, $symbol@{gotpc}rel32@hi; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // fixups or relocations are emitted to replace $symbol@*@lo and; // $symbol@*@hi with lower 32 bits and higher 32 bits of a literal constant,; // which is a 64-bit pc-relative offset from the encoding of the $symbol; // operand to the global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:971,Modifiability,variab,variable,971,"// In order to support pc-relative addressing, the PC_ADD_REL_OFFSET SDNode is; // lowered to the following code sequence:; //; // For constant address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol; // s_addc_u32 s1, s1, 0; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // a fixup or relocation is emitted to replace $symbol with a literal; // constant, which is a pc-relative offset from the encoding of the $symbol; // operand to the global variable.; //; // For global address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol@{gotpc}rel32@lo; // s_addc_u32 s1, s1, $symbol@{gotpc}rel32@hi; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // fixups or relocations are emitted to replace $symbol@*@lo and; // $symbol@*@hi with lower 32 bits and higher 32 bits of a literal constant,; // which is a 64-bit pc-relative offset from the encoding of the $symbol; // operand to the global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:208,Energy Efficiency,allocate,allocated,208,// HIP uses an unsized array `extern __shared__ T s[]` or similar; // zero-sized type in other languages to declare the dynamic shared; // memory which size is not known at the compile time. They will be; // allocated by the runtime and placed directly after the static; // allocated ones. They all share the same offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:274,Energy Efficiency,allocate,allocated,274,// HIP uses an unsized array `extern __shared__ T s[]` or similar; // zero-sized type in other languages to declare the dynamic shared; // memory which size is not known at the compile time. They will be; // allocated by the runtime and placed directly after the static; // allocated ones. They all share the same offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:218,Availability,redundant,redundant,218,"// We can't use S_MOV_B32 directly, because there is no way to specify m0 as; // the destination register.; //; // We can't use CopyToReg, because MachineCSE won't combine COPY instructions,; // so we will end up with redundant moves to m0.; //; // We use a pseudo to ensure we emit s_mov_b32 with m0 as the direct result.; // A Null SDValue creates a glue result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:218,Safety,redund,redundant,218,"// We can't use S_MOV_B32 directly, because there is no way to specify m0 as; // the destination register.; //; // We can't use CopyToReg, because MachineCSE won't combine COPY instructions,; // so we will end up with redundant moves to m0.; //; // We use a pseudo to ensure we emit s_mov_b32 with m0 as the direct result.; // A Null SDValue creates a glue result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:54,Performance,load,load,54,// Re-construct the required return value for a image load intrinsic.; // This is more complicated due to the optional use TexFailCtrl which means the required; // return type is an aggregate,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Availability,error,error,23,// Expecting to get an error flag since TFC is on - and dmask is 0; // Force dmask to be at least 1 otherwise the instruction will fail,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:110,Performance,load,load,110,// Has something earlier tagged that the return type needs adjusting; // This happens if the instruction is a load or has set TexFailCtrl flags,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Performance,load,load,19,// This is a no-op load. This can be eliminated,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:18,Performance,optimiz,optimization,18,// TODO no-return optimization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Performance,load,load,30,"// Lower llvm.amdgcn.s.buffer.load.{i16, u16} intrinsics. Initially, the; // s_buffer_load_u16 instruction is emitted for both signed and unsigned; // loads. Later, DAG combiner tries to combine s_buffer_load_u16 with sext; // and generates s_buffer_load_i16 (performSignExtendInRegCombine).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:151,Performance,load,loads,151,"// Lower llvm.amdgcn.s.buffer.load.{i16, u16} intrinsics. Initially, the; // s_buffer_load_u16 instruction is emitted for both signed and unsigned; // loads. Later, DAG combiner tries to combine s_buffer_load_u16 with sext; // and generates s_buffer_load_i16 (performSignExtendInRegCombine).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:260,Performance,perform,performSignExtendInRegCombine,260,"// Lower llvm.amdgcn.s.buffer.load.{i16, u16} intrinsics. Initially, the; // s_buffer_load_u16 instruction is emitted for both signed and unsigned; // loads. Later, DAG combiner tries to combine s_buffer_load_u16 with sext; // and generates s_buffer_load_i16 (performSignExtendInRegCombine).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:14,Performance,load,load,14,// Widen vec3 load to vec4.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:51,Performance,load,load,51,// We have a divergent offset. Emit a MUBUF buffer load instead. We can; // assume that the buffer is unswizzled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:81,Availability,mask,masking,81,// Don't bother inserting AssertZext for packed IDs since we're emitting the; // masking operations anyway.; //; // TODO: We could assert the top bit is 0 for the source copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:26,Testability,Assert,AssertZext,26,// Don't bother inserting AssertZext for packed IDs since we're emitting the; // masking operations anyway.; //; // TODO: We could assert the top bit is 0 for the source copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:131,Testability,assert,assert,131,// Don't bother inserting AssertZext for packed IDs since we're emitting the; // masking operations anyway.; //; // TODO: We could assert the top bit is 0 for the source copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:38,Performance,optimiz,optimized,38,"// s_buffer_load, because of how it's optimized, can't be volatile; // so reject ones with the volatile bit set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:86,Safety,avoid,avoid,86,"// On targets not supporting constant in soffset field, turn zero to; // SGPR_NULL to avoid generating an extra s_mov with zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:38,Performance,load,load,38,"// Call DAG.getMemIntrinsicNode for a load, but first widen a dwordx3 type to; // dwordx4 if on SI and handle TFE loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:114,Performance,load,loads,114,"// Call DAG.getMemIntrinsicNode for a load, but first widen a dwordx3 type to; // dwordx4 if on SI and handle TFE loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:101,Performance,Perform,Perform,101,// If reference to barrier id is not an inline constant then it must be; // referenced with M0[4:0]. Perform an OR with the member count to; // include it in M0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:353,Availability,down,down,353,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:206,Energy Efficiency,power,power,206,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:304,Performance,load,load,304,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Integrability,Wrap,Wrap,3,// Wrap a global or flat pointer into a buffer intrinsic using the flags; // specified in the intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:34,Performance,load,loads,34,// Handle 8 bit and 16 bit buffer loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:35,Performance,load,loads,35,// Try to turn 8 and 16-bit scalar loads into SMEM eligible 32-bit loads.; // TODO: Skip this on GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:67,Performance,load,loads,67,// Try to turn 8 and 16-bit scalar loads into SMEM eligible 32-bit loads.; // TODO: Skip this on GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:136,Performance,load,loads,136,// Try to turn 8 and 16-bit scalar loads into SMEM eligible 32-bit loads.; // TODO: Skip this on GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Performance,load,loads,19,// FIXME: Constant loads should all be marked invariant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:61,Performance,load,load,61,"// Don't do this early, since it may interfere with adjacent load merging for; // illegal types. We can avoid losing alignment information for exotic types; // pre-legalize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:104,Safety,avoid,avoid,104,"// Don't do this early, since it may interfere with adjacent load merging for; // illegal types. We can avoid losing alignment information for exotic types; // pre-legalize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:122,Performance,load,load,122,"// We may need to handle exotic cases, such as i16->i64 extloads, so insert; // the appropriate extension from the 32-bit load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:56,Security,access,access,56,// TODO: Should check if the address can definitely not access stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:37,Performance,load,load,37,"// FIXME: Copied from PPC; // First, load into 32 bits, then truncate to 1 bit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:51,Security,access,access,51,// If there is a possibility that flat instruction access scratch memory; // then we need to use the same legalization rules we use for private.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:15,Performance,load,loads,15,"// Non-uniform loads will be selected to MUBUF instructions, so they; // have the same legalization requirements as global and private; // loads.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:139,Performance,load,loads,139,"// Non-uniform loads will be selected to MUBUF instructions, so they; // have the same legalization requirements as global and private; // loads.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:15,Performance,load,loads,15,"// Non-uniform loads will be selected to MUBUF instructions, so they; // have the same legalization requirements as global and private; // loads.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:139,Performance,load,loads,139,"// Non-uniform loads will be selected to MUBUF instructions, so they; // have the same legalization requirements as global and private; // loads.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Performance,load,loads,6,// v3 loads not supported on SI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:13,Performance,load,loads,13,// v3 and v4 loads are supported for private and global memory.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Integrability,Depend,Depending,3,"// Depending on the setting of the private_element_size field in the; // resource descriptor, we can only make private accesses up to a certain; // size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:119,Security,access,accesses,119,"// Depending on the setting of the private_element_size field in the; // resource descriptor, we can only make private accesses up to a certain; // size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Performance,load,loads,6,// v3 loads not supported on SI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:112,Availability,error,error,112,"// v_rcp_f32 and v_rsq_f32 do not support denormals, and according to; // the CI documentation has a worst case error of 1 ulp.; // OpenCL requires <= 2.5 ulp for 1.0 / x, so it should always be OK to; // use it as long as we aren't trying to use denormals.; //; // v_rcp_f16 and v_rsq_f16 DO support denormals and 0.51ulp.; // 1.0 / sqrt(x) -> rsq(x); // XXX - Is UnsafeFPMath sufficient to do this for f64? The maximum ULP; // error seems really high at 2^29 ULP.; // 1.0 / x -> rcp(x)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:429,Availability,error,error,429,"// v_rcp_f32 and v_rsq_f32 do not support denormals, and according to; // the CI documentation has a worst case error of 1 ulp.; // OpenCL requires <= 2.5 ulp for 1.0 / x, so it should always be OK to; // use it as long as we aren't trying to use denormals.; //; // v_rcp_f16 and v_rsq_f16 DO support denormals and 0.51ulp.; // 1.0 / sqrt(x) -> rsq(x); // XXX - Is UnsafeFPMath sufficient to do this for f64? The maximum ULP; // error seems really high at 2^29 ULP.; // 1.0 / x -> rcp(x)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:365,Safety,Unsafe,UnsafeFPMath,365,"// v_rcp_f32 and v_rsq_f32 do not support denormals, and according to; // the CI documentation has a worst case error of 1 ulp.; // OpenCL requires <= 2.5 ulp for 1.0 / x, so it should always be OK to; // use it as long as we aren't trying to use denormals.; //; // v_rcp_f16 and v_rsq_f16 DO support denormals and 0.51ulp.; // 1.0 / sqrt(x) -> rsq(x); // XXX - Is UnsafeFPMath sufficient to do this for f64? The maximum ULP; // error seems really high at 2^29 ULP.; // 1.0 / x -> rcp(x)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:96,Integrability,depend,dependence,96,"// Note we can't use the STRICT_FMA/STRICT_FMUL for the non-strict FDIV; // lowering. The chain dependence is insufficient, and we need glue. We do; // not need the glue variants in a strictfp function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:88,Usability,usab,usable,88,// Workaround a hardware bug on SI where the condition output from div_scale; // is not usable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:51,Security,access,access,51,// If there is a possibility that flat instruction access scratch memory; // then we need to use the same legalization rules we use for private.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:70,Availability,error,error,70,// Probably an invalid store. If so we'll end up emitting a selection error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid the full correct expansion for f32 sqrt when promoting from f16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:96,Performance,optimiz,optimizations,96,//===----------------------------------------------------------------------===//; // Custom DAG optimizations; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Reduce,Reduce,3,"// Reduce width of sign operand, we only need the highest bit.; //; // fcopysign f64:x, f64:y ->; // fcopysign f64:x, (extract_vector_elt (bitcast f64:y to v2f32), 1); // TODO: In some cases it might make sense to go all the way to f16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:741,Usability,simpl,simplified,741,"// (shl (add x, c1), c2) -> add (shl x, c2), (shl c1, c2); // (shl (or x, c1), c2) -> add (shl x, c2), (shl c1, c2) iff x and c1 share no; // bits; // This is a variant of; // (mul (add x, c1), c2) -> add (mul x, c2), (mul c1, c2),; //; // The normal DAG combiner will do this, but only if the add has one use since; // that would increase the number of instructions.; //; // This prevents us from seeing a constant offset that can be folded into a; // memory instruction's addressing mode. If we know the resulting add offset of; // a pointer can be folded into an addressing offset, we can replace the pointer; // operand with the add of new constant offset. This eliminates one of the uses,; // and may allow the remaining use to also be simplified.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:141,Security,expose,exposes,141,// Break up 64-bit bit operation of a constant into two 32-bit and/or/xor. This; // will typically happen anyway for a VALU 64-bit and. This exposes other 32-bit; // integer combine opportunities since most 64-bit operations are decomposed; // this way. TODO: We won't want this for SALU especially if it is an inline; // immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:87,Safety,Avoid,Avoid,87,"// If we need to materialize a 64-bit immediate, it will be split up later; // anyway. Avoid creating the harder to understand 64-bit immediate; // materialization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:33,Availability,mask,mask,33,// 0xff for any zero byte in the mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:96,Availability,mask,masking,96,// Check if a node selects whole bytes from its operand 0 starting at a byte; // boundary while masking the rest. Returns select mask as in the v_perm_b32; // or -1 if not succeeded.; // Note byte select encoding:; // value 0-3 selects corresponding source byte;; // value 0xc selects zero;; // value 0xff selects 0xff.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:129,Availability,mask,mask,129,// Check if a node selects whole bytes from its operand 0 starting at a byte; // boundary while masking the rest. Returns select mask as in the v_perm_b32; // or -1 if not succeeded.; // Note byte select encoding:; // value 0-3 selects corresponding source byte;; // value 0xc selects zero;; // value 0xff selects 0xff.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Availability,mask,mask,19,"// and (srl x, c), mask => shl (bfe x, nb + c, mask >> nb), nb; // nb = number of trailing zeroes in mask; // It can be optimized out using SDWA for GFX8+ in the SDWA peephole pass,; // given that we are selecting 8 or 16 bit fields starting at byte boundary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:47,Availability,mask,mask,47,"// and (srl x, c), mask => shl (bfe x, nb + c, mask >> nb), nb; // nb = number of trailing zeroes in mask; // It can be optimized out using SDWA for GFX8+ in the SDWA peephole pass,; // given that we are selecting 8 or 16 bit fields starting at byte boundary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:101,Availability,mask,mask,101,"// and (srl x, c), mask => shl (bfe x, nb + c, mask >> nb), nb; // nb = number of trailing zeroes in mask; // It can be optimized out using SDWA for GFX8+ in the SDWA peephole pass,; // given that we are selecting 8 or 16 bit fields starting at byte boundary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:120,Performance,optimiz,optimized,120,"// and (srl x, c), mask => shl (bfe x, nb + c, mask >> nb), nb; // nb = number of trailing zeroes in mask; // It can be optimized out using SDWA for GFX8+ in the SDWA peephole pass,; // given that we are selecting 8 or 16 bit fields starting at byte boundary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:33,Availability,mask,mask,33,"// and (fcmp seto), (fp_class x, mask) -> fp_class x, mask & ~(p_nan | n_nan); // and (fcmp setuo), (fp_class x, mask) -> fp_class x, mask & (p_nan | n_nan)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:54,Availability,mask,mask,54,"// and (fcmp seto), (fp_class x, mask) -> fp_class x, mask & ~(p_nan | n_nan); // and (fcmp setuo), (fp_class x, mask) -> fp_class x, mask & (p_nan | n_nan)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:113,Availability,mask,mask,113,"// and (fcmp seto), (fp_class x, mask) -> fp_class x, mask & ~(p_nan | n_nan); // and (fcmp setuo), (fp_class x, mask) -> fp_class x, mask & (p_nan | n_nan)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:134,Availability,mask,mask,134,"// and (fcmp seto), (fp_class x, mask) -> fp_class x, mask & ~(p_nan | n_nan); // and (fcmp setuo), (fp_class x, mask) -> fp_class x, mask & (p_nan | n_nan)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:66,Availability,mask,masks,66,// Canonicalize the expression in an attempt to have fewer unique masks; // and therefore fewer registers used to hold the masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:123,Availability,mask,masks,123,// Canonicalize the expression in an attempt to have fewer unique masks; // and therefore fewer registers used to hold the masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:67,Availability,mask,mask,67,"// Select 0xc for each lane used from source operand. Zero has 0xc mask; // set, 0xff have 0xff in the mask, actual lanes are in the 0-3 range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:103,Availability,mask,mask,103,"// Select 0xc for each lane used from source operand. Zero has 0xc mask; // set, 0xff have 0xff in the mask, actual lanes are in the 0-3 range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:21,Availability,mask,mask,21,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:45,Availability,mask,mask,45,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:95,Availability,mask,masks,95,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:172,Availability,mask,mask,172,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:216,Availability,mask,mask,216,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:260,Availability,mask,masks,260,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:478,Performance,Load,LoadCombine,478,"// A key component of v_perm is a mapping between byte position of the src; // operands, and the byte position of the dest. To provide such, we need: 1. the; // node that provides x byte of the dest of the OR, and 2. the byte of the node; // used to provide that x byte. calculateByteProvider finds which node provides; // a certain byte of the dest of the OR, and calculateSrcByte takes that node,; // and finds an ultimate src and byte position For example: The supported; // LoadCombine pattern for vector loads is as follows; // t1; // or; // / \; // t2 t3; // zext shl; // | | \; // t4 t5 16; // or anyext; // / \ |; // t6 t7 t8; // srl shl or; // / | / \ / \; // t9 t10 t11 t12 t13 t14; // trunc* 8 trunc* 8 and and; // | | / | | \; // t15 t16 t17 t18 t19 t20; // trunc* 255 srl -256; // | / \; // t15 t15 16; //; // *In this example, the truncs are from i32->i16; //; // calculateByteProvider would find t6, t7, t13, and t14 for bytes 0-3; // respectively. calculateSrcByte would find (given node) -> ultimate src &; // byteposition: t6 -> t15 & 1, t7 -> t16 & 0, t13 -> t15 & 0, t14 -> t15 & 3.; // After finding the mapping, we can combine the tree into vperm t15, t16,; // 0x05000407; // Find the source and byte position from a node.; // \p DestByte is the byte position of the dest of the or that the src; // ultimately provides. \p SrcIndex is the byte of the src that maps to this; // dest of the or byte. \p Depth tracks how many recursive iterations we have; // performed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:509,Performance,load,loads,509,"// A key component of v_perm is a mapping between byte position of the src; // operands, and the byte position of the dest. To provide such, we need: 1. the; // node that provides x byte of the dest of the OR, and 2. the byte of the node; // used to provide that x byte. calculateByteProvider finds which node provides; // a certain byte of the dest of the OR, and calculateSrcByte takes that node,; // and finds an ultimate src and byte position For example: The supported; // LoadCombine pattern for vector loads is as follows; // t1; // or; // / \; // t2 t3; // zext shl; // | | \; // t4 t5 16; // or anyext; // / \ |; // t6 t7 t8; // srl shl or; // / | / \ / \; // t9 t10 t11 t12 t13 t14; // trunc* 8 trunc* 8 and and; // | | / | | \; // t15 t16 t17 t18 t19 t20; // trunc* 255 srl -256; // | / \; // t15 t15 16; //; // *In this example, the truncs are from i32->i16; //; // calculateByteProvider would find t6, t7, t13, and t14 for bytes 0-3; // respectively. calculateSrcByte would find (given node) -> ultimate src &; // byteposition: t6 -> t15 & 1, t7 -> t16 & 0, t13 -> t15 & 0, t14 -> t15 & 3.; // After finding the mapping, we can combine the tree into vperm t15, t16,; // 0x05000407; // Find the source and byte position from a node.; // \p DestByte is the byte position of the dest of the or that the src; // ultimately provides. \p SrcIndex is the byte of the src that maps to this; // dest of the or byte. \p Depth tracks how many recursive iterations we have; // performed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1478,Performance,perform,performed,1478,"// A key component of v_perm is a mapping between byte position of the src; // operands, and the byte position of the dest. To provide such, we need: 1. the; // node that provides x byte of the dest of the OR, and 2. the byte of the node; // used to provide that x byte. calculateByteProvider finds which node provides; // a certain byte of the dest of the OR, and calculateSrcByte takes that node,; // and finds an ultimate src and byte position For example: The supported; // LoadCombine pattern for vector loads is as follows; // t1; // or; // / \; // t2 t3; // zext shl; // | | \; // t4 t5 16; // or anyext; // / \ |; // t6 t7 t8; // srl shl or; // / | / \ / \; // t9 t10 t11 t12 t13 t14; // trunc* 8 trunc* 8 and and; // | | / | | \; // t15 t16 t17 t18 t19 t20; // trunc* 255 srl -256; // | / \; // t15 t15 16; //; // *In this example, the truncs are from i32->i16; //; // calculateByteProvider would find t6, t7, t13, and t14 for bytes 0-3; // respectively. calculateSrcByte would find (given node) -> ultimate src &; // byteposition: t6 -> t15 & 1, t7 -> t16 & 0, t13 -> t15 & 0, t14 -> t15 & 3.; // After finding the mapping, we can combine the tree into vperm t15, t16,; // 0x05000407; // Find the source and byte position from a node.; // \p DestByte is the byte position of the dest of the or that the src; // ultimately provides. \p SrcIndex is the byte of the src that maps to this; // dest of the or byte. \p Depth tracks how many recursive iterations we have; // performed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:386,Performance,perform,performed,386,"// For a byte position in the result of an Or, traverse the tree and find the; // node (and the byte of the node) which ultimately provides this {Or,; // BytePosition}. \p Op is the operand we are currently examining. \p Index is; // the byte position of the Op that corresponds with the originally requested; // byte of the Or \p Depth tracks how many recursive iterations we have; // performed. \p StartingIndex is the originally requested byte of the Or",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Performance,load,load,23,"// If the width of the load does not reach byte we are trying to provide for; // and it is not a ZEXTLOAD, then the load does not provide for the byte in; // question",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:116,Performance,load,load,116,"// If the width of the load does not reach byte we are trying to provide for; // and it is not a ZEXTLOAD, then the load does not provide for the byte in; // question",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Availability,mask,mask,23,"// Returns true if the mask matches consecutive bytes, and the first byte; // begins at a power of 2 byte offset from 0th byte",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:90,Energy Efficiency,power,power,90,"// Returns true if the mask matches consecutive bytes, and the first byte; // begins at a power of 2 byte offset from 0th byte",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:13,Availability,mask,mask,13,"// Since the mask is applied to Src1:Src2, Src1 bytes must be offset; // by sizeof(Src2) = 4",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:284,Modifiability,extend,extended,284,"// If the ultimate src is less than 32 bits, then we will only be; // using bytes 0: Op.getValueSizeInBytes() - 1 in the or.; // CalculateByteProvider would not have returned Op as source if we; // used a byte that is outside its ValueType. Thus, we are free to; // ANY_EXTEND as the extended bits are dont-cares.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:66,Availability,mask,masks,66,// Canonicalize the expression in an attempt to have fewer unique masks; // and therefore fewer registers used to hold the masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:123,Availability,mask,masks,123,// Canonicalize the expression in an attempt to have fewer unique masks; // and therefore fewer registers used to hold the masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:67,Availability,mask,mask,67,"// Select 0xc for each lane used from source operand. Zero has 0xc mask; // set, 0xff have 0xff in the mask, actual lanes are in the 0-3 range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:103,Availability,mask,mask,103,"// Select 0xc for each lane used from source operand. Zero has 0xc mask; // set, 0xff have 0xff in the mask, actual lanes are in the 0-3 range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:37,Availability,mask,mask,37,// Kill zero bytes selected by other mask. Zero value is 0xc.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:11,Availability,mask,masks,11,// Combine masks,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachePolicy,3,// cachePolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:148,Safety,unsafe,unsafe,148,"// If it's free to do so, push canonicalizes further up the source, which may; // find a canonical source.; //; // TODO: More opcodes. Note this is unsafe for the _ieee minnum/maxnum for; // sNaNs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:77,Availability,avail,available,77,"// Note: we could also extend to i32 and use i32 med3 if i16 med3 is; // not available, but this is unlikely to be profitable as constants; // will often need to be materialized & extended, especially on; // pre-GFX10 where VOP3 instructions couldn't take literal operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Modifiability,extend,extend,23,"// Note: we could also extend to i32 and use i32 med3 if i16 med3 is; // not available, but this is unlikely to be profitable as constants; // will often need to be materialized & extended, especially on; // pre-GFX10 where VOP3 instructions couldn't take literal operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:180,Modifiability,extend,extended,180,"// Note: we could also extend to i32 and use i32 med3 if i16 med3 is; // not available, but this is unlikely to be profitable as constants; // will often need to be materialized & extended, especially on; // pre-GFX10 where VOP3 instructions couldn't take literal operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:24,Availability,avail,available,24,"// med3 for f16 is only available on gfx9+, and not available for v2f16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:52,Availability,avail,available,52,"// med3 for f16 is only available on gfx9+, and not available for v2f16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:14,Safety,safe,safe,14,"// This isn't safe with signaling NaNs because in IEEE mode, min/max on a; // signaling NaN gives a quiet NaN. The quiet NaN input to the min would; // then give the other result, which is different from med3 with a NaN; // input.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:35,Safety,safe,safe,35,"// const_a, const_b, x -> clamp is safe in all cases including signaling; // nans.; // FIXME: Should this be allowing -0.0?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:46,Availability,avail,available,46,// On some architectures (GFX9) movrel is not available and it's better; // to expand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:16,Availability,avail,available,16,"// If movrel is available, use it instead of expanding for vector of 8; // elements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:110,Performance,load,load,110,// Try to turn sub-dword accesses of vectors into accesses of the same 32-bit; // elements. This exposes more load reduction opportunities by replacing; // multiple small extract_vector_elements with a single 32-bit extract.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:25,Security,access,accesses,25,// Try to turn sub-dword accesses of vectors into accesses of the same 32-bit; // elements. This exposes more load reduction opportunities by replacing; // multiple small extract_vector_elements with a single 32-bit extract.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:50,Security,access,accesses,50,// Try to turn sub-dword accesses of vectors into accesses of the same 32-bit; // elements. This exposes more load reduction opportunities by replacing; // multiple small extract_vector_elements with a single 32-bit extract.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:97,Security,expose,exposes,97,// Try to turn sub-dword accesses of vectors into accesses of the same 32-bit; // elements. This exposes more load reduction opportunities by replacing; // multiple small extract_vector_elements with a single 32-bit extract.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize f16 fmed3 pattern performed on f32. On gfx8 there is no f16 fmed3,; // and expanding it with min/max saves 1 instruction vs. casting to f32 and; // casting back.; // fptrunc (f32 (fmed3 (fpext f16:a, fpext f16:b, fpext f16:c))) =>; // fmin(fmax(a, b), fmax(fmin(a, b), c))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Performance,perform,performed,30,"// Optimize f16 fmed3 pattern performed on f32. On gfx8 there is no f16 fmed3,; // and expanding it with min/max saves 1 instruction vs. casting to f32 and; // casting back.; // fptrunc (f32 (fmed3 (fpext f16:a, fpext f16:b, fpext f16:c))) =>; // fmin(fmax(a, b), fmax(fmin(a, b), c))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:31,Performance,perform,perform,31,"// For a reassociatable opcode perform:; // op x, (op y, z) -> op (op x, z), y, if x and z are uniform",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,"// Avoid the fold if it would unduly increase the number of multiplies due to; // multiple uses, except on hardware with full-rate multiply-add (which is; // part of full-rate 64-bit ops).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:94,Modifiability,extend,extended,94,"// The operands and final result all have the same number of bits. If; // operands need to be extended, they can be extended with garbage. The; // resulting garbage in the high bits of the mad_[iu]64_[iu]32 result is; // truncated away in the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:116,Modifiability,extend,extended,116,"// The operands and final result all have the same number of bits. If; // operands need to be extended, they can be extended with garbage. The; // resulting garbage in the high bits of the mad_[iu]64_[iu]32 result is; // truncated away in the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:271,Integrability,depend,depending,271,"// The basic code generated is conceptually straightforward. Pseudo code:; //; // accum = mad_64_32 lhs.lo, rhs.lo, accum; // accum.hi = add (mul lhs.hi, rhs.lo), accum.hi; // accum.hi = add (mul lhs.lo, rhs.hi), accum.hi; //; // The second and third lines are optional, depending on whether the factors; // are {sign,zero}-extended or not.; //; // The actual DAG is noisier than the pseudo code, but only due to; // instructions that disassemble values into low and high parts, and; // assemble the final result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:324,Modifiability,extend,extended,324,"// The basic code generated is conceptually straightforward. Pseudo code:; //; // accum = mad_64_32 lhs.lo, rhs.lo, accum; // accum.hi = add (mul lhs.hi, rhs.lo), accum.hi; // accum.hi = add (mul lhs.lo, rhs.hi), accum.hi; //; // The second and third lines are optional, depending on whether the factors; // are {sign,zero}-extended or not.; //; // The actual DAG is noisier than the pseudo code, but only due to; // instructions that disassemble values into low and high parts, and; // assemble the final result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:82,Availability,mask,mask,82,"// Attempt to find Src vector which contains our SDValue, if so, add our; // perm mask to the existing one. If we are unable to find a match for the; // first SDValue, attempt to find match for the second.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:95,Availability,mask,mask,95,"// If we have multiple sources in the chain, combine them via perms (using; // calculated perm mask) and Ors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:182,Usability,simpl,simply,182,"// There are 9 possible permutations of; // {S0IsUnsigned, S0IsSigned, S1IsUnsigned, S1IsSigned}; // In two permutations, the sign bits are known to be the same for both Ops,; // so simply return Signed / Unsigned corresponding to the MSB",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:269,Modifiability,extend,extended,269,"// In the remaining five permutations, we don't know the value of the sign; // bit for at least one Op. Since we have a valid ByteProvider, we know that; // the upper bits must be extension bits. Thus, the only ways for the sign; // bit to be unknown is if it was sign extended from unknown value, or if it; // was any extended. In either case, it is correct to use the signed; // version of the signedness semantics of dot4; // In two of such permutations, we known the sign bit is set for; // one op, and the other is unknown. It is okay to used signed version of; // dot4.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:319,Modifiability,extend,extended,319,"// In the remaining five permutations, we don't know the value of the sign; // bit for at least one Op. Since we have a valid ByteProvider, we know that; // the upper bits must be extension bits. Thus, the only ways for the sign; // bit to be unknown is if it was sign extended from unknown value, or if it; // was any extended. In either case, it is correct to use the signed; // version of the signedness semantics of dot4; // In two of such permutations, we known the sign bit is set for; // one op, and the other is unknown. It is okay to used signed version of; // dot4.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Availability,Mask,Masks,3,"// Masks were constructed with assumption that we would find a chain of; // length 4. If not, then we need to 0 out the MSB bits (via perm mask of; // 0x0c) so they do not affect dot calculation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:139,Availability,mask,mask,139,"// Masks were constructed with assumption that we would find a chain of; // length 4. If not, then we need to 0 out the MSB bits (via perm mask of; // 0x0c) so they do not affect dot calculation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:62,Usability,undo,undoes,62,// Try to get the fneg to fold into the source modifier. This undoes generic; // DAG combines and folds them into the mad.; //; // Only do this if we are not trying to support denormals. v_mad_f32 does; // not support denormals ever.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:8,Integrability,contract,contract,8,"// fdiv contract 1.0, (sqrt contract x) -> rsq for f16; // fdiv contract -1.0, (sqrt contract x) -> fneg(rsq) for f16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:28,Integrability,contract,contract,28,"// fdiv contract 1.0, (sqrt contract x) -> rsq for f16; // fdiv contract -1.0, (sqrt contract x) -> fneg(rsq) for f16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:64,Integrability,contract,contract,64,"// fdiv contract 1.0, (sqrt contract x) -> rsq for f16; // fdiv contract -1.0, (sqrt contract x) -> fneg(rsq) for f16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:85,Integrability,contract,contract,85,"// fdiv contract 1.0, (sqrt contract x) -> rsq for f16; // fdiv contract -1.0, (sqrt contract x) -> fneg(rsq) for f16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:150,Integrability,contract,contract,150,"// fdot2_f32_f16 always flushes fp32 denormal operand and output to zero,; // regardless of the denorm mode setting. Therefore,; // unsafe-fp-math/fp-contract is sufficient to allow generating fdot2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:132,Safety,unsafe,unsafe-fp-math,132,"// fdot2_f32_f16 always flushes fp32 denormal operand and output to zero,; // regardless of the denorm mode setting. Therefore,; // unsafe-fp-math/fp-contract is sufficient to allow generating fdot2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:9,Modifiability,Extend,Extend,9,// TODO: Extend type shouldn't matter (assuming legal types).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Usability,simpl,simplified,6,"// We simplified Src. If this node is not dead, visit it again so it is; // folded properly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:60,Testability,assert,assert,60,"// These are folded out, but on the chance it happens don't assert.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Abort,Abort,3,// Abort if we can't understand the usage,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Abort,Abort,3,// Abort if we have more than one user per component.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Abort,Abort,3,// Abort if there's no change,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,// Update chain.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,// Update the users of the node with the new indices,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:88,Deployability,update,updated,88,/// Fold the instructions after selecting them.; /// Returns null if users were already updated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:150,Availability,failure,failure,150,// Any MIMG instructions that use tfe or lwe require an initialization of the; // result register that will be written in the case of a memory access failure.; // The required code is also added to tie this init code to the result of the; // img instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:143,Security,access,access,143,// Any MIMG instructions that use tfe or lwe require an initialization of the; // result register that will be written in the case of a memory access failure.; // The required code is also added to tie this init code to the result of the; // img instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:79,Availability,error,error,79,// Abandon attempt if the dst size isn't large enough; // - this is in fact an error but this is picked up elsewhere and; // reported correctly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:167,Availability,error,error,167,"// Final initialized value will be in here; // If PRTStrictNull feature is enabled (the default) then initialize; // all the result registers to 0, otherwise just the error indication; // register (VGPRn+1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Integrability,depend,depending,30,/// Assign the register class depending on the number of; /// bits set in the writemask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:59,Security,access,access,59,// Figure out which registers should be reserved for stack access. Only after; // the function is legalized do we know all of the non-spill stack objects or if; // calls are present.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:58,Security,access,access,58,// Callable functions have fixed registers used for stack access.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Testability,log,logic,19,// TODO: Move this logic to getReservedRegs(); // Reserve the SGPR(s) to save/restore EXEC for WWM spill/copy handling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:88,Testability,test,testcases,88,// We need to worry about replacing the default register with itself in case; // of MIR testcases missing the MFI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:31,Performance,cache,cache,31,"// On GFX10 I$ is 4 x 64 bytes cache lines.; // By default prefetcher keeps one cache line behind and reads two ahead.; // We can modify it with S_INST_PREFETCH for larger loops to have two lines; // behind and one ahead.; // Therefor we can benefit from aligning loop headers if loop fits 192 bytes.; // If loop fits 64 bytes it always spans no more than two cache lines and; // does not need an alignment.; // Else if loop is less or equal 128 bytes we do not need to modify prefetch,; // Else if loop is less or equal 192 bytes we need two lines behind.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:80,Performance,cache,cache,80,"// On GFX10 I$ is 4 x 64 bytes cache lines.; // By default prefetcher keeps one cache line behind and reads two ahead.; // We can modify it with S_INST_PREFETCH for larger loops to have two lines; // behind and one ahead.; // Therefor we can benefit from aligning loop headers if loop fits 192 bytes.; // If loop fits 64 bytes it always spans no more than two cache lines and; // does not need an alignment.; // Else if loop is less or equal 128 bytes we do not need to modify prefetch,; // Else if loop is less or equal 192 bytes we need two lines behind.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:360,Performance,cache,cache,360,"// On GFX10 I$ is 4 x 64 bytes cache lines.; // By default prefetcher keeps one cache line behind and reads two ahead.; // We can modify it with S_INST_PREFETCH for larger loops to have two lines; // behind and one ahead.; // Therefor we can benefit from aligning loop headers if loop fits 192 bytes.; // If loop fits 64 bytes it always spans no more than two cache lines and; // does not need an alignment.; // Else if loop is less or equal 128 bytes we do not need to modify prefetch,; // Else if loop is less or equal 192 bytes we need two lines behind.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Performance,load,load,10,// A flat load may access private memory.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Security,access,access,19,// A flat load may access private memory.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:129,Energy Efficiency,efficient,efficient,129,"// The amdgpu-unsafe-fp-atomics attribute enables generation of unsafe; // floating point atomic instructions. May generate more efficient code,; // but may not respect rounding and denormal modes, and may give incorrect; // results for certain memory destinations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:14,Safety,unsafe,unsafe-fp-atomics,14,"// The amdgpu-unsafe-fp-atomics attribute enables generation of unsafe; // floating point atomic instructions. May generate more efficient code,; // but may not respect rounding and denormal modes, and may give incorrect; // results for certain memory destinations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:64,Safety,unsafe,unsafe,64,"// The amdgpu-unsafe-fp-atomics attribute enables generation of unsafe; // floating point atomic instructions. May generate more efficient code,; // but may not respect rounding and denormal modes, and may give incorrect; // results for certain memory destinations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:118,Availability,mask,mask,118,// FIXME: This is a workaround for DivergenceAnalysis not understanding always; // uniform values (as produced by the mask results of control flow intrinsics); // used outside of divergent blocks. The phi users need to also be treated as; // always uniform.; //; // FIXME: DA is no longer in-use. Does this still apply to UniformityAnalysis?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:38,Availability,mask,mask,38,// FIXME: We assume we never cast the mask results of a control flow; // intrinsic.; // Early exit if the type won't be consistent as a compile time hack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:53,Security,access,access,53,// Check if we have a good chance to form the memory access pattern with the; // base and offset,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:73,Performance,load,load,73,// Propagate metadata set by AMDGPUAnnotateUniformValues to the MMO of a load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:425,Performance,load,loaded,425,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:805,Performance,load,loaded,805,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:822,Performance,load,load,822,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:892,Performance,load,loaded,892,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1092,Performance,load,loaded,1092,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1234,Performance,load,loaded,1234,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1260,Performance,load,loaded,1260,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1303,Performance,load,loaded,1303,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1348,Performance,load,loaded,1348,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:125,Deployability,release,release,125,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:181,Deployability,release,release,181,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:271,Deployability,release,release,271,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:7,Performance,optimiz,optimization,7,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:70,Performance,cache,cache,70,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:232,Safety,Avoid,Avoid,232,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:43,Integrability,Interface,Interface,43,"//===-- SIISelLowering.h - SI DAG Lowering Interface ------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI DAG Lowering interface definition; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:411,Integrability,interface,interface,411,"//===-- SIISelLowering.h - SI DAG Lowering Interface ------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI DAG Lowering interface definition; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:38,Performance,load,load,38,"// Call DAG.getMemIntrinsicNode for a load, but first widen a dwordx3 type to; // dwordx4 if on SI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:107,Modifiability,extend,extending,107,"/// Converts \p Op, which must be of floating point type, to the; /// floating point type \p VT, by either extending or truncating it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:213,Testability,log,logic,213,"// Convert the i128 that an addrspace(8) pointer is natively represented as; // into the v4i32 that all the buffer intrinsics expect to receive. We can't; // add register classes for i128 on pain of the promotion logic going haywire,; // so this slightly ugly hack is what we've got. If passed a non-pointer; // argument (as would be seen in older buffer intrinsics), does nothing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:3,Integrability,Wrap,Wrap,3,// Wrap a 64-bit pointer into a v4i32 (which is how all SelectionDAG code; // represents ptr addrspace(8)) using the flags specified in the intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:34,Performance,load,loads,34,// Handle 8 bit and 16 bit buffer loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:2,Availability,Mask,Mask,2,/*Mask*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:2,Security,Access,AccessTy,2,/*AccessTy*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp:30,Modifiability,config,configured,30,// Check if hardware has been configured to expect color or depth exports.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp:3,Deployability,Update,Update,3,// Update dominator tree,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize out branches to the next block.; // This only occurs in -O0 when BranchFolding is not executed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:1797,Energy Efficiency,schedul,scheduling,1797,"nstructions with close by immediate offsets.; // This will fuse operations such as; // ds_read_b32 v0, v2 offset:16; // ds_read_b32 v1, v2 offset:32; // ==>; // ds_read2_b32 v[0:1], v2, offset0:4 offset1:8; //; // The same is done for certain SMEM and VMEM opcodes, e.g.:; // s_buffer_load_dword s4, s[0:3], 4; // s_buffer_load_dword s5, s[0:3], 8; // ==>; // s_buffer_load_dwordx2 s[4:5], s[0:3], 4; //; // This pass also tries to promote constant offset to the immediate by; // adjusting the base. It tries to use a base from the nearby instructions that; // allows it to have a 13bit constant offset and then promotes the 13bit offset; // to the immediate.; // E.g.; // s_movk_i32 s0, 0x1800; // v_add_co_u32_e32 v0, vcc, s0, v2; // v_addc_co_u32_e32 v1, vcc, 0, v6, vcc; //; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[0:1], off; // =>; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[5:6], off offset:2048; //; // Future improvements:; //; // - This is currently missing stores of constants because loading; // the constant into the data register is placed between the stores, although; // this is arguably a scheduling problem.; //; // - Live interval recomputing seems inefficient. This currently only matches; // one pair, and recomputes live intervals and moves on to the next pair. It; // would be better to compute a list of all merges that need to occur.; //; // - With a list of instructions to process, we can also merge more. If a; // cluster of loads have offsets that are too large to fit in the 8-bit; // offsets, but are close enough to fit in the 8 bits, we can add to the base; // pointer and use the new reduced offsets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:2309,Energy Efficiency,reduce,reduced,2309,"nstructions with close by immediate offsets.; // This will fuse operations such as; // ds_read_b32 v0, v2 offset:16; // ds_read_b32 v1, v2 offset:32; // ==>; // ds_read2_b32 v[0:1], v2, offset0:4 offset1:8; //; // The same is done for certain SMEM and VMEM opcodes, e.g.:; // s_buffer_load_dword s4, s[0:3], 4; // s_buffer_load_dword s5, s[0:3], 8; // ==>; // s_buffer_load_dwordx2 s[4:5], s[0:3], 4; //; // This pass also tries to promote constant offset to the immediate by; // adjusting the base. It tries to use a base from the nearby instructions that; // allows it to have a 13bit constant offset and then promotes the 13bit offset; // to the immediate.; // E.g.; // s_movk_i32 s0, 0x1800; // v_add_co_u32_e32 v0, vcc, s0, v2; // v_addc_co_u32_e32 v1, vcc, 0, v6, vcc; //; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[0:1], off; // =>; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[5:6], off offset:2048; //; // Future improvements:; //; // - This is currently missing stores of constants because loading; // the constant into the data register is placed between the stores, although; // this is arguably a scheduling problem.; //; // - Live interval recomputing seems inefficient. This currently only matches; // one pair, and recomputes live intervals and moves on to the next pair. It; // would be better to compute a list of all merges that need to occur.; //; // - With a list of instructions to process, we can also merge more. If a; // cluster of loads have offsets that are too large to fit in the 8-bit; // offsets, but are close enough to fit in the 8 bits, we can add to the base; // pointer and use the new reduced offsets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:1687,Performance,load,loading,1687,"nstructions with close by immediate offsets.; // This will fuse operations such as; // ds_read_b32 v0, v2 offset:16; // ds_read_b32 v1, v2 offset:32; // ==>; // ds_read2_b32 v[0:1], v2, offset0:4 offset1:8; //; // The same is done for certain SMEM and VMEM opcodes, e.g.:; // s_buffer_load_dword s4, s[0:3], 4; // s_buffer_load_dword s5, s[0:3], 8; // ==>; // s_buffer_load_dwordx2 s[4:5], s[0:3], 4; //; // This pass also tries to promote constant offset to the immediate by; // adjusting the base. It tries to use a base from the nearby instructions that; // allows it to have a 13bit constant offset and then promotes the 13bit offset; // to the immediate.; // E.g.; // s_movk_i32 s0, 0x1800; // v_add_co_u32_e32 v0, vcc, s0, v2; // v_addc_co_u32_e32 v1, vcc, 0, v6, vcc; //; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[0:1], off; // =>; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[5:6], off offset:2048; //; // Future improvements:; //; // - This is currently missing stores of constants because loading; // the constant into the data register is placed between the stores, although; // this is arguably a scheduling problem.; //; // - Live interval recomputing seems inefficient. This currently only matches; // one pair, and recomputes live intervals and moves on to the next pair. It; // would be better to compute a list of all merges that need to occur.; //; // - With a list of instructions to process, we can also merge more. If a; // cluster of loads have offsets that are too large to fit in the 8-bit; // offsets, but are close enough to fit in the 8 bits, we can add to the base; // pointer and use the new reduced offsets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:2144,Performance,load,loads,2144,"nstructions with close by immediate offsets.; // This will fuse operations such as; // ds_read_b32 v0, v2 offset:16; // ds_read_b32 v1, v2 offset:32; // ==>; // ds_read2_b32 v[0:1], v2, offset0:4 offset1:8; //; // The same is done for certain SMEM and VMEM opcodes, e.g.:; // s_buffer_load_dword s4, s[0:3], 4; // s_buffer_load_dword s5, s[0:3], 8; // ==>; // s_buffer_load_dwordx2 s[4:5], s[0:3], 4; //; // This pass also tries to promote constant offset to the immediate by; // adjusting the base. It tries to use a base from the nearby instructions that; // allows it to have a 13bit constant offset and then promotes the 13bit offset; // to the immediate.; // E.g.; // s_movk_i32 s0, 0x1800; // v_add_co_u32_e32 v0, vcc, s0, v2; // v_addc_co_u32_e32 v1, vcc, 0, v6, vcc; //; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[0:1], off; // =>; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[5:6], off offset:2048; //; // Future improvements:; //; // - This is currently missing stores of constants because loading; // the constant into the data register is placed between the stores, although; // this is arguably a scheduling problem.; //; // - Live interval recomputing seems inefficient. This currently only matches; // one pair, and recomputes live intervals and moves on to the next pair. It; // would be better to compute a list of all merges that need to occur.; //; // - With a list of instructions to process, we can also merge more. If a; // cluster of loads have offsets that are too large to fit in the 8-bit; // offsets, but are close enough to fit in the 8 bits, we can add to the base; // pointer and use the new reduced offsets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:10,Performance,load,loads,10,"// GLOBAL loads and stores are classified as FLAT initially. If both combined; // instructions are FLAT GLOBAL adjust the class to GLOBAL_LOAD or GLOBAL_STORE.; // If either or both instructions are non segment specific FLAT the resulting; // combined operation will be FLAT, potentially promoting one of the GLOBAL; // operations to FLAT.; // For other instructions return the original unmodified class.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:125,Security,access,access,125,// Given that \p CI and \p Paired are adjacent memory operations produce a new; // MMO for the combined operation with a new access size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:86,Energy Efficiency,power,power,86,"// Return the value in the inclusive range [Lo,Hi] that is aligned to the; // highest power of two. Note that the result is well defined for all inputs; // including corner cases like:; // - if Lo == Hi, return that value; // - if Lo == 0, return 0 (even though the ""- 1"" below underflows; // - if Lo > Hi, return 0 (as if the range wrapped around)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:333,Integrability,wrap,wrapped,333,"// Return the value in the inclusive range [Lo,Hi] that is aligned to the; // highest power of two. Note that the result is well defined for all inputs; // including corner cases like:; // - if Lo == Hi, return that value; // - if Lo == 0, return 0 (even though the ""- 1"" below underflows; // - if Lo > Hi, return 0 (as if the range wrapped around)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:67,Performance,load,loads,67,"// TODO: Should be possible to support more formats, but if format loads; // are not dword-aligned, the merged load might not be valid.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:111,Performance,load,load,111,"// TODO: Should be possible to support more formats, but if format loads; // are not dword-aligned, the merged load might not be valid.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:191,Performance,load,load,191,"// Reject cases like:; // dword + dwordx2 -> dwordx3; // dword + dwordx3 -> dwordx4; // If we tried to combine these cases, we would fail to extract a subreg; // for the result of the second load due to SGPR alignment requirements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:39,Energy Efficiency,reduce,reduced,39,// Check if the new offsets fit in the reduced 8-bit range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:104,Energy Efficiency,power,power,104,"// From the range of values we could use for BaseOff, choose the one that; // is aligned to the highest power of two, to maximise the chance that; // the same offset can be reused for other load/store pairs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:190,Performance,load,load,190,"// From the range of values we could use for BaseOff, choose the one that; // is aligned to the highest power of two, to maximise the chance that; // the same offset can be reused for other load/store pairs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:104,Energy Efficiency,power,power,104,"// From the range of values we could use for BaseOff, choose the one that; // is aligned to the highest power of two, to maximise the chance that; // the same offset can be reused for other load/store pairs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:190,Performance,load,load,190,"// From the range of values we could use for BaseOff, choose the one that; // is aligned to the highest power of two, to maximise the chance that; // the same offset can be reused for other load/store pairs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:144,Availability,failure,failure,144,/// This function assumes that CI comes before Paired in a basic block. Return; /// an insertion point for the merged instruction or nullptr on failure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:26,Availability,mask,masks,26,// Check both offsets (or masks for MIMG) can be combined and fit in the; // reduced range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:77,Energy Efficiency,reduce,reduced,77,// Check both offsets (or masks for MIMG) can be combined and fit in the; // reduced range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:18,Availability,down,down,18,// Try to sink CI down to Paired.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:3,Deployability,Update,Update,3,// Update base and offset with the NewBase and NewOffset in MI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:167,Performance,load,loads,167,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:213,Performance,load,load,213,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:259,Performance,load,load,259,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:305,Performance,load,load,305,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:352,Performance,load,load,352,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:399,Performance,load,load,399,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:446,Performance,load,load,446,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:456,Performance,optimiz,optimization,456,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:921,Performance,load,load,921,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:951,Performance,load,load,951,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:981,Performance,load,load,981,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:1007,Performance,load,load,1007,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:1056,Performance,load,load,1056,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:89,Performance,load,load,89,// TODO: Support finding an anchor(with same base) from store addresses or; // any other load addresses where the opcodes are different.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:18,Security,access,accesses,18,"// Treat volatile accesses, ordered accesses and unmodeled side effects as; // barriers. We can look after this barrier for separate merges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:36,Security,access,accesses,36,"// Treat volatile accesses, ordered accesses and unmodeled side effects as; // barriers. We can look after this barrier for separate merges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:15,Usability,resume,resume,15,// Search will resume after this instruction in a separate merge list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:122,Energy Efficiency,schedul,scheduler,122,"// Scan through looking for adjacent LDS operations with constant offsets from; // the same base register. We rely on the scheduler to do the hard work of; // clustering nearby loads, and assume these are all adjacent.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:177,Performance,load,loads,177,"// Scan through looking for adjacent LDS operations with constant offsets from; // the same base register. We rely on the scheduler to do the hard work of; // clustering nearby loads, and assume these are all adjacent.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:129,Performance,optimiz,optimize,129,"// We weren't able to make any changes, so delete the list so we don't; // process the same instructions the next time we try to optimize this; // block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:64,Performance,optimiz,optimization,64,"// We made changes, but also determined that there were no more optimization; // opportunities, so we don't need to reprocess the list",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1352,Availability,mask,mask,1352,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1430,Availability,mask,mask,1430,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1744,Availability,mask,mask,1744,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1832,Availability,mask,mask,1832,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:2054,Availability,mask,mask,2054,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:663,Deployability,update,update,663,"//===-- SILowerControlFlow.cpp - Use predicates for control flow ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %e",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1022,Deployability,update,update,1022,"//===-- SILowerControlFlow.cpp - Use predicates for control flow ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %e",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1336,Deployability,update,update,1336,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1816,Deployability,Update,Update,1816,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1506,Performance,optimiz,optimization,1506,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1882,Performance,optimiz,optimization,1882,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1398,Usability,Clear,Clear,1398,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:10,Availability,redundant,redundant,10,// Remove redundant SI_END_CF instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:10,Safety,redund,redundant,10,// Remove redundant SI_END_CF instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:136,Availability,mask,mask,136,"// If there is only one use of save exec register and that use is SI_END_CF,; // we can optimize SI_IF by returning the full saved exec mask instead of; // just cleared bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:88,Performance,optimiz,optimize,88,"// If there is only one use of save exec register and that use is SI_END_CF,; // we can optimize SI_IF by returning the full saved exec mask instead of; // just cleared bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:161,Usability,clear,cleared,161,"// If there is only one use of save exec register and that use is SI_END_CF,; // we can optimize SI_IF by returning the full saved exec mask instead of; // just cleared bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:120,Safety,safe,safe,120,// Check for SI_KILL_*_TERMINATOR on path from if to endif.; // if there is any such terminator simplifications are not safe.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:96,Usability,simpl,simplifications,96,// Check for SI_KILL_*_TERMINATOR on path from if to endif.; // if there is any such terminator simplifications are not safe.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:45,Energy Efficiency,schedul,scheduling,45,// Add an implicit def of exec to discourage scheduling VALU after this which; // will interfere with trying to form s_and_saveexec_b64 later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:56,Performance,optimiz,optimized,56,// Insert the S_CBRANCH_EXECZ instruction which will be optimized later; // during SIRemoveShortExecBranches.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:142,Deployability,update,update,142,// FIXME: Is there a better way of adjusting the liveness? It shouldn't be; // hard to add another def here but I'm not sure how to correctly update the; // valno.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:50,Availability,mask,mask,50,// This accounts for any modification of the EXEC mask within the block and; // can be optimized out pre-RA when not required.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:87,Performance,optimiz,optimized,87,// This accounts for any modification of the EXEC mask within the block and; // can be optimized out pre-RA when not required.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:59,Availability,mask,masked,59,"// Skip ANDing with exec if the break condition is already masked by exec; // because it is a V_CMP in the same basic block. (We know the break; // condition operand was an i1 in IR, so if it is a VALU instruction it must; // be one with a carry-out.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:84,Availability,mask,mask,84,"// AND the break condition operand with exec, then OR that into the ""loop; // exit"" mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:115,Availability,Alive,AliveBlocks,115,"// Track the set of registers defined in the original block so we don't; // accidentally add the original block to AliveBlocks. AliveBlocks only; // includes blocks which are live through, which excludes live outs and; // local defs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:128,Availability,Alive,AliveBlocks,128,"// Track the set of registers defined in the original block so we don't; // accidentally add the original block to AliveBlocks. AliveBlocks only; // includes blocks which are live through, which excludes live outs and; // local defs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:34,Testability,log,logical,34,"// Returns replace operands for a logical operation, either single result; // for exec or two operands if source was another equivalent operation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:194,Availability,mask,mask,194,"// Search and combine pairs of equivalent instructions, like; // S_AND_B64 x, (S_AND_B64 x, y) => S_AND_B64 x, y; // S_OR_B64 x, (S_OR_B64 x, y) => S_OR_B64 x, y; // One of the operands is exec mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:132,Availability,mask,mask,132,// If the only instruction immediately following this END_CF is another; // END_CF in the only successor we can avoid emitting exec mask restore here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:112,Safety,avoid,avoid,112,// If the only instruction immediately following this END_CF is another; // END_CF in the only successor we can avoid emitting exec mask restore here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:101,Availability,mask,mask,101,// Only skip inner END_CF if outer ENDCF belongs to SI_IF.; // If that belongs to SI_ELSE then saved mask has an inverted value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:37,Availability,mask,mask,37,// Cleanup bit manipulations on exec mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:19,Deployability,update,update,19,// Note: we cannot update block layout and preserve live intervals;; // hence we must insert a branch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:472,Availability,mask,masks,472,"//===-- SILowerI1Copies.cpp - Lower I1 Copies -----------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass lowers all occurrences of i1 values (with a vreg_1 register class); // to lane masks (32 / 64-bit scalar registers). The pass assumes machine SSA; // form and a wave-level control flow graph.; //; // Before this pass, values that are semantically i1 and are defined and used; // within the same basic block are already represented as lane masks in scalar; // registers. However, values that cross basic blocks are always transferred; // between basic blocks in vreg_1 virtual registers and are lowered by this; // pass.; //; // The only instructions that use or define vreg_1 virtual registers are COPY,; // PHI, and IMPLICIT_DEF.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:732,Availability,mask,masks,732,"//===-- SILowerI1Copies.cpp - Lower I1 Copies -----------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass lowers all occurrences of i1 values (with a vreg_1 register class); // to lane masks (32 / 64-bit scalar registers). The pass assumes machine SSA; // form and a wave-level control flow graph.; //; // Before this pass, values that are semantically i1 and are defined and used; // within the same basic block are already represented as lane masks in scalar; // registers. However, values that cross basic blocks are always transferred; // between basic blocks in vreg_1 virtual registers and are lowered by this; // pass.; //; // The only instructions that use or define vreg_1 virtual registers are COPY,; // PHI, and IMPLICIT_DEF.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:193,Availability,mask,mask,193,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:280,Availability,mask,mask,280,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:603,Availability,mask,mask,603,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:777,Availability,mask,mask,777,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:926,Availability,mask,mask,926,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:964,Availability,avail,available,964,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:160,Usability,simpl,simply,160,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:1133,Performance,cache,cache,1133,"/// Helper class that detects loops which require us to lower an i1 COPY into; /// bitwise manipulation.; ///; /// Unfortunately, we cannot use LoopInfo because LoopInfo does not distinguish; /// between loops with the same header. Consider this example:; ///; /// A-+-+; /// | | |; /// B-+ |; /// | |; /// C---+; ///; /// A is the header of a loop containing A, B, and C as far as LoopInfo is; /// concerned. However, an i1 COPY in B that is used in C must be lowered to; /// bitwise operations to combine results from different loop iterations when; /// B has a divergent branch (since by default we will compile this code such; /// that threads in a wave are merged at the entry of C).; ///; /// The following rule is implemented to determine whether bitwise operations; /// are required: use the bitwise lowering for a def in block B if a backward; /// edge to B is reachable without going through the nearest common; /// post-dominator of B and all uses of the def.; ///; /// TODO: This rule is conservative because it does not check whether the; /// relevant branches are actually divergent.; ///; /// The class is designed to cache the CFG traversal so that it can be re-used; /// for multiple defs within the same basic block.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:22,Safety,detect,detects,22,"/// Helper class that detects loops which require us to lower an i1 COPY into; /// bitwise manipulation.; ///; /// Unfortunately, we cannot use LoopInfo because LoopInfo does not distinguish; /// between loops with the same header. Consider this example:; ///; /// A-+-+; /// | | |; /// B-+ |; /// | |; /// C---+; ///; /// A is the header of a loop containing A, B, and C as far as LoopInfo is; /// concerned. However, an i1 COPY in B that is used in C must be lowered to; /// bitwise operations to combine results from different loop iterations when; /// B has a divergent branch (since by default we will compile this code such; /// that threads in a wave are merged at the entry of C).; ///; /// The following rule is implemented to determine whether bitwise operations; /// are required: use the bitwise lowering for a def in block B if a backward; /// edge to B is reachable without going through the nearest common; /// post-dominator of B and all uses of the def.; ///; /// TODO: This rule is conservative because it does not check whether the; /// relevant branches are actually divergent.; ///; /// The class is designed to cache the CFG traversal so that it can be re-used; /// for multiple defs within the same basic block.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:106,Deployability,update,updater,106,"/// Add undef values dominating the loop and the optionally given additional; /// blocks, so that the SSA updater doesn't have to search all the way to the; /// function entry.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:247,Availability,mask,mask,247,"/// Lower all instructions that def or use vreg_1 registers.; ///; /// In a first pass, we lower COPYs from vreg_1 to vector registers, as can; /// occur around inline assembly. We do this first, before vreg_1 registers; /// are changed to scalar mask registers.; ///; /// Then we lower all defs of vreg_1 registers. Phi nodes are lowered before; /// all others, because phi lowering looks through copies and can therefore; /// often make copy lowering unnecessary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:63,Usability,simpl,simple,63,// Phis in a loop that are observed outside the loop receive a simple but; // conservatively correct treatment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:92,Availability,mask,mask,92,/// Return a point at the end of the given \p MBB to insert SALU instructions; /// for lane mask calculation. Take terminators and SCC into account.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:41,Availability,mask,masked,41,// TODO: check whether CurReg is already masked by EXEC,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:473,Availability,mask,mask,473,"//===-- SILowerI1Copies.h --------------------------------------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition of the PhiLoweringHelper class that implements lane; /// mask merging algorithm for divergent i1 phis.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:395,Integrability,Interface,Interface,395,"//===-- SILowerI1Copies.h --------------------------------------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition of the PhiLoweringHelper class that implements lane; /// mask merging algorithm for divergent i1 phis.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:194,Availability,mask,mask,194,"/// Incoming for lane maks phi as machine instruction, incoming register \p Reg; /// and incoming block \p Block are taken from machine instruction.; /// \p UpdatedReg (if valid) is \p Reg lane mask merged with another lane mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:224,Availability,mask,mask,224,"/// Incoming for lane maks phi as machine instruction, incoming register \p Reg; /// and incoming block \p Block are taken from machine instruction.; /// \p UpdatedReg (if valid) is \p Reg lane mask merged with another lane mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:157,Deployability,Update,UpdatedReg,157,"/// Incoming for lane maks phi as machine instruction, incoming register \p Reg; /// and incoming block \p Block are taken from machine instruction.; /// \p UpdatedReg (if valid) is \p Reg lane mask merged with another lane mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:28,Performance,load,loadRegFromStackSlot,28,// Insert in reverse order. loadRegFromStackSlot can insert; // multiple instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:183,Integrability,wrap,wrapping,183,"// Even when we do not change any CSR, we still want to insert the; // prologue and epilogue of the function.; // So set the save points for those.; // Use the points found by shrink-wrapping, if any.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:88,Deployability,update,updateLiveness,88,"// TODO: To support shrink wrapping, this would need to copy; // PrologEpilogInserter's updateLiveness.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:27,Integrability,wrap,wrapping,27,"// TODO: To support shrink wrapping, this would need to copy; // PrologEpilogInserter's updateLiveness.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:115,Energy Efficiency,allocate,allocated,115,"// TODO: This is a workaround to avoid the unmodelled liveness computed with; // whole-wave virtual registers when allocated together with the regular VGPR; // virtual registers. Presently, the liveness computed during the regalloc is; // only uniform (or single lane aware) and it doesn't take account of the; // divergent control flow that exists for our GPUs. Since the WWM registers; // can modify inactive lanes, the wave-aware liveness should be computed for; // the virtual registers to accurately plot their interferences. Without; // having the divergent CFG for the function, it is difficult to implement the; // wave-aware liveness info. Until then, we conservatively extend the liveness; // of the wwm registers into the entire function so that they won't be reused; // without first spilling/splitting their liveranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:679,Modifiability,extend,extend,679,"// TODO: This is a workaround to avoid the unmodelled liveness computed with; // whole-wave virtual registers when allocated together with the regular VGPR; // virtual registers. Presently, the liveness computed during the regalloc is; // only uniform (or single lane aware) and it doesn't take account of the; // divergent control flow that exists for our GPUs. Since the WWM registers; // can modify inactive lanes, the wave-aware liveness should be computed for; // the virtual registers to accurately plot their interferences. Without; // having the divergent CFG for the function, it is difficult to implement the; // wave-aware liveness info. Until then, we conservatively extend the liveness; // of the wwm registers into the entire function so that they won't be reused; // without first spilling/splitting their liveranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:33,Safety,avoid,avoid,33,"// TODO: This is a workaround to avoid the unmodelled liveness computed with; // whole-wave virtual registers when allocated together with the regular VGPR; // virtual registers. Presently, the liveness computed during the regalloc is; // only uniform (or single lane aware) and it doesn't take account of the; // divergent control flow that exists for our GPUs. Since the WWM registers; // can modify inactive lanes, the wave-aware liveness should be computed for; // the virtual registers to accurately plot their interferences. Without; // having the divergent CFG for the function, it is difficult to implement the; // wave-aware liveness info. Until then, we conservatively extend the liveness; // of the wwm registers into the entire function so that they won't be reused; // without first spilling/splitting their liveranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:43,Modifiability,extend,extend,43,// Insert the KILL in the return blocks to extend their liveness untill the; // end of function. Insert a separate KILL for each VGPR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:10,Security,expose,expose,10,"// First, expose any CSR SGPR spills. This is mostly the same as what PEI; // does, but somewhat simpler.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:97,Usability,simpl,simpler,97,"// First, expose any CSR SGPR spills. This is mostly the same as what PEI; // does, but somewhat simpler.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:106,Energy Efficiency,efficient,efficient,106,// Spill callee-saved SGPRs into physical VGPR lanes.; // TODO: This is to ensure the CFIs are static for efficient frame; // unwinding in the debugger. Spilling them into virtual VGPR lanes; // involve regalloc to allocate the physical VGPRs and that might; // cause intermediate spill/split of such liveranges for successful; // allocation. This would result in broken CFI encoding unless the; // regalloc aware CFI generation to insert new CFIs along with the; // intermediate spills is implemented. There is no such support; // currently exist in the LLVM compiler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:215,Energy Efficiency,allocate,allocate,215,// Spill callee-saved SGPRs into physical VGPR lanes.; // TODO: This is to ensure the CFIs are static for efficient frame; // unwinding in the debugger. Spilling them into virtual VGPR lanes; // involve regalloc to allocate the physical VGPRs and that might; // cause intermediate spill/split of such liveranges for successful; // allocation. This would result in broken CFI encoding unless the; // regalloc aware CFI generation to insert new CFIs along with the; // intermediate spills is implemented. There is no such support; // currently exist in the LLVM compiler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:125,Deployability,update,update,125,"// FIXME: The dead frame indices are replaced with a null register from; // the debug value instructions. We should instead, update it with the; // correct register value. But not sure the register value alone is; // adequate to lower the DIExpression. It should be worked out later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp:578,Availability,mask,mask,578,"//===-- SILowerWWMCopies.cpp - Lower Copies after regalloc ---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Lowering the WWM_COPY instructions for various register classes.; /// AMDGPU target generates WWM_COPY instruction to differentiate WWM; /// copy from COPY. This pass generates the necessary exec mask manipulation; /// instructions to replicate 'Whole Wave Mode' and lowers WWM_COPY back to; /// COPY.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp:53,Availability,avail,available,53,// We can't determine the liveness info if LIS isn't available. Early return; // in that case and always assume SCC is live.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp:46,Availability,mask,mask,46,"// For WWM vector copies, manipulate the exec mask around the copy; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:41,Availability,down,down,41,"// TODO: Pick a high register, and shift down, similar to a kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:96,Security,access,access,96,"// Non-entry functions have no special inputs for now, other registers; // required for scratch access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:86,Availability,avail,available,86,"// On GFX908, in order to guarantee copying between AGPRs, we need a scratch; // VGPR available at all times. For now, reserve highest available VGPR. After; // RA, shift it to the lowest available unused VGPR if the one exist.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:135,Availability,avail,available,135,"// On GFX908, in order to guarantee copying between AGPRs, we need a scratch; // VGPR available at all times. For now, reserve highest available VGPR. After; // RA, shift it to the lowest available unused VGPR if the one exist.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:188,Availability,avail,available,188,"// On GFX908, in order to guarantee copying between AGPRs, we need a scratch; // VGPR available at all times. For now, reserve highest available VGPR. After; // RA, shift it to the lowest available unused VGPR if the one exist.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:10,Availability,avail,available,10,"// If the available register tuples are aligned with the kernarg to be; // preloaded use that register, otherwise we need to use a set of SGPRs and; // merge them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:159,Energy Efficiency,allocate,allocate,159,// Skip if this is a function with the amdgpu_cs_chain or; // amdgpu_cs_chain_preserve calling convention and this is a scratch register.; // We never need to allocate a spill for these because we don't even need to; // restore the inactive lanes for them (they're scratchier than the usual; // scratch registers).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:3,Deployability,Update,Update,3,// Update various tables with the new VGPR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:20,Availability,avail,available,20,"// Find the highest available register if called before RA to ensure the; // lowest registers are available for allocation. The LaneVGPR, in that; // case, will be shifted back to the lowest range after VGPR allocation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:98,Availability,avail,available,98,"// Find the highest available register if called before RA to ensure the; // lowest registers are available for allocation. The LaneVGPR, in that; // case, will be shifted back to the lowest range after VGPR allocation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:25,Energy Efficiency,allocate,allocated,25,// This has already been allocated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:25,Energy Efficiency,allocate,allocated,25,// This has already been allocated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:26,Testability,log,logic,26,// FIXME: Move allocation logic out of MachineFunctionInfo and initialize; // once.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:27,Energy Efficiency,allocate,allocated,27,"// All other SGPRs must be allocated on the default stack, so reset the; // stack ID.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:33,Availability,mask,mask,33,// Check and update the optional mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:13,Deployability,update,update,13,// Check and update the optional mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:54,Integrability,interface,interface,54,"//==- SIMachineFunctionInfo.h - SIMachineFunctionInfo interface --*- C++ -*-==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:52,Modifiability,config,config,52,"/// This class keeps track of the SPI_SP_INPUT_ADDR config register, which; /// tells the hardware which interpolation parameters to load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:133,Performance,load,load,133,"/// This class keeps track of the SPI_SP_INPUT_ADDR config register, which; /// tells the hardware which interpolation parameters to load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:50,Energy Efficiency,allocate,allocate,50,// Registers that may be reserved when RA doesn't allocate enough; // registers to plan for the case where an indirect branch ends up; // being needed during branch relaxation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:168,Usability,usab,usable,168,"/// Number of bytes of arguments this function has on the stack. If the callee; /// is expected to restore the argument stack this should be a multiple of 16,; /// all usable during a tail call.; ///; /// The alternative would forbid tail call optimisation in some cases: if we; /// want to transfer control from a function with 8-bytes of stack-argument; /// space to a function with 16-bytes then misalignment of this value would; /// make a stack adjustment necessary, which could not be undone by the; /// callee.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:491,Usability,undo,undone,491,"/// Number of bytes of arguments this function has on the stack. If the callee; /// is expected to restore the argument stack this should be a multiple of 16,; /// all usable during a tail call.; ///; /// The alternative would forbid tail call optimisation in some cases: if we; /// want to transfer control from a function with 8-bytes of stack-argument; /// space to a function with 16-bytes then misalignment of this value would; /// make a stack adjustment necessary, which could not be undone by the; /// callee.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:314,Safety,avoid,avoid,314,"// To track the registers used in instructions that can potentially modify the; // inactive lanes. The WWM instructions and the writelane instructions for; // spilling SGPRs to VGPRs fall under such category of operations. The VGPRs; // modified by them should be spilled/restored at function prolog/epilog to; // avoid any undesired outcome. Each entry in this map holds a pair of values,; // the VGPR and its stack slot index.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:24,Availability,MASK,MASK,24,// To save/restore EXEC MASK around WWM spills and copies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:27,Energy Efficiency,allocate,allocated,27,// Get the scratch SGPR if allocated to save/restore \p Reg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:25,Energy Efficiency,allocate,allocated,25,// Get all scratch SGPRs allocated to copy/restore the SGPR spills.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:21,Energy Efficiency,allocate,allocated,21,// Check if \p FI is allocated for any SGPR spill to a VGPR lane during PEI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:104,Availability,avail,available,104,// To bring the Physical VGPRs in the highest range allocated for CSR SGPR; // spilling into the lowest available range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:52,Energy Efficiency,allocate,allocated,52,// To bring the Physical VGPRs in the highest range allocated for CSR SGPR; // spilling into the lowest available range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:95,Security,access,accesses,95,/// Returns the physical register reserved for use as the resource; /// descriptor for scratch accesses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:115,Testability,test,tests,115,// Note the unset value for this is AMDGPU::SP_REG rather than; // NoRegister. This is mostly a workaround for MIR tests where state that; // can't be directly computed from the function is not preserved in serialized; // MIR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:36,Energy Efficiency,Schedul,Scheduler,36,"//===-- SIMachineScheduler.cpp - SI Scheduler Interface -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:406,Energy Efficiency,Schedul,Scheduler,406,"//===-- SIMachineScheduler.cpp - SI Scheduler Interface -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:46,Integrability,Interface,Interface,46,"//===-- SIMachineScheduler.cpp - SI Scheduler Interface -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:416,Integrability,interface,interface,416,"//===-- SIMachineScheduler.cpp - SI Scheduler Interface -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:8,Energy Efficiency,schedul,scheduler,8,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:41,Energy Efficiency,schedul,scheduling,41,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:763,Energy Efficiency,schedul,scheduler,763,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1225,Energy Efficiency,schedul,scheduling,1225,"ction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typica",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1692,Energy Efficiency,schedul,scheduler,1692,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1721,Energy Efficiency,schedul,scheduling,1721,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1827,Energy Efficiency,schedul,schedules,1827,"ge to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; /",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:2858,Energy Efficiency,schedul,scheduling,2858,"ons into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefro",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:5147,Energy Efficiency,schedul,scheduling,5147,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:731,Integrability,depend,dependencies,731,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:2636,Integrability,depend,dependencies,2636,"W can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is hi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:240,Performance,load,load,240,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:296,Performance,load,load,296,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:411,Performance,load,load,411,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:467,Performance,load,load,467,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:506,Performance,load,load,506,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:607,Performance,load,load,607,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1107,Performance,load,loading,1107,"tecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1379,Performance,load,loading,1379,"/ . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be co",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1437,Performance,latency,latency,1437,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1468,Performance,perform,performance,1468,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1522,Performance,perform,performance,1522,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1564,Performance,cache,cache,1564,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3032,Performance,latency,latency,3032,"; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3293,Performance,latency,latency,3293,"/ right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associate",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3311,Performance,latency,latency,3311,"/ right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associate",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3345,Performance,latency,latency,3345,"/ right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associate",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3364,Performance,cache,cache,3364,"/ right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associate",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3711,Performance,latency,latency,3711,"he block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3898,Performance,cache,caches,3898,"cond the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_load",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3965,Performance,cache,cache,3965," do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload th",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4129,Performance,latency,latency,4129,"is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to p",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4532,Performance,latency,latency,4532,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4558,Performance,load,loading,4558,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4571,Performance,cache,cache,4571,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4619,Performance,throughput,throughput,4619,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4718,Performance,cache,cache,4718,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4911,Performance,cache,cache,4911,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4987,Performance,cache,cache,4987,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:5013,Performance,load,loads,5013,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:5029,Performance,latency,latency,5029,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1551,Safety,predict,predict,1551,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:208,Security,access,accessing,208,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:379,Security,access,accessing,379,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3754,Security,access,access,3754,"he block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:5065,Security,access,access,5065,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1764,Usability,simpl,simpler,1764,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4767,Usability,simpl,simple,4767,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:734,Deployability,release,release,734,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:127,Integrability,depend,depend,127,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:232,Integrability,depend,depend,232,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:492,Integrability,depend,depend,492,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:16,Performance,latency,latency,16,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:93,Performance,latency,latency,93,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:147,Performance,latency,latency,147,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:246,Performance,latency,latency,246,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:366,Performance,latency,latency,366,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:446,Performance,latency,latency,446,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:516,Performance,latency,latency,516,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:585,Performance,load,loads,585,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:704,Performance,load,loaded,704,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:765,Performance,load,loading,765,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Safety,Predict,Predict,3,// Predict register usage after this instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,// Schedule something valid.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:57,Availability,alive,alive,57,"// Goes though all SU. RPTracker captures what had to be alive for the SUs; // to execute, and what is still alive at the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:109,Availability,alive,alive,109,"// Goes though all SU. RPTracker captures what had to be alive for the SUs; // to execute, and what is still alive at the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1056,Energy Efficiency,schedul,scheduling,1056,"// There is several possibilities to distinguish:; // 1) Reg is not input to any instruction in the block, but is output of one; // 2) 1) + read in the block and not needed after it; // 3) 1) + read in the block but needed in another block; // 4) Reg is input of an instruction but another block will read it too; // 5) Reg is input of an instruction and then rewritten in the block.; // result is not read in the block (implies used in another block); // 6) Reg is input of an instruction and then rewritten in the block.; // result is read in the block and not needed in another block; // 7) Reg is input of an instruction and then rewritten in the block.; // result is read in the block but also needed in another block; // LiveInRegs will contains all the regs in situation 4, 5, 6, 7; // We want LiveOutRegs to contain only Regs whose content will be read after; // in another block, and whose content was written in the current block,; // that is we want it to get 1, 3, 5, 7; // Since we made the MIs of a block to be packed all together before; // scheduling, then the LiveIntervals were correct, and the RPTracker was; // able to correctly handle 5 vs 6, 2 vs 3.; // (Note: This is not sufficient for RPTracker to not do mistakes for case 4); // The RPTracker's LiveOutRegs has 1, 3, (some correct or incorrect)4, 5, 7; // Comparing to LiveInRegs is not sufficient to differentiate 4 vs 5, 7; // The use of findDefBetween removes the case 4.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:33,Availability,down,down,33,// Prepares TopRPTracker for top down scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:38,Energy Efficiency,schedul,scheduling,38,// Prepares TopRPTracker for top down scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,// Schedule for real now.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4,Deployability,Release,Release,4,/// Release Successors of the SU that are in the block or not.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Scheduling,3,"// Scheduling this node will trigger a wait,; // thus propagate to other instructions that they do not need to wait either.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:49,Energy Efficiency,schedul,scheduling,49,// We remove links from outside blocks to enable scheduling inside the block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:81,Integrability,depend,depend,81,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:300,Integrability,depend,dependencies,300,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:386,Integrability,depend,dependency,386,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:55,Performance,latency,latency,55,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:232,Performance,latency,latency,232,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:334,Performance,latency,latency,334,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:6,Integrability,depend,dependencies,6,// No dependencies between each other,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:21,Integrability,depend,dependency,21,// Check the type of dependency,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:74,Performance,latency,latency,74,"// If in the path to join the two instructions,; // there is another high latency instruction,; // or instructions colored for another block; // abort the merge.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:145,Safety,abort,abort,145,"// If in the path to join the two instructions,; // there is another high latency instruction,; // or instructions colored for another block; // abort the merge.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:36,Integrability,depend,depends,36,"// If one of the SU in the subgraph depends on the result of SU j,; // there'll be a data dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:90,Integrability,depend,dependency,90,"// If one of the SU in the subgraph depends on the result of SU j,; // there'll be a data dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:107,Integrability,depend,depend,107,"// Add all the required instructions to the block; // These cannot live in another block (because they; // depend (order dependency) on one of the; // instruction in the block, and are required for the; // high latency instruction we add.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:121,Integrability,depend,dependency,121,"// Add all the required instructions to the block; // These cannot live in another block (because they; // depend (order dependency) on one of the; // instruction in the block, and are required for the; // high latency instruction we add.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:211,Performance,latency,latency,211,"// Add all the required instructions to the block; // These cannot live in another block (because they; // depend (order dependency) on one of the; // instruction in the block, and are required for the; // high latency instruction we add.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:54,Integrability,depend,depending,54,"// Traverse TopDown, and give different colors to SUs depending; // on which combination of High Latencies they depend on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:112,Integrability,depend,depend,112,"// Traverse TopDown, and give different colors to SUs depending; // on which combination of High Latencies they depend on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:48,Availability,down,down,48,// Every combination of colors given by the top down; // and bottom up Reserved node dependency,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:85,Integrability,depend,dependency,85,// Every combination of colors given by the top down; // and bottom up Reserved node dependency,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:8,Performance,latency,latency,8,// High latency instructions: already given.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:30,Integrability,depend,depending,30,// TODO: Attribute new colors depending on color; // combination of children.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:33,Performance,load,loading,33,// No predecessor: Vgpr constant loading.; // Low latency instructions usually have a predecessor (the address),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:50,Performance,latency,latency,50,// No predecessor: Vgpr constant loading.; // Low latency instructions usually have a predecessor (the address),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:82,Energy Efficiency,schedul,scheduled,82,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:141,Energy Efficiency,schedul,schedule,141,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:354,Energy Efficiency,schedul,scheduling,354,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:175,Performance,perform,performance,175,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:220,Safety,safe,safety,220,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:71,Integrability,depend,dependencies,71,"// SU is an export instruction. Check whether one of its successor; // dependencies is a non-export, in which case we skip export grouping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:16,Integrability,depend,dependencies,16,// Ignore these dependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:16,Integrability,depend,depends,16,"// A non-export depends on us. Skip export grouping.; // Note that this is a bit pessimistic: We could still group all other; // exports that are not depended on by non-exports, directly or; // indirectly. Simply skipping this particular export but grouping all; // others would not account for indirect dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:150,Integrability,depend,depended,150,"// A non-export depends on us. Skip export grouping.; // Note that this is a bit pessimistic: We could still group all other; // exports that are not depended on by non-exports, directly or; // indirectly. Simply skipping this particular export but grouping all; // others would not account for indirect dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:304,Integrability,depend,dependencies,304,"// A non-export depends on us. Skip export grouping.; // Note that this is a bit pessimistic: We could still group all other; // exports that are not depended on by non-exports, directly or; // indirectly. Simply skipping this particular export but grouping all; // others would not account for indirect dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:206,Usability,Simpl,Simply,206,"// A non-export depends on us. Skip export grouping.; // Note that this is a bit pessimistic: We could still group all other; // exports that are not depended on by non-exports, directly or; // indirectly. Simply skipping this particular export but grouping all; // others would not account for indirect dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:26,Energy Efficiency,schedul,scheduling,26,// Restore links previous scheduling variant has overridden.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:9,Integrability,depend,dependencies,9,// Build dependencies between blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:47,Energy Efficiency,schedul,scheduling,47,// Free root and leafs of all blocks to enable scheduling inside them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:9,Energy Efficiency,schedul,schedule,9,// We do schedule a valid scheduling such that a Block corresponds; // to a range of instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:26,Energy Efficiency,schedul,scheduling,26,// We do schedule a valid scheduling such that a Block corresponds; // to a range of instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:142,Deployability,update,update,142,"// Note: the following code, and the part restoring previous position; // is by far the most expensive operation of the Scheduler.; // Do not update CurrentTop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:120,Energy Efficiency,Schedul,Scheduler,120,"// Note: the following code, and the part restoring previous position; // is by far the most expensive operation of the Scheduler.; // Do not update CurrentTop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Deployability,Update,Update,3,// Update the instruction stream.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Deployability,Update,Update,3,// Update LiveIntervals.; // Note: Moving all instructions and calling handleMove every time; // is the most cpu intensive operation of the scheduler.; // It would gain a lot if there was a way to recompute the; // LiveIntervals for the entire scheduling region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:140,Energy Efficiency,schedul,scheduler,140,// Update LiveIntervals.; // Note: Moving all instructions and calling handleMove every time; // is the most cpu intensive operation of the scheduler.; // It would gain a lot if there was a way to recompute the; // LiveIntervals for the entire scheduling region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:244,Energy Efficiency,schedul,scheduling,244,// Update LiveIntervals.; // Note: Moving all instructions and calling handleMove every time; // is the most cpu intensive operation of the scheduler.; // It would gain a lot if there was a way to recompute the; // LiveIntervals for the entire scheduling region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:2,Deployability,Update,UpdateFlags,2,/*UpdateFlags=*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:64,Energy Efficiency,schedul,schedule,64,// Now we have Block of SUs == Block of MI.; // We do the final schedule for the instructions inside the block.; // The property that all the SUs of the Block are grouped together as MI; // is used for correct reg usage tracking.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Deployability,Update,Update,3,// Update the instruction stream.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Deployability,Update,Update,3,// Update LiveIntervals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:2,Deployability,Update,UpdateFlags,2,/*UpdateFlags=*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:92,Energy Efficiency,schedul,scheduling,92,// Increase LiveOutRegsNumUsages for blocks; // producing registers consumed in another; // scheduling region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:72,Energy Efficiency,schedul,scheduling,72,// Fill LiveRegsConsumers for regs that were already; // defined before scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,// Schedule high latencies early so you can hide them better.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:25,Availability,alive,alive,25,// Tracking of currently alive registers to determine VGPR Usage.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:60,Availability,alive,alive,60,"// We produce this register, thus it must not be previously alive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:8,Energy Efficiency,adapt,adapted,8,// Code adapted from scheduleDAG.cpp; // Does a topological sort over the SUs.; // Both TopDown and BottomUp,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:21,Energy Efficiency,schedul,scheduleDAG,21,// Code adapted from scheduleDAG.cpp; // Does a topological sort over the SUs.; // Both TopDown and BottomUp,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:8,Modifiability,adapt,adapted,8,// Code adapted from scheduleDAG.cpp; // Does a topological sort over the SUs.; // Both TopDown and BottomUp,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:284,Performance,load,loads,284,"// Move low latencies further from their user without; // increasing SGPR usage (in general); // This is to be replaced by a better pass that would; // take into account SGPR usage (based on VGPR Usage; // and the corresponding wavefront count), that would; // try to merge groups of loads if it make sense, etc",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:36,Integrability,depend,depends,36,// Moves COPY instructions on which depends; // the low latency instructions too.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:56,Performance,latency,latency,56,// Moves COPY instructions on which depends; // the low latency instructions too.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:20,Energy Efficiency,Schedul,ScheduleDAGMI,20,"// We reuse several ScheduleDAGMI and ScheduleDAGMILive; // functions, but to make them happy we must initialize; // the default Scheduler implementation (even if we do not; // run it)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:38,Energy Efficiency,Schedul,ScheduleDAGMILive,38,"// We reuse several ScheduleDAGMI and ScheduleDAGMILive; // functions, but to make them happy we must initialize; // the default Scheduler implementation (even if we do not; // run it)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:129,Energy Efficiency,Schedul,Scheduler,129,"// We reuse several ScheduleDAGMI and ScheduleDAGMILive; // functions, but to make them happy we must initialize; // the default Scheduler implementation (even if we do not; // run it)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:27,Energy Efficiency,schedul,scheduling,27,// Fill some stats to help scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:51,Performance,perform,performing,51,"// if VGPR usage is extremely high, try other good performing variants; // which could lead to lower VGPR usage",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:94,Performance,perform,performing,94,"// if VGPR usage is still extremely high, we may spill. Try other variants; // which are less performing, but that could lead to lower VGPR usage.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:50,Energy Efficiency,schedul,scheduling,50,// Tell the outside world about the result of the scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:34,Energy Efficiency,Schedul,Scheduler,34,"//===-- SIMachineScheduler.h - SI Scheduler Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:406,Energy Efficiency,Schedul,Scheduler,406,"//===-- SIMachineScheduler.h - SI Scheduler Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:44,Integrability,Interface,Interface,44,"//===-- SIMachineScheduler.h - SI Scheduler Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:416,Integrability,interface,interface,416,"//===-- SIMachineScheduler.h - SI Scheduler Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:100,Energy Efficiency,schedul,schedule,100,// The block Predecessors and Successors must be all registered; // before fastSchedule().; // Fast schedule with no particular requirement.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:12,Energy Efficiency,schedul,schedule,12,"// Complete schedule that will try to minimize reg pressure and; // low latencies, and will fill liveins and liveouts.; // Needs all MIs to be grouped between BeginBlock and EndBlock.; // The MIs can be moved after the scheduling,; // it is just used to allow correct track of live registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:219,Energy Efficiency,schedul,scheduling,219,"// Complete schedule that will try to minimize reg pressure and; // low latencies, and will fill liveins and liveouts.; // Needs all MIs to be grouped between BeginBlock and EndBlock.; // The MIs can be moved after the scheduling,; // it is just used to allow correct track of live registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:25,Energy Efficiency,schedul,scheduled,25,// Needs the block to be scheduled inside; // TODO: find a way to compute it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:39,Performance,latency,latency,39,// Give a Reserved color to every high latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:80,Integrability,depend,depending,80,// Compute coloring for topdown and bottom traversals with; // different colors depending on dependencies on Reserved colors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:93,Integrability,depend,dependencies,93,// Compute coloring for topdown and bottom traversals with; // different colors depending on dependencies on Reserved colors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:66,Integrability,depend,dependencies,66,// Give color to all non-colored SUs according to Reserved groups dependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:45,Availability,down,down,45,// Divides Blocks having no bottom up or top down dependencies on Reserved groups.; // The new colors are computed according to the dependencies on the other blocks; // formed with colorAccordingToReservedDependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:50,Integrability,depend,dependencies,50,// Divides Blocks having no bottom up or top down dependencies on Reserved groups.; // The new colors are computed according to the dependencies on the other blocks; // formed with colorAccordingToReservedDependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:132,Integrability,depend,dependencies,132,// Divides Blocks having no bottom up or top down dependencies on Reserved groups.; // The new colors are computed according to the dependencies on the other blocks; // formed with colorAccordingToReservedDependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:118,Integrability,depend,depend,118,"// Merge Constant loads that have all their users into another group to the group.; // (TODO: else if all their users depend on the same group, put them there)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:18,Performance,load,loads,18,"// Merge Constant loads that have all their users into another group to the group.; // (TODO: else if all their users depend on the same group, put them there)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:88,Integrability,depend,depending,88,// Divides Blocks with important size.; // Idea of implementation: attribute new colors depending on topdown and; // bottom up links to other blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:59,Energy Efficiency,schedul,scheduling,59,// Put in one group all instructions with no users in this scheduling region; // (we'd want these groups be at the end).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:10,Energy Efficiency,schedul,schedulable,10,// Num of schedulable unscheduled blocks reading the register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:41,Energy Efficiency,schedul,scheduling,41,// Check register pressure change; // by scheduling a block with these LiveIn and LiveOut.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:35,Energy Efficiency,Schedul,Scheduling,35,// For moveLowLatencies. After all Scheduling variants are tested.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:59,Testability,test,tested,59,// For moveLowLatencies. After all Scheduling variants are tested.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:23,Energy Efficiency,schedul,schedule,23,// Entry point for the schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:9,Energy Efficiency,schedul,scheduling,9,"// After scheduling is done, improve low latency placements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:41,Performance,latency,latency,41,"// After scheduling is done, improve low latency placements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:18,Energy Efficiency,schedul,scheduling,18,// Some stats for scheduling inside blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Integrability,synchroniz,synchronization,15,/// The atomic synchronization scopes supported by the AMDGPU target.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:35,Security,access,accessed,35,/// The address spaces that can be accessed by a FLAT instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:20,Integrability,synchroniz,synchronization,20,/// \returns Atomic synchronization scope of the machine instruction used to; /// create this SIMemOpInfo.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:13,Availability,Failure,Failure,13,/// \returns Failure ordering constraint of the machine instruction used to; /// create this SIMemOpInfo.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:35,Security,access,accessed,35,/// \returns The address spaces be accessed by the machine; /// instruction used to create this SIMemOpInfo.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:28,Security,access,access,28,"/// \returns True if memory access of the machine instruction used to; /// create this SIMemOpInfo is volatile, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:28,Security,access,access,28,"/// \returns True if memory access of the machine instruction used to; /// create this SIMemOpInfo is nontemporal, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Integrability,message,message,24,/// Reports unsupported message \p Msg for \p MI to LLVM context.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Integrability,synchroniz,synchronization,24,"/// Inspects the target synchronization scope \p SSID and determines; /// the SI atomic scope it corresponds to, the address spaces it; /// covers, and whether the memory ordering applies between address; /// spaces.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:51,Security,access,accessed,51,/// \return Return a bit set of the address spaces accessed by \p AS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:31,Security,access,accessing,31,/// Construct class to support accessing the machine memory operands; /// of instructions in the machine function \p MF.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:13,Performance,Load,Load,13,"/// \returns Load info if \p MI is a load operation, ""std::nullopt"" otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:37,Performance,load,load,37,"/// \returns Load info if \p MI is a load operation, ""std::nullopt"" otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:22,Performance,cache,cache,22,/// Whether to insert cache invalidating instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:13,Performance,cache,cache,13,/// Create a cache control for the subtarget \p ST.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Deployability,Update,Update,4,/// Update \p MI memory load instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Performance,load,load,24,/// Update \p MI memory load instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:55,Performance,cache,caches,55,/// Update \p MI memory load instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Deployability,Update,Update,4,/// Update \p MI memory store instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:56,Performance,cache,caches,56,/// Update \p MI memory store instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Deployability,Update,Update,4,/// Update \p MI memory read-modify-write instruction to bypass any caches up; /// to the \p Scope memory scope for address spaces \p AddrSpace. Return true; /// iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:68,Performance,cache,caches,68,/// Update \p MI memory read-modify-write instruction to bypass any caches up; /// to the \p Scope memory scope for address spaces \p AddrSpace. Return true; /// iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Deployability,Update,Update,4,/// Update \p MI memory instruction of kind \p Op associated with address; /// spaces \p AddrSpace to indicate it is volatile and/or nontemporal. Return; /// true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Performance,Cache,Cache,4,/// Cache Control.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:12,Performance,load,load,12,"/// Expands load operation \p MI. Returns true if instructions are; /// added/deleted or \p MI is modified, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:3,Security,Validat,Validator,3,// Validator should check whether or not MMOs cover the entire set of; // locations accessed by the memory instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:84,Security,access,accessed,84,// Validator should check whether or not MMOs cover the entire set of; // locations accessed by the memory instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// Set L1 cache policy to MISS_EVICT.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:63,Performance,cache,cache,63,// Set L1 cache policy to MISS_EVICT.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:11,Performance,cache,cache,11,/// The L1 cache is write through so does not need to be bypassed. There is no; /// bypass control for the L2 cache at the isa level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:110,Performance,cache,cache,110,/// The L1 cache is write through so does not need to be bypassed. There is no; /// bypass control for the L2 cache at the isa level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:54,Performance,cache,cache,54,"/// Do not set GLC for RMW atomic operations as L0/L1 cache is automatically; /// bypassed, and the GLC bit is instead used to indicate if they are; /// return or no-return.; /// Note: there is no L2 cache coherent bypass control at the ISA level.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:200,Performance,cache,cache,200,"/// Do not set GLC for RMW atomic operations as L0/L1 cache is automatically; /// bypassed, and the GLC bit is instead used to indicate if they are; /// return or no-return.; /// Note: there is no L2 cache coherent bypass control at the ISA level.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:44,Performance,load,load,44,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:128,Performance,cache,cache,128,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:28,Modifiability,config,configures,28,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:42,Performance,cache,cache,42,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:82,Performance,load,loads,82,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:111,Performance,cache,cache,111,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// The L1 cache keeps all memory operations in order for; // wavefronts in the same work-group.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Integrability,synchroniz,synchronizing,210,"// If no cross address space ordering then an ""S_WAITCNT lgkmcnt(0)"" is; // not needed as LDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/GDS memory as LDS operations could be; // reordered with respect to later global/GDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:214,Integrability,synchroniz,synchronizing,214,"// If no cross address space ordering then an GDS ""S_WAITCNT lgkmcnt(0)""; // is not needed as GDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/LDS memory as GDS operations could be; // reordered with respect to later global/LDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to invalidate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to invalidate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:14,Performance,cache,cache,14,// Set the L1 cache policy to MISS_LRU.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:65,Performance,cache,cache,65,// Set the L1 cache policy to MISS_LRU.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:92,Performance,cache,cache,92,/// Do not set glc for store atomic operations as they implicitly write; /// through the L1 cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass. Store atomics implicitly write through the L1; // cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:73,Performance,cache,cache,73,// No cache to bypass. Store atomics implicitly write through the L1; // cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:83,Performance,cache,cache,83,"/// Do not set glc for RMW atomic operations as they implicitly bypass; /// the L1 cache, and the glc bit is instead used to indicate if they are; /// return or no-return.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass. RMW atomics implicitly bypass the L1 cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:60,Performance,cache,cache,60,// No cache to bypass. RMW atomics implicitly bypass the L1 cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:44,Performance,load,load,44,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:128,Performance,cache,cache,128,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:28,Modifiability,config,configures,28,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:42,Performance,cache,cache,42,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:82,Performance,load,loads,82,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:111,Performance,cache,cache,111,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:389,Security,access,access,389,"// In threadgroup split mode the waves of a work-group can be executing on; // different CUs. Therefore need to wait for global or GDS memory operations; // to complete to ensure they are visible to waves in the other CUs.; // Otherwise in non-threadgroup split mode all waves of a work-group are on; // the same CU, so no need to wait for global memory as all waves in the; // work-group access the same the L1, nor wait for GDS as access are ordered; // on a CU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:433,Security,access,access,433,"// In threadgroup split mode the waves of a work-group can be executing on; // different CUs. Therefore need to wait for global or GDS memory operations; // to complete to ensure they are visible to waves in the other CUs.; // Otherwise in non-threadgroup split mode all waves of a work-group are on; // the same CU, so no need to wait for global memory as all waves in the; // work-group access the same the L1, nor wait for GDS as access are ordered; // on a CU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:43,Energy Efficiency,allocate,allocated,43,// In threadgroup split mode LDS cannot be allocated so no need to wait for; // LDS memory operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:26,Performance,load,loads,26,// Ensures that following loads will not see stale remote VMEM data or; // stale local VMEM data with MTYPE NC. Local VMEM data with MTYPE RW and; // CC will never be stale due to the local memory probes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:230,Performance,cache,cache,230,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INVL2"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:339,Performance,cache,cache,339,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INVL2"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:254,Performance,cache,cache,254,"// Inserting a ""S_WAITCNT vmcnt(0)"" before is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a following ""BUFFER_WBL2"". The ""BUFFER_WBL2"" is guaranteed; // to initiate writeback of any dirty cache lines of earlier writes by the; // same wave. A ""S_WAITCNT vmcnt(0)"" is needed after to ensure the; // writeback has completed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:50,Performance,cache,cache,50,// RMW atomic operations implicitly bypass the L1 cache and only use SC1; // to indicate system or agent scope. The SC0 bit is used to indicate if; // they are return or no-return. Leave SC1 bit unset to indicate agent; // scope.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:26,Performance,load,loads,26,// Ensures that following loads will not see stale remote VMEM data or; // stale local VMEM data with MTYPE NC. Local VMEM data with MTYPE RW and; // CC will never be stale due to the local memory probes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:228,Performance,cache,cache,228,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INV"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:337,Performance,cache,cache,337,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INV"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:26,Performance,load,loads,26,// Ensures that following loads will not see stale remote date or local; // MTYPE NC global data. Local MTYPE RW and CC memory will never be stale; // due to the memory probes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:203,Performance,cache,cache,203,"// Inserting ""S_WAITCNT vmcnt(0)"" is not required because the hardware; // does not reorder memory operations with respect to preceeding buffer; // invalidate. The invalidate is guaranteed to remove any cache lines of; // earlier writes and ensures later writes will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:279,Performance,cache,cache,279,"// Inserting ""S_WAITCNT vmcnt(0)"" is not required because the hardware; // does not reorder memory operations with respect to preceeding buffer; // invalidate. The invalidate is guaranteed to remove any cache lines of; // earlier writes and ensures later writes will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:203,Performance,cache,cache,203,"// Inserting ""S_WAITCNT vmcnt(0)"" is not required because the hardware; // does not reorder memory operations with respect to preceeding buffer; // invalidate. The invalidate is guaranteed to remove any cache lines of; // earlier writes and ensures later writes will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:279,Performance,cache,cache,279,"// Inserting ""S_WAITCNT vmcnt(0)"" is not required because the hardware; // does not reorder memory operations with respect to preceeding buffer; // invalidate. The invalidate is guaranteed to remove any cache lines of; // earlier writes and ensures later writes will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:75,Performance,cache,caches,75,"// Could generate ""BUFFER_INV"" but it would do nothing as there are no; // caches to invalidate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:254,Performance,cache,cache,254,"// Inserting a ""S_WAITCNT vmcnt(0)"" before is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a following ""BUFFER_WBL2"". The ""BUFFER_WBL2"" is guaranteed; // to initiate writeback of any dirty cache lines of earlier writes by the; // same wave. A ""S_WAITCNT vmcnt(0)"" is needed after to ensure the; // writeback has completed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:49,Performance,cache,caches,49,"// Do not generate ""BUFFER_WBL2"" as there are no caches it would; // writeback, and would require an otherwise unnecessary; // ""S_WAITCNT vmcnt(0)"".",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:21,Performance,cache,cache,21,// Set the L0 and L1 cache policies to MISS_EVICT.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:76,Performance,cache,cache,76,// Set the L0 and L1 cache policies to MISS_EVICT.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:17,Performance,cache,cache,17,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:51,Performance,load,load,51,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:135,Performance,cache,cache,135,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:25,Modifiability,config,configures,25,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:147,Modifiability,config,configures,147,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:7,Performance,load,loads,7,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:46,Performance,cache,cache,46,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:83,Performance,cache,cache,83,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:168,Performance,cache,cache,168,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Performance,cache,cache,210,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// The L0 cache keeps all memory operations in order for; // work-items in the same wavefront.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Integrability,synchroniz,synchronizing,210,"// If no cross address space ordering then an ""S_WAITCNT lgkmcnt(0)"" is; // not needed as LDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/GDS memory as LDS operations could be; // reordered with respect to later global/GDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:214,Integrability,synchroniz,synchronizing,214,"// If no cross address space ordering then an GDS ""S_WAITCNT lgkmcnt(0)""; // is not needed as GDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/LDS memory as GDS operations could be; // reordered with respect to later global/LDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to invalidate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:21,Performance,cache,cache,21,// Set the L0 and L1 cache policies to MISS_EVICT.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:76,Performance,cache,cache,76,// Set the L0 and L1 cache policies to MISS_EVICT.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:17,Performance,cache,cache,17,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:51,Performance,load,load,51,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:135,Performance,cache,cache,135,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Performance,load,load,24,// Set MALL NOALLOC for load and store instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:25,Modifiability,config,configures,25,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:147,Modifiability,config,configures,147,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:7,Performance,load,loads,7,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:46,Performance,cache,cache,46,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:83,Performance,cache,cache,83,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:168,Performance,cache,cache,168,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Performance,cache,cache,210,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Performance,load,load,24,// Set MALL NOALLOC for load and store instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// The L0 cache keeps all memory operations in order for; // work-items in the same wavefront.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Integrability,synchroniz,synchronizing,210,"// If no cross address space ordering then an ""S_WAITCNT lgkmcnt(0)"" is; // not needed as LDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/GDS memory as LDS operations could be; // reordered with respect to later global/GDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to invalidate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:33,Performance,cache,cache,33,// Set non-temporal hint for all cache levels.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:38,Performance,cache,caches,38,// Atomic instructions already bypass caches to the scope specified by the; // SyncScope operand. Only non-atomic volatile and nontemporal instructions; // need additional treatment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:38,Performance,cache,caches,38,// Atomic instructions already bypass caches to the scope specified by the; // SyncScope operand. Only non-atomic volatile and nontemporal instructions; // need additional treatment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:17,Deployability,release,release,17,"// TODO: If both release and invalidate are happening they could be combined; // to use the single ""BUFFER_WBINV*"" instruction. This could be done by; // reorganizing this code or as part of optimizing SIInsertWaitcnt pass to; // track cache invalidate and write back instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Performance,optimiz,optimizing,191,"// TODO: If both release and invalidate are happening they could be combined; // to use the single ""BUFFER_WBINV*"" instruction. This could be done by; // reorganizing this code or as part of optimizing SIInsertWaitcnt pass to; // track cache invalidate and write back instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:236,Performance,cache,cache,236,"// TODO: If both release and invalidate are happening they could be combined; // to use the single ""BUFFER_WBINV*"" instruction. This could be done by; // reorganizing this code or as part of optimizing SIInsertWaitcnt pass to; // track cache invalidate and write back instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:43,Energy Efficiency,schedul,scheduler,43,// Unbundle instructions after the post-RA scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:3,Availability,Mask,Mask,3,// Mask is a bitmask where a '1' indicates the corresponding Mode bit has a; // known value,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:55,Availability,mask,mask,55,// merge an unknown value by using the unknown value's mask to remove bits; // from the result,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:53,Availability,mask,mask,53,// intersect two Status values to produce a mode and mask that is a subset; // of both values,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:34,Deployability,update,update,34,// Insert a setreg instruction to update the Mode register.; // It is possible (though unlikely) for an instruction to require a change to; // the value of disjoint parts of the Mode register when we don't know the; // value of the intervening bits. In that case we need to use more than one; // setreg instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:171,Deployability,update,update,171,"// In Phase 1 we iterate through the instructions of the block and for each; // instruction we get its mode usage. If the instruction uses the Mode register; // we:; // - update the Change status, which tracks the changes to the Mode register; // made by this block; // - if this instruction's requirements are compatible with the current setting; // of the Mode register we merge the modes; // - if it isn't compatible and an InsertionPoint isn't set, then we set the; // InsertionPoint to the current instruction, and we remember the current; // mode; // - if it isn't compatible and InsertionPoint is set we insert a seteg before; // that instruction (unless this instruction forms part of the block's; // entry requirements in which case the insertion is deferred until Phase 3; // when predecessor exit values are known), and move the insertion point to; // this instruction; // - if this is a setreg instruction we treat it as an incompatible instruction.; // This is sub-optimal but avoids some nasty corner cases, and is expected to; // occur very rarely.; // - on exit we have set the Require, Change, and initial Exit modes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:990,Safety,avoid,avoids,990,"// In Phase 1 we iterate through the instructions of the block and for each; // instruction we get its mode usage. If the instruction uses the Mode register; // we:; // - update the Change status, which tracks the changes to the Mode register; // made by this block; // - if this instruction's requirements are compatible with the current setting; // of the Mode register we merge the modes; // - if it isn't compatible and an InsertionPoint isn't set, then we set the; // InsertionPoint to the current instruction, and we remember the current; // mode; // - if it isn't compatible and InsertionPoint is set we insert a seteg before; // that instruction (unless this instruction forms part of the block's; // entry requirements in which case the insertion is deferred until Phase 3; // when predecessor exit values are known), and move the insertion point to; // this instruction; // - if this is a setreg instruction we treat it as an incompatible instruction.; // This is sub-optimal but avoids some nasty corner cases, and is expected to; // occur very rarely.; // - on exit we have set the Require, Change, and initial Exit modes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:121,Availability,Mask,Mask,121,"// Build a status that is common to all the predecessors by intersecting; // all the predecessor exit status values.; // Mask bits (which represent the Mode bits with a known value) can only be; // added by explicit SETREG instructions or the initial default value -; // the intersection process may remove Mask bits.; // If we find a predecessor that has not yet had an exit value determined; // (this can happen for example if a block is its own predecessor) we defer; // use of that value as the Mask will be all zero, and we will revisit this; // block again later (unless the only predecessor without an exit value is; // this block).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:307,Availability,Mask,Mask,307,"// Build a status that is common to all the predecessors by intersecting; // all the predecessor exit status values.; // Mask bits (which represent the Mode bits with a known value) can only be; // added by explicit SETREG instructions or the initial default value -; // the intersection process may remove Mask bits.; // If we find a predecessor that has not yet had an exit value determined; // (this can happen for example if a block is its own predecessor) we defer; // use of that value as the Mask will be all zero, and we will revisit this; // block again later (unless the only predecessor without an exit value is; // this block).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:499,Availability,Mask,Mask,499,"// Build a status that is common to all the predecessors by intersecting; // all the predecessor exit status values.; // Mask bits (which represent the Mode bits with a known value) can only be; // added by explicit SETREG instructions or the initial default value -; // the intersection process may remove Mask bits.; // If we find a predecessor that has not yet had an exit value determined; // (this can happen for example if a block is its own predecessor) we defer; // use of that value as the Mask will be all zero, and we will revisit this; // block again later (unless the only predecessor without an exit value is; // this block).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:17,Performance,perform,performed,17,"// Processing is performed in a number of phases; // Phase 1 - determine the initial mode required by each block, and add setreg; // instructions for intra block requirements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h:166,Modifiability,extend,extended,166,"/// Return values used for llvm.get.rounding; ///; /// When both the F32 and F64/F16 modes are the same, returns the standard; /// values. If they differ, returns an extended mode starting at 8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h:3,Modifiability,Inherit,Inherit,3,// Inherit everything from RoundingMode,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:18,Testability,log,logical,18,"/// If \p MI is a logical operation on an exec value,; /// return the register copied to.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:153,Availability,mask,mask,153,// XXX - Seems LivePhysRegs doesn't work correctly since it will incorrectly; // report the register as unavailable because a super-register with a lane mask; // is unavailable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:87,Safety,detect,detected,87,"// Check for kills that appear after the terminator instruction, that; // would not be detected by clearKillFlags, since they will cause the; // register to be dead at a later place, causing the verifier to fail.; // We use the candidates to clear the kill flags later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:99,Usability,clear,clearKillFlags,99,"// Check for kills that appear after the terminator instruction, that; // would not be detected by clearKillFlags, since they will cause the; // register to be dead at a later place, causing the verifier to fail.; // We use the candidates to clear the kill flags later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:242,Usability,clear,clear,242,"// Check for kills that appear after the terminator instruction, that; // would not be detected by clearKillFlags, since they will cause the; // register to be dead at a later place, causing the verifier to fail.; // We use the candidates to clear the kill flags later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize sequences emitted for control flow lowering. They are originally; // emitted as the separate operations because spill code may need to be; // inserted for the saved copy of exec.; //; // x = copy exec; // z = s_<op>_b64 x, y; // exec = copy z; // =>; // x = s_<op>_saveexec_b64 y; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:72,Energy Efficiency,schedul,scheduled,72,// Make sure this is inserted after any VALU ops that may have been; // scheduled in between.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:15,Performance,optimiz,optimized,15,"// Inserts the optimized s_mov_b32 / v_cmpx sequence based on the; // operands extracted from a v_cmp ..., s_and_saveexec pattern.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:218,Deployability,pipeline,pipeline,218,"// Record (on GFX10.3 and later) occurences of; // v_cmp_* SGPR, IMM, VGPR; // s_and_saveexec_b32 EXEC_SGPR_DEST, SGPR; // to be replaced with; // s_mov_b32 EXEC_SGPR_DEST, exec_lo; // v_cmpx_* IMM, VGPR; // to reduce pipeline stalls.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:211,Energy Efficiency,reduce,reduce,211,"// Record (on GFX10.3 and later) occurences of; // v_cmp_* SGPR, IMM, VGPR; // s_and_saveexec_b32 EXEC_SGPR_DEST, SGPR; // to be replaced with; // s_mov_b32 EXEC_SGPR_DEST, exec_lo; // v_cmpx_* IMM, VGPR; // to reduce pipeline stalls.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:341,Integrability,depend,dependencies,341,"// Tries to find a possibility to optimize a v_cmp ..., s_and_saveexec; // sequence by looking at an instance of an s_and_saveexec instruction.; // Returns a pointer to the v_cmp instruction if it is safe to replace the; // sequence (see the conditions in the function body). This is after register; // allocation, so some checks on operand dependencies need to be considered.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:34,Performance,optimiz,optimize,34,"// Tries to find a possibility to optimize a v_cmp ..., s_and_saveexec; // sequence by looking at an instance of an s_and_saveexec instruction.; // Returns a pointer to the v_cmp instruction if it is safe to replace the; // sequence (see the conditions in the function body). This is after register; // allocation, so some checks on operand dependencies need to be considered.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:200,Safety,safe,safe,200,"// Tries to find a possibility to optimize a v_cmp ..., s_and_saveexec; // sequence by looking at an instance of an s_and_saveexec instruction.; // Returns a pointer to the v_cmp instruction if it is safe to replace the; // sequence (see the conditions in the function body). This is after register; // allocation, so some checks on operand dependencies need to be considered.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:108,Performance,optimiz,optimization,108,"// If the v_cmp target is in use between v_cmp and s_and_saveexec or after the; // s_and_saveexec, skip the optimization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:419,Availability,mask,mask,419,"//===-- SIOptimizeExecMaskingPreRA.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs exec mask handling peephole optimizations which needs; /// to be done before register allocation to reduce register pressure.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:514,Energy Efficiency,reduce,reduce,514,"//===-- SIOptimizeExecMaskingPreRA.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs exec mask handling peephole optimizations which needs; /// to be done before register allocation to reduce register pressure.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:405,Performance,perform,performs,405,"//===-- SIOptimizeExecMaskingPreRA.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs exec mask handling peephole optimizations which needs; /// to be done before register allocation to reduce register pressure.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:442,Performance,optimiz,optimizations,442,"//===-- SIOptimizeExecMaskingPreRA.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs exec mask handling peephole optimizations which needs; /// to be done before register allocation to reduce register pressure.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize sequence; // %sel = V_CNDMASK_B32_e64 0, 1, %cc; // %cmp = V_CMP_NE_U32 1, %sel; // $vcc = S_AND_B64 $exec, %cmp; // S_CBRANCH_VCC[N]Z; // =>; // $vcc = S_ANDN2_B64 $exec, %cc; // S_CBRANCH_VCC[N]Z; //; // It is the negation pattern inserted by DAGCombiner::visitBRCOND() in the; // rebuildSetCC(). We start with S_CBRANCH to avoid exhaustive search, but; // only 3 first instructions are really needed. S_AND_B64 with exec is a; // required part of the pattern since V_CNDMASK_B32 writes zeroes for inactive; // lanes.; //; // Returns true on success.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:338,Safety,avoid,avoid,338,"// Optimize sequence; // %sel = V_CNDMASK_B32_e64 0, 1, %cc; // %cmp = V_CMP_NE_U32 1, %sel; // $vcc = S_AND_B64 $exec, %cmp; // S_CBRANCH_VCC[N]Z; // =>; // $vcc = S_ANDN2_B64 $exec, %cc; // S_CBRANCH_VCC[N]Z; //; // It is the negation pattern inserted by DAGCombiner::visitBRCOND() in the; // rebuildSetCC(). We start with S_CBRANCH to avoid exhaustive search, but; // only 3 first instructions are really needed. S_AND_B64 with exec is a; // required part of the pattern since V_CNDMASK_B32 writes zeroes for inactive; // lanes.; //; // Returns true on success.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:85,Performance,optimiz,optimization,85,"// Cannot safely mirror live intervals with PHI nodes, so check for these; // before optimization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:10,Safety,safe,safely,10,"// Cannot safely mirror live intervals with PHI nodes, so check for these; // before optimization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:3,Deployability,Update,Update,3,"// Update live intervals for CCReg before potentially removing CmpReg/SelReg,; // and their associated liveness information.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize sequence; // %dst = S_OR_SAVEEXEC %src; // ... instructions not modifying exec ...; // %tmp = S_AND $exec, %dst; // $exec = S_XOR_term $exec, %tmp; // =>; // %dst = S_OR_SAVEEXEC %src; // ... instructions not modifying exec ...; // $exec = S_XOR_term $exec, %dst; //; // Clean up potentially unnecessary code added for safety during; // control flow lowering.; //; // Return whether any changes were made to MBB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:331,Safety,safe,safety,331,"// Optimize sequence; // %dst = S_OR_SAVEEXEC %src; // ... instructions not modifying exec ...; // %tmp = S_AND $exec, %dst; // $exec = S_XOR_term $exec, %tmp; // =>; // %dst = S_OR_SAVEEXEC %src; // ... instructions not modifying exec ...; // $exec = S_XOR_term $exec, %dst; //; // Clean up potentially unnecessary code added for safety during; // control flow lowering.; //; // Return whether any changes were made to MBB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:95,Deployability,update,update,95,"// Skip this if the endpgm has any implicit uses, otherwise we would need; // to be careful to update / remove them.; // S_ENDPGM always has a single imm operand that is not used other than to; // end up in the encoding",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:25,Testability,log,logical,25,"// If the only user of a logical operation is move to exec, fold it now; // to prevent forming of saveexec. I.e.:; //; // %0:sreg_64 = COPY $exec; // %1:sreg_64 = S_AND_B64 %0:sreg_64, %2:sreg_64; // =>; // %1 = S_AND_B64 $exec, %2:sreg_64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1105,Availability,alive,alive,1105,"-----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There sh",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1252,Availability,mask,mask,1252,"ormation.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There should be no further uses after the IF-ELSE region.; ///; ///; /// Waterfall loops get inserted around instructions that use divergent values; /// but can onl",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1369,Safety,safe,safe,1369," This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There should be no further uses after the IF-ELSE region.; ///; ///; /// Waterfall loops get inserted around instructions that use divergent values; /// but can only be executed with a uniform value. For example an indirect call; /// to a divergent address:; /// bb.start:; /// %a = ...; /// %fun = ...; /// ...; /// bb.loop:; /// call %f",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1217,Security,access,accessed,1217,"ormation.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There should be no further uses after the IF-ELSE region.; ///; ///; /// Waterfall loops get inserted around instructions that use divergent values; /// but can onl",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1558,Security,access,accessed,1558,"ation, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There should be no further uses after the IF-ELSE region.; ///; ///; /// Waterfall loops get inserted around instructions that use divergent values; /// but can only be executed with a uniform value. For example an indirect call; /// to a divergent address:; /// bb.start:; /// %a = ...; /// %fun = ...; /// ...; /// bb.loop:; /// call %fun (%a); /// ... // %a can be dead here; /// loop %bb.loop; ///; /// The loop block is executed multiple times, but it is run exactly once for; /",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:673,Usability,simpl,simple,673,"//===--------------------- SIOptimizeVGPRLiveRange.cpp -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the s",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:66,Availability,alive,alive,66,/// Collect the killed registers in the ELSE region which are not alive through; /// the whole THEN region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:15,Performance,optimiz,optimize,15,// We can only optimize AGPR/VGPR virtual register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:84,Performance,optimiz,optimize,84,// The register is live through the path If->Flow or Flow->Endif.; // we should not optimize for such cases.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:15,Performance,optimiz,optimize,15,// We can only optimize AGPR/VGPR virtual register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:10,Modifiability,variab,variable,10,"// If the variable is used after the loop, the register coalescer will; // merge the newly created register and remove the phi node again.; // Just do nothing in that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:3,Usability,Clear,Clear,3,"// Clear Live bit, as we will recalculate afterwards",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:36,Availability,alive,alive,36,// Get the blocks the Reg should be alive through,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:12,Availability,alive,alive,12,// Mark Reg alive through the block if this is a PHI incoming block,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:12,Availability,alive,aliveBlocks,12,// Transfer aliveBlocks from Reg to NewReg,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:118,Deployability,update,update,118,// Replace all uses in the ELSE region or the PHIs in ENDIF block; // Use early increment range because setReg() will update the linked list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:3,Safety,Detect,DetectDeadLanes,3,"// DetectDeadLanes may mark register uses as undef without removing; // them, in which case a non-phi instruction using the original register; // may exist in the Endif block even though the register is not live; // into it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:28,Availability,alive,alive,28,// The optimized Reg is not alive through Flow blocks anymore.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:7,Performance,optimiz,optimized,7,// The optimized Reg is not alive through Flow blocks anymore.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:91,Deployability,update,update,91,// Replace all uses in the LOOP region; // Use early increment range because setReg() will update the linked list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:3,Safety,Detect,Detect,3,// Detect the if-else blocks,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:32,Performance,optimiz,optimized,32,// Collect the registers can be optimized,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:22,Performance,optimiz,optimize,22,// Now we are safe to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:14,Safety,safe,safe,14,// Now we are safe to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:22,Performance,optimiz,optimize,22,// Now we are safe to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:14,Safety,safe,safe,14,// Now we are safe to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:37,Performance,optimiz,optimization,37,"//===- SIPeepholeSDWA.cpp - Peephole optimization for SDWA instructions ---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass tries to apply several peephole SDWA patterns.; ///; /// E.g. original:; /// V_LSHRREV_B32_e32 %0, 16, %1; /// V_ADD_CO_U32_e32 %2, %0, %3; /// V_LSHLREV_B32_e32 %4, 16, %2; ///; /// Replace:; /// V_ADD_CO_U32_sdwa %4, %1, %3; /// dst_sel:WORD_1 dst_unused:UNUSED_PAD src0_sel:WORD_1 src1_sel:DWORD; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:37,Security,access,accessing,37,"// This will work if the tied src is accessing WORD_0, and the dst is; // writing WORD_1. Modifiers don't matter because all the bits that; // would be impacted are being overwritten by the dst.; // Any other case will not work.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:68,Usability,clear,clear,68,// MI should be moved right before v_or_b32.; // For this we should clear all kill flags on uses of MI src-operands or else; // we can encounter problem with use of killed operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:94,Performance,perform,perform,94,"// Convert the V_ADD_CO_U32_e64 into V_ADD_CO_U32_e32. This allows; // isConvertibleToSDWA to perform its transformation on V_ADD_CO_U32_e32 into; // V_ADD_CO_U32_sdwa.; //; // We are transforming from a VOP3 into a VOP2 form of the instruction.; // %19:vgpr_32 = V_AND_B32_e32 255,; // killed %16:vgpr_32, implicit $exec; // %47:vgpr_32, %49:sreg_64_xexec = V_ADD_CO_U32_e64; // %26.sub0:vreg_64, %19:vgpr_32, implicit $exec; // %48:vgpr_32, dead %50:sreg_64_xexec = V_ADDC_U32_e64; // %26.sub1:vreg_64, %54:vgpr_32, killed %49:sreg_64_xexec, implicit $exec; //; // becomes; // %47:vgpr_32 = V_ADD_CO_U32_sdwa; // 0, %26.sub0:vreg_64, 0, killed %16:vgpr_32, 0, 6, 0, 6, 0,; // implicit-def $vcc, implicit $exec; // %48:vgpr_32, dead %50:sreg_64_xexec = V_ADDC_U32_e64; // %26.sub1:vreg_64, %54:vgpr_32, killed $vcc, implicit $exec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:44,Deployability,update,update,44,"// Since the carry output of MI is now VCC, update its use in MISucc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPostRABundler.cpp:464,Performance,load,loads,464,"//===-- SIPostRABundler.cpp -----------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass creates bundles of memory instructions to protect adjacent loads; /// and stores from being rescheduled apart from each other post-RA.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPostRABundler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPostRABundler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp:407,Energy Efficiency,allocate,allocated,407,"//===- SIPreAllocateWWMRegs.cpp - WWM Register Pre-allocation -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Pass to pre-allocated WWM registers; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp:3,Deployability,Update,Update,3,// Update the set of reserved registers to include WWM ones.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:396,Performance,perform,performs,396,"//===-- SIPreEmitPeephole.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs the peephole optimizations before code emission.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:418,Performance,optimiz,optimizations,418,"//===-- SIPreEmitPeephole.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs the peephole optimizations before code emission.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:281,Availability,mask,mask,281,"// Match:; // sreg = -1 or 0; // vcc = S_AND_B64 exec, sreg or S_ANDN2_B64 exec, sreg; // S_CBRANCH_VCC[N]Z; // =>; // S_CBRANCH_EXEC[N]Z; // We end up with this pattern sometimes after basic block placement.; // It happens while combining a block which assigns -1 or 0 to a saved mask; // and another block which consumes that saved mask and then a branch.; //; // While searching this also performs the following substitution:; // vcc = V_CMP; // vcc = S_AND exec, vcc; // S_CBRANCH_VCC[N]Z; // =>; // vcc = V_CMP; // S_CBRANCH_VCC[N]Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:334,Availability,mask,mask,334,"// Match:; // sreg = -1 or 0; // vcc = S_AND_B64 exec, sreg or S_ANDN2_B64 exec, sreg; // S_CBRANCH_VCC[N]Z; // =>; // S_CBRANCH_EXEC[N]Z; // We end up with this pattern sometimes after basic block placement.; // It happens while combining a block which assigns -1 or 0 to a saved mask; // and another block which consumes that saved mask and then a branch.; //; // While searching this also performs the following substitution:; // vcc = V_CMP; // vcc = S_AND exec, vcc; // S_CBRANCH_VCC[N]Z; // =>; // vcc = V_CMP; // S_CBRANCH_VCC[N]Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:392,Performance,perform,performs,392,"// Match:; // sreg = -1 or 0; // vcc = S_AND_B64 exec, sreg or S_ANDN2_B64 exec, sreg; // S_CBRANCH_VCC[N]Z; // =>; // S_CBRANCH_EXEC[N]Z; // We end up with this pattern sometimes after basic block placement.; // It happens while combining a block which assigns -1 or 0 to a saved mask; // and another block which consumes that saved mask and then a branch.; //; // While searching this also performs the following substitution:; // vcc = V_CMP; // vcc = S_AND exec, vcc; // S_CBRANCH_VCC[N]Z; // =>; // vcc = V_CMP; // S_CBRANCH_VCC[N]Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:10,Availability,mask,mask,10,// Invert mask for s_andn2,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:11,Deployability,update,updated,11,// EXEC is updated directly,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:3,Modifiability,Rewrite,Rewrite,3,// Rewrite to unconditional branch,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:3,Integrability,Depend,Depends,3,// Depends only on EXEC,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:42,Performance,optimiz,optimize,42,// Check first terminator for branches to optimize,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:121,Performance,optimiz,optimizeSetGPR,121,// Scan the block for two S_SET_GPR_IDX_ON instructions to see if a; // second is not needed. Do expensive checks in the optimizeSetGPR(); // and limit the distance to 20 instructions for compile time purposes.; // Note: this needs to work on bundles as S_SET_GPR_IDX* instructions; // may be bundled with the instructions they modify.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:294,Performance,load,loads,294,"// A temporary struct to spill SGPRs.; // This is mostly to spill SGPRs to memory. Spilling SGPRs into VGPR lanes emits; // just v_writelane and v_readlane.; //; // When spilling to memory, the SGPRs are written into VGPR lanes and the VGPR; // is saved to scratch (or the other way around for loads).; // For this, a VGPR is required where the needed lanes can be clobbered. The; // RegScavenger can provide a VGPR where currently active lanes can be; // clobbered, but we still need to save inactive lanes.; // The high-level steps are:; // - Try to scavenge SGPR(s) to save exec; // - Try to scavenge VGPR; // - Save needed, all or inactive lanes of a TmpVGPR; // - Spill/Restore SGPRs using TmpVGPR; // - Restore TmpVGPR; //; // To save all lanes of TmpVGPR, exec needs to be saved and modified. If we; // cannot scavenge temporary SGPRs to save exec, we use the following code:; // buffer_store_dword TmpVGPR ; only if active lanes need to be saved; // s_not exec, exec; // buffer_store_dword TmpVGPR ; save inactive lanes; // s_not exec, exec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:127,Performance,load,loads,127,"/* When spilling to stack */; // The SGPRs are written into this VGPR, which is then written to scratch; // (or vice versa for loads).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:30,Performance,load,load,30,// Add an implicit use of the load so it is not dead.; // FIXME This inserts an unnecessary waitcnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:129,Availability,mask,mask,129,"// Write TmpVGPR to memory or read TmpVGPR from memory.; // Either using a single buffer_load/store if exec is set to the needed mask; // or using; // buffer_load; // s_not exec, exec; // buffer_load; // s_not exec, exec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:139,Security,access,accessing,139,"// During ISel lowering we always reserve the stack pointer in entry and chain; // functions, but never actually want to reference it when accessing our own; // frame. If we need a frame pointer we use it, but otherwise we can just use; // an immediate ""0"" which we represent by returning NoRegister.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:75,Energy Efficiency,allocate,allocated,75,"// Reserve special purpose registers.; //; // EXEC_LO and EXEC_HI could be allocated and used as regular register, but; // this seems likely to result in bugs, so I'm marking them as reserved.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:45,Energy Efficiency,allocate,allocated,45,// Reserve null register - it shall never be allocated,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:51,Energy Efficiency,allocate,allocated,51,// Disallow vcc_hi allocation in wave32. It may be allocated but most likely; // will result in bugs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:92,Safety,detect,detected,92,"// We have to assume the SP is needed in case there are calls in the function,; // which is detected after the function is lowered. If we aren't really going; // to need SP, don't bother reserving it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:90,Availability,MASK,MASK,90,// FIXME: Use same reserved register introduced in D149775; // SGPR used to preserve EXEC MASK around WWM spill/copy instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:86,Availability,avail,available,86,"// On GFX908, in order to guarantee copying between AGPRs, we need a scratch; // VGPR available at all times.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:223,Safety,avoid,avoid,223,"// Do not use frame virtual registers. They used to be used for SGPRs, but; // once we reach PrologEpilogInserter, we can no longer spill SGPRs. If the; // scavenger fails, we can increment/decrement the necessary SGPRs to avoid a; // spill.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:26,Integrability,depend,depends,26,"// This is inaccurate. It depends on the instruction and address space. The; // only place where we should hit this is for dealing with frame indexes /; // private accesses, so this is correct in that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:164,Security,access,accesses,164,"// This is inaccurate. It depends on the instruction and address space. The; // only place where we should hit this is for dealing with frame indexes /; // private accesses, so this is correct in that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:52,Performance,load,loads,52,// On gfx90a+ AGPR is a regular VGPR acceptable for loads and stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:83,Performance,perform,perform,83,"// We only have 1 VGPR offset, or 1 SGPR offset. We don't have a free; // SGPR, so perform the add as vector.; // We don't need a base SGPR in the kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:17,Security,access,access,17,// We don't have access to the register scavenger if this function is called; // during PEI::scavengeFrameVirtualRegs() so use LiveUnits in this case.; // TODO: Clobbering SCC is not necessary for scratch instructions in the; // entry.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:86,Usability,simpl,simplify,86,"// We currently only support spilling VGPRs to EltSize boundaries, meaning; // we can simplify the adjustment of Offset here to just scale with; // WavefrontSize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:43,Energy Efficiency,allocate,allocated,43,"// AGPRs to spill VGPRs and vice versa are allocated in a reverse order,; // starting from the last lane. In case if a register cannot be completely; // spilled into another register that will ensure its alignment does not; // change. For targets with VGPR alignment requirement this is important; // in case of flat scratch usage as we might get a scratch_load or; // scratch_store of an unaligned register otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:72,Performance,optimiz,optimization,72,"// The epilog restore of a wwm-scratch register can cause undesired; // optimization during machine-cp post PrologEpilogInserter if the same; // register was assigned for return value ABI lowering with a COPY; // instruction. As given below, with the epilog reload, the earlier COPY; // appeared to be dead during machine-cp.; // ...; // v0 in WWM operation, needs the WWM spill at prolog/epilog.; // $vgpr0 = V_WRITELANE_B32 $sgpr20, 0, $vgpr0; // ...; // Epilog block:; // $vgpr0 = COPY $vgpr1 // outgoing value moved to v0; // ...; // WWM spill restore to preserve the inactive lanes of v0.; // $sgpr4_sgpr5 = S_XOR_SAVEEXEC_B64 -1; // $vgpr0 = BUFFER_LOAD $sgpr0_sgpr1_sgpr2_sgpr3, $sgpr32, 0, 0, 0; // $exec = S_MOV_B64 killed $sgpr4_sgpr5; // ...; // SI_RETURN implicit $vgpr0; // ...; // To fix it, mark the same reg as a tied op for such restore instructions; // so that it marks a usage for the preceding COPY.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:3,Performance,Load,Load,3,// Load/store VGPR,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:81,Safety,detect,detect,81,// There could be undef components of a spilled super register.; // TODO: Can we detect this and skip the spill?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:3,Performance,Load,Load,3,// Load in VGPR data,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:81,Safety,detect,detect,81,// There could be undef components of a spilled super register.; // TODO: Can we detect this and skip the spill?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:17,Performance,load,load,17,// Don't need to load VGPR in.; // Unpack lanes,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:119,Safety,safe,safely,119,/// Special case of eliminateFrameIndex. Returns true if the SGPR was spilled to; /// a VGPR and the stack slot can be safely eliminated when all other users are; /// handled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:9,Security,access,access,9,// Other access to frame index,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:74,Testability,assert,asserts,74,"// removeOperand doesn't fixup tied operand indexes as it goes, so; // it asserts. Untie vdst_in for now and retie them afterwards.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:3,Usability,Undo,Undo,3,// Undo frame register modification.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:129,Safety,avoid,avoid,129,"// We have to produce a carry out, and there isn't a free SGPR pair; // for it. We can keep the whole computation on the SALU to avoid; // clobbering an additional register at the cost of an extra mov.; // We may have 1 free scratch SGPR even though a carry out is; // unavailable. Only one additional mov is needed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:49,Usability,undo,undo,49,"// If there were truly no free SGPRs, we need to undo everything.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:20,Usability,simpl,simply,20,"// If the offset is simply too big, don't convert to a scratch wave offset; // relative index.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:92,Modifiability,rewrite,rewrite,92,"// We want to prefer the smallest register class possible, so we don't want to; // stop and rewrite on anything that looks like a subregister; // extract. Operations mostly don't care about the super register class, so we; // only want to stop on the most basic of copies between the same register; // class.; //; // e.g. if we have something like; // %0 = ...; // %1 = ...; // %2 = REG_SEQUENCE %0, sub0, %1, sub1, %2, sub2; // %3 = COPY %2, sub0; //; // We want to look through the COPY to find:; // => %3 = COPY %0; // Plain copy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:30,Modifiability,extend,extending,30,// TODO: 64-bit operands have extending behavior from 32-bit literal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:68,Energy Efficiency,allocate,allocate,68,"// Do not increase size of registers beyond dword, we would need to allocate; // adjacent registers and constraint regalloc more than needed.; // Always allow dword coalescing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:44,Integrability,Interface,Interface,44,"//===-- SIRegisterInfo.h - SI Register Info Interface ----------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for SIRegisterInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:395,Integrability,Interface,Interface,395,"//===-- SIRegisterInfo.h - SI Register Info Interface ----------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for SIRegisterInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:278,Availability,mask,mask,278,"/// Sub reg indexes for getRegSplitParts.; /// First index represents subreg size from 1 to 16 DWORDs.; /// The inner vector is sorted by bit offset.; /// Provided a register can be fully split with given subregs,; /// all elements of the inner vector combined give a full lane mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:23,Availability,avail,available,23,/// Return the largest available SGPR aligned to \p Align for the register; /// class \p RC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:9,Security,access,access,9,"// Stack access is very expensive. CSRs are also the high registers, and we; // want to minimize the number of used registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:191,Availability,avail,available,191,"// Insert spill or restore instructions.; // When lowering spill pseudos, the RegScavenger should be set.; // For creating spill instructions during frame lowering, where no scavenger; // is available, LiveUnits can be used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:3,Safety,Detect,Detect,3,"// Detect ""Dst = VSrc * VGPR + Imm"" and convert to AK form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:3,Safety,Detect,Detect,3,"// Detect ""Dst = VSrc * Imm + VGPR"" and convert to MK form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:126,Usability,clear,clear,126,"/// Attempt to shrink AND/OR/XOR operations requiring non-inlineable literals.; /// For AND or OR, try using S_BITSET{0,1} to clear or set bits.; /// If the inverse of the immediate is legal, use ANDN2, ORN2 or; /// XNOR (as a ^ b == ~(a ^ ~b)).; /// \returns true if the caller should continue the machine function iterator",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:383,Deployability,release,release,383,"// Match:; // mov t, x; // mov x, y; // mov y, t; //; // =>; //; // mov t, x (t is potentially dead and move eliminated); // v_swap_b32 x, y; //; // Returns next valid instruction pointer if was able to create v_swap_b32.; //; // This shall not be done too early not to prevent possible folding which may; // remove matched moves, and this should preferably be done before RA to; // release saved registers and also possibly after RA which can insert copies; // too.; //; // This is really just a generic peephole that is not a canonical shrinking,; // although requirements match the pass placement and it reduces code size too.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:607,Energy Efficiency,reduce,reduces,607,"// Match:; // mov t, x; // mov x, y; // mov y, t; //; // =>; //; // mov t, x (t is potentially dead and move eliminated); // v_swap_b32 x, y; //; // Returns next valid instruction pointer if was able to create v_swap_b32.; //; // This shall not be done too early not to prevent possible folding which may; // remove matched moves, and this should preferably be done before RA to; // release saved registers and also possibly after RA which can insert copies; // too.; //; // This is really just a generic peephole that is not a canonical shrinking,; // although requirements match the pass placement and it reduces code size too.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:298,Performance,optimiz,optimizations,298,"// If this has a literal constant source that is the same as the; // reversed bits of an inline immediate, replace with a bitreverse of; // that constant. This saves 4 bytes in the common case of materializing; // sign bits.; // Test if we are after regalloc. We only want to do this after any; // optimizations happen because this will confuse them.; // XXX - not exactly a check for post-regalloc run.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:229,Testability,Test,Test,229,"// If this has a literal constant source that is the same as the; // reversed bits of an inline immediate, replace with a bitreverse of; // that constant. This saves 4 bytes in the common case of materializing; // sign bits.; // Test if we are after regalloc. We only want to do this after any; // optimizations happen because this will confuse them.; // XXX - not exactly a check for post-regalloc run.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:17,Testability,log,logic,17,// Shrink scalar logic operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:2429,Availability,recover,recovering,2429,"; ///; /// Whole quad mode is required for derivative computations, but it interferes; /// with shader side effects (stores and atomics). It ensures that WQM is; /// enabled when necessary, but disabled around stores and atomics.; ///; /// When necessary, this pass creates a function prolog; ///; /// S_MOV_B64 LiveMask, EXEC; /// S_WQM_B64 EXEC, EXEC; ///; /// to enter WQM at the top of the function and surrounds blocks of Exact; /// instructions by; ///; /// S_AND_SAVEEXEC_B64 Tmp, LiveMask; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// We also compute when a sequence of instructions requires strict whole; /// wavefront mode (StrictWWM) and insert instructions to save and restore it:; ///; /// S_OR_SAVEEXEC_B64 Tmp, -1; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// When a sequence of instructions requires strict whole quad mode (StrictWQM); /// we use a similar save and restore mechanism and force whole quad mode for; /// those instructions:; ///; /// S_MOV_B64 Tmp, EXEC; /// S_WQM_B64 EXEC, EXEC; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// In order to avoid excessive switching during sequences of Exact; /// instructions, the pass first analyzes which instructions must be run in WQM; /// (aka which instructions produce values that lead to derivative; /// computations).; ///; /// Basic blocks are always exited in WQM as long as some successor needs WQM.; ///; /// There is room for improvement given better control flow analysis:; ///; /// (1) at the top level (outside of control flow statements, and as long as; /// kill hasn't been used), one SGPR can be saved by recovering WQM from; /// the LiveMask (this is implemented for the entry block).; ///; /// (2) when entire regions (e.g. if-else blocks or entire loops) only; /// consist of exact and don't-care instructions, the switch only has to; /// be done at the entry and exit points rather than potentially in each; /// block of the region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:1910,Safety,avoid,avoid,1910,"; ///; /// Whole quad mode is required for derivative computations, but it interferes; /// with shader side effects (stores and atomics). It ensures that WQM is; /// enabled when necessary, but disabled around stores and atomics.; ///; /// When necessary, this pass creates a function prolog; ///; /// S_MOV_B64 LiveMask, EXEC; /// S_WQM_B64 EXEC, EXEC; ///; /// to enter WQM at the top of the function and surrounds blocks of Exact; /// instructions by; ///; /// S_AND_SAVEEXEC_B64 Tmp, LiveMask; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// We also compute when a sequence of instructions requires strict whole; /// wavefront mode (StrictWWM) and insert instructions to save and restore it:; ///; /// S_OR_SAVEEXEC_B64 Tmp, -1; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// When a sequence of instructions requires strict whole quad mode (StrictWQM); /// we use a similar save and restore mechanism and force whole quad mode for; /// those instructions:; ///; /// S_MOV_B64 Tmp, EXEC; /// S_WQM_B64 EXEC, EXEC; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// In order to avoid excessive switching during sequences of Exact; /// instructions, the pass first analyzes which instructions must be run in WQM; /// (aka which instructions produce values that lead to derivative; /// computations).; ///; /// Basic blocks are always exited in WQM as long as some successor needs WQM.; ///; /// There is room for improvement given better control flow analysis:; ///; /// (1) at the top level (outside of control flow statements, and as long as; /// kill hasn't been used), one SGPR can be saved by recovering WQM from; /// the LiveMask (this is implemented for the entry block).; ///; /// (2) when entire regions (e.g. if-else blocks or entire loops) only; /// consist of exact and don't-care instructions, the switch only has to; /// be done at the entry and exit points rather than potentially in each; /// block of the region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:2429,Safety,recover,recovering,2429,"; ///; /// Whole quad mode is required for derivative computations, but it interferes; /// with shader side effects (stores and atomics). It ensures that WQM is; /// enabled when necessary, but disabled around stores and atomics.; ///; /// When necessary, this pass creates a function prolog; ///; /// S_MOV_B64 LiveMask, EXEC; /// S_WQM_B64 EXEC, EXEC; ///; /// to enter WQM at the top of the function and surrounds blocks of Exact; /// instructions by; ///; /// S_AND_SAVEEXEC_B64 Tmp, LiveMask; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// We also compute when a sequence of instructions requires strict whole; /// wavefront mode (StrictWWM) and insert instructions to save and restore it:; ///; /// S_OR_SAVEEXEC_B64 Tmp, -1; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// When a sequence of instructions requires strict whole quad mode (StrictWQM); /// we use a similar save and restore mechanism and force whole quad mode for; /// those instructions:; ///; /// S_MOV_B64 Tmp, EXEC; /// S_WQM_B64 EXEC, EXEC; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// In order to avoid excessive switching during sequences of Exact; /// instructions, the pass first analyzes which instructions must be run in WQM; /// (aka which instructions produce values that lead to derivative; /// computations).; ///; /// Basic blocks are always exited in WQM as long as some successor needs WQM.; ///; /// There is room for improvement given better control flow analysis:; ///; /// (1) at the top level (outside of control flow statements, and as long as; /// kill hasn't been used), one SGPR can be saved by recovering WQM from; /// the LiveMask (this is implemented for the entry block).; ///; /// (2) when entire regions (e.g. if-else blocks or entire loops) only; /// consist of exact and don't-care instructions, the switch only has to; /// be done at the entry and exit points rather than potentially in each; /// block of the region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:37,Availability,mask,masks,37,// Note: this code assumes that lane masks on AMDGPU completely; // cover registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Performance,Perform,Perform,3,// Perform a depth-first iteration of the LiveRange graph marking defs.; // Stop processing of a given branch when all use lanes have been defined.; // The first definition stops processing for a physical register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:26,Usability,simpl,simply,26,// For physical registers simply mark the defining instruction,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:77,Safety,avoid,avoids,77,// Only generate implicit WQM if implicit derivatives are required.; // This avoids inserting unintended WQM if a shader type without; // implicit derivatives uses an image sampling instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:82,Safety,avoid,avoid,82,"// Mark these STRICTWQM, but only for the instruction, not its operands.; // This avoid unnecessarily marking M0 as requiring WQM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Deployability,Update,Update,3,// Update dominator trees,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:166,Availability,mask,mask,166,// Comparison is for live lanes; however here we compute the inverse; // (killed lanes). This is because VCMP will always generate 0 bits; // for inactive lanes so a mask of live lanes would not be correct; // inside control flow.; // Invert the comparison by swapping the operands and adjusting; // the comparison codes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:57,Availability,mask,mask,57,"// State of SCC represents whether any lanes are live in mask,; // if SCC is 0 then no lanes will be alive anymore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:101,Availability,alive,alive,101,"// State of SCC represents whether any lanes are live in mask,; // if SCC is 0 then no lanes will be alive anymore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Deployability,Update,Update,3,// Update live intervals,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:52,Availability,mask,mask,52,"// Op represents live lanes after kill,; // so exec mask needs to be factored in.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:57,Availability,mask,mask,57,"// State of SCC represents whether any lanes are live in mask,; // if SCC is 0 then no lanes will be alive anymore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:101,Availability,alive,alive,101,"// State of SCC represents whether any lanes are live in mask,; // if SCC is 0 then no lanes will be alive anymore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:62,Deployability,update,update,62,"// In the case we got this far some lanes are still live,; // update EXEC to deactivate lanes as appropriate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:45,Availability,mask,mask,45,// Kill - deactivate lanes no longer in live mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Deployability,Update,Update,3,// Update live intervals,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:146,Availability,mask,mask,146,"// Convert a strict mode transition to a pseudo transition.; // This still pre-allocates registers to prevent clobbering,; // but avoids any EXEC mask changes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:79,Energy Efficiency,allocate,allocates,79,"// Convert a strict mode transition to a pseudo transition.; // This still pre-allocates registers to prevent clobbering,; // but avoids any EXEC mask changes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:130,Safety,avoid,avoids,130,"// Convert a strict mode transition to a pseudo transition.; // This still pre-allocates registers to prevent clobbering,; // but avoids any EXEC mask changes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:55,Availability,mask,mask,55,// Replace (or supplement) instructions accessing live mask.; // This can only happen once all the live mask registers have been created; // and the execute state (WQM/StrictWWM/Exact) of instructions is known.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:104,Availability,mask,mask,104,// Replace (or supplement) instructions accessing live mask.; // This can only happen once all the live mask registers have been created; // and the execute state (WQM/StrictWWM/Exact) of instructions is known.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:40,Security,access,accessing,40,// Replace (or supplement) instructions accessing live mask.; // This can only happen once all the live mask registers have been created; // and the execute state (WQM/StrictWWM/Exact) of instructions is known.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:38,Safety,detect,detected,38,// Transition WQM -> StrictWQM -> WQM detected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Performance,Perform,Perform,3,// Perform splitting after instruction scan to simplify iteration.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:47,Usability,simpl,simplify,47,// Perform splitting after instruction scan to simplify iteration.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:94,Safety,safe,safely,94,"// Return an iterator in the (inclusive) range [First, Last] at which; // instructions can be safely inserted, keeping in mind that some of the; // instructions we want to add necessarily clobber SCC.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:48,Safety,safe,safe,48,// This stores the first instruction where it's safe to switch from WQM to; // Exact or vice versa.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:48,Safety,safe,safe,48,"// This stores the first instruction where it's safe to switch from Strict; // mode to Exact/WQM or to switch to Strict mode. It must always be the same; // as, or after, FirstWQM since if it's safe to switch to/from Strict, it must; // be safe to switch to/from WQM as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:194,Safety,safe,safe,194,"// This stores the first instruction where it's safe to switch from Strict; // mode to Exact/WQM or to switch to Strict mode. It must always be the same; // as, or after, FirstWQM since if it's safe to switch to/from Strict, it must; // be safe to switch to/from WQM as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:240,Safety,safe,safe,240,"// This stores the first instruction where it's safe to switch from Strict; // mode to Exact/WQM or to switch to Strict mode. It must always be the same; // as, or after, FirstWQM since if it's safe to switch to/from Strict, it must; // be safe to switch to/from WQM as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:76,Safety,safe,safely,76,"// If the instruction doesn't actually need a correct EXEC, then we can; // safely leave Strict mode enabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:31,Integrability,depend,depends,31,// Whether we need to save SCC depends on start and end states.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:77,Availability,mask,mask,77,// Exact/Strict -> Strict: save SCC; // Exact/Strict -> WQM: save SCC if WQM mask is generated from exec; // Exact/Strict -> Exact: no save,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:36,Integrability,depend,depends,36,// Check that it already implicitly depends on exec (like all VALU movs; // should do).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:33,Integrability,depend,dependency,33,// Remove early-clobber and exec dependency from simple SGPR copies.; // This allows some to be eliminated during/post RA.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:49,Usability,simpl,simple,49,// Remove early-clobber and exec dependency from simple SGPR copies.; // This allows some to be eliminated during/post RA.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:107,Usability,simpl,simple,107,// the only reason we should be here is V_SET_INACTIVE has; // an undef input so it is being replaced by a simple copy.; // There should be a second undef source that we should remove.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:13,Usability,simpl,simple,13,// Shader is simple does not need any state changes or any complex lowering,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:37,Availability,mask,mask,37,// Store a copy of the original live mask when required,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:45,Performance,perform,perform,45,// Lowering blocks causes block splitting so perform as a second pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:31,Availability,mask,mask,31,// Compute live range for live mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:116,Usability,simpl,simplest,116,"// Physical registers like SCC aren't tracked by default anyway, so just; // removing the ranges we computed is the simplest option for maintaining; // the analysis results.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:9,Performance,perform,performed,9,// If we performed any kills then recompute EXEC,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:98,Availability,error,error,98,"// Immediate operand kind.; // It helps to identify the location of an offending operand after an error.; // Note that regular literals and mandatory literals (KImm) must be handled; // differently. When looking for an offending operand, we should usually; // ignore mandatory literals because they are part of the instruction and; // cannot be changed. Report location of mandatory operands only for VOPD,; // when both OpX and OpY have a KImm and there are no other literals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:20,Availability,error,error,20,// Instruction will error in AMDGPUAsmParser::MatchAndEmitInstruction,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:8,Deployability,update,update,8,// Also update vgpr_count (dependent on agpr_count for gfx908/gfx90a),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:27,Integrability,depend,dependent,27,// Also update vgpr_count (dependent on agpr_count for gfx908/gfx90a),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:32,Modifiability,variab,variables,32,"// TODO: make those pre-defined variables read-only.; // Currently there is none suitable machinery in the core llvm-mc for this.; // MCSymbol::isRedefinable is intended for another purpose, and; // AsmParser::parseDirectiveSet() cannot be specialized for specific target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:156,Availability,failure,failures,156,// TODO: We should avoid using host float here. It would be better to; // check the float bit values which is what a few other places do.; // We've had bot failures before due to weird NaN support on mips hosts.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:19,Safety,avoid,avoid,19,// TODO: We should avoid using host float here. It would be better to; // check the float bit values which is what a few other places do.; // We've had bot failures before due to weird NaN support on mips hosts.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:135,Safety,avoid,avoid,135,"// Cannot apply fp modifiers to int literals preserving the same semantics; // for VOP1/2/C and VOP3 because of integer truncation. To avoid ambiguity,; // disable these cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:35,Modifiability,extend,extend,35,"// FIXME: 64-bit operands can zero extend, sign extend, or pad zeroes for FP; // types.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:48,Modifiability,extend,extend,48,"// FIXME: 64-bit operands can zero extend, sign extend, or pad zeroes for FP; // types.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:43,Modifiability,extend,extend,43,// We got int literal token.; // Only sign extend inline immediates.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:113,Availability,error,error,113,"// Currently all regular registers have their .l and .h subregisters, so; // we should never need to generate an error here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:129,Safety,avoid,avoid,129,// Check if this is an operand modifier or an opcode modifier; // which may look like an expression but it is not. We should; // avoid parsing these modifiers as expressions. Currently; // recognized sequences are:; // |...|; // abs(...); // neg(...); // sext(...); // -reg; // -|...|; // -abs(...); // name:...; //,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:556,Usability,intuit,intuitive,556,"// Check if the current token is an SP3 'neg' modifier.; // Currently this modifier is allowed in the following context:; //; // 1. Before a register, e.g. ""-v0"", ""-v[...]"" or ""-[v0,v1]"".; // 2. Before an 'abs' modifier: -abs(...); // 3. Before an SP3 'abs' modifier: -|...|; //; // In all other cases ""-"" is handled as a part; // of an expression that follows the sign.; //; // Note: When ""-"" is followed by an integer literal,; // this is interpreted as integer negation rather; // than a floating-point NEG modifier applied to N.; // Beside being contr-intuitive, such use of floating-point; // NEG modifier would have resulted in different meaning; // of integer literals used with VOP1/2/C and VOP3,; // for example:; // v_exp_f32_e32 v5, -1 // VOP1: src0 = 0xFFFFFFFF; // v_exp_f32_e64 v5, -1 // VOP3: src0 = 0x80000001; // Negative fp literals with preceding ""-"" are; // handled likewise for uniformity; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:105,Security,validat,validated,105,"// special operand like VINTERP attr_chan; // An instruction may use only one literal.; // This has been validated on the previous step.; // See validateVOPLiteral.; // This literal may be used as more than one operand.; // If all these operands are of the same size,; // this literal counts as one scalar value.; // Otherwise it counts as 2 scalar values.; // See ""GFX10 Shader Programming"", section 3.6.2.3.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:145,Security,validat,validateVOPLiteral,145,"// special operand like VINTERP attr_chan; // An instruction may use only one literal.; // This has been validated on the previous step.; // See validateVOPLiteral.; // This literal may be used as more than one operand.; // If all these operands are of the same size,; // this literal counts as one scalar value.; // Otherwise it counts as 2 scalar values.; // See ""GFX10 Shader Programming"", section 3.6.2.3.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:72,Performance,cache,cache,72,// On GFX12 if both OpX and OpY are V_MOV_B32 then OPY uses SRC2 source-cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:144,Energy Efficiency,green,green,144,"// GATHER4 instructions use dmask in a different fashion compared to; // other MIMG instructions. The only useful DMASK values are; // 1=red, 2=green, 4=blue, 8=alpha. (e.g. 1 returns; // (red,red,red,red) etc.) The ISA document doesn't mention; // this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:8,Security,validat,validate,8,// Only validate GDS for non-GWS instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:17,Usability,undo,undocumented,17,// gfx90a has an undocumented limitation:; // DS_GWS opcodes must use even aligned registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:67,Security,validat,validate,67,"// For MUBUF/MTBUF d16 is a part of opcode, so there is nothing to validate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:66,Availability,error,errors,66,// This instruction is not supported.; // Clear any other pending errors because they are no longer relevant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:42,Usability,Clear,Clear,42,// This instruction is not supported.; // Clear any other pending errors because they are no longer relevant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:104,Security,access,accessed,104,// No flat_scr on SI.; // On GFX10Plus flat scratch is not a valid register operand and can only be; // accessed with s_setreg/s_getreg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:59,Availability,error,error,59,"// If we successfully parsed the operand or if there as an error parsing,; // we are done.; //; // If we are parsing after we reach EndOfStatement then this means we; // are appending default values to the Operands list. This is only done; // by custom parser, so we shouldn't continue on to the generic parsing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:3,Usability,Clear,Clear,3,// Clear any forced encodings from the previous instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:71,Availability,error,error,71,"// We are expecting an soffset operand,; // but let matcher handle the error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:8,Integrability,message,message,8,// skip message name,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:25,Integrability,depend,depends,25,// Validation strictness depends on whether message is specified; // in a symbolic or in a numeric form. In the latter case; // only encoding possibility is checked.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:44,Integrability,message,message,44,// Validation strictness depends on whether message is specified; // in a symbolic or in a numeric form. In the latter case; // only encoding possibility is checked.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:3,Security,Validat,Validation,3,// Validation strictness depends on whether message is specified; // in a symbolic or in a numeric form. In the latter case; // only encoding possibility is checked.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:121,Availability,error,error,121,// Make sure we are not parsing something; // that looks like a label or an expression but is not.; // This will improve error messages.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:127,Integrability,message,messages,127,// Make sure we are not parsing something; // that looks like a label or an expression but is not.; // This will improve error messages.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:36,Security,validat,validator,36,// Offset range is checked later by validator.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:144,Security,validat,validate,144,"// Tokens like ""glc"" would be parsed as immediate operands in ParseOperand().; // But MatchInstructionImpl() expects to meet token and fails to validate; // operand. This method checks if we are given immediate operand but expect to; // get corresponding token.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:123,Availability,error,error,123,// The NSA encoding does not contain enough operands for the; // combination of base opcode / dimension. Should this be an error?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:25,Availability,error,error,25,// ToDo: add support for error operands to MCInst.h; // return MCOperand::createError(V);,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:34,Availability,avail,available,34,// ToDo: unclear if s[100:104] is available on VI. Can we use VCC as SGPR in; // this bundle?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:33,Availability,avail,available,33,// ToDo: unclear if s[96:104] is available on VI. Can we use VCC as SGPR in; // this bundle?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:33,Availability,avail,available,33,// ToDo: unclear if s[88:104] is available on VI. Can we use VCC as SGPR in; // this bundle?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:236,Availability,error,error,236,"// ToDo: case 248: 1/(2*PI) - is allowed only on VI; // ImmWidth 0 is a default case where operand should not allow immediates.; // Imm value is still decoded into 32 bit immediate operand, inst printer will; // use it to print verbose error message.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:242,Integrability,message,message,242,"// ToDo: case 248: 1/(2*PI) - is allowed only on VI; // ImmWidth 0 is a default case where operand should not allow immediates.; // Imm value is still decoded into 32 bit immediate operand, inst printer will; // use it to print verbose error message.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:33,Safety,avoid,avoid,33,// XXX: cast to int is needed to avoid stupid warning:; // compare with unsigned is always true,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:218,Usability,simpl,simply,218,// We cannot accurately backward compute #VGPRs used from; // GRANULATED_WORKITEM_VGPR_COUNT. But we are concerned with getting the same; // value of GRANULATED_WORKITEM_VGPR_COUNT in the reassembled binary. So we; // simply calculate the inverse of what the assembler does.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:681,Availability,recover,recover,681,// We cannot backward compute values used to calculate; // GRANULATED_WAVEFRONT_SGPR_COUNT. Hence the original values for following; // directives can't be computed:; // .amdhsa_reserve_vcc; // .amdhsa_reserve_flat_scratch; // .amdhsa_reserve_xnack_mask; // They take their respective default values if not specified in the assembly.; //; // GRANULATED_WAVEFRONT_SGPR_COUNT; // = f(NEXT_FREE_SGPR + VCC + FLAT_SCRATCH + XNACK_MASK); //; // We compute the inverse as though all directives apart from NEXT_FREE_SGPR; // are set to 0. So while disassembling we consider that:; //; // GRANULATED_WAVEFRONT_SGPR_COUNT; // = f(NEXT_FREE_SGPR + 0 + 0 + 0); //; // The disassembler cannot recover the original values of those 3 directives.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:681,Safety,recover,recover,681,// We cannot backward compute values used to calculate; // GRANULATED_WAVEFRONT_SGPR_COUNT. Hence the original values for following; // directives can't be computed:; // .amdhsa_reserve_vcc; // .amdhsa_reserve_flat_scratch; // .amdhsa_reserve_xnack_mask; // They take their respective default values if not specified in the assembly.; //; // GRANULATED_WAVEFRONT_SGPR_COUNT; // = f(NEXT_FREE_SGPR + VCC + FLAT_SCRATCH + XNACK_MASK); //; // We compute the inverse as though all directives apart from NEXT_FREE_SGPR; // are set to 0. So while disassembling we consider that:; //; // GRANULATED_WAVEFRONT_SGPR_COUNT; // = f(NEXT_FREE_SGPR + 0 + 0 + 0); //; // The disassembler cannot recover the original values of those 3 directives.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:94,Usability,simpl,simply,94,"// KERNEL_CODE_ENTRY_BYTE_OFFSET; // So far no directive controls this for Code Object V3, so simply skip for; // disassembly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:38,Availability,failure,failure,38,// Size = 64 regardless of success or failure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h:14,Integrability,interface,interface,14,// Exposes an interface expected by autogenerated code in; // FixedLenDecoderEmitter,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h:3,Security,Expose,Exposes,3,// Exposes an interface expected by autogenerated code in; // FixedLenDecoderEmitter,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:201,Deployability,pipeline,pipeline,201,"// s_endpgm also behaves as if there is an implicit; // s_waitcnt 0, but I'm not sure if it would be appropriate; // to model this in llvm-mca based on how the iterations work; // while simulating the pipeline over and over.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:3,Availability,Redundant,Redundant,3,// Redundant switch so I don't have to repeat the code above; // for each case. There are more clever ways to avoid this; // extra switch and anyone can feel free to implement one of them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:3,Safety,Redund,Redundant,3,// Redundant switch so I don't have to repeat the code above; // for each case. There are more clever ways to avoid this; // extra switch and anyone can feel free to implement one of them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:110,Safety,avoid,avoid,110,// Redundant switch so I don't have to repeat the code above; // for each case. There are more clever ways to avoid this; // extra switch and anyone can feel free to implement one of them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:73,Deployability,update,updateEventWaitcntAfter,73,"// The core logic from this function is taken from; // SIInsertWaitcnts::updateEventWaitcntAfter() In that pass, the instructions; // that are being looked at are in the MachineInstr format, whereas we have; // access to the MCInst format. The side effects of this are that we can't use; // the mayAccessVMEMThroughFlat(Inst) or mayAccessLDSThroughFlat(Inst); // functions. Therefore, we conservatively assume that these functions will; // return true. This may cause a few instructions to be incorrectly tagged; // with an extra CNT. However, these are instructions that do interact with at; // least one CNT so giving them an extra CNT shouldn't cause issues in most; // scenarios.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:211,Security,access,access,211,"// The core logic from this function is taken from; // SIInsertWaitcnts::updateEventWaitcntAfter() In that pass, the instructions; // that are being looked at are in the MachineInstr format, whereas we have; // access to the MCInst format. The side effects of this are that we can't use; // the mayAccessVMEMThroughFlat(Inst) or mayAccessLDSThroughFlat(Inst); // functions. Therefore, we conservatively assume that these functions will; // return true. This may cause a few instructions to be incorrectly tagged; // with an extra CNT. However, these are instructions that do interact with at; // least one CNT so giving them an extra CNT shouldn't cause issues in most; // scenarios.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:12,Testability,log,logic,12,"// The core logic from this function is taken from; // SIInsertWaitcnts::updateEventWaitcntAfter() In that pass, the instructions; // that are being looked at are in the MachineInstr format, whereas we have; // access to the MCInst format. The side effects of this are that we can't use; // the mayAccessVMEMThroughFlat(Inst) or mayAccessLDSThroughFlat(Inst); // functions. Therefore, we conservatively assume that these functions will; // return true. This may cause a few instructions to be incorrectly tagged; // with an extra CNT. However, these are instructions that do interact with at; // least one CNT so giving them an extra CNT shouldn't cause issues in most; // scenarios.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:259,Security,access,access,259,// We conservatively assume that mayAccessVMEMThroughFlat(Inst); // and mayAccessLDSThroughFlat(Inst) would both return true for this; // instruction. We have to do this because those functions use; // information about the memory operands that we don't have access to.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:616,Energy Efficiency,schedul,scheduling,616,"//===------------------- AMDGPUCustomBehaviour.h ----------------*-C++ -* -===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; ///; /// This file defines the AMDGPUCustomBehaviour class which inherits from; /// CustomBehaviour. This class is used by the tool llvm-mca to enforce; /// target specific behaviour that is not expressed well enough in the; /// scheduling model for mca to enforce it automatically.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:452,Modifiability,inherit,inherits,452,"//===------------------- AMDGPUCustomBehaviour.h ----------------*-C++ -* -===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; ///; /// This file defines the AMDGPUCustomBehaviour class which inherits from; /// CustomBehaviour. This class is used by the tool llvm-mca to enforce; /// target specific behaviour that is not expressed well enough in the; /// scheduling model for mca to enforce it automatically.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:310,Availability,avail,available,310,"/// This method gets called from the constructor and is; /// where we setup the InstrWaitCntInfo vector.; /// The core logic for determining which CNTs an instruction; /// interacts with is taken from SIInsertWaitcnts::updateEventWaitcntAfter().; /// Unfortunately, some of the logic from that function is not available to us; /// in this scope so we conservatively end up assuming that some; /// instructions interact with more CNTs than they do in reality.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:219,Deployability,update,updateEventWaitcntAfter,219,"/// This method gets called from the constructor and is; /// where we setup the InstrWaitCntInfo vector.; /// The core logic for determining which CNTs an instruction; /// interacts with is taken from SIInsertWaitcnts::updateEventWaitcntAfter().; /// Unfortunately, some of the logic from that function is not available to us; /// in this scope so we conservatively end up assuming that some; /// instructions interact with more CNTs than they do in reality.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:119,Testability,log,logic,119,"/// This method gets called from the constructor and is; /// where we setup the InstrWaitCntInfo vector.; /// The core logic for determining which CNTs an instruction; /// interacts with is taken from SIInsertWaitcnts::updateEventWaitcntAfter().; /// Unfortunately, some of the logic from that function is not available to us; /// in this scope so we conservatively end up assuming that some; /// instructions interact with more CNTs than they do in reality.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:278,Testability,log,logic,278,"/// This method gets called from the constructor and is; /// where we setup the InstrWaitCntInfo vector.; /// The core logic for determining which CNTs an instruction; /// interacts with is taken from SIInsertWaitcnts::updateEventWaitcntAfter().; /// Unfortunately, some of the logic from that function is not available to us; /// in this scope so we conservatively end up assuming that some; /// instructions interact with more CNTs than they do in reality.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:224,Deployability,pipeline,pipeline,224,/// This method gets called from checkCustomHazard when mca is attempting to; /// dispatch an s_waitcnt instruction (or one of its variants). The method; /// looks at each of the instructions that are still executing in the pipeline; /// to determine if the waitcnt should force a wait.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:266,Integrability,depend,dependencies,266,/// This method is used to determine if an instruction; /// should be allowed to be dispatched. The return value is; /// how many cycles until the instruction can be dispatched.; /// This method is called after MCA has already checked for; /// register and hardware dependencies so this method should only; /// implement custom behaviour and dependencies that are not picked up; /// by MCA naturally.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:342,Integrability,depend,dependencies,342,/// This method is used to determine if an instruction; /// should be allowed to be dispatched. The return value is; /// how many cycles until the instruction can be dispatched.; /// This method is called after MCA has already checked for; /// register and hardware dependencies so this method should only; /// implement custom behaviour and dependencies that are not picked up; /// by MCA naturally.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp:57,Availability,mask,mask,57,"// For each byte of the fragment that the fixup touches, mask in the bits from; // the fixup value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUELFObjectWriter.cpp:46,Modifiability,variab,variable,46,// SCRATCH_RSRC_DWORD[01] is a special global variable that represents; // the scratch buffer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUELFObjectWriter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUELFObjectWriter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp:667,Modifiability,extend,extend,667,"// FIXME: The current implementation of; // AsmParser::parseRegisterOrRegisterNumber in MC implies we either emit this; // as an integer or we provide a name which represents a physical register.; // For CFI instructions we really want to emit a name for the DWARF register; // instead, because there may be multiple DWARF registers corresponding to a; // single physical register. One case where this problem manifests is with; // wave32/wave64 where using the physical register name is ambiguous: if we; // write e.g. `.cfi_undefined v0` we lose information about the wavefront; // size which we need to encode the register in the final DWARF. Ideally we; // would extend MC to support parsing DWARF register names so we could do; // something like `.cfi_undefined dwarf_wave32_v0`. For now we just live with; // non-pretty DWARF register names in assembly text.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp:33,Performance,load,load,33,// This will default to printing load variants when neither MayStore nor; // MayLoad flag is present which is the case with instructions like; // image_get_resinfo.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp:93,Integrability,message,message,93,"// Check if operand register class contains register used.; // Intention: print disassembler message when invalid code is decoded,; // for example sgpr register used in VReg or VISrc(VReg or imm) operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp:36,Safety,avoid,avoid,36,// Use 'neg(...)' instead of '-' to avoid ambiguity.; // This is important for integer literals because; // -1 is not the same value as neg(1).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.h:52,Integrability,interface,interface,52,"//===-- AMDGPUInstPrinter.h - AMDGPU MC Inst -> ASM interface ---*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp:97,Energy Efficiency,reduce,reduced,97,"// This is the maximum instruction encoded size for gfx10. With a known; // subtarget, it can be reduced to 8 bytes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp:16,Modifiability,Variab,Variable,16,//===--- Global Variable Emission Directives --------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h:54,Integrability,Interface,Interface,54,"//===-- MCTargetDesc/AMDGPUMCAsmInfo.h - AMDGPU MCAsm Interface -*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h:56,Modifiability,inherit,inherits,56,"// If you need to create another MCAsmInfo class, which inherits from MCAsmInfo,; // you will need to make sure your new class sets PrivateGlobalPrefix to; // a prefix that won't appear in a function name. The default value; // for PrivateGlobalPrefix is 'L', so it will consider any function starting; // with 'L' as a local symbol.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCCodeEmitter.cpp:59,Integrability,depend,depend,59,"// FIXME: If this is expression is PCRel or not should not depend on what; // the expression looks like. Given that this is just a general expression,; // it should probably be FK_Data_4 and whatever is producing; //; // s_add_u32 s2, s2, (extern_const_addrspace+16; //; // And expecting a PCRel should instead produce; //; // .Ltmp1:; // s_add_u32 s2, s2, (extern_const_addrspace+16)-.Ltmp1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:15,Performance,cache,cache,15,// Instruction cache line size in bytes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:22,Testability,test,tests,22,"// TODO: Why are some tests have ""mingw"" listed as OS?; // llvm_unreachable(""Unsupported OS"");",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:22,Testability,test,tests,22,"// TODO: Why are some tests have ""mingw"" listed as OS?; // assert(STI.getTargetTriple().getOS() == Triple::UnknownOS);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:59,Testability,assert,assert,59,"// TODO: Why are some tests have ""mingw"" listed as OS?; // assert(STI.getTargetTriple().getOS() == Triple::UnknownOS);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:15,Performance,cache,cache,15,// Instruction cache line size in bytes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:81,Deployability,update,update,81,"// Assume the default COV for now, EmitDirectiveAMDHSACodeObjectVersion; // will update this if it is encountered.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:281,Availability,failure,failure,281,"/// Emit HSA Metadata; ///; /// When \p Strict is true, known metadata elements must already be; /// well-typed. When \p Strict is false, known types are inferred and; /// the \p HSAMetadata structure is updated with the correct types.; ///; /// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:204,Deployability,update,updated,204,"/// Emit HSA Metadata; ///; /// When \p Strict is true, known metadata elements must already be; /// well-typed. When \p Strict is false, known types are inferred and; /// the \p HSAMetadata structure is updated with the correct types.; ///; /// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/R600InstPrinter.h:50,Integrability,interface,interface,50,"//===-- R600InstPrinter.h - AMDGPU MC Inst -> ASM interface -----*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/R600InstPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/R600InstPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:17,Availability,mask,mask,17,/// \returns Bit mask for given bit \p Shift and bit \p Width.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:13,Performance,Load,Loadcnt,13,/// \returns Loadcnt bit width,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:42,Integrability,depend,depending,42,"/// \returns Storecnt or Vscnt bit width, depending on VersionMajor.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:23,Performance,Load,Loadcnt,23,/// \returns shift for Loadcnt/Storecnt in combined S_WAIT instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:3,Integrability,Wrap,Wrapper,3,"// Wrapper for Tablegen'd function. enum Subtarget is not defined in any; // header files, so we need to wrap it in a function that takes unsigned; // instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:105,Integrability,wrap,wrap,105,"// Wrapper for Tablegen'd function. enum Subtarget is not defined in any; // header files, so we need to wrap it in a function that takes unsigned; // instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:55,Testability,test,tested,55,"// Some subtargets allow encoding 2048, but this isn't tested or supported.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:43,Energy Efficiency,power,powers,43,"// These alignment values are specified in powers of two, so alignment =; // 2^n. The minimum alignment is 2^4 = 16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:377,Energy Efficiency,efficient,efficient,377,"//===----------------------------------------------------------------------===//; // Custom Operands.; //; // A table of custom operands shall describe ""primary"" operand names; // first followed by aliases if any. It is not required but recommended; // to arrange operands so that operand encoding match operand position; // in the table. This will make disassembly a bit more efficient.; // Unused slots in the table shall have an empty name.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:14,Performance,optimiz,optimization,14,"// This is an optimization that should work in most cases.; // As a side effect, it may cause selection of an alias; // instead of a primary operand name in case of sparse tables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:8,Safety,safe,safe,8,// As a safe default always respond as if PS has color exports.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:3,Safety,Avoid,Avoid,3,"// Avoid using MCRegisterClass::getSize, since that function will go away; // (move from MC* level to Target* level). Return size in bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:268,Modifiability,extend,extended,268,"// Unfortunately, the Instruction Set Architecture Reference Guide is; // misleading about how the inline operands work for (packed) 16-bit; // instructions. In a nutshell, the actual HW behavior is:; //; // - integer encodings (-16 .. 64) are always produced as sign-extended; // 32-bit values; // - float encodings are produced as:; // - for F16 instructions: corresponding half-precision float values in; // the LSBs, 0 in the MSBs; // - for UI16 instructions: corresponding single-precision float value",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:61,Usability,Guid,Guide,61,"// Unfortunately, the Instruction Set Architecture Reference Guide is; // misleading about how the inline operands work for (packed) 16-bit; // instructions. In a nutshell, the actual HW behavior is:; //; // - integer encodings (-16 .. 64) are always produced as sign-extended; // 32-bit values; // - float encodings are produced as:; // - for F16 instructions: corresponding half-precision float values in; // the LSBs, 0 in the MSBs; // - for UI16 instructions: corresponding single-precision float value",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:7,Availability,mask,mask,7,"// LSB mask for VGPR banks per VOPD component operand.; // 4 banks result in a mask 3, setting 2 lower bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:79,Availability,mask,mask,79,"// LSB mask for VGPR banks per VOPD component operand.; // 4 banks result in a mask 3, setting 2 lower bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:3,Integrability,Interface,Interface,3,"// Interface functions of this class map VOPD component operand indices; // to indices of operands in MachineInstr/MCInst or parsed operands array.; //; // Note that this class operates with 3 kinds of indices:; // - VOPD component operand indices (Component::DST, Component::SRC0, etc.);; // - MC operand indices (they refer operands in a MachineInstr/MCInst);; // - parsed operand indices (they refer operands in parsed operands array).; //; // For SINGLE components mapping between these indices is trivial.; // But things get more complicated for COMPONENT_X and; // COMPONENT_Y because these components share the same; // MachineInstr/MCInst and the same parsed operands array.; // Below is an example of component operand to parsed operand; // mapping for the following instruction:; //; // v_dual_add_f32 v255, v4, v5 :: v_dual_mov_b32 v6, v1; //; // PARSED COMPONENT PARSED; // COMPONENT OPERANDS OPERAND INDEX OPERAND INDEX; // -------------------------------------------------------------------; // ""v_dual_add_f32"" 0; // v_dual_add_f32 v255 0 (DST) --> 1; // v4 1 (SRC0) --> 2; // v5 2 (SRC1) --> 3; // ""::"" 4; // ""v_dual_mov_b32"" 5; // v_dual_mov_b32 v6 0 (DST) --> 6; // v1 1 (SRC0) --> 7; // -------------------------------------------------------------------; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:39,Integrability,depend,depends,39,// Create layout for COMPONENT_Y which depends on COMPONENT_X layout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:46,Integrability,depend,depends,46,// Create ComponentInfo for COMPONENT_Y which depends on COMPONENT_X layout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:168,Availability,error,error,168,/// \returns Integer value requested using \p F's \p Name attribute.; ///; /// \returns \p Default if attribute is not present.; ///; /// \returns \p Default and emits error if requested value cannot be converted; /// to integer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:276,Availability,error,error,276,"/// \returns A pair of integer values requested using \p F's \p Name attribute; /// in ""first[,second]"" format (""second"" is optional unless \p OnlyFirstRequired; /// is false).; ///; /// \returns \p Default if attribute is not present.; ///; /// \returns \p Default and emits error if one of the requested values cannot be; /// converted to integer, or \p OnlyFirstRequired is false and ""second"" value is; /// not present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:108,Availability,mask,mask,108,// The following methods are only meaningful on targets that support; // S_WAITCNT.; /// \returns Vmcnt bit mask for given isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:24,Availability,mask,mask,24,/// \returns Expcnt bit mask for given isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:25,Availability,mask,mask,25,/// \returns Lgkmcnt bit mask for given isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:25,Availability,mask,mask,25,/// \returns Waitcnt bit mask for given isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:135,Availability,mask,mask,135,"// The following methods are only meaningful on targets that support; // S_WAIT_*CNT, introduced with gfx12.; /// \returns Loadcnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support LOADcnt",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:123,Performance,Load,Loadcnt,123,"// The following methods are only meaningful on targets that support; // S_WAIT_*CNT, introduced with gfx12.; /// \returns Loadcnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support LOADcnt",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:214,Performance,LOAD,LOADcnt,214,"// The following methods are only meaningful on targets that support; // S_WAIT_*CNT, introduced with gfx12.; /// \returns Loadcnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support LOADcnt",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:27,Availability,mask,mask,27,/// \returns Samplecnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support SAMPLEcnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:24,Availability,mask,mask,24,/// \returns Bvhcnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support BVHcnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:23,Availability,mask,mask,23,/// \returns Dscnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support DScnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:23,Availability,mask,mask,23,/// \returns Dscnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support KMcnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:34,Availability,mask,mask,34,"/// \return STOREcnt or VScnt bit mask for given isa \p Version.; /// returns 0 for versions that do not support STOREcnt or VScnt.; /// STOREcnt and VScnt are the same counter, the name used; /// depends on the ISA version.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:197,Integrability,depend,depends,197,"/// \return STOREcnt or VScnt bit mask for given isa \p Version.; /// returns 0 for versions that do not support STOREcnt or VScnt.; /// STOREcnt and VScnt are the same counter, the name used; /// depends on the ISA version.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:167,Performance,Load,LoadcntDscnt,167,// The following are only meaningful on targets that support; // S_WAIT_LOADCNT_DSCNT and S_WAIT_STORECNT_DSCNT.; /// \returns Decoded Waitcnt structure from given \p LoadcntDscnt for given; /// isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:16,Performance,Load,Loadcnt,16,/// \returns \p Loadcnt and \p Dscnt components of \p Decoded encoded as an; /// immediate that can be used with S_WAIT_LOADCNT_DSCNT for given isa; /// \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:459,Energy Efficiency,allocate,allocate,459,"// These functions are considered entrypoints into the current module, i.e. they; // are allowed to be called from outside the current module. This is different; // from isEntryFunctionCC, which is only true for functions that are entered by; // the hardware. Module entry points include all entry functions but also; // include functions that can be called from other functions inside or outside; // the current module. Module entry functions are allowed to allocate LDS.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:142,Availability,avail,available,142,/// For pre-GFX12 FLAT instructions the offset must be positive;; /// MSB is ignored and forced to zero.; ///; /// \return The number of bits available for the signed offset field in flat; /// instructions. Note that some forms of the instruction disallow negative; /// offsets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:119,Modifiability,variab,variable,119,// external zero size addrspace(3) without initializer implies cuda/hip extern; // __shared__ the semantics for such a variable appears to be that all extern; // __shared__ variables alias one another. This hits different handling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:173,Modifiability,variab,variables,173,// external zero size addrspace(3) without initializer implies cuda/hip extern; // __shared__ the semantics for such a variable appears to be that all extern; // __shared__ variables alias one another. This hits different handling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:20,Modifiability,variab,variable,20,"// A constant undef variable can't be written to, and any load is; // undef, so it should be eliminated by the optimizer. It could be; // dropped by the back end if not. This pass skips over it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:58,Performance,load,load,58,"// A constant undef variable can't be written to, and any load is; // undef, so it should be eliminated by the optimizer. It could be; // dropped by the back end if not. This pass skips over it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:111,Performance,optimiz,optimizer,111,"// A constant undef variable can't be written to, and any load is; // undef, so it should be eliminated by the optimizer. It could be; // dropped by the back end if not. This pass skips over it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:106,Availability,error,error,106,// Initializers are unimplemented for LDS address space.; // Leave such variables in place for consistent error reporting.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:72,Modifiability,variab,variables,72,// Initializers are unimplemented for LDS address space.; // Leave such variables in place for consistent error reporting.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:49,Performance,load,load,49,"// Ignore atomics not aliasing with the original load, any atomic is a; // universal MemoryDef from MSSA's point of view too, just like a fence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:106,Performance,load,load,106,"// Start with a nearest dominating clobbering access, it will be either; // live on entry (nothing to do, load is not clobbered), MemoryDef, or; // MemoryPhi if several MemoryDefs can define this memory state. In that; // case add all Defs to WorkList and continue going up and checking all; // the definitions of this memory location until the root. When all the; // defs are exhausted and came to the entry state we have no clobber.; // Along the scan ignore barriers and fences which are considered clobbers; // by the MemorySSA, but not really writing anything into the memory.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:46,Security,access,access,46,"// Start with a nearest dominating clobbering access, it will be either; // live on entry (nothing to do, load is not clobbered), MemoryDef, or; // MemoryPhi if several MemoryDefs can define this memory state. In that; // case add all Defs to WorkList and continue going up and checking all; // the definitions of this memory location until the root. When all the; // defs are exhausted and came to the entry state we have no clobber.; // Along the scan ignore barriers and fences which are considered clobbers; // by the MemorySSA, but not really writing anything into the memory.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h:111,Deployability,update,update,111,/// Given a \p Def clobbering a load from \p Ptr according to the MSSA check; /// if this is actually a memory update or an artificial clobber to facilitate; /// ordering constraints.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h:32,Performance,load,load,32,/// Given a \p Def clobbering a load from \p Ptr according to the MSSA check; /// if this is actually a memory update or an artificial clobber to facilitate; /// ordering constraints.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h:18,Performance,Load,Load,18,/// Check is a \p Load is clobbered in its function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:100,Security,access,access,100,// Calculate the PAL metadata key for *S_SCRATCH_SIZE. It can be used; // with a constant offset to access any non-register shader-specific PAL; // metadata key.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:218,Energy Efficiency,allocate,allocate,218,// Set the number of used vgprs in the metadata. This is an optional; // advisory record for logging etc; wave dispatch actually uses the rsrc1; // register for the shader stage to determine the number of vgprs to; // allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:93,Testability,log,logging,93,// Set the number of used vgprs in the metadata. This is an optional; // advisory record for logging etc; wave dispatch actually uses the rsrc1; // register for the shader stage to determine the number of vgprs to; // allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:214,Energy Efficiency,allocate,allocate,214,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:93,Testability,log,logging,93,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:214,Energy Efficiency,allocate,allocate,214,// Set the number of used vgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of vgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used vgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of vgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used agprs in the metadata. This is an optional advisory; // record for logging etc;,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:214,Energy Efficiency,allocate,allocate,214,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:219,Energy Efficiency,allocate,allocate,219,// Set the amount of LDS used in bytes in the metadata. This is an optional; // advisory record for logging etc; wave dispatch actually uses the rsrc1; // register for the shader stage to determine the amount of LDS to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:100,Testability,log,logging,100,// Set the amount of LDS used in bytes in the metadata. This is an optional; // advisory record for logging etc; wave dispatch actually uses the rsrc1; // register for the shader stage to determine the amount of LDS to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:214,Energy Efficiency,allocate,allocate,214,// Set the number of used vgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of vgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used vgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of vgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:214,Energy Efficiency,allocate,allocate,214,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:101,Integrability,wrap,wrapper,101,// Get the PAL version major (idx 0) or minor (idx 1). This is an internal; // helper for the public wrapper functions that request Major or Minor,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARC.h:25,Integrability,interface,interface,25,"//===- ARC.h - Top-level interface for ARC representation -------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the entry points for global functions defined in the LLVM; // ARC back-end.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARC.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARC.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCAsmPrinter.cpp:474,Integrability,depend,dependent,474,"//===- ARCAsmPrinter.cpp - ARC LLVM assembly writer -------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a printer that converts from our internal representation; // of machine-dependent LLVM code to GNU format ARC assembly language.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCExpandPseudos.cpp:48,Performance,load,loads,48,"//===- ARCExpandPseudosPass - ARC expand pseudo loads -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass expands stores with large offsets into an appropriate sequence.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCExpandPseudos.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCExpandPseudos.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:113,Availability,down,downward,113,// Allocate by adjusting by the negative of what the record holder tracked; // it tracked a positive offset in a downward growing stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate by adjusting by the negative of what the record holder tracked; // it tracked a positive offset in a downward growing stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:14,Modifiability,variab,variable,14,"// If we have variable sized frame objects, then we have to move; // the stack pointer to a known spot (fp - StackSize).; // Then, replace the frame pointer by (new) [sp,StackSize-4].; // Then, move the stack pointer the rest of the way (sp = sp + StackSize).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:25,Availability,down,down,25,// Create slots for last down to r13.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:13,Integrability,rout,routines,13,"// There are routines for saving at least 3 registers (r13 to r15, etc.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:13,Integrability,rout,routines,13,"// There are routines for saving at least 3 registers (r13 to r15, etc.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:16,Modifiability,variab,variables,16,// Adjust local variables that are 4-bytes or larger to 4-byte boundary,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:58,Performance,load,load,58,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:186,Performance,load,loaded,186,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:313,Performance,load,loading,313,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:184,Performance,load,loaded,184,"/// If the specified machine instruction is a direct; /// store to a stack slot, return the virtual or physical register number of; /// the source reg along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than storing to the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:1294,Integrability,rout,routine,1294,"/// Analyze the branching code at the end of MBB, returning; /// true if it cannot be understood (e.g. it's a switch dispatch or isn't; /// implemented for a target). Upon success, this returns false and returns; /// with the following information in various cases:; ///; /// 1. If this block ends with no branches (it just falls through to its succ); /// just return false, leaving TBB/FBB null.; /// 2. If this block ends with only an unconditional branch, it sets TBB to be; /// the destination block.; /// 3. If this block ends with a conditional branch and it falls through to a; /// successor block, it sets TBB to be the branch destination block and a; /// list of operands that evaluate the condition. These operands can be; /// passed to other TargetInstrInfo methods to create new branches.; /// 4. If this block ends with a conditional branch followed by an; /// unconditional branch, it returns the 'true' destination in TBB, the; /// 'false' destination in FBB, and a list of operands that evaluate the; /// condition. These operands can be passed to other TargetInstrInfo; /// methods to create new branches.; ///; /// Note that RemoveBranch and insertBranch must be implemented to support; /// cases where this method returns success.; ///; /// If AllowModify is true, then this routine is allowed to modify the basic; /// block (e.g. delete instructions after the unconditional branch).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:58,Performance,load,load,58,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:186,Performance,load,loaded,186,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:313,Performance,load,loading,313,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:184,Performance,load,loaded,184,"/// If the specified machine instruction is a direct; /// store to a stack slot, return the virtual or physical register number of; /// the source reg along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than storing to the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:28,Performance,load,load,28,// Emit code before MBBI to load immediate value into physical register Reg.; // Returns an iterator to the new instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelDAGToDAG.cpp:91,Energy Efficiency,schedul,scheduling,91,"/// This pass converts a legalized DAG into a ARC-specific DAG, ready for; /// instruction scheduling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:31,Modifiability,extend,extend,31,// We read the TIMER0 and zero-extend it to 64-bits as the intrinsic; // requires.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:8,Modifiability,extend,extend,8,// Sign extend inreg,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:107,Availability,avail,available,107,// TODO: Predicate these with `options.hasBitScan() ? Legal : Expand`; // when the HasBitScan predicate is available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:58,Performance,load,loads,58,"// Walk the register/memloc assignments, inserting copies/loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:18,Availability,mask,mask,18,// Add a register mask operand representing the call-preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:17,Performance,load,loads,17,// Transform all loads nodes into one single node because; // all load nodes are independent of each other.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:66,Performance,load,load,66,// Transform all loads nodes into one single node because; // all load nodes are independent of each other.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:70,Performance,load,load,70,"/// Transform physical registers into virtual registers, and generate load; /// operations for argument places on the stack.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:73,Energy Efficiency,schedul,scheduler,73,// All getCopyFromReg ops must precede any getMemcpys to prevent the; // scheduler clobbering a register before it has been copied.; // The stages are:; // 1. CopyFromReg (and load) arg & vararg registers.; // 2. Chain CopyFromReg nodes into a TokenFactor.; // 3. Memcpy 'byVal' args & push final InVals.; // 4. Chain mem ops nodes into a TokenFactor.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:176,Performance,load,load,176,// All getCopyFromReg ops must precede any getMemcpys to prevent the; // scheduler clobbering a register before it has been copied.; // The stages are:; // 1. CopyFromReg (and load) arg & vararg registers.; // 2. Chain CopyFromReg nodes into a TokenFactor.; // 3. Memcpy 'byVal' args & push final InVals.; // 4. Chain mem ops nodes into a TokenFactor.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:24,Performance,load,load,24,// 1a. CopyFromReg (and load) arg registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:3,Performance,Load,Load,3,// Load the argument to a virtual register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:52,Performance,load,load,52,// Create the SelectionDAG nodes corresponding to a load; // from this parameter,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:61,Safety,avoid,avoiding,61,"// guarantee that all emitted copies are; // stuck together, avoiding something bad",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:3,Deployability,Update,Update,3,// Update chain.; // Add the glue if we have it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:92,Performance,Optimiz,Optimization,92,//===----------------------------------------------------------------------===//; // Target Optimization Hooks; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:296,Performance,load,load,296,"//===----------------------------------------------------------------------===//; // Addressing mode description hooks; //===----------------------------------------------------------------------===//; /// Return true if the addressing mode represented by AM is legal for this; /// target, for a load/store of the specified type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h:44,Integrability,Interface,Interface,44,"//===- ARCISelLowering.h - ARC DAG Lowering Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines the interfaces that ARC uses to lower LLVM code into a; // selection DAG.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h:405,Integrability,interface,interfaces,405,"//===- ARCISelLowering.h - ARC DAG Lowering Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines the interfaces that ARC uses to lower LLVM code into a; // selection DAG.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h:18,Integrability,Wrap,Wrapper,18,// Global Address Wrapper,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h:94,Performance,load,load,94,"/// Return true if the addressing mode represented by AM is legal for this; /// target, for a load/store of the specified type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:462,Performance,load,load,462,"//===- ARCOptAddrMode.cpp ---------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass folds LD/ST + ADD pairs into Pre/Post-increment form of; /// load/store instructions.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:19,Performance,load,load,19,// Returns true if load/store instruction \p Ldst can be hoisted up to; // instruction \p To,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:65,Availability,down,down,65,"// // Returns true if load/store instruction \p Ldst can be sunk down; // // to instruction \p To; // bool canSinkLoadStoreTo(MachineInstr *Ldst, MachineInstr *To);; // Check if instructions \p Ldst and \p Add can be moved to become adjacent; // If they can return instruction which need not to move.; // If \p Uses is not null, fill it with instructions after \p Ldst which use; // \p Ldst's base register",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:22,Performance,load,load,22,"// // Returns true if load/store instruction \p Ldst can be sunk down; // // to instruction \p To; // bool canSinkLoadStoreTo(MachineInstr *Ldst, MachineInstr *To);; // Check if instructions \p Ldst and \p Add can be moved to become adjacent; // If they can return instruction which need not to move.; // If \p Uses is not null, fill it with instructions after \p Ldst which use; // \p Ldst's base register",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:3,Deployability,Update,Update,3,// Update all instructions in \p Uses to accomodate increment; // of \p BaseReg by \p Offset,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:88,Deployability,update,update,88,// Change instruction \p Ldst to postincrement form.; // \p NewBase is register to hold update base value; // \p NewOffset is instruction's new offset,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:72,Performance,load,load,72,// Return true if \p Off can be used as immediate offset; // operand of load/store instruction (S9 literal),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:27,Performance,load,load,27,// Return true if \p MI is load/store instruction with immediate offset; // which can be adjusted by \p Disp,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:104,Performance,load,load,104,"// bool ARCOptAddrMode::canSinkLoadStoreTo(MachineInstr *Ldst, MachineInstr *To) {; // // Can only sink load/store within same BB; // if (Ldst->getParent() != To->getParent()); // return false;; // MachineBasicBlock::const_iterator MI(Ldst), ME(To),; // End(Ldst->getParent()->end());; // bool IsStore = Ldst->mayStore();; // bool IsLoad = Ldst->mayLoad();; // Register ValReg = IsLoad ? Ldst->getOperand(0).getReg() : Register();; // for (; MI != ME && MI != End; ++MI) {; // if (MI->isDebugValue()); // continue;; // if (MI->mayStore() || MI->isCall() || MI->isInlineAsm() ||; // MI->hasUnmodeledSideEffects()); // return false;; // if (IsStore && MI->mayLoad()); // return false;; // if (ValReg && MI->readsVirtualRegister(ValReg)); // return false;; // }; // return true;; // }",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp:3,Performance,Load,Loads,3,// Loads can always be reached with LD_rlimm.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp:82,Performance,load,load,82,// We can be sure that the scavenged-register slot is within the range; // of the load offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp:29,Performance,load,load,29,"// TODO: assert based on the load type:; // ldb needs no alignment,; // ldh needs 2 byte alignment; // ld needs 4 byte alignment",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp:9,Testability,assert,assert,9,"// TODO: assert based on the load type:; // ldb needs no alignment,; // ldh needs 2 byte alignment; // ld needs 4 byte alignment",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCSubtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU=*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp:28,Deployability,Configurat,Configuration,28,/// ARC Code Generator Pass Configuration Options.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp:28,Modifiability,Config,Configuration,28,/// ARC Code Generator Pass Configuration Options.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h:8,Deployability,Pipeline,Pipeline,8,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h:17,Deployability,Configurat,Configuration,17,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h:17,Modifiability,Config,Configuration,17,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/MCTargetDesc/ARCMCTargetDesc.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU=*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/MCTargetDesc/ARCMCTargetDesc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/MCTargetDesc/ARCMCTargetDesc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:31,Performance,Optimiz,Optimize,31,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:879,Performance,perform,performance,879,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:1236,Performance,optimiz,optimized,1236,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:61,Security,access,accesses,61,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:1246,Security,access,access,1246,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:15,Performance,optimiz,optimization,15,//; // Pattern optimization methods; //,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:7,Security,Sanitiz,Sanitizing,7,//; // Sanitizing method - used to make sure if don't leave dead code around.; //,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:20,Performance,optimiz,optimized,20,// Creates the more optimized patterns and generally does all the code; // transformations in this pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:74,Performance,optimiz,optimizer,74,// See if all bar one of the operands are IMPLICIT_DEF and insert the; // optimizer pattern accordingly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:45,Deployability,update,update,45,"// The only way we can do a partial register update is through a COPY,; // INSERT_SUBREG or REG_SEQUENCE.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:50,Performance,optimiz,optimize,50,"// This function inserts instructions in order to optimize interactions between; // SPR registers and DPR/QPR registers. It does so by performing VDUPs on all; // lanes, and the using VEXT instructions to recompose the result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:135,Performance,perform,performing,135,"// This function inserts instructions in order to optimize interactions between; // SPR registers and DPR/QPR registers. It does so by performing VDUPs on all; // lanes, and the using VEXT instructions to recompose the result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:618,Energy Efficiency,efficient,efficiently,618,"// We look for instructions that write S registers that are then read as; // D/Q registers. These can only be caused by COPY, INSERT_SUBREG and; // REG_SEQUENCE pseudos that insert an SPR value into a DPR register or; // merge two SPR values to form a DPR register. In order avoid false; // positives we make sure that there is an SPR producer so we look past; // COPY and PHI nodes to find it.; //; // The best code pattern for when an SPR producer is going to be used by a; // DPR or QPR consumer depends on whether the other lanes of the; // corresponding DPR/QPR are currently defined.; //; // We can handle these efficiently, depending on the type of; // pseudo-instruction that is producing the pattern; //; // * COPY: * VDUP all lanes and merge the results together; // using VEXTs.; //; // * INSERT_SUBREG: * If the SPR value was originally in another DPR/QPR; // lane, and the other lane(s) of the DPR/QPR register; // that we are inserting in are undefined, use the; // original DPR/QPR value.; // * Otherwise, fall back on the same stategy as COPY.; //; // * REG_SEQUENCE: * If all except one of the input operands are; // IMPLICIT_DEFs, insert the VDUP pattern for just the; // defined input operand; // * Otherwise, fall back on the same stategy as COPY.; //; // First, get all the reads of D-registers done by this instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:499,Integrability,depend,depends,499,"// We look for instructions that write S registers that are then read as; // D/Q registers. These can only be caused by COPY, INSERT_SUBREG and; // REG_SEQUENCE pseudos that insert an SPR value into a DPR register or; // merge two SPR values to form a DPR register. In order avoid false; // positives we make sure that there is an SPR producer so we look past; // COPY and PHI nodes to find it.; //; // The best code pattern for when an SPR producer is going to be used by a; // DPR or QPR consumer depends on whether the other lanes of the; // corresponding DPR/QPR are currently defined.; //; // We can handle these efficiently, depending on the type of; // pseudo-instruction that is producing the pattern; //; // * COPY: * VDUP all lanes and merge the results together; // using VEXTs.; //; // * INSERT_SUBREG: * If the SPR value was originally in another DPR/QPR; // lane, and the other lane(s) of the DPR/QPR register; // that we are inserting in are undefined, use the; // original DPR/QPR value.; // * Otherwise, fall back on the same stategy as COPY.; //; // * REG_SEQUENCE: * If all except one of the input operands are; // IMPLICIT_DEFs, insert the VDUP pattern for just the; // defined input operand; // * Otherwise, fall back on the same stategy as COPY.; //; // First, get all the reads of D-registers done by this instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:631,Integrability,depend,depending,631,"// We look for instructions that write S registers that are then read as; // D/Q registers. These can only be caused by COPY, INSERT_SUBREG and; // REG_SEQUENCE pseudos that insert an SPR value into a DPR register or; // merge two SPR values to form a DPR register. In order avoid false; // positives we make sure that there is an SPR producer so we look past; // COPY and PHI nodes to find it.; //; // The best code pattern for when an SPR producer is going to be used by a; // DPR or QPR consumer depends on whether the other lanes of the; // corresponding DPR/QPR are currently defined.; //; // We can handle these efficiently, depending on the type of; // pseudo-instruction that is producing the pattern; //; // * COPY: * VDUP all lanes and merge the results together; // using VEXTs.; //; // * INSERT_SUBREG: * If the SPR value was originally in another DPR/QPR; // lane, and the other lane(s) of the DPR/QPR register; // that we are inserting in are undefined, use the; // original DPR/QPR value.; // * Otherwise, fall back on the same stategy as COPY.; //; // * REG_SEQUENCE: * If all except one of the input operands are; // IMPLICIT_DEFs, insert the VDUP pattern for just the; // defined input operand; // * Otherwise, fall back on the same stategy as COPY.; //; // First, get all the reads of D-registers done by this instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:275,Safety,avoid,avoid,275,"// We look for instructions that write S registers that are then read as; // D/Q registers. These can only be caused by COPY, INSERT_SUBREG and; // REG_SEQUENCE pseudos that insert an SPR value into a DPR register or; // merge two SPR values to form a DPR register. In order avoid false; // positives we make sure that there is an SPR producer so we look past; // COPY and PHI nodes to find it.; //; // The best code pattern for when an SPR producer is going to be used by a; // DPR or QPR consumer depends on whether the other lanes of the; // corresponding DPR/QPR are currently defined.; //; // We can handle these efficiently, depending on the type of; // pseudo-instruction that is producing the pattern; //; // * COPY: * VDUP all lanes and merge the results together; // using VEXTs.; //; // * INSERT_SUBREG: * If the SPR value was originally in another DPR/QPR; // lane, and the other lane(s) of the DPR/QPR register; // that we are inserting in are undefined, use the; // original DPR/QPR value.; // * Otherwise, fall back on the same stategy as COPY.; //; // * REG_SEQUENCE: * If all except one of the input operands are; // IMPLICIT_DEFs, insert the VDUP pattern for just the; // defined input operand; // * Otherwise, fall back on the same stategy as COPY.; //; // First, get all the reads of D-registers done by this instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:54,Integrability,depend,dependency,54,"// Now, work out if the instruction causes a SPR->DPR dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:10,Performance,optimiz,optimize,10,// We can optimize this.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:117,Performance,optimiz,optimize,117,"// Make sure to constrain the register class of the new register to; // match what we're replacing. Otherwise we can optimize a DPR_VFP2; // reference into a plain DPR, and that will end poorly. NewReg is; // always virtual here, so there will always be a matching subclass; // to find.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:103,Availability,avail,available,103,"// Since the A15SDOptimizer pass can insert VDUP instructions, it can only be; // enabled when NEON is available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARM.h:26,Integrability,interface,interface,26,"//===-- ARM.h - Top-level interface for ARM representation ------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the entry points for global functions defined in the LLVM; // ARM back-end.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARM.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARM.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:474,Integrability,depend,dependent,474,"//===-- ARMAsmPrinter.cpp - Print machine code to an ARM .s file ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a printer that converts from our internal representation; // of machine-dependent LLVM code to GAS-format ARM assembly language.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:28,Security,secur,secure,28,// Emit symbol for CMSE non-secure entry point,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:108,Modifiability,variab,variables,108,"// Collect all globals that had their storage promoted to a constant pool.; // Functions are emitted before variables, so this accumulates promoted; // globals from all functions in PromotedGlobals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:29,Performance,optimiz,optimization,29,// Calculate this function's optimization goal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:17,Performance,optimiz,optimization,17,// Combine a new optimization goal with existing ones.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:224,Availability,down,down,224,"// FIXME: The register allocator not only may not have given us the; // registers in sequence, but may not be in ascending registers. This; // will require changes in the register allocator that'll need to be; // propagated down here if the operands change.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:147,Integrability,depend,depends,147,// 'Q' should correspond to the low order register and 'R' to the high; // order register. Whether this corresponds to the upper or lower half; // depends on the endianess mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:9,Testability,stub,stub,9,// L_foo$stub:,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:59,Modifiability,variab,variables,59,// Output non-lazy-pointers for external and common global variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:245,Performance,perform,perform,245,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:238,Safety,safe,safely,238,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:339,Safety,safe,safe,339,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:92,Integrability,rout,routines,92,"//===----------------------------------------------------------------------===//; // Helper routines for emitStartOfAsmFile() and emitEndOfAsmFile(); // FIXME:; // The following seem like one-off assembler flags, but they actually need; // to appear in the .ARM.attributes section in ELF.; // Instead of subclassing the MCELFStreamer, we do the work here.; // Returns true if all functions have the same function attribute value.; // It also returns true when the module has no functions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:33,Availability,avail,available,33,// Emit build attributes for the available hardware.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:167,Security,expose,exposed,167,"// FIXME: To support emitting this build attribute as GCC does, the; // -mfp16-format option and associated plumbing must be; // supported. For now the __fp16 type is exposed by default, so this; // attribute should be emitted with value 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:97,Performance,load,load,97,"// special cases:; // 1) for Thumb1 code we sometimes materialize the constant via constpool; // load.; // 2) for Thumb1 execute only code we materialize the constant via the; // following pattern:; // movs r3, #:upper8_15:<const>; // lsls r3, #8; // adds r3, #:upper0_7:<const>; // lsls r3, #8; // adds r3, #:lower8_15:<const>; // lsls r3, #8; // adds r3, #:lower0_7:<const>; // So we need to special-case MOVS, ADDS and LSLS, and keep track of; // where we are in the sequence with the simplest of state machines.; // 3) for Thumb2 execute only code we materialize the constant via; // immediate constants in 2 separate instructions (MOVW/MOVT).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:488,Usability,simpl,simplest,488,"// special cases:; // 1) for Thumb1 code we sometimes materialize the constant via constpool; // load.; // 2) for Thumb1 execute only code we materialize the constant via the; // following pattern:; // movs r3, #:upper8_15:<const>; // lsls r3, #8; // adds r3, #:upper0_7:<const>; // lsls r3, #8; // adds r3, #:lower8_15:<const>; // lsls r3, #8; // adds r3, #:lower0_7:<const>; // So we need to special-case MOVS, ADDS and LSLS, and keep track of; // where we are in the sequence with the simplest of state machines.; // 3) for Thumb2 execute only code we materialize the constant via; // immediate constants in 2 separate instructions (MOVW/MOVT).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:48,Deployability,update,update,48,"// Registers, pushed as a part of folding an SP update into the; // push instruction are marked as undef and should not be; // restored when unwinding, because the function can modify the; // corresponding stack slots.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:3,Usability,Simpl,Simple,3,// Simple pseudo-instructions have their lowering (with expansion to real; // instructions) auto-generated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:61,Testability,test,test,61,"// TODOD FIXME: Enable feature predicate checks once all the test pass.; // ARM_MC::verifyInstructionPredicates(MI->getOpcode(),; // getSubtargetInfo().getFeatureBits());",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:21,Performance,load,load,21,// Form and emit the load,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:489,Performance,load,load,489,"// TBB [base, idx] =; // ADDS idx, idx, base; // LDRB idx, [idx, #4] ; or LDRH if TBH; // LSLS idx, #1; // ADDS pc, pc, idx; // When using PC as the base, it's important that there is no padding; // between the last ADDS and the start of the jump table. The jump table; // is 4-byte aligned, so we ensure we're 4 byte aligned here too.; //; // FIXME: Ideally we could vary the LDRB index based on the padding; // between the sequence and jump table, however that relies on MCExprs; // for load indexes which are currently not supported.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h:4,Performance,Optimiz,OptimizationGoals,4,"/// OptimizationGoals - Maintain a combined optimization goal for all; /// functions in a module: one of Tag_ABI_optimization_goals values,; /// -1 if uninitialized, 0 if conflicting goals",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h:44,Performance,optimiz,optimization,44,"/// OptimizationGoals - Maintain a combined optimization goal for all; /// functions in a module: one of Tag_ABI_optimization_goals values,; /// -1 if uninitialized, 0 if conflicting goals",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Energy Efficiency,schedul,scheduling,52,// Use a ScoreboardHazardRecognizer for prepass ARM scheduling. TargetInstrImpl; // currently defaults to no prepass hazard recognizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:117,Safety,hazard,hazard,117,// Use a ScoreboardHazardRecognizer for prepass ARM scheduling. TargetInstrImpl; // currently defaults to no prepass hazard recognizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:31,Energy Efficiency,schedul,scheduling,31,// Called during:; // - pre-RA scheduling; // - post-RA scheduling when FeatureUseMISched is set,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:56,Energy Efficiency,schedul,scheduling,56,// Called during:; // - pre-RA scheduling; // - post-RA scheduling when FeatureUseMISched is set,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:72,Energy Efficiency,schedul,scheduling,72,// We would like to restrict this hazard recognizer to only; // post-RA scheduling; we can tell that we're post-RA because we don't; // track VRegLiveness.; // Cortex-M7: TRM indicates that there is a single ITCM bank and two DTCM; // banks banked on bit 2. Assume that TCMs are in use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:34,Safety,hazard,hazard,34,// We would like to restrict this hazard recognizer to only; // post-RA scheduling; we can tell that we're post-RA because we don't; // track VRegLiveness.; // Cortex-M7: TRM indicates that there is a single ITCM bank and two DTCM; // banks banked on bit 2. Assume that TCMs are in use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:25,Energy Efficiency,schedul,scheduling,25,// Called during post-RA scheduling when FeatureUseMISched is not set,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:28,Performance,load,load,28,// Try splitting an indexed load/store to an un-indexed one plus an add/sub; // operation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:305,Testability,test,test,305,"// Branch analysis.; // Cond vector output format:; // 0 elements indicates an unconditional branch; // 2 elements indicates a conditional branch; the elements are; // the condition to check and the CPSR.; // 3 elements indicates a hardware loop end; the elements; // are the opcode, the operand value to test, and a dummy; // operand used to pad out to 3 operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:224,Integrability,interface,interface,224,// VMOVRRD is also a copy instruction but it requires; // special way of handling. It is more complex copy version; // and since that we are not considering it. For recognition; // of such instruction isExtractSubregLike MI interface fuction; // could be used.; // VORRq is considered as a move only if two inputs are; // the same register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:30,Security,access,access,30,// FIXME: don't use t2STRs to access frame.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:30,Security,access,access,30,// FIXME: don't use t2LDRs to access frame.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:69,Integrability,depend,depending,69,/// Expands MEMCPY to either LDMIA/STMIA or LDMIA_UPD/STMID_UPD; /// depending on whether the result is used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:248,Availability,down,down,248,// This hook gets to expand COPY instructions before they become; // copyPhysReg() calls. Look for VMOVS instructions that can legally be; // widened to VMOVD. We prefer the VMOVD when possible because it may be; // changed into a VORR that can go down the NEON pipeline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:262,Deployability,pipeline,pipeline,262,// This hook gets to expand COPY instructions before they become; // copyPhysReg() calls. Look for VMOVS instructions that can legally be; // widened to VMOVD. We prefer the VMOVD when possible because it may be; // changed into a VORR that can go down the NEON pipeline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:7,Usability,clear,clear,7,"// All clear, widen the COPY.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:41,Deployability,Update,Update,41,/// Create a copy of a const pool value. Update CPI to the new index and return; /// the label UID.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:16,Performance,load,loaded,16,"// Check if the loaded value, e.g. a constantpool of a global address, are; // the same.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:63,Energy Efficiency,schedul,scheduler,63,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only differences; /// between the two addresses is the offset. It also returns the offsets by; /// reference.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:386,Integrability,interface,interface,386,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only differences; /// between the two addresses is the offset. It also returns the offsets by; /// reference.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:98,Performance,load,loads,98,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only differences; /// between the two addresses is the offset. It also returns the offsets by; /// reference.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:108,Performance,load,loading,108,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only differences; /// between the two addresses is the offset. It also returns the offsets by; /// reference.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:65,Energy Efficiency,schedul,scheduler,65,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:167,Energy Efficiency,schedul,scheduled,167,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:295,Energy Efficiency,schedul,scheduled,295,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:473,Energy Efficiency,schedul,schedule,473,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:568,Energy Efficiency,schedul,scheduled,568,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:649,Integrability,interface,interface,649,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:146,Performance,load,loads,146,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:209,Performance,load,loads,209,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:219,Performance,load,loading,219,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:259,Performance,cache,cache,259,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:372,Performance,load,load,372,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:490,Performance,load,loads,490,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:534,Performance,load,loads,534,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:169,Performance,load,loads,169,"// Check if the machine opcodes are different. If they are different; // then we consider them to not be of the same base address,; // EXCEPT in the case of Thumb2 byte loads where one is LDRBi8 and the other LDRBi12.; // In this case, they are considered to be the same because they are different; // encoding forms of the same basic instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:40,Performance,load,loads,40,// FIXME: overly conservative?; // Four loads in a row should be sufficient.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:25,Energy Efficiency,schedul,scheduling,25,"// Debug info is never a scheduling boundary. It's necessary to be explicit; // due to the special treatment of IT instructions below, otherwise a; // dbg_value followed by an IT will result in the IT instruction being; // considered a scheduling hazard, which is wrong. It should be the actual; // instruction preceding the dbg_value instruction(s), just like it is; // when debug info is not present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:236,Energy Efficiency,schedul,scheduling,236,"// Debug info is never a scheduling boundary. It's necessary to be explicit; // due to the special treatment of IT instructions below, otherwise a; // dbg_value followed by an IT will result in the IT instruction being; // considered a scheduling hazard, which is wrong. It should be the actual; // instruction preceding the dbg_value instruction(s), just like it is; // when debug info is not present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:247,Safety,hazard,hazard,247,"// Debug info is never a scheduling boundary. It's necessary to be explicit; // due to the special treatment of IT instructions below, otherwise a; // dbg_value followed by an IT will result in the IT instruction being; // considered a scheduling hazard, which is wrong. It should be the actual; // instruction preceding the dbg_value instruction(s), just like it is; // when debug info is not present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:35,Energy Efficiency,schedul,scheduled,35,// Terminators and labels can't be scheduled around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:40,Energy Efficiency,schedul,scheduling,40,"// Treat the start of the IT block as a scheduling boundary, but schedule; // t2IT along with all instructions following it.; // FIXME: This is a big hammer. But the alternative is to add all potential; // true and anti dependencies to IT block instructions as implicit operands; // to the t2IT instruction. The added compile time and complexity does not; // seem worth it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:65,Energy Efficiency,schedul,schedule,65,"// Treat the start of the IT block as a scheduling boundary, but schedule; // t2IT along with all instructions following it.; // FIXME: This is a big hammer. But the alternative is to add all potential; // true and anti dependencies to IT block instructions as implicit operands; // to the t2IT instruction. The added compile time and complexity does not; // seem worth it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:220,Integrability,depend,dependencies,220,"// Treat the start of the IT block as a scheduling boundary, but schedule; // t2IT along with all instructions following it.; // FIXME: This is a big hammer. But the alternative is to add all potential; // true and anti dependencies to IT block instructions as implicit operands; // to the t2IT instruction. The added compile time and complexity does not; // seem worth it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:20,Energy Efficiency,schedul,schedule,20,"// Don't attempt to schedule around any instruction that defines; // a stack-oriented pointer, as it's unlikely to be profitable. This; // saves compile time, because it doesn't require every single; // stack slot reference to depend on the instruction that does the; // modification.; // Calls don't actually change the stack pointer, even if they have imp-defs.; // No ARM calling conventions change the stack pointer. (X86 calling; // conventions sometimes do).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:227,Integrability,depend,depend,227,"// Don't attempt to schedule around any instruction that defines; // a stack-oriented pointer, as it's unlikely to be profitable. This; // saves compile time, because it doesn't require every single; // stack slot reference to depend on the instruction that does the; // modification.; // Calls don't actually change the stack pointer, even if they have imp-defs.; // No ARM calling conventions change the stack pointer. (X86 calling; // conventions sometimes do).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:13,Performance,optimiz,optimizing,13,"// If we are optimizing for size, see if the branch in the predecessor can be; // lowered to cbn?z by the constant island lowering pass, and return false if; // so. This results in a shorter instruction sequence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:128,Safety,avoid,avoid,128,// Attempt to estimate the relative costs of predication versus branching.; // Here we scale up each component of UnpredCost to avoid precision issue when; // scaling TCycles/FCycles by Probability.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:31,Safety,predict,predictor,31,"// When we don't have a branch predictor it's always cheaper to not take a; // branch than take it, so we have to take that into account.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:106,Energy Efficiency,reduce,reduce,106,"// If this branch is likely to be folded into the comparison to form a; // CB(N)Z, then removing it won't reduce code size at all, because that will; // just replace the CB(N)Z with a CMP.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Energy Efficiency,Reduce,Reduce,3,// Reduce false anti-dependencies to let the target's out-of-order execution; // engine do its thing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:21,Integrability,depend,dependencies,21,// Reduce false anti-dependencies to let the target's out-of-order execution; // engine do its thing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:64,Safety,detect,detects,64,// Check if MI has any non-dead defs or physreg uses. This also detects; // predicated instructions which will be reading CPSR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:180,Energy Efficiency,allocate,allocated,180,// The output register value when the predicate is false is an implicit; // register operand tied to the first def.; // The tie makes the register allocator ensure the FalseReg is allocated the; // same register as operand 0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Deployability,Update,Update,3,// Update SeenMIs set: register newly created MI and erase removed DefMI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:42,Usability,clear,clear,42,"// We will handle these bits from offset, clear them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:46,Performance,load,load,46,"// This optimisation potentially adds lots of load and store; // micro-operations, it's only really a great benefit to code-size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:50,Energy Efficiency,allocate,allocate,50,// Now try to find enough space in the reglist to allocate NumBytes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,Performance,perform,perform,37,// Finally we know we can profitably perform the optimisation so go; // ahead: strip all existing registers off and add them back again; // in the right order.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:42,Usability,clear,clear,42,"// We will handle these bits from offset, clear them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:46,Usability,simpl,simplify,46,"// FIXME: When addrmode2 goes away, this will simplify (like the; // T2 version), as the LDR.i12 versions don't need the encoding; // tricks for the offset value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Usability,simpl,simplify,52,"// Otherwise, it didn't fit. Pull in what we can to simplify the immed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:131,Availability,mask,mask,131,"/// isSuitableForMask - Identify a suitable 'and' instruction that; /// operates on the given source register and applies the same mask; /// as a 'tst' instruction. Provide a limited look-through for copies.; /// When successful, MI will hold the found instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:120,Availability,redundant,redundant,120,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:154,Availability,redundant,redundant,154,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:226,Availability,redundant,redundant,226,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:306,Availability,redundant,redundant,306,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:94,Deployability,update,update,94,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:366,Modifiability,extend,extended,366,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:120,Safety,redund,redundant,120,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:154,Safety,redund,redundant,154,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:226,Safety,redund,redundant,226,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:306,Safety,redund,redundant,306,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:166,Availability,redundant,redundant,166,"/// optimizeCompareInstr - Convert the instruction supplying the argument to the; /// comparison into one that sets the zero bit in the flags register;; /// Remove a redundant Compare instruction if an earlier instruction can set the; /// flags in the same way as Compare.; /// E.g. SUBrr(r1,r2) and CMPrr(r1,r2). We also handle the case where two; /// operands are swapped: SUBrr(r1,r2) and CMPrr(r2,r1), by updating the; /// condition code of instructions which use the flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:4,Performance,optimiz,optimizeCompareInstr,4,"/// optimizeCompareInstr - Convert the instruction supplying the argument to the; /// comparison into one that sets the zero bit in the flags register;; /// Remove a redundant Compare instruction if an earlier instruction can set the; /// flags in the same way as Compare.; /// E.g. SUBrr(r1,r2) and CMPrr(r1,r2). We also handle the case where two; /// operands are swapped: SUBrr(r1,r2) and CMPrr(r2,r1), by updating the; /// condition code of instructions which use the flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:166,Safety,redund,redundant,166,"/// optimizeCompareInstr - Convert the instruction supplying the argument to the; /// comparison into one that sets the zero bit in the flags register;; /// Remove a redundant Compare instruction if an earlier instruction can set the; /// flags in the same way as Compare.; /// E.g. SUBrr(r1,r2) and CMPrr(r1,r2). We also handle the case where two; /// operands are swapped: SUBrr(r1,r2) and CMPrr(r2,r1), by updating the; /// condition code of instructions which use the flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Availability,Mask,Masked,3,// Masked compares sometimes use the same register as the corresponding 'and'.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:202,Performance,load,load,202,"// We also want to do this peephole for cases like this: if (a*b == 0),; // and optimise away the CMP instruction from the generated code sequence:; // MULS, MOVS, MOVS, CMP. Here the MOVS instructions load the boolean values; // resulting from the select instruction, but these MOVS instructions for; // Thumb1 (V6M) are flag setting and are thus preventing this optimisation.; // However, if we only have MOVS instructions in between the CMP and the; // other instruction (the MULS in this example), then the CPSR is dead so we; // can safely reorder the sequence into: MOVS, MOVS, MULS, CMP. We do this; // reordering and then continue the analysis hoping we can eliminate the; // CMP. This peephole works on the vregs, so is still in SSA form. As a; // consequence, the movs won't redefine/kill the MUL operands which would; // make this reordering illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:538,Safety,safe,safely,538,"// We also want to do this peephole for cases like this: if (a*b == 0),; // and optimise away the CMP instruction from the generated code sequence:; // MULS, MOVS, MOVS, CMP. Here the MOVS instructions load the boolean values; // resulting from the select instruction, but these MOVS instructions for; // Thumb1 (V6M) are flag setting and are thus preventing this optimisation.; // However, if we only have MOVS instructions in between the CMP and the; // other instruction (the MULS in this example), then the CPSR is dead so we; // can safely reorder the sequence into: MOVS, MOVS, MULS, CMP. We do this; // reordering and then continue the analysis hoping we can eliminate the; // CMP. This peephole works on the vregs, so is still in SSA form. As a; // consequence, the movs won't redefine/kill the MUL operands which would; // make this reordering illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:38,Availability,redundant,redundant,38,// Check whether CmpInstr can be made redundant by the current instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:38,Safety,redund,redundant,38,// Check whether CmpInstr can be made redundant by the current instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:116,Energy Efficiency,schedul,scheduled,116,"// In some cases, we scan the use-list of an instruction for an AND;; // that AND is in the same BB, but may not be scheduled before the; // corresponding TST. In that case, bail out.; //; // FIXME: We could try to reschedule the AND.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:160,Safety,safe,safe,160,"// Scan forward for the use of CPSR; // When checking against MI: if it's a conditional code that requires; // checking of the V bit or C bit, then this is not safe to do.; // It is safe to remove CmpInstr if CPSR is redefined or killed.; // If we are done with the basic block, we need to check whether CPSR is; // live-out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:182,Safety,safe,safe,182,"// Scan forward for the use of CPSR; // When checking against MI: if it's a conditional code that requires; // checking of the V bit or C bit, then this is not safe to do.; // It is safe to remove CmpInstr if CPSR is redefined or killed.; // If we are done with the basic block, we need to check whether CPSR is; // live-out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:91,Deployability,update,updated,91,"// If we have SUB(r1, r2) and CMP(r2, r1), the condition code based; // on CMP needs to be updated to be based on SUB.; // If we have ADD(r1, r2, X) and CMP(r1, r2), the condition code also; // needs to be modified.; // Push the condition code operands to OperandsToUpdate.; // If it is safe to remove CmpInstr, the condition code of these; // operands will be modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:287,Safety,safe,safe,287,"// If we have SUB(r1, r2) and CMP(r2, r1), the condition code based; // on CMP needs to be updated to be based on SUB.; // If we have ADD(r1, r2, X) and CMP(r1, r2), the condition code also; // needs to be modified.; // Push the condition code operands to OperandsToUpdate.; // If it is safe to remove CmpInstr, the condition code of these; // operands will be modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:39,Deployability,update,update,39,// VSel doesn't support condition code update.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:66,Safety,safe,safe,66,// Z N V; // The instruction uses the V bit or C bit which is not safe.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:111,Performance,optimiz,optimize,111,"// If CPSR is not killed nor re-defined, we should check whether it is; // live-out. If it is live-out, do not optimize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Deployability,Toggle,Toggle,3,// Toggle the optional operand to CPSR (if it exists - in Thumb1 we always; // set CPSR so this is represented as an explicit output),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Availability,redundant,redundant,52,// Do not sink MI if it might be used to optimize a redundant compare.; // We heuristically only look at the instruction immediately following MI to; // avoid potentially searching the entire basic block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:41,Performance,optimiz,optimize,41,// Do not sink MI if it might be used to optimize a redundant compare.; // We heuristically only look at the instruction immediately following MI to; // avoid potentially searching the entire basic block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Safety,redund,redundant,52,// Do not sink MI if it might be used to optimize a redundant compare.; // We heuristically only look at the instruction immediately following MI to; // avoid potentially searching the entire basic block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:153,Safety,avoid,avoid,153,// Do not sink MI if it might be used to optimize a redundant compare.; // We heuristically only look at the instruction immediately following MI to; // avoid potentially searching the entire basic block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:64,Safety,safe,safe,64,"// If DefMI defines CPSR and it is not dead, it's obviously not safe; // to delete DefMI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:57,Performance,optimiz,optimization,57,"// If the instruction sets the flag, do not attempt this optimization; // since it may change the semantics of the code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:877,Energy Efficiency,schedul,scheduling,877,"// Return the number of 32-bit words loaded by LDM or stored by STM. If this; // can't be easily determined return 0 (missing MachineMemOperand).; //; // FIXME: The current MachineInstr design does not support relying on machine; // mem operands to determine the width of a memory access. Instead, we expect; // the target to provide this information based on the instruction opcode and; // operands. However, using MachineMemOperand is the best solution now for; // two reasons:; //; // 1) getNumMicroOps tries to infer LDM memory width from the total number of MI; // operands. This is much more dangerous than using the MachineMemOperand; // sizes because CodeGen passes can insert/remove optional machine operands. In; // fact, it's totally incorrect for preRA passes and appears to be wrong for; // postRA passes as well.; //; // 2) getNumLDMAddresses is only used by the scheduling machine model and any; // machine model that calls this should handle the unknown (zero size) case.; //; // Long term, we should require a target hook that verifies MachineMemOperand; // sizes during MC lowering. That target hook should be local to MC lowering; // because we can't ensure that it is aware of other MI forms. Doing this will; // ensure that MachineMemOperands are correctly propagated through all passes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,Performance,load,loaded,37,"// Return the number of 32-bit words loaded by LDM or stored by STM. If this; // can't be easily determined return 0 (missing MachineMemOperand).; //; // FIXME: The current MachineInstr design does not support relying on machine; // mem operands to determine the width of a memory access. Instead, we expect; // the target to provide this information based on the instruction opcode and; // operands. However, using MachineMemOperand is the best solution now for; // two reasons:; //; // 1) getNumMicroOps tries to infer LDM memory width from the total number of MI; // operands. This is much more dangerous than using the MachineMemOperand; // sizes because CodeGen passes can insert/remove optional machine operands. In; // fact, it's totally incorrect for preRA passes and appears to be wrong for; // postRA passes as well.; //; // 2) getNumLDMAddresses is only used by the scheduling machine model and any; // machine model that calls this should handle the unknown (zero size) case.; //; // Long term, we should require a target hook that verifies MachineMemOperand; // sizes during MC lowering. That target hook should be local to MC lowering; // because we can't ensure that it is aware of other MI forms. Doing this will; // ensure that MachineMemOperands are correctly propagated through all passes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:281,Security,access,access,281,"// Return the number of 32-bit words loaded by LDM or stored by STM. If this; // can't be easily determined return 0 (missing MachineMemOperand).; //; // FIXME: The current MachineInstr design does not support relying on machine; // mem operands to determine the width of a memory access. Instead, we expect; // the target to provide this information based on the instruction opcode and; // operands. However, using MachineMemOperand is the best solution now for; // two reasons:; //; // 1) getNumMicroOps tries to infer LDM memory width from the total number of MI; // operands. This is much more dangerous than using the MachineMemOperand; // sizes because CodeGen passes can insert/remove optional machine operands. In; // fact, it's totally incorrect for preRA passes and appears to be wrong for; // postRA passes as well.; //; // 2) getNumLDMAddresses is only used by the scheduling machine model and any; // machine model that calls this should handle the unknown (zero size) case.; //; // Long term, we should require a target hook that verifies MachineMemOperand; // sizes during MC lowering. That target hook should be local to MC lowering; // because we can't ensure that it is aware of other MI forms. Doing this will; // ensure that MachineMemOperands are correctly propagated through all passes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:14,Energy Efficiency,schedul,scheduler,14,"// FIXME: The scheduler currently can't handle values larger than 16. But; // the values can actually go up to 32 for floating-point load/store; // multiple (VLDMIA etc.). Also, the way this code is reasoning about memory; // operations isn't right; we could end up with ""extra"" memory operands for; // various reasons, like tail merge merging two memory operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:133,Performance,load,load,133,"// FIXME: The scheduler currently can't handle values larger than 16. But; // the values can actually go up to 32 for floating-point load/store; // multiple (VLDMIA etc.). Also, the way this code is reasoning about memory; // operations isn't right; we could end up with ""extra"" memory operands for; // various reasons, like tail merge merging two memory operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:158,Energy Efficiency,schedul,scheduled,158,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:195,Energy Efficiency,schedul,scheduling,195,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:26,Performance,load,load,26,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:136,Performance,load,loads,136,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:220,Performance,load,load,220,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:474,Performance,load,load,474,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:345,Usability,simpl,simply,345,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:10,Performance,latency,latency,10,// Result latency is issue cycle + 2: E2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:10,Performance,latency,latency,10,// Result latency is AGU cycles + 2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:74,Performance,latency,latency,74,"// This may be a def / use of a variable_ops instruction, the operand; // latency might be determinable dynamically. Let the target try to; // figure it out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:41,Performance,latency,latency,41,"// We can't seem to determine the result latency of the def, assume it's 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:168,Performance,latency,latency,168,/// Return the number of cycles to add to (or subtract from) the static; /// itinerary based on the def opcode and alignment. The caller will ensure that; /// adjusted latency is at least one cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,Performance,latency,latency,37,// FIXME: Properly handle all of the latency adjustments for address; // writeback.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:14,Performance,latency,latency,14,// No operand latency. The caller may fall back to getInstrLatency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:38,Performance,latency,latency,38,// Otherwise it takes the instruction latency (generally one).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:30,Energy Efficiency,schedul,scheduling,30,"// For Thumb2 and -Os, prefer scheduling CPSR setting instruction close to; // its uses. Instructions which are otherwise scheduled between them may; // incur a code size penalty (not able to use the CPSR setting 16-bit; // instructions).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:122,Energy Efficiency,schedul,scheduled,122,"// For Thumb2 and -Os, prefer scheduling CPSR setting instruction close to; // its uses. Instructions which are otherwise scheduled between them may; // incur a code size penalty (not able to use the CPSR setting 16-bit; // instructions).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:23,Performance,latency,latency,23,"// Get the itinerary's latency if possible, and handle variable_ops.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:26,Performance,latency,latency,26,// Unable to find operand latency. The caller may resort to getInstrLatency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Performance,latency,latency,24,"// Return the itinerary latency, which may be zero but not less than zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,Performance,latency,latency,37,// FIXME: Properly handle all of the latency adjustments for address; // writeback.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:18,Energy Efficiency,schedul,scheduler,18,"// An instruction scheduler typically runs on unbundled instructions, however; // other passes may query the latency of a bundled instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:109,Performance,latency,latency,109,"// An instruction scheduler typically runs on unbundled instructions, however; // other passes may query the latency of a bundled instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:25,Modifiability,variab,variable,25,"// For instructions with variable uops, use uops as latency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Performance,latency,latency,52,"// For instructions with variable uops, use uops as latency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:53,Performance,latency,latency,53,"// For the common case, fall back on the itinerary's latency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:50,Performance,latency,latency,50,// Hoist VFP / NEON instructions with 4 or higher latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:215,Availability,down,down,215,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:247,Availability,down,down,247,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:290,Availability,down,down,290,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:326,Availability,down,down,326,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:229,Deployability,pipeline,pipeline,229,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:260,Deployability,pipeline,pipeline,260,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:339,Deployability,pipeline,pipeline,339,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:431,Deployability,pipeline,pipeline,431,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:20,Security,access,access,20,// If we don't have access to NEON instructions then we won't be able; // to swizzle anything to the NEON domain. Check to make sure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:53,Performance,perform,perform,53,"// In general there's no single instruction that can perform an S <-> S; // move in NEON space, but a pair of VEXT instructions *can* do the; // job. It turns out that the VEXTs needed will only use DSrc once, with; // the position based purely on the combination of lane-0 and lane-1; // involved. For example; // vmov s0, s2 -> vext.32 d0, d0, d1, #1 vext.32 d0, d0, d0, #1; // vmov s1, s3 -> vext.32 d0, d1, d0, #1 vext.32 d0, d0, d0, #1; // vmov s0, s3 -> vext.32 d0, d0, d0, #1 vext.32 d0, d1, d0, #1; // vmov s1, s2 -> vext.32 d0, d0, d0, #1 vext.32 d0, d0, d1, #1; //; // Pattern of the MachineInstrs is:; // %DDst = VEXTd32 %DSrc1, %DSrc2, Lane, 14, %noreg (;implicits)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:102,Deployability,update,updates,102,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:586,Deployability,update,update,586,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:648,Deployability,update,update,648,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:715,Deployability,update,update,715,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:764,Integrability,depend,dependency-breaking,764,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:453,Performance,load,loads,453,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:511,Performance,load,load,511,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:367,Safety,avoid,avoided,367,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Integrability,depend,dependency,24,// Explicitly reads the dependency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:81,Integrability,depend,dependency,81,"// If this instruction actually reads a value from Reg, there is no unwanted; // dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:33,Integrability,depend,dependency,33,// MI has an unwanted D-register dependency.; // Avoid defs in the previous N instructrions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:49,Safety,Avoid,Avoid,49,// MI has an unwanted D-register dependency.; // Avoid defs in the previous N instructrions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:28,Integrability,depend,dependency,28,// Break a partial register dependency after getPartialRegUpdateClearance; // returned non-zero.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:232,Energy Efficiency,schedul,schedule,232,"// FIXME: In some cases, VLDRS can be changed to a VLD1DUPd32 which defines; // the full D-register by loading the same value to both lanes. The; // instruction is micro-coded with 2 uops, so don't do this until we can; // properly schedule micro-coded instructions. The dispatcher stalls cause; // too big regressions.; // Insert the dependency-breaking FCONSTD before MI.; // 96 is the encoding of 0.5, but the actual value doesn't matter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:335,Integrability,depend,dependency-breaking,335,"// FIXME: In some cases, VLDRS can be changed to a VLD1DUPd32 which defines; // the full D-register by loading the same value to both lanes. The; // instruction is micro-coded with 2 uops, so don't do this until we can; // properly schedule micro-coded instructions. The dispatcher stalls cause; // too big regressions.; // Insert the dependency-breaking FCONSTD before MI.; // 96 is the encoding of 0.5, but the actual value doesn't matter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:103,Performance,load,loading,103,"// FIXME: In some cases, VLDRS can be changed to a VLD1DUPd32 which defines; // the full D-register by loading the same value to both lanes. The; // instruction is micro-coded with 2 uops, so don't do this until we can; // properly schedule micro-coded instructions. The dispatcher stalls cause; // too big regressions.; // Insert the dependency-breaking FCONSTD before MI.; // 96 is the encoding of 0.5, but the actual value doesn't matter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:16,Performance,load,load,16,// Literal pool load,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:2150,Availability,avail,available,2150,"nction is; /// called with a BL instruction, and the outlined function tail-calls the; /// original call destination.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// BL f I2; /// B f; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 4 | 4 |; /// | Frame overhead in Bytes | 0 | 0 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerNoLRSave implies that the function should be called using; /// a BL instruction, but doesn't require LR to be saved and restored. This; /// happens when LR is known to be dead.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 4 | 4 |; /// | Frame overhead in Bytes | 2 | 4 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerRegSave implies that the function should be called with a; /// save and restore of LR to an available register. This allows us to avoid; /// stack fixups. Note that this outlining variant is compatible with the; /// NoLRSave case.; ///; /// That is,; ///; /// I1 Save LR OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 Restore LR I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 8 | 12 |; /// | Frame overhead in Bytes | 2 | 4 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerDefault implies that the function should be called with; /// a save and restore of LR to the stack.; ///; /// That is,; ///; /// I1 Save LR OUTLIN",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:2188,Safety,avoid,avoid,2188," ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 4 | 4 |; /// | Frame overhead in Bytes | 0 | 0 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerNoLRSave implies that the function should be called using; /// a BL instruction, but doesn't require LR to be saved and restored. This; /// happens when LR is known to be dead.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 4 | 4 |; /// | Frame overhead in Bytes | 2 | 4 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerRegSave implies that the function should be called with a; /// save and restore of LR to an available register. This allows us to avoid; /// stack fixups. Note that this outlining variant is compatible with the; /// NoLRSave case.; ///; /// That is,; ///; /// I1 Save LR OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 Restore LR I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 8 | 12 |; /// | Frame overhead in Bytes | 2 | 4 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerDefault implies that the function should be called with; /// a save and restore of LR to the stack.; ///; /// That is,; ///; /// I1 Save LR OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 Restore LR I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 8 | 12 |; /// | Frame overhead in Bytes | 2 | 4 |;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Availability,avail,available,24,// Check if there is an available register across the sequence that we can; // use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:10,Safety,unsafe,unsafe,10,"// If the unsafe registers in this block are all dead, then we don't need; // to compute liveness here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:116,Security,authenticat,authentication,116,"// We expect the majority of the outlining candidates to be in consensus with; // regard to return address sign and authentication, and branch target; // enforcement, in other words, partitioning according to all the four; // possible combinations of PAC-RET and BTI is going to yield one big subset; // and three small (likely empty) subsets. That allows us to cull incompatible; // candidates separately for PAC-RET and BTI.; // Partition the candidates in two sets: one with BTI enabled and one with BTI; // disabled. Remove the candidates from the smaller set. If they are the same; // number prefer the non-BTI ones for outlining, since they have less; // overhead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:32,Safety,safe,safe,32,"// At this point, we have only ""safe"" candidates to outline. Figure out; // frame + call instruction information.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:40,Security,authenticat,authentication,40,// Adjust costs to account for sign and authentication instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:25,Availability,avail,available,25,"// Is an unused register available? If so, we won't modify the stack, so; // we can outline with the same frame type as those that don't save LR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:88,Deployability,update,update,88,"// If there are no places where we have to save LR, then note that we don't; // have to update the stack. Otherwise, give every candidate the default; // call type",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Performance,Load,Load,3,// Load/Store Multiple,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:8,Performance,Load,Load,8,// Neon Load/Store Multiple,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:9,Security,access,access,9,// PCrel access,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:8,Safety,safe,safe,8,// It's safe to outline from MF.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:18,Availability,avail,available,18,"// Check if LR is available through all of the MBB. If it's not, then set; // a flag.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:45,Availability,avail,available,45,// Check if each of the unsafe registers are available...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Safety,unsafe,unsafe,24,// Check if each of the unsafe registers are available...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:32,Availability,avail,available,32,"// If any of these registers is available in the MBB, but also a live out of; // the block, then we know outlining is unsafe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:118,Safety,unsafe,unsafe,118,"// If any of these registers is available in the MBB, but also a live out of; // the block, then we know outlining is unsafe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:57,Integrability,depend,depends,57,"// If we don't know anything about the callee, assume it depends on the; // stack layout of the caller. In that case, it's only legal to outline; // as a tail-call. Explicitly list the call instructions we know about so; // we don't get unexpected results with call pseudo-instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:84,Safety,safe,safely,84,// We have a function we have information about. Check if it's something we; // can safely outline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:132,Availability,avail,available,132,"// True if there is no chance that any outlined candidate from this range; // could require stack fixups. That is, both; // * LR is available in the range (No save/restore around call); // * The range doesn't include calls (No save/restore in outlined frame); // are true.; // These conditions also ensure correctness of the return address; // authentication - we insert sign and authentication instructions only if; // we save/restore LR on stack, but then this condition ensures that the; // outlined range does not modify the SP, therefore the SP value used for; // signing is the same as the one used for authentication.; // FIXME: This is very restrictive; the flags check the whole block,; // not just the bit we will try to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:344,Security,authenticat,authentication,344,"// True if there is no chance that any outlined candidate from this range; // could require stack fixups. That is, both; // * LR is available in the range (No save/restore around call); // * The range doesn't include calls (No save/restore in outlined frame); // are true.; // These conditions also ensure correctness of the return address; // authentication - we insert sign and authentication instructions only if; // we save/restore LR on stack, but then this condition ensures that the; // outlined range does not modify the SP, therefore the SP value used for; // signing is the same as the one used for authentication.; // FIXME: This is very restrictive; the flags check the whole block,; // not just the bit we will try to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:380,Security,authenticat,authentication,380,"// True if there is no chance that any outlined candidate from this range; // could require stack fixups. That is, both; // * LR is available in the range (No save/restore around call); // * The range doesn't include calls (No save/restore in outlined frame); // are true.; // These conditions also ensure correctness of the return address; // authentication - we insert sign and authentication instructions only if; // we save/restore LR on stack, but then this condition ensures that the; // outlined range does not modify the SP, therefore the SP value used for; // signing is the same as the one used for authentication.; // FIXME: This is very restrictive; the flags check the whole block,; // not just the bit we will try to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:609,Security,authenticat,authentication,609,"// True if there is no chance that any outlined candidate from this range; // could require stack fixups. That is, both; // * LR is available in the range (No save/restore around call); // * The range doesn't include calls (No save/restore in outlined frame); // are true.; // These conditions also ensure correctness of the return address; // authentication - we insert sign and authentication instructions only if; // we save/restore LR on stack, but then this condition ensures that the; // outlined range does not modify the SP, therefore the SP value used for; // signing is the same as the one used for authentication.; // FIXME: This is very restrictive; the flags check the whole block,; // not just the bit we will try to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:110,Performance,load,load,110,"// At this point, we have a stack instruction that we might need to fix up.; // up. We'll handle it if it's a load or store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:6,Security,authenticat,authentication,6,"// LR authentication is after the CFI instructions, below.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Modifiability,rewrite,rewrite,24,"// For thunk outlining, rewrite the last instruction from a call to a; // tail-call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:81,Security,access,accesses,81,// We modified the stack.; // Walk over the basic block and fix up all the stack accesses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:101,Energy Efficiency,schedul,schedule,101,// Bitset[0 .. MAX_STAGES-1] ... iterations needed; // [LAST_IS_USE] : last reference to register in schedule is a use; // [SEEN_AS_LIVE] : Normal pressure algorithm believes register is live,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:57,Energy Efficiency,schedul,schedule,57,// Determine which values will be loop-carried after the schedule is; // applied,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:44,Energy Efficiency,schedul,schedule,44,// Determine more-or-less what the proposed schedule (reversed) is going to; // be; it might not be quite the same because the within-cycle ordering; // created by SMSchedule depends upon changes to help with address offsets and; // the like.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:175,Integrability,depend,depends,175,// Determine more-or-less what the proposed schedule (reversed) is going to; // be; it might not be quite the same because the within-cycle ordering; // created by SMSchedule depends upon changes to help with address offsets and; // the like.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Usability,Learn,Learn,3,"// Learn whether the last use/def of each cross-iteration register is a use or; // def. If it is a def, RegisterPressure will implicitly increase max pressure; // and we do not have to add the pressure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:176,Deployability,pipeline,pipelineable,176,"// If the branch is a Bcc, then the CPSR should be set somewhere within the; // block. We need to determine the reaching definition of CPSR so that; // it can be marked as non-pipelineable, allowing the pipeliner to force; // it into stage 0 or give up if it cannot or will not do so.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:203,Deployability,pipeline,pipeliner,203,"// If the branch is a Bcc, then the CPSR should be set somewhere within the; // block. We need to determine the reaching definition of CPSR so that; // it can be marked as non-pipelineable, allowing the pipeliner to force; // it into stage 0 or give up if it cannot or will not do so.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:65,Deployability,pipeline,pipeline,65,"// Unable to find the CC setter, so unable to guarantee; // that pipeline will work",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:78,Modifiability,enhance,enhance,78,"/// Specialization of \ref TargetInstrInfo::describeLoadedValue, used to; /// enhance debug entry value descriptions for ARM targets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:63,Energy Efficiency,schedul,scheduler,63,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only; /// differences between the two addresses is the offset. It also returns the; /// offsets by reference.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:98,Performance,load,loads,98,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only; /// differences between the two addresses is the offset. It also returns the; /// offsets by reference.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:108,Performance,load,loading,108,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only; /// differences between the two addresses is the offset. It also returns the; /// offsets by reference.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:65,Energy Efficiency,schedul,scheduler,65,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:167,Energy Efficiency,schedul,scheduled,167,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:295,Energy Efficiency,schedul,scheduled,295,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:473,Energy Efficiency,schedul,schedule,473,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:568,Energy Efficiency,schedul,scheduled,568,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:146,Performance,load,loads,146,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:209,Performance,load,loads,209,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:219,Performance,load,loading,219,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:259,Performance,cache,cache,259,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:372,Performance,load,load,372,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:490,Performance,load,loads,490,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:534,Performance,load,loads,534,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:134,Availability,redundant,redundant,134,"/// optimizeCompareInstr - Convert the instruction to set the zero flag so; /// that we can remove a ""comparison with zero""; Remove a redundant CMP; /// instruction if the flags can be updated in the same way by an earlier; /// instruction such as SUB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:185,Deployability,update,updated,185,"/// optimizeCompareInstr - Convert the instruction to set the zero flag so; /// that we can remove a ""comparison with zero""; Remove a redundant CMP; /// instruction if the flags can be updated in the same way by an earlier; /// instruction such as SUB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:4,Performance,optimiz,optimizeCompareInstr,4,"/// optimizeCompareInstr - Convert the instruction to set the zero flag so; /// that we can remove a ""comparison with zero""; Remove a redundant CMP; /// instruction if the flags can be updated in the same way by an earlier; /// instruction such as SUB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:134,Safety,redund,redundant,134,"/// optimizeCompareInstr - Convert the instruction to set the zero flag so; /// that we can remove a ""comparison with zero""; Remove a redundant CMP; /// instruction if the flags can be updated in the same way by an earlier; /// instruction such as SUB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:123,Deployability,Pipeline,PipelinerLoopInfo,123,"/// Analyze loop L, which must be a single-basic-block loop, and if the; /// conditions can be understood enough produce a PipelinerLoopInfo object.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:178,Security,authenticat,authentication,178,"/// Adds an instruction which saves the link register on top of the stack into; /// the MachineBasicBlock \p MBB at position \p It. If \p Auth is true,; /// compute and store an authentication code alongiside the link register.; /// If \p CFI is true, emit CFI instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:174,Security,authenticat,authentication,174,"/// Adds an instruction which restores the link register from the top the; /// stack into the MachineBasicBlock \p MBB at position \p It. If \p Auth is; /// true, restore an authentication code and authenticate LR.; /// If \p CFI is true, emit CFI instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:198,Security,authenticat,authenticate,198,"/// Adds an instruction which restores the link register from the top the; /// stack into the MachineBasicBlock \p MBB at position \p It. If \p Auth is; /// true, restore an authentication code and authenticate LR.; /// If \p CFI is true, emit CFI instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:87,Deployability,update,updates,87,/// Returns true if the machine instruction offset can handle the stack fixup; /// and updates it if requested.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:24,Performance,Perform,Perform,24,/// verifyInstruction - Perform target specific instruction verification.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:45,Safety,hazard,hazards,45,/// Modeling special VFP / NEON fp MLA / MLS hazards.; /// MLxEntryMap - Map fp MLA / MLS to the corresponding entry in the internal; /// MLx table.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:95,Energy Efficiency,schedul,scheduled,95,/// MLxHazardOpcodes - Set of add / sub and multiply opcodes that would cause; /// stalls when scheduled together with fp MLA / MLS opcodes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:107,Energy Efficiency,schedul,scheduled,107,/// canCauseFpMLxStall - Return true if an instruction of the specified opcode; /// will cause stalls when scheduled after (within 4-cycle window) a fp; /// MLA / MLS instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:73,Availability,mask,mask,73,"// This table shows the VPT instruction variants, i.e. the different; // mask field encodings, see also B5.6. Predication/conditional execution in; // the ArmARM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:4,Modifiability,rewrite,rewriteARMFrameIndex,4,"/// rewriteARMFrameIndex / rewriteT2FrameIndex -; /// Rewrite MI to access 'Offset' bytes from the FP. Return false if the; /// offset could not be handled directly in MI, and return the left-over; /// portion by reference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:54,Modifiability,Rewrite,Rewrite,54,"/// rewriteARMFrameIndex / rewriteT2FrameIndex -; /// Rewrite MI to access 'Offset' bytes from the FP. Return false if the; /// offset could not be handled directly in MI, and return the left-over; /// portion by reference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:68,Security,access,access,68,"/// rewriteARMFrameIndex / rewriteT2FrameIndex -; /// Rewrite MI to access 'Offset' bytes from the FP. Return false if the; /// offset could not be handled directly in MI, and return the left-over; /// portion by reference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:125,Performance,load,load,125,"/// Returns the number of instructions required to materialize the given; /// constant in a register, or 3 if a literal pool load is needed.; /// If ForCodesize is specified, an approximate cost in bytes is returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:18,Security,access,access,18,"// Given a memory access Opcode, check that the give Imm would be a valid Offset; // for this instruction using its addressing mode.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:33,Availability,mask,mask,33,"// This should return a register mask that is the same as that returned by; // getCallPreservedMask but that additionally preserves the register used for; // the first i32 argument (which must also be the register used to return a; // single i32 return value); //; // In case that the calling convention does not use the same register for; // both or otherwise does not want to enable this optimization, the function; // should return NULL",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:390,Performance,optimiz,optimization,390,"// This should return a register mask that is the same as that returned by; // getCallPreservedMask but that additionally preserves the register used for; // the first i32 argument (which must also be the register used to return a; // single i32 return value); //; // In case that the calling convention does not use the same register for; // both or otherwise does not want to enable this optimization, the function; // should return NULL",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:10,Safety,avoid,avoid,10,// FIXME: avoid re-calculating this every time.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:72,Availability,avail,available,72,// hasFP ends up calling getMaxCallFrameComputed() which may not be; // available when getPressureLimit() is called as part of; // ScheduleDAGRRList.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:131,Energy Efficiency,Schedul,ScheduleDAGRRList,131,// hasFP ends up calling getMaxCallFrameComputed() which may not be; // available when getPressureLimit() is called as part of; // ScheduleDAGRRList.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:179,Deployability,update,updated,179,// If 'Reg' is one of the even / odd register pair and it's now changed; // (e.g. coalesced) into a different register. The other register of the; // pair allocation hint must be updated to reflect the relationship; // change.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:171,Energy Efficiency,allocate,allocate,171,"// If we have stack realignment and VLAs, we have no pointer to use to; // access the stack. If we have stack realignment, and a large call frame,; // we have no place to allocate the emergency spill slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:75,Security,access,access,75,"// If we have stack realignment and VLAs, we have no pointer to use to; // access the stack. If we have stack realignment, and a large call frame,; // we have no place to allocate the emergency spill slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:242,Modifiability,variab,variable,242,"// Thumb has trouble with negative offsets from the FP. Thumb2 has a limited; // negative range for ldr/str (255), and Thumb1 is positive offsets only.; //; // It's going to be better to use the SP or Base Pointer instead. When there; // are variable sized objects, we can't reference off of the SP, so we; // reserve a Base Pointer.; //; // For Thumb2, estimate whether a negative offset from the frame pointer; // will be sufficient to reach the whole stack frame. If a function has a; // smallish frame, it's less likely to have lots of spills and callee saved; // space, so it's all more likely to be within range of the frame pointer.; // If it's wrong, the scavenger will still enable access to work, it just; // won't be optimal. (We should always be able to reach the emergency; // spill slot from the frame pointer.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:691,Security,access,access,691,"// Thumb has trouble with negative offsets from the FP. Thumb2 has a limited; // negative range for ldr/str (255), and Thumb1 is positive offsets only.; //; // It's going to be better to use the SP or Base Pointer instead. When there; // are variable sized objects, we can't reference off of the SP, so we; // reserve a Base Pointer.; //; // For Thumb2, estimate whether a negative offset from the frame pointer; // will be sufficient to reach the whole stack frame. If a function has a; // smallish frame, it's less likely to have lots of spills and callee saved; // space, so it's all more likely to be within range of the frame pointer.; // If it's wrong, the scavenger will still enable access to work, it just; // won't be optimal. (We should always be able to reach the emergency; // spill slot from the frame pointer.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:32,Performance,load,load,32,/// emitLoadConstPool - Emits a load from constpool to materialize the; /// specified immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:12,Performance,load,load,12,"// It's the load/store FI references that cause issues, as it can be difficult; // to materialize the offset if it won't fit in the literal field. Estimate; // based on the size of the local frame and some conservative assumptions; // about the rest of the stack frame (note, this is pre-regalloc, so; // we don't know everything for certain yet) whether this offset is likely; // to be out of range of the immediate. Return true if so.; // We only generate virtual base registers for loads and stores, so; // return false for everything else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:485,Performance,load,loads,485,"// It's the load/store FI references that cause issues, as it can be difficult; // to materialize the offset if it won't fit in the literal field. Estimate; // based on the size of the local frame and some conservative assumptions; // about the rest of the stack frame (note, this is pre-regalloc, so; // we don't know everything for certain yet) whether this offset is likely; // to be out of range of the immediate. Return true if so.; // We only generate virtual base registers for loads and stores, so; // return false for everything else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:56,Modifiability,variab,variable,56,"// Without a virtual base register, if the function has variable sized; // objects, all fixed-size local references will be via the frame pointer,; // Approximate the offset and see if it's legal for the instruction.; // Note that the incoming offset is based on the SP value at function entry,; // so it'll be negative.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:138,Security,access,access,138,"// Estimate an offset from the stack pointer.; // The incoming offset is relating to the SP at the start of the function,; // but when we access the local it'll be relative to the SP after local; // allocation, so adjust our SP-relative offset by that allocation size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:52,Energy Efficiency,allocate,allocated,52,// Assume that we'll have at least some spill slots allocated.; // FIXME: This is a total SWAG number. We should run some statistics; // and pick a real one.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:127,Availability,avail,available,127,"// 128 bytes of spill slots; // If there's a frame pointer and the addressing mode allows it, try using it.; // The FP is only available if there is no dynamic realignment. We; // don't know for sure yet whether we'll need that, so we guess based; // on whether there are any local variables that would trigger it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:282,Modifiability,variab,variables,282,"// 128 bytes of spill slots; // If there's a frame pointer and the addressing mode allows it, try using it.; // The FP is only available if there is no dynamic realignment. We; // don't know for sure yet whether we'll need that, so we guess based; // on whether there are any local variables that would trigger it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:45,Energy Efficiency,allocate,allocate,45,"// The offset likely isn't legal, we want to allocate a virtual base register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:200,Security,access,access,200,// PEI::scavengeFrameVirtualRegs() cannot accurately track SPAdj because the; // call frame setup/destroy instructions have already been eliminated. That; // means the stack pointer cannot be used to access the emergency spill slot; // when !hasReservedCallFrame().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:3,Deployability,Update,Update,3,// Update the original instruction to use the scratch register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:138,Testability,test,test,138,"// This number is the largest round number that which meets the criteria:; // (1) addresses PR18825; // (2) generates better code in some test cases (like vldm-shed-a9.ll); // (3) Doesn't regress any test cases (in-tree, test-suite, and SPEC); // In practice the SizeMultiplier will only factor in for straight line code; // that uses a lot of NEON vectors, which isn't terribly common.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:200,Testability,test,test,200,"// This number is the largest round number that which meets the criteria:; // (1) addresses PR18825; // (2) generates better code in some test cases (like vldm-shed-a9.ll); // (3) Doesn't regress any test cases (in-tree, test-suite, and SPEC); // In practice the SizeMultiplier will only factor in for straight line code; // that uses a lot of NEON vectors, which isn't terribly common.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:221,Testability,test,test-suite,221,"// This number is the largest round number that which meets the criteria:; // (1) addresses PR18825; // (2) generates better code in some test cases (like vldm-shed-a9.ll); // (3) Doesn't regress any test cases (in-tree, test-suite, and SPEC); // In practice the SizeMultiplier will only factor in for straight line code; // that uses a lot of NEON vectors, which isn't terribly common.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h:148,Modifiability,variab,variable,148,"/// BasePtr - ARM physical register used as a base ptr in complex stack; /// frames. I.e., when we need a 3rd base, not just SP and FP, due to; /// variable size stack objects.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h:58,Availability,mask,mask,58,/// getThisReturnPreservedMask - Returns a call preserved mask specific to the; /// case that 'returned' is on an i32 first argument if the calling convention; /// is one that can (partially) model this attribute with a preserved mask; /// (i.e. it is a calling convention that uses the same register for the first; /// i32 argument and an i32 return value); ///; /// Should return NULL in the case that the calling convention does not have; /// this property,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h:230,Availability,mask,mask,230,/// getThisReturnPreservedMask - Returns a call preserved mask specific to the; /// case that 'returned' is on an i32 first argument if the calling convention; /// is one that can (partially) model this attribute with a preserved mask; /// (i.e. it is a calling convention that uses the same register for the first; /// i32 argument and an i32 return value); ///; /// Should return NULL in the case that the calling convention does not have; /// this property,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h:32,Performance,load,load,32,/// emitLoadConstPool - Emits a load from constpool to materialize the; /// specified immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.cpp:88,Deployability,update,updated,88,"// This is where block i begins. Stop if the offset is already correct,; // and we have updated 2 blocks. This is the maximum number of blocks; // changed before calling this function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h:105,Safety,predict,predict,105,/// Compute the number of known offset bits internally to this block.; /// This number should be used to predict worst case padding when; /// splitting the block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h:235,Testability,Log,LogAlign,235,"/// Compute the number of known low bits of postOffset. If this block; /// contains inline asm, the number of known bits drops to the; /// instruction alignment. An aligned terminator may increase the number; /// of know bits.; /// If LogAlign is given, also consider the alignment of the next block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp:3,Usability,Clear,Clear,3,"// Clear the kill flags, as the cmp/bcc will no longer kill any operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp:4,Deployability,Update,Updates,4,/// Updates ordering (of WLS BB and their loopExits) in inner loops first; /// Returns true if any change was made in any of the loops,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp
