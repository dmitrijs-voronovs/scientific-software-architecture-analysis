id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/issues/2092:1256,Performance,perform,performance-sensitive,1256,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092
https://github.com/broadinstitute/gatk/issues/2092:1306,Performance,perform,performance,1306,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092
https://github.com/broadinstitute/gatk/issues/2092:1448,Testability,test,test,1448,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092
https://github.com/broadinstitute/gatk/issues/2092:1473,Testability,test,tests,1473,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092
https://github.com/broadinstitute/gatk/pull/2094:182,Deployability,Integrat,Integration,182,"The first commit has the raw GATK3 files, the second has the ported files. In order to minimize the diffs from GATK3 for the initial port, there are only very minimal style changes. Integration tests will follow in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2094
https://github.com/broadinstitute/gatk/pull/2094:182,Integrability,Integrat,Integration,182,"The first commit has the raw GATK3 files, the second has the ported files. In order to minimize the diffs from GATK3 for the initial port, there are only very minimal style changes. Integration tests will follow in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2094
https://github.com/broadinstitute/gatk/pull/2094:194,Testability,test,tests,194,"The first commit has the raw GATK3 files, the second has the ported files. In order to minimize the diffs from GATK3 for the initial port, there are only very minimal style changes. Integration tests will follow in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2094
https://github.com/broadinstitute/gatk/pull/2097:233,Deployability,update,update,233,updating wrapper from 2.13 -> 3.0. disable daemon on travis since it's now enabled by default and gradle recommends disabling it on CI servers; remove jacoco version specification since 3.0 specifies a reasonable version by default; update the test result html path on travis since it changed in 3.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2097
https://github.com/broadinstitute/gatk/pull/2097:9,Integrability,wrap,wrapper,9,updating wrapper from 2.13 -> 3.0. disable daemon on travis since it's now enabled by default and gradle recommends disabling it on CI servers; remove jacoco version specification since 3.0 specifies a reasonable version by default; update the test result html path on travis since it changed in 3.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2097
https://github.com/broadinstitute/gatk/pull/2097:244,Testability,test,test,244,updating wrapper from 2.13 -> 3.0. disable daemon on travis since it's now enabled by default and gradle recommends disabling it on CI servers; remove jacoco version specification since 3.0 specifies a reasonable version by default; update the test result html path on travis since it changed in 3.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2097
https://github.com/broadinstitute/gatk/pull/2098:51,Deployability,integrat,integration,51,@droazen I plugged this into the `HaplotypeCaller` integration tests and it looks really good. The concordance tests fail due to a handful of false positives but these are _all_ obvious true positive multiallelics that the old qual model behaves stupidly on. I'm not sure who the right reviewer for this is but I'm guessing it's @ldgauthier and/or @vruano.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098
https://github.com/broadinstitute/gatk/pull/2098:51,Integrability,integrat,integration,51,@droazen I plugged this into the `HaplotypeCaller` integration tests and it looks really good. The concordance tests fail due to a handful of false positives but these are _all_ obvious true positive multiallelics that the old qual model behaves stupidly on. I'm not sure who the right reviewer for this is but I'm guessing it's @ldgauthier and/or @vruano.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098
https://github.com/broadinstitute/gatk/pull/2098:63,Testability,test,tests,63,@droazen I plugged this into the `HaplotypeCaller` integration tests and it looks really good. The concordance tests fail due to a handful of false positives but these are _all_ obvious true positive multiallelics that the old qual model behaves stupidly on. I'm not sure who the right reviewer for this is but I'm guessing it's @ldgauthier and/or @vruano.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098
https://github.com/broadinstitute/gatk/pull/2098:111,Testability,test,tests,111,@droazen I plugged this into the `HaplotypeCaller` integration tests and it looks really good. The concordance tests fail due to a handful of false positives but these are _all_ obvious true positive multiallelics that the old qual model behaves stupidly on. I'm not sure who the right reviewer for this is but I'm guessing it's @ldgauthier and/or @vruano.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098
https://github.com/broadinstitute/gatk/pull/2100:9,Testability,test,test,9,updating test files due to change in VCF FT field from htsjdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2100
https://github.com/broadinstitute/gatk/pull/2104:69,Testability,test,tests,69,Implements #2059.; Ignore loci inside spanning deletions in Rank Sum tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2104
https://github.com/broadinstitute/gatk/pull/2106:187,Energy Efficiency,power,power,187,"this includes the new VCF2TileDB java API; replacing process builder calls to GenomicsDB binaries with usage of VCF2TileDB api in GenomicsDBTestUtils. these tests will likely now fail on power systems since GenomicsDB doesn't bundle a power library, power users should build genomics db and specify it's path on the LD_PATH",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2106
https://github.com/broadinstitute/gatk/pull/2106:235,Energy Efficiency,power,power,235,"this includes the new VCF2TileDB java API; replacing process builder calls to GenomicsDB binaries with usage of VCF2TileDB api in GenomicsDBTestUtils. these tests will likely now fail on power systems since GenomicsDB doesn't bundle a power library, power users should build genomics db and specify it's path on the LD_PATH",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2106
https://github.com/broadinstitute/gatk/pull/2106:250,Energy Efficiency,power,power,250,"this includes the new VCF2TileDB java API; replacing process builder calls to GenomicsDB binaries with usage of VCF2TileDB api in GenomicsDBTestUtils. these tests will likely now fail on power systems since GenomicsDB doesn't bundle a power library, power users should build genomics db and specify it's path on the LD_PATH",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2106
https://github.com/broadinstitute/gatk/pull/2106:157,Testability,test,tests,157,"this includes the new VCF2TileDB java API; replacing process builder calls to GenomicsDB binaries with usage of VCF2TileDB api in GenomicsDBTestUtils. these tests will likely now fail on power systems since GenomicsDB doesn't bundle a power library, power users should build genomics db and specify it's path on the LD_PATH",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2106
https://github.com/broadinstitute/gatk/issues/2110:1259,Deployability,install,installedProviders,1259,7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:27); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:64); at com.google.cloud.storage.StorageOptions.defaultInstance(StorageOptions.java:91); at com.google.cloud.storage.contrib.nio.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110
https://github.com/broadinstitute/gatk/issues/2110:871,Performance,load,loadInstalledProviders,871,The presence of gcloud-java-nio on the classpath prevents other providers being used if the GCS provider ID has not been specified (e.g. by setting `GOOGLE_APPLICATION_CREDENTIALS`). Here's an example test case: https://github.com/broadinstitute/gatk/commit/8b217f82352ceb55d21d7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110
https://github.com/broadinstitute/gatk/issues/2110:589,Security,access,access,589,The presence of gcloud-java-nio on the classpath prevents other providers being used if the GCS provider ID has not been specified (e.g. by setting `GOOGLE_APPLICATION_CREDENTIALS`). Here's an example test case: https://github.com/broadinstitute/gatk/commit/8b217f82352ceb55d21d7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110
https://github.com/broadinstitute/gatk/issues/2110:964,Security,access,access,964,The presence of gcloud-java-nio on the classpath prevents other providers being used if the GCS provider ID has not been specified (e.g. by setting `GOOGLE_APPLICATION_CREDENTIALS`). Here's an example test case: https://github.com/broadinstitute/gatk/commit/8b217f82352ceb55d21d7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110
https://github.com/broadinstitute/gatk/issues/2110:1164,Security,secur,security,1164,CREDENTIALS`). Here's an example test case: https://github.com/broadinstitute/gatk/commit/8b217f82352ceb55d21d7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:27); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:64); at com.google.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110
https://github.com/broadinstitute/gatk/issues/2110:1173,Security,Access,AccessController,1173,. Here's an example test case: https://github.com/broadinstitute/gatk/commit/8b217f82352ceb55d21d7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:27); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:64); at com.google.cloud.storage.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110
https://github.com/broadinstitute/gatk/issues/2110:201,Testability,test,test,201,The presence of gcloud-java-nio on the classpath prevents other providers being used if the GCS provider ID has not been specified (e.g. by setting `GOOGLE_APPLICATION_CREDENTIALS`). Here's an example test case: https://github.com/broadinstitute/gatk/commit/8b217f82352ceb55d21d7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110
https://github.com/broadinstitute/gatk/issues/2110:1439,Testability,test,test,1439,ageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:27); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:64); at com.google.cloud.storage.StorageOptions.defaultInstance(StorageOptions.java:91); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.<init>(CloudStorageFileSystemProvider.java:141); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.<init>(CloudStorageFileSyst,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110
https://github.com/broadinstitute/gatk/issues/2112:254,Deployability,update,updated,254,"We should provide a way for tools to generate standardized VCF header metadata lines. Most tools start with the lines from the input header, but after that there is a variation since each tool has custom code. We should rationalize how these are created/updated, for at least:; - ## contig: Some tools use the ones from the input directly, even if a specific reference is provided on the command line, but some update them based on the reference. Since we don't require a reference, but will accept one, we should be consistent about whether we keep or destroy the old attributes in each case, and whether we use an assembly attribute, URL attribute, etc.; - ## reference: same as above; - ## source: it seems like we should always add/update this. This is all somewhat related to https://github.com/broadinstitute/gatk/issues/1116.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2112
https://github.com/broadinstitute/gatk/issues/2112:411,Deployability,update,update,411,"We should provide a way for tools to generate standardized VCF header metadata lines. Most tools start with the lines from the input header, but after that there is a variation since each tool has custom code. We should rationalize how these are created/updated, for at least:; - ## contig: Some tools use the ones from the input directly, even if a specific reference is provided on the command line, but some update them based on the reference. Since we don't require a reference, but will accept one, we should be consistent about whether we keep or destroy the old attributes in each case, and whether we use an assembly attribute, URL attribute, etc.; - ## reference: same as above; - ## source: it seems like we should always add/update this. This is all somewhat related to https://github.com/broadinstitute/gatk/issues/1116.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2112
https://github.com/broadinstitute/gatk/issues/2112:736,Deployability,update,update,736,"We should provide a way for tools to generate standardized VCF header metadata lines. Most tools start with the lines from the input header, but after that there is a variation since each tool has custom code. We should rationalize how these are created/updated, for at least:; - ## contig: Some tools use the ones from the input directly, even if a specific reference is provided on the command line, but some update them based on the reference. Since we don't require a reference, but will accept one, we should be consistent about whether we keep or destroy the old attributes in each case, and whether we use an assembly attribute, URL attribute, etc.; - ## reference: same as above; - ## source: it seems like we should always add/update this. This is all somewhat related to https://github.com/broadinstitute/gatk/issues/1116.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2112
https://github.com/broadinstitute/gatk/issues/2113:186,Availability,ERROR,ERROR,186,"Attempting to get wgs coverage with 10000 bp bin size on; /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam; produces the following stacktrace (see first line for command line). ```; ERROR 14:06:46,283 FunctionEdge - Error: java -Xmx16g -jar /dsde/working/slee/acnv-eval/resources/gatk-protected-536b94e-create-apon.jar SparkGenomeReadCounts -o /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv -I /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam -bins 10000 -R /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --sparkMaster local[1] ; ERROR 14:06:46,288 FunctionEdge - Contents of /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv.out:; 16/08/24 14:06:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 284.6 KB); 16/08/24 14:06:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56998 (size: 23.2 KB, free: 10.4 GB); 16/08/24 14:06:09 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:105; 16/08/24 14:06:10 INFO FileInputFormat: Total input paths to process : 1; 16/08/24 14:06:21 INFO SparkUI: Stopped Spark web UI at http://10.200.98.30:4040; 16/08/24 14:06:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/08/24 14:06:21 INFO MemoryStore: MemoryStore cleared; 16/08/24 14:06:21 INFO BlockManager: BlockManager stopped; 16/08/24 14:06:21 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/08/24 14:06:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/08/24 14:06:21 INFO SparkContext: Successfully stopped SparkContext; 14:06:21.109 INFO SparkGenomeReadCounts - Shutting down engine; [August 24, 2016 2:06:21 PM EDT] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=3192389632; java.lang.IndexOutOfBoundsException; at java.nio.ByteBuffer.wrap(ByteBuffer.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:220,Availability,Error,Error,220,"Attempting to get wgs coverage with 10000 bp bin size on; /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam; produces the following stacktrace (see first line for command line). ```; ERROR 14:06:46,283 FunctionEdge - Error: java -Xmx16g -jar /dsde/working/slee/acnv-eval/resources/gatk-protected-536b94e-create-apon.jar SparkGenomeReadCounts -o /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv -I /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam -bins 10000 -R /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --sparkMaster local[1] ; ERROR 14:06:46,288 FunctionEdge - Contents of /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv.out:; 16/08/24 14:06:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 284.6 KB); 16/08/24 14:06:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56998 (size: 23.2 KB, free: 10.4 GB); 16/08/24 14:06:09 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:105; 16/08/24 14:06:10 INFO FileInputFormat: Total input paths to process : 1; 16/08/24 14:06:21 INFO SparkUI: Stopped Spark web UI at http://10.200.98.30:4040; 16/08/24 14:06:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/08/24 14:06:21 INFO MemoryStore: MemoryStore cleared; 16/08/24 14:06:21 INFO BlockManager: BlockManager stopped; 16/08/24 14:06:21 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/08/24 14:06:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/08/24 14:06:21 INFO SparkContext: Successfully stopped SparkContext; 14:06:21.109 INFO SparkGenomeReadCounts - Shutting down engine; [August 24, 2016 2:06:21 PM EDT] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=3192389632; java.lang.IndexOutOfBoundsException; at java.nio.ByteBuffer.wrap(ByteBuffer.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:584,Availability,ERROR,ERROR,584,"Attempting to get wgs coverage with 10000 bp bin size on; /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam; produces the following stacktrace (see first line for command line). ```; ERROR 14:06:46,283 FunctionEdge - Error: java -Xmx16g -jar /dsde/working/slee/acnv-eval/resources/gatk-protected-536b94e-create-apon.jar SparkGenomeReadCounts -o /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv -I /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam -bins 10000 -R /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --sparkMaster local[1] ; ERROR 14:06:46,288 FunctionEdge - Contents of /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv.out:; 16/08/24 14:06:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 284.6 KB); 16/08/24 14:06:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56998 (size: 23.2 KB, free: 10.4 GB); 16/08/24 14:06:09 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:105; 16/08/24 14:06:10 INFO FileInputFormat: Total input paths to process : 1; 16/08/24 14:06:21 INFO SparkUI: Stopped Spark web UI at http://10.200.98.30:4040; 16/08/24 14:06:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/08/24 14:06:21 INFO MemoryStore: MemoryStore cleared; 16/08/24 14:06:21 INFO BlockManager: BlockManager stopped; 16/08/24 14:06:21 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/08/24 14:06:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/08/24 14:06:21 INFO SparkContext: Successfully stopped SparkContext; 14:06:21.109 INFO SparkGenomeReadCounts - Shutting down engine; [August 24, 2016 2:06:21 PM EDT] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=3192389632; java.lang.IndexOutOfBoundsException; at java.nio.ByteBuffer.wrap(ByteBuffer.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:1743,Availability,down,down,1743,"oadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 284.6 KB); 16/08/24 14:06:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56998 (size: 23.2 KB, free: 10.4 GB); 16/08/24 14:06:09 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:105; 16/08/24 14:06:10 INFO FileInputFormat: Total input paths to process : 1; 16/08/24 14:06:21 INFO SparkUI: Stopped Spark web UI at http://10.200.98.30:4040; 16/08/24 14:06:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/08/24 14:06:21 INFO MemoryStore: MemoryStore cleared; 16/08/24 14:06:21 INFO BlockManager: BlockManager stopped; 16/08/24 14:06:21 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/08/24 14:06:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/08/24 14:06:21 INFO SparkContext: Successfully stopped SparkContext; 14:06:21.109 INFO SparkGenomeReadCounts - Shutting down engine; [August 24, 2016 2:06:21 PM EDT] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=3192389632; java.lang.IndexOutOfBoundsException; at java.nio.ByteBuffer.wrap(ByteBuffer.java:375); at htsjdk.samtools.BAMRecord.getCigar(BAMRecord.java:246); at org.seqdoop.hadoop_bam.BAMSplitGuesser.guessNextBAMRecordStart(BAMSplitGuesser.java:189); at org.seqdoop.hadoop_bam.BAMInputFormat.addProbabilisticSplits(BAMInputFormat.java:244); at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:159); at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAMInputFormat.java:253); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:120); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:5473,Energy Efficiency,reduce,reduceByKey,5473,rk.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:65); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:331); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:331); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:330); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:381); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:381); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:380); at org.apache.spark.rdd.RDD$$anonfun$countByVal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:5572,Energy Efficiency,reduce,reduceByKey,5572,ons$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:65); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:331); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:331); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:330); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:381); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:381); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:380); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1187); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:118,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:5882,Energy Efficiency,reduce,reduceByKey,5882,$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:65); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:331); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:331); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:330); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:381); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:381); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:380); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1187); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1187); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.RDD.countByValue(RDD.scala:1186); at org.apache.spark.api.java.JavaRDDLike$class.co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:1983,Integrability,wrap,wrap,1983,"xt: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:105; 16/08/24 14:06:10 INFO FileInputFormat: Total input paths to process : 1; 16/08/24 14:06:21 INFO SparkUI: Stopped Spark web UI at http://10.200.98.30:4040; 16/08/24 14:06:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/08/24 14:06:21 INFO MemoryStore: MemoryStore cleared; 16/08/24 14:06:21 INFO BlockManager: BlockManager stopped; 16/08/24 14:06:21 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/08/24 14:06:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/08/24 14:06:21 INFO SparkContext: Successfully stopped SparkContext; 14:06:21.109 INFO SparkGenomeReadCounts - Shutting down engine; [August 24, 2016 2:06:21 PM EDT] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=3192389632; java.lang.IndexOutOfBoundsException; at java.nio.ByteBuffer.wrap(ByteBuffer.java:375); at htsjdk.samtools.BAMRecord.getCigar(BAMRecord.java:246); at org.seqdoop.hadoop_bam.BAMSplitGuesser.guessNextBAMRecordStart(BAMSplitGuesser.java:189); at org.seqdoop.hadoop_bam.BAMInputFormat.addProbabilisticSplits(BAMInputFormat.java:244); at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:159); at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAMInputFormat.java:253); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:120); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:369,Testability,test,testing,369,"Attempting to get wgs coverage with 10000 bp bin size on; /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam; produces the following stacktrace (see first line for command line). ```; ERROR 14:06:46,283 FunctionEdge - Error: java -Xmx16g -jar /dsde/working/slee/acnv-eval/resources/gatk-protected-536b94e-create-apon.jar SparkGenomeReadCounts -o /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv -I /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam -bins 10000 -R /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --sparkMaster local[1] ; ERROR 14:06:46,288 FunctionEdge - Contents of /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv.out:; 16/08/24 14:06:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 284.6 KB); 16/08/24 14:06:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56998 (size: 23.2 KB, free: 10.4 GB); 16/08/24 14:06:09 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:105; 16/08/24 14:06:10 INFO FileInputFormat: Total input paths to process : 1; 16/08/24 14:06:21 INFO SparkUI: Stopped Spark web UI at http://10.200.98.30:4040; 16/08/24 14:06:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/08/24 14:06:21 INFO MemoryStore: MemoryStore cleared; 16/08/24 14:06:21 INFO BlockManager: BlockManager stopped; 16/08/24 14:06:21 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/08/24 14:06:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/08/24 14:06:21 INFO SparkContext: Successfully stopped SparkContext; 14:06:21.109 INFO SparkGenomeReadCounts - Shutting down engine; [August 24, 2016 2:06:21 PM EDT] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=3192389632; java.lang.IndexOutOfBoundsException; at java.nio.ByteBuffer.wrap(ByteBuffer.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:651,Testability,test,testing,651,"Attempting to get wgs coverage with 10000 bp bin size on; /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam; produces the following stacktrace (see first line for command line). ```; ERROR 14:06:46,283 FunctionEdge - Error: java -Xmx16g -jar /dsde/working/slee/acnv-eval/resources/gatk-protected-536b94e-create-apon.jar SparkGenomeReadCounts -o /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv -I /seq/picard_aggregation/D4491/HCC1143/v1/HCC1143.bam -bins 10000 -R /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --sparkMaster local[1] ; ERROR 14:06:46,288 FunctionEdge - Contents of /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv.out:; 16/08/24 14:06:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 284.6 KB); 16/08/24 14:06:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56998 (size: 23.2 KB, free: 10.4 GB); 16/08/24 14:06:09 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:105; 16/08/24 14:06:10 INFO FileInputFormat: Total input paths to process : 1; 16/08/24 14:06:21 INFO SparkUI: Stopped Spark web UI at http://10.200.98.30:4040; 16/08/24 14:06:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/08/24 14:06:21 INFO MemoryStore: MemoryStore cleared; 16/08/24 14:06:21 INFO BlockManager: BlockManager stopped; 16/08/24 14:06:21 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/08/24 14:06:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/08/24 14:06:21 INFO SparkContext: Successfully stopped SparkContext; 14:06:21.109 INFO SparkGenomeReadCounts - Shutting down engine; [August 24, 2016 2:06:21 PM EDT] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=3192389632; java.lang.IndexOutOfBoundsException; at java.nio.ByteBuffer.wrap(ByteBuffer.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2113:1367,Usability,clear,cleared,1367,"1/HCC1143.bam -bins 10000 -R /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --sparkMaster local[1] ; ERROR 14:06:46,288 FunctionEdge - Contents of /dsde/working/aaronc/testing/case/HCC1143/cov/HCC1143-coverage.tsv.out:; 16/08/24 14:06:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 284.6 KB); 16/08/24 14:06:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56998 (size: 23.2 KB, free: 10.4 GB); 16/08/24 14:06:09 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:105; 16/08/24 14:06:10 INFO FileInputFormat: Total input paths to process : 1; 16/08/24 14:06:21 INFO SparkUI: Stopped Spark web UI at http://10.200.98.30:4040; 16/08/24 14:06:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/08/24 14:06:21 INFO MemoryStore: MemoryStore cleared; 16/08/24 14:06:21 INFO BlockManager: BlockManager stopped; 16/08/24 14:06:21 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/08/24 14:06:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/08/24 14:06:21 INFO SparkContext: Successfully stopped SparkContext; 14:06:21.109 INFO SparkGenomeReadCounts - Shutting down engine; [August 24, 2016 2:06:21 PM EDT] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=3192389632; java.lang.IndexOutOfBoundsException; at java.nio.ByteBuffer.wrap(ByteBuffer.java:375); at htsjdk.samtools.BAMRecord.getCigar(BAMRecord.java:246); at org.seqdoop.hadoop_bam.BAMSplitGuesser.guessNextBAMRecordStart(BAMSplitGuesser.java:189); at org.seqdoop.hadoop_bam.BAMInputFormat.addProbabilisticSplits(BAMInputFormat.java:244); at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:159); at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAMInputFormat.java:253); at org.apache.spark.rdd.NewHadoopRDD.getPartit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113
https://github.com/broadinstitute/gatk/issues/2116:510,Availability,repair,repair,510,"The new version of SplitNCigarReads now sets reads that are split from one original read to be supplementary to each other. Unfortunately, the tool does not respect any existing supplementary reads as being supplementary to each other. Currently the tool clobbers any existing SA tag and sets one read from each read group as primary which is undesirable as it corresponds to loosing information from the aligner. One solution is to use the same ""predicting"" approach for existing SA information to attempt to repair the corresponding SA tag for each read based on how it would be split given its cigar string. Perhaps there is another solution that is more in line with what the SA tag gets used for. . In order to make these tags 100% accurate with overhang fixing on however, the OverhangFixingManager will probably need to be changed to store the mate information for every read it changes between file iterations. Currently it only stores information on a single read from every read group as it would be the only one that affects mate information but this assumption is invalidated as the SA tag needs to keep information on EVERY read in a canonical alignment to be correct, not just the first one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2116
https://github.com/broadinstitute/gatk/issues/2116:447,Safety,predict,predicting,447,"The new version of SplitNCigarReads now sets reads that are split from one original read to be supplementary to each other. Unfortunately, the tool does not respect any existing supplementary reads as being supplementary to each other. Currently the tool clobbers any existing SA tag and sets one read from each read group as primary which is undesirable as it corresponds to loosing information from the aligner. One solution is to use the same ""predicting"" approach for existing SA information to attempt to repair the corresponding SA tag for each read based on how it would be split given its cigar string. Perhaps there is another solution that is more in line with what the SA tag gets used for. . In order to make these tags 100% accurate with overhang fixing on however, the OverhangFixingManager will probably need to be changed to store the mate information for every read it changes between file iterations. Currently it only stores information on a single read from every read group as it would be the only one that affects mate information but this assumption is invalidated as the SA tag needs to keep information on EVERY read in a canonical alignment to be correct, not just the first one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2116
https://github.com/broadinstitute/gatk/issues/2117:481,Testability,test,test,481,"The twoPassReadsWalker currently only iterates twice through the reads as intended for BAM files. If given a text SAM file as input it will fail to iterate through the file a second time. This has to do with the SAMTextReader not using a seekable stream for the SAM file in htsjdk, which proves to be complicated to fix given the current api implementation there. A workaround for this is to reinitialize ReadsDataSource between passes through the input for twoPassReadsWalker and test that it works for all types of read files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2117
https://github.com/broadinstitute/gatk/pull/2118:256,Energy Efficiency,power,power,256,"previously jbwaIntegrationTest was being skipped if running on ppc; this was a hold over from when we didn't yet have a ppc library for it; now it will always run and fail if it's on an unsupported system; it should be compatible with most linux, osx, and power systems now though",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2118
https://github.com/broadinstitute/gatk/issues/2119:173,Testability,test,test,173,"Trying to read a headerless sam file fails with:. ```; java.lang.IllegalArgumentException: unknown SAM format, cannot create RecordReader: file:/Users/louisb/Workspace/gatk/test.sam; at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:186); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:156); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:129); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:64); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270). ...; ```. It fails because of some problems in `AnySAMInputFormat`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2119
https://github.com/broadinstitute/gatk/issues/2120:98,Security,validat,validate,98,"BwaSpark is using WELLFORMED as it's read filter. This includes a number of checks that the reads validate against the header. However, most unaligned sam files are likely to have no or incomplete headers. This causes many reads to be unexpectedly filtered. We should define a better filter criteria BwaSpark.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2120
https://github.com/broadinstitute/gatk/issues/2121:44,Security,validat,validate,44,"Since it's realigning the bam, it shouldn't validate that the file to be aligned already matches the header. This makes it impossible to realign to a different reference or align a headerless file. . This can currently be worked around using `--disableSequenceDictionaryValidation`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2121
https://github.com/broadinstitute/gatk/pull/2122:95,Deployability,Integrat,IntegrationTestSpec,95,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:564,Deployability,Integrat,IntegrationTestSpec,564,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:95,Integrability,Integrat,IntegrationTestSpec,95,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:564,Integrability,Integrat,IntegrationTestSpec,564,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:364,Modifiability,extend,extending,364,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:295,Performance,load,loading,295,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:37,Testability,test,testing,37,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:211,Testability,test,test,211,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:349,Testability,test,test,349,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:547,Testability,test,test,547,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/pull/2122:357,Usability,simpl,simply,357,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122
https://github.com/broadinstitute/gatk/issues/2123:219,Availability,error,error,219,"If you run AlignAssembledContigsSpark with an incorrect BWA index version (ie. one that was generated with BWA index 0.7.12 or previous), you get executor logs that trace back to an ""IOException: File system is closed"" error, which is very misleading. I believe that this happens because Spark retries the tasks multiple times after they have failed, and in the subsequent tries the filesystem is in a bad state. It would be nice if we could either catch this error earlier, or check to make sure that the reference is compatible before trying to load it somehow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123
https://github.com/broadinstitute/gatk/issues/2123:460,Availability,error,error,460,"If you run AlignAssembledContigsSpark with an incorrect BWA index version (ie. one that was generated with BWA index 0.7.12 or previous), you get executor logs that trace back to an ""IOException: File system is closed"" error, which is very misleading. I believe that this happens because Spark retries the tasks multiple times after they have failed, and in the subsequent tries the filesystem is in a bad state. It would be nice if we could either catch this error earlier, or check to make sure that the reference is compatible before trying to load it somehow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123
https://github.com/broadinstitute/gatk/issues/2123:547,Performance,load,load,547,"If you run AlignAssembledContigsSpark with an incorrect BWA index version (ie. one that was generated with BWA index 0.7.12 or previous), you get executor logs that trace back to an ""IOException: File system is closed"" error, which is very misleading. I believe that this happens because Spark retries the tasks multiple times after they have failed, and in the subsequent tries the filesystem is in a bad state. It would be nice if we could either catch this error earlier, or check to make sure that the reference is compatible before trying to load it somehow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123
https://github.com/broadinstitute/gatk/issues/2123:155,Testability,log,logs,155,"If you run AlignAssembledContigsSpark with an incorrect BWA index version (ie. one that was generated with BWA index 0.7.12 or previous), you get executor logs that trace back to an ""IOException: File system is closed"" error, which is very misleading. I believe that this happens because Spark retries the tasks multiple times after they have failed, and in the subsequent tries the filesystem is in a bad state. It would be nice if we could either catch this error earlier, or check to make sure that the reference is compatible before trying to load it somehow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123
https://github.com/broadinstitute/gatk/issues/2125:61,Testability,test,test,61,BaseTest instantiates a GenomeLocParser that uses one of our test references. This causes BaseTest to be unusable by external projects that don't have the file included. We should either package the reference file in the jar or we should remove the GenomeLocParser from BaseTest and instantiate it in the classes that actually use it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2125
https://github.com/broadinstitute/gatk/issues/2127:323,Performance,load,loading,323,"It seems like it's taking perceptibly longer time to start the gatk. This was an annoyance with GATK3 and we should see if there's anything we can do for gatk4 to avoid it. . `time gatk-launch PrintReads --help` reports ~1.2 seconds to run. . There's no significant overhead from gatk-launch, so all the time is in our own loading process. It's probably time spent searching the class path like it was in gatk3 but I haven't done any profiling to be sure. . The `4.alpha.1` milestone launches in ~0.9 seconds, so we've definitely been adding some extra time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2127
https://github.com/broadinstitute/gatk/issues/2127:163,Safety,avoid,avoid,163,"It seems like it's taking perceptibly longer time to start the gatk. This was an annoyance with GATK3 and we should see if there's anything we can do for gatk4 to avoid it. . `time gatk-launch PrintReads --help` reports ~1.2 seconds to run. . There's no significant overhead from gatk-launch, so all the time is in our own loading process. It's probably time spent searching the class path like it was in gatk3 but I haven't done any profiling to be sure. . The `4.alpha.1` milestone launches in ~0.9 seconds, so we've definitely been adding some extra time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2127
https://github.com/broadinstitute/gatk/issues/2128:429,Availability,redundant,redundant,429,"GATKVariantContextUtils.createVCFWriter attempts to determine the output vcf type based on the file extension provided by the user, and defaults to vcf if the extension isn't a recognized type. There is code in VariantContextWriterBuilder (determineOutputTypeFromFile) in htsjdk that attempts to do the same mapping, but isn't public. We should expose that in htsjdk and use it in GATKVariantConextUtils so we can get rid of the redundant code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2128
https://github.com/broadinstitute/gatk/issues/2128:429,Safety,redund,redundant,429,"GATKVariantContextUtils.createVCFWriter attempts to determine the output vcf type based on the file extension provided by the user, and defaults to vcf if the extension isn't a recognized type. There is code in VariantContextWriterBuilder (determineOutputTypeFromFile) in htsjdk that attempts to do the same mapping, but isn't public. We should expose that in htsjdk and use it in GATKVariantConextUtils so we can get rid of the redundant code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2128
https://github.com/broadinstitute/gatk/issues/2128:345,Security,expose,expose,345,"GATKVariantContextUtils.createVCFWriter attempts to determine the output vcf type based on the file extension provided by the user, and defaults to vcf if the extension isn't a recognized type. There is code in VariantContextWriterBuilder (determineOutputTypeFromFile) in htsjdk that attempts to do the same mapping, but isn't public. We should expose that in htsjdk and use it in GATKVariantConextUtils so we can get rid of the redundant code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2128
https://github.com/broadinstitute/gatk/issues/2129:365,Deployability,pipeline,pipeline,365,"A user rightly [points out](http://gatkforums.broadinstitute.org/gatk/discussion/comment/32631#Comment_32631) that different versions of HaplotypeCaller may produce GVCFs that are not directly compatible, causing weirdness when you joint-genotype them with GenotypeGVCFs. . Obviously this is primarily a data management problem (user should control what's in their pipeline) -- but it would be good to provide an additional safety layer by having GenotypeGVCFs, CombineGVCFs or whatever demon is used to invoke TileDB at least emit a WARN message if they see GVCFs produced by different versions of HC within the same input cohort. . Note that the VCF version number is not directly useable for this purpose since changes in the contents of GVCFs can arise within the same version of VCF spec. Also, one could argue that the GVCFs really should all be produced using exactly the same command line arguments -- but validating the entire command line would probably be overkill...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2129
https://github.com/broadinstitute/gatk/issues/2129:539,Integrability,message,message,539,"A user rightly [points out](http://gatkforums.broadinstitute.org/gatk/discussion/comment/32631#Comment_32631) that different versions of HaplotypeCaller may produce GVCFs that are not directly compatible, causing weirdness when you joint-genotype them with GenotypeGVCFs. . Obviously this is primarily a data management problem (user should control what's in their pipeline) -- but it would be good to provide an additional safety layer by having GenotypeGVCFs, CombineGVCFs or whatever demon is used to invoke TileDB at least emit a WARN message if they see GVCFs produced by different versions of HC within the same input cohort. . Note that the VCF version number is not directly useable for this purpose since changes in the contents of GVCFs can arise within the same version of VCF spec. Also, one could argue that the GVCFs really should all be produced using exactly the same command line arguments -- but validating the entire command line would probably be overkill...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2129
https://github.com/broadinstitute/gatk/issues/2129:424,Safety,safe,safety,424,"A user rightly [points out](http://gatkforums.broadinstitute.org/gatk/discussion/comment/32631#Comment_32631) that different versions of HaplotypeCaller may produce GVCFs that are not directly compatible, causing weirdness when you joint-genotype them with GenotypeGVCFs. . Obviously this is primarily a data management problem (user should control what's in their pipeline) -- but it would be good to provide an additional safety layer by having GenotypeGVCFs, CombineGVCFs or whatever demon is used to invoke TileDB at least emit a WARN message if they see GVCFs produced by different versions of HC within the same input cohort. . Note that the VCF version number is not directly useable for this purpose since changes in the contents of GVCFs can arise within the same version of VCF spec. Also, one could argue that the GVCFs really should all be produced using exactly the same command line arguments -- but validating the entire command line would probably be overkill...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2129
https://github.com/broadinstitute/gatk/issues/2129:914,Security,validat,validating,914,"A user rightly [points out](http://gatkforums.broadinstitute.org/gatk/discussion/comment/32631#Comment_32631) that different versions of HaplotypeCaller may produce GVCFs that are not directly compatible, causing weirdness when you joint-genotype them with GenotypeGVCFs. . Obviously this is primarily a data management problem (user should control what's in their pipeline) -- but it would be good to provide an additional safety layer by having GenotypeGVCFs, CombineGVCFs or whatever demon is used to invoke TileDB at least emit a WARN message if they see GVCFs produced by different versions of HC within the same input cohort. . Note that the VCF version number is not directly useable for this purpose since changes in the contents of GVCFs can arise within the same version of VCF spec. Also, one could argue that the GVCFs really should all be produced using exactly the same command line arguments -- but validating the entire command line would probably be overkill...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2129
https://github.com/broadinstitute/gatk/issues/2130:246,Deployability,update,updated,246,"The package org.broadinstitute.hellbender.utils.commandline contains annotation classes called AdvancedOption and HiddenOption that are duplicates of Advanced and Hidden in org.broadinstitute.hellbender.commandline. All usages of these should be updated and this entire package should be removed. Also, it looks like Hidden is not integrated with the command line parser.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2130
https://github.com/broadinstitute/gatk/issues/2130:331,Deployability,integrat,integrated,331,"The package org.broadinstitute.hellbender.utils.commandline contains annotation classes called AdvancedOption and HiddenOption that are duplicates of Advanced and Hidden in org.broadinstitute.hellbender.commandline. All usages of these should be updated and this entire package should be removed. Also, it looks like Hidden is not integrated with the command line parser.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2130
https://github.com/broadinstitute/gatk/issues/2130:331,Integrability,integrat,integrated,331,"The package org.broadinstitute.hellbender.utils.commandline contains annotation classes called AdvancedOption and HiddenOption that are duplicates of Advanced and Hidden in org.broadinstitute.hellbender.commandline. All usages of these should be updated and this entire package should be removed. Also, it looks like Hidden is not integrated with the command line parser.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2130
https://github.com/broadinstitute/gatk/pull/2131:2,Deployability,Update,Update,2,- Update htsjdk to 2.6.1; - Fix #2080,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2131
https://github.com/broadinstitute/gatk/pull/2136:83,Deployability,Update,Updated,83,"1st step towards bringing in fermi-lite as alternative option for local assembly.; Updated other classes accordingly.; Some teeny clean ups elsewhere. @cwhelan mind reviewing? Mostly trivial changes, should be very quick.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2136
https://github.com/broadinstitute/gatk/issues/2150:13,Deployability,pipeline,pipelines,13,"The existing pipelines and multiple collectors are not consistent in how the user's filter requests are merged and propagated to different stages of the pipeline. The BQSR pipeline for instance first retrieves the initial RDD via getReads, which honor's the user's filter requests, but subsequently manually applies the BQSR-specific filter, which might re-enable a filter that was disabled by the user. We should have a more explicit (and user-transparent) set of rules for propagating/applying filters in the cases where we're composing multiple tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2150
https://github.com/broadinstitute/gatk/issues/2150:153,Deployability,pipeline,pipeline,153,"The existing pipelines and multiple collectors are not consistent in how the user's filter requests are merged and propagated to different stages of the pipeline. The BQSR pipeline for instance first retrieves the initial RDD via getReads, which honor's the user's filter requests, but subsequently manually applies the BQSR-specific filter, which might re-enable a filter that was disabled by the user. We should have a more explicit (and user-transparent) set of rules for propagating/applying filters in the cases where we're composing multiple tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2150
https://github.com/broadinstitute/gatk/issues/2150:172,Deployability,pipeline,pipeline,172,"The existing pipelines and multiple collectors are not consistent in how the user's filter requests are merged and propagated to different stages of the pipeline. The BQSR pipeline for instance first retrieves the initial RDD via getReads, which honor's the user's filter requests, but subsequently manually applies the BQSR-specific filter, which might re-enable a filter that was disabled by the user. We should have a more explicit (and user-transparent) set of rules for propagating/applying filters in the cases where we're composing multiple tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2150
https://github.com/broadinstitute/gatk/pull/2153:56,Availability,error,error,56,AFCalculator.getLog10PNonRef() was throwing a confusing error due to a typo.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2153
https://github.com/broadinstitute/gatk/pull/2154:42,Modifiability,Refactor,Refactoring,42,This PR tries to solve several issues:; - Refactoring constructors for LIBS (#1879); - Adding maximum depth per sample argument to `LocusWalker` to avoid memory overload; - Fixing `Pileup`/`CheckPileup` tools for command line read filters; - Method for fix overlaps in `ReadPileup` in the same way as samtools (#2034),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2154
https://github.com/broadinstitute/gatk/pull/2154:148,Safety,avoid,avoid,148,This PR tries to solve several issues:; - Refactoring constructors for LIBS (#1879); - Adding maximum depth per sample argument to `LocusWalker` to avoid memory overload; - Fixing `Pileup`/`CheckPileup` tools for command line read filters; - Method for fix overlaps in `ReadPileup` in the same way as samtools (#2034),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2154
https://github.com/broadinstitute/gatk/issues/2158:45,Testability,Test,Tests,45,There is no `AlleleSubsettingUtilsUnitTest`. Tests for `subsetAlleles` live `GATKVariantContextUtilsUnitTest` and there are no tests for `calculateMostLikelyAlleles`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2158
https://github.com/broadinstitute/gatk/issues/2158:127,Testability,test,tests,127,There is no `AlleleSubsettingUtilsUnitTest`. Tests for `subsetAlleles` live `GATKVariantContextUtilsUnitTest` and there are no tests for `calculateMostLikelyAlleles`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2158
https://github.com/broadinstitute/gatk/issues/2159:10,Testability,test,tests,10,There are tests for basic `ReadWalker` functionality that live in`PrintReadsIntegrationTest`. These should move into their own `ReadWalkerUnitTest`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2159
https://github.com/broadinstitute/gatk/issues/2160:23,Modifiability,plugin,pluging,23,"After merging #2085, a pluging for `ReadTransformer` should be added using the pluging framework. The arguments should include if it is going to be applied before or after filtering. This was decided after discussion in #2084.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2160
https://github.com/broadinstitute/gatk/issues/2160:79,Modifiability,plugin,pluging,79,"After merging #2085, a pluging for `ReadTransformer` should be added using the pluging framework. The arguments should include if it is going to be applied before or after filtering. This was decided after discussion in #2084.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2160
https://github.com/broadinstitute/gatk/issues/2161:22,Energy Efficiency,efficient,efficient,22,"Sharding permits more efficient reads and writes. This is similar to ReadsSparkSink, which can produce sharded BAMs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2161
https://github.com/broadinstitute/gatk/pull/2163:90,Testability,test,tests,90,GVCF extension was whimsically wrong in a few places. Will probably require updating some tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2163
https://github.com/broadinstitute/gatk/issues/2165:216,Availability,redundant,redundant,216,"PrimaryAlignmentReadFilter filters out secondary alignments, but not supplementary alignments (this is what GATK3 does as well). This doesn't match the SAM spec (though it does match the language used by htsjdk); is redundant given that in GATK4 we have a NotSecondaryAlignmentReadFilter for use with InsertSizeMetricsCollector; and is likely confusing. Proposed change is to visit the usages of the PrimaryAlignmentReadFilter (BaseRecalibrator, Pileup, CalculateTargetBaseCallCoverage and HaplotypCaller) and for any that don't want supplementary alignments filtered, replace the filter with NotSecondary, and for those that do, change Primary to also reject supplementary reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2165
https://github.com/broadinstitute/gatk/issues/2165:216,Safety,redund,redundant,216,"PrimaryAlignmentReadFilter filters out secondary alignments, but not supplementary alignments (this is what GATK3 does as well). This doesn't match the SAM spec (though it does match the language used by htsjdk); is redundant given that in GATK4 we have a NotSecondaryAlignmentReadFilter for use with InsertSizeMetricsCollector; and is likely confusing. Proposed change is to visit the usages of the PrimaryAlignmentReadFilter (BaseRecalibrator, Pileup, CalculateTargetBaseCallCoverage and HaplotypCaller) and for any that don't want supplementary alignments filtered, replace the filter with NotSecondary, and for those that do, change Primary to also reject supplementary reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2165
https://github.com/broadinstitute/gatk/pull/2167:241,Testability,test,tests,241,wanted to see how codecov.io works. don't merge this. It looks like they fixed all the problems we have with coveralls though and actually generate a useful report. They have a browser extension that lets you view which lines are covered by tests in github. It seems like it would actually be pretty useful in code reviews.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2167
https://github.com/broadinstitute/gatk/pull/2168:37,Security,hash,hashCode,37,"This PR primarily fixes a bug in the hashCode method of BreakpointAllele that was causing Spark's groupByKey() method to not group identical inversion events together, resulting in duplicate variant calls in the output VCF. . All changes:; - Fix hashCode of BreakpointAllele to use the ordinal of the InversionType enum, which is consistent across executors; - Added Kryo Serializers to several SV classes; - Made the INSERTED_SEQUENCE_MAPPINGS VCF annotation include the assembly id and contig name for each mapping, which is necessary to read them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2168
https://github.com/broadinstitute/gatk/pull/2168:246,Security,hash,hashCode,246,"This PR primarily fixes a bug in the hashCode method of BreakpointAllele that was causing Spark's groupByKey() method to not group identical inversion events together, resulting in duplicate variant calls in the output VCF. . All changes:; - Fix hashCode of BreakpointAllele to use the ordinal of the InversionType enum, which is consistent across executors; - Added Kryo Serializers to several SV classes; - Made the INSERTED_SEQUENCE_MAPPINGS VCF annotation include the assembly id and contig name for each mapping, which is necessary to read them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2168
https://github.com/broadinstitute/gatk/pull/2170:161,Testability,test,tests,161,AlleleSubsettingUtils.calculateMostLikelyAlleles was incorrectly removing all alleles if the best alleles were tied and numAltAllelesToKeep=1; fixes #2157. move tests into AlleleSubsettingUtilsUnitTest from GATKVariantContextUtilsUnitTest as part of #2158; a few typo fixes that I noticed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2170
https://github.com/broadinstitute/gatk/issues/2171:4943,Availability,ERROR,ERROR,4943,licable; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; 15:47:34.944 ERROR Executor:95 - Exception in task 5.0 in stage 0.0 (TID 5); org.broadinstitute.hellbender.exceptions.GATKException: Cannot run BWA-MEM; at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$null$1(BwaSparkEngine.java:113); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:760,Deployability,install,install,760,"Below are the contents of my reference folder. The index is there, but I don't know why the tool can't recognize it. Please help, thanks!; kh3@rgcaahauva08091 ~/Resources/genome_b37 $> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:824,Deployability,install,install,824,"Below are the contents of my reference folder. The index is there, but I don't know why the tool can't recognize it. Please help, thanks!; kh3@rgcaahauva08091 ~/Resources/genome_b37 $> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1224,Deployability,install,install,1224,"> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1447,Deployability,pipeline,pipelines,1447,"2 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.; 2-45-ga30af5a-SNAPSHOT; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.BUFFER_SIZE : 131072; 15:47:28.835 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:6953,Energy Efficiency,schedul,scheduler,6953,r(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.io.IOException: bwa_idx_load failed; at com.github.lindenb.jbwa.jni.BwaIndex._open(Native Method); at com.github.lindenb.jbwa.jni.BwaIndex.<init>(BwaIndex.java:216); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$null$1(BwaSparkEngine.java:109); ... 32 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:7032,Energy Efficiency,schedul,scheduler,7032,r(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.io.IOException: bwa_idx_load failed; at com.github.lindenb.jbwa.jni.BwaIndex._open(Native Method); at com.github.lindenb.jbwa.jni.BwaIndex.<init>(BwaIndex.java:216); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$null$1(BwaSparkEngine.java:109); ... 32 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:7111,Energy Efficiency,schedul,scheduler,7111,r(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.io.IOException: bwa_idx_load failed; at com.github.lindenb.jbwa.jni.BwaIndex._open(Native Method); at com.github.lindenb.jbwa.jni.BwaIndex.<init>(BwaIndex.java:216); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$null$1(BwaSparkEngine.java:109); ... 32 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:714,Integrability,wrap,wrapper,714,"Below are the contents of my reference folder. The index is there, but I don't know why the tool can't recognize it. Please help, thanks!; kh3@rgcaahauva08091 ~/Resources/genome_b37 $> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1154,Performance,load,load,1154,"> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1345,Performance,load,loaded,1345,"06669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.; 2-45-ga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:3877,Performance,load,load,3877,p://www.ebi.ac.uk/ena/cram/md5/%s; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.REFERENCE_FASTA : null; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater IntelDeflater; 15:47:28.836 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 15:47:28.836 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; 15:47:29.287 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:7233,Performance,concurren,concurrent,7233,r(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.io.IOException: bwa_idx_load failed; at com.github.lindenb.jbwa.jni.BwaIndex._open(Native Method); at com.github.lindenb.jbwa.jni.BwaIndex.<init>(BwaIndex.java:216); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$null$1(BwaSparkEngine.java:109); ... 32 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:7317,Performance,concurren,concurrent,7317,r(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.io.IOException: bwa_idx_load failed; at com.github.lindenb.jbwa.jni.BwaIndex._open(Native Method); at com.github.lindenb.jbwa.jni.BwaIndex.<init>(BwaIndex.java:216); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$null$1(BwaSparkEngine.java:109); ... 32 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:922,Testability,TEST,TEST,922,"Below are the contents of my reference folder. The index is there, but I don't know why the tool can't recognize it. Please help, thanks!; kh3@rgcaahauva08091 ~/Resources/genome_b37 $> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:927,Testability,test,test,927,"Below are the contents of my reference folder. The index is there, but I don't know why the tool can't recognize it. Please help, thanks!; kh3@rgcaahauva08091 ~/Resources/genome_b37 $> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1081,Testability,TEST,TEST,1081," folder. The index is there, but I don't know why the tool can't recognize it. Please help, thanks!; kh3@rgcaahauva08091 ~/Resources/genome_b37 $> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1086,Testability,test,test,1086," folder. The index is there, but I don't know why the tool can't recognize it. Please help, thanks!; kh3@rgcaahauva08091 ~/Resources/genome_b37 $> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1552,Testability,TEST,TEST,1552,"eb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.; 2-45-ga30af5a-SNAPSHOT; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.BUFFER_SIZE : 131072; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.COMPRESSIO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1557,Testability,test,test,1557,"eb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.; 2-45-ga30af5a-SNAPSHOT; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.BUFFER_SIZE : 131072; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.COMPRESSIO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1684,Testability,TEST,TEST,1684,"3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.; 2-45-ga30af5a-SNAPSHOT; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.BUFFER_SIZE : 131072; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.COMPRESSION_LEVEL : 1; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.CREATE_INDEX : false; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2171:1689,Testability,test,test,1689,"3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.; 2-45-ga30af5a-SNAPSHOT; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.BUFFER_SIZE : 131072; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.COMPRESSION_LEVEL : 1; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.CREATE_INDEX : false; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171
https://github.com/broadinstitute/gatk/issues/2172:73,Deployability,integrat,integration,73,GATK public doesn't have any example of an `AssemblyRegionWalker` and no integration tests. This makes it easy to accidentally break the haplotype caller without realizing it. We should have an ExampleAssemblyRegionWalker and integration tests for it to prevent this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2172
https://github.com/broadinstitute/gatk/issues/2172:226,Deployability,integrat,integration,226,GATK public doesn't have any example of an `AssemblyRegionWalker` and no integration tests. This makes it easy to accidentally break the haplotype caller without realizing it. We should have an ExampleAssemblyRegionWalker and integration tests for it to prevent this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2172
https://github.com/broadinstitute/gatk/issues/2172:73,Integrability,integrat,integration,73,GATK public doesn't have any example of an `AssemblyRegionWalker` and no integration tests. This makes it easy to accidentally break the haplotype caller without realizing it. We should have an ExampleAssemblyRegionWalker and integration tests for it to prevent this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2172
https://github.com/broadinstitute/gatk/issues/2172:226,Integrability,integrat,integration,226,GATK public doesn't have any example of an `AssemblyRegionWalker` and no integration tests. This makes it easy to accidentally break the haplotype caller without realizing it. We should have an ExampleAssemblyRegionWalker and integration tests for it to prevent this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2172
https://github.com/broadinstitute/gatk/issues/2172:85,Testability,test,tests,85,GATK public doesn't have any example of an `AssemblyRegionWalker` and no integration tests. This makes it easy to accidentally break the haplotype caller without realizing it. We should have an ExampleAssemblyRegionWalker and integration tests for it to prevent this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2172
https://github.com/broadinstitute/gatk/issues/2172:238,Testability,test,tests,238,GATK public doesn't have any example of an `AssemblyRegionWalker` and no integration tests. This makes it easy to accidentally break the haplotype caller without realizing it. We should have an ExampleAssemblyRegionWalker and integration tests for it to prevent this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2172
https://github.com/broadinstitute/gatk/pull/2174:14,Testability,test,test,14,Adds a simple test to test a cram in UnmarkDuplicatesIntergrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2174
https://github.com/broadinstitute/gatk/pull/2174:22,Testability,test,test,22,Adds a simple test to test a cram in UnmarkDuplicatesIntergrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2174
https://github.com/broadinstitute/gatk/pull/2174:7,Usability,simpl,simple,7,Adds a simple test to test a cram in UnmarkDuplicatesIntergrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2174
https://github.com/broadinstitute/gatk/issues/2176:412,Deployability,configurat,configuration,412,"As explain in the documentation. for logging GATK is using `org.apache.logging.log4j.Logger`. Nevertheless, because I would like to use GATK4 as a framework, I think that API users could benefit from using [SLF4J](http://www.slf4j.org/) as a plugin system for allow users to decide which system use. Because it exists a wrapper for log4j, I think that this could be done easily without changing any behaviour or configuration. In addition, I believe that the usage of SLF4J is similar as the one used in log4j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176
https://github.com/broadinstitute/gatk/issues/2176:320,Integrability,wrap,wrapper,320,"As explain in the documentation. for logging GATK is using `org.apache.logging.log4j.Logger`. Nevertheless, because I would like to use GATK4 as a framework, I think that API users could benefit from using [SLF4J](http://www.slf4j.org/) as a plugin system for allow users to decide which system use. Because it exists a wrapper for log4j, I think that this could be done easily without changing any behaviour or configuration. In addition, I believe that the usage of SLF4J is similar as the one used in log4j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176
https://github.com/broadinstitute/gatk/issues/2176:242,Modifiability,plugin,plugin,242,"As explain in the documentation. for logging GATK is using `org.apache.logging.log4j.Logger`. Nevertheless, because I would like to use GATK4 as a framework, I think that API users could benefit from using [SLF4J](http://www.slf4j.org/) as a plugin system for allow users to decide which system use. Because it exists a wrapper for log4j, I think that this could be done easily without changing any behaviour or configuration. In addition, I believe that the usage of SLF4J is similar as the one used in log4j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176
https://github.com/broadinstitute/gatk/issues/2176:412,Modifiability,config,configuration,412,"As explain in the documentation. for logging GATK is using `org.apache.logging.log4j.Logger`. Nevertheless, because I would like to use GATK4 as a framework, I think that API users could benefit from using [SLF4J](http://www.slf4j.org/) as a plugin system for allow users to decide which system use. Because it exists a wrapper for log4j, I think that this could be done easily without changing any behaviour or configuration. In addition, I believe that the usage of SLF4J is similar as the one used in log4j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176
https://github.com/broadinstitute/gatk/issues/2176:37,Testability,log,logging,37,"As explain in the documentation. for logging GATK is using `org.apache.logging.log4j.Logger`. Nevertheless, because I would like to use GATK4 as a framework, I think that API users could benefit from using [SLF4J](http://www.slf4j.org/) as a plugin system for allow users to decide which system use. Because it exists a wrapper for log4j, I think that this could be done easily without changing any behaviour or configuration. In addition, I believe that the usage of SLF4J is similar as the one used in log4j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176
https://github.com/broadinstitute/gatk/issues/2176:71,Testability,log,logging,71,"As explain in the documentation. for logging GATK is using `org.apache.logging.log4j.Logger`. Nevertheless, because I would like to use GATK4 as a framework, I think that API users could benefit from using [SLF4J](http://www.slf4j.org/) as a plugin system for allow users to decide which system use. Because it exists a wrapper for log4j, I think that this could be done easily without changing any behaviour or configuration. In addition, I believe that the usage of SLF4J is similar as the one used in log4j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176
https://github.com/broadinstitute/gatk/issues/2176:85,Testability,Log,Logger,85,"As explain in the documentation. for logging GATK is using `org.apache.logging.log4j.Logger`. Nevertheless, because I would like to use GATK4 as a framework, I think that API users could benefit from using [SLF4J](http://www.slf4j.org/) as a plugin system for allow users to decide which system use. Because it exists a wrapper for log4j, I think that this could be done easily without changing any behaviour or configuration. In addition, I believe that the usage of SLF4J is similar as the one used in log4j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176
https://github.com/broadinstitute/gatk/issues/2177:541,Usability,simpl,simple,541,"`CreateSequenceDictionary` have several things that could be improved:; - The ouptut is necessary, but a default value could be created within the tool. Because the framework reads the sequence dictionary from the same path and a default extension (**.dict**), it would be nice to allow the creation without specifying the output file and create it according to the convention.; - An option to create the **.fai** index could be included. The `samtools faidx` indexing is well [documented](http://www.htslib.org/doc/faidx.html) and it looks simple to implement. I don't know if this should be implemented here or in htsjdk. I can contribute with both if the GATK developers think that this will improve the tool. I think that the first point is good because is fairly simple and consistent with the framework. The second could be an improvement to be able to create the two required indexes using the same bundled tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2177
https://github.com/broadinstitute/gatk/issues/2177:768,Usability,simpl,simple,768,"`CreateSequenceDictionary` have several things that could be improved:; - The ouptut is necessary, but a default value could be created within the tool. Because the framework reads the sequence dictionary from the same path and a default extension (**.dict**), it would be nice to allow the creation without specifying the output file and create it according to the convention.; - An option to create the **.fai** index could be included. The `samtools faidx` indexing is well [documented](http://www.htslib.org/doc/faidx.html) and it looks simple to implement. I don't know if this should be implemented here or in htsjdk. I can contribute with both if the GATK developers think that this will improve the tool. I think that the first point is good because is fairly simple and consistent with the framework. The second could be an improvement to be able to create the two required indexes using the same bundled tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2177
https://github.com/broadinstitute/gatk/issues/2178:18,Testability,test,test,18,"We don't have any test that check the correctness of `AlelleSubsettingUtils.calculateLikelihoodSums` and we probably should. (it's tested indirectly in protected, but that's not really ideal.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2178
https://github.com/broadinstitute/gatk/issues/2178:131,Testability,test,tested,131,"We don't have any test that check the correctness of `AlelleSubsettingUtils.calculateLikelihoodSums` and we probably should. (it's tested indirectly in protected, but that's not really ideal.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2178
https://github.com/broadinstitute/gatk/pull/2179:21,Deployability,integrat,integration,21,"Previously, the only integration tests involving assembly region traversal were in gatk-protected,; which led to breakage. Resolves #2172",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2179
https://github.com/broadinstitute/gatk/pull/2179:21,Integrability,integrat,integration,21,"Previously, the only integration tests involving assembly region traversal were in gatk-protected,; which led to breakage. Resolves #2172",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2179
https://github.com/broadinstitute/gatk/pull/2179:33,Testability,test,tests,33,"Previously, the only integration tests involving assembly region traversal were in gatk-protected,; which led to breakage. Resolves #2172",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2179
https://github.com/broadinstitute/gatk/issues/2180:57,Availability,error,errors,57,Our jenkins worker nodes are dead with out of disk space errors. We need to either clean stuff off of them or give them more space. . We should determine what's eating their disk space as well so we can prevent this in the future. We may need to do some clean up after some of our tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2180
https://github.com/broadinstitute/gatk/issues/2180:281,Testability,test,tests,281,Our jenkins worker nodes are dead with out of disk space errors. We need to either clean stuff off of them or give them more space. . We should determine what's eating their disk space as well so we can prevent this in the future. We may need to do some clean up after some of our tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2180
https://github.com/broadinstitute/gatk/pull/2181:130,Performance,load,loader,130,"Example codes are include to demonstrate the two ways variant contexts can be read to Spark RDD from GenomicsDB. The code takes a loader JSON file, a query JSON file and a hosts file as input.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2181
https://github.com/broadinstitute/gatk/pull/2182:329,Testability,test,tests,329,"MultiVariantWalker implementation, with VariantRecalibrator and ApplyVQSR switched over to the new base class. GatherVCFs, MergeVCFs, SelectVariants, and possibly SortVCF are all candidates to be changed to MultiVariantWalkers, but are not included here. Also, this is a branch of the branch that contains the VQSR and ApplyVQSR tests so I could use those tests to verify the MultiVariantWalker. As result there are two commits here, but the tests are already included a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2182
https://github.com/broadinstitute/gatk/pull/2182:356,Testability,test,tests,356,"MultiVariantWalker implementation, with VariantRecalibrator and ApplyVQSR switched over to the new base class. GatherVCFs, MergeVCFs, SelectVariants, and possibly SortVCF are all candidates to be changed to MultiVariantWalkers, but are not included here. Also, this is a branch of the branch that contains the VQSR and ApplyVQSR tests so I could use those tests to verify the MultiVariantWalker. As result there are two commits here, but the tests are already included a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2182
https://github.com/broadinstitute/gatk/pull/2182:442,Testability,test,tests,442,"MultiVariantWalker implementation, with VariantRecalibrator and ApplyVQSR switched over to the new base class. GatherVCFs, MergeVCFs, SelectVariants, and possibly SortVCF are all candidates to be changed to MultiVariantWalkers, but are not included here. Also, this is a branch of the branch that contains the VQSR and ApplyVQSR tests so I could use those tests to verify the MultiVariantWalker. As result there are two commits here, but the tests are already included a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2182
https://github.com/broadinstitute/gatk/issues/2183:5823,Availability,ERROR,ERROR,5823,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5924,Availability,ERROR,ERROR,5924,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:4939,Deployability,deploy,deploy,4939,.runTool(MarkDuplicatesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waitin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:4976,Deployability,deploy,deploy,4976,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5048,Deployability,deploy,deploy,5048,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5124,Deployability,deploy,deploy,5124,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5195,Deployability,deploy,deploy,5195,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5264,Deployability,deploy,deploy,5264,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:1266,Performance,load,loadClass,1266,"rkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:274); at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:259); at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:1349,Performance,load,loadClass,1349,"; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:274); at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:259); at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:175); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:1417,Performance,load,loadClass,1417,"-driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:274); at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:259); at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:175); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5544,Performance,load,loadClass,5544,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5620,Performance,load,loadClass,5620,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5709,Performance,load,loadClass,5709,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5777,Performance,load,loadClass,5777,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:801,Security,secur,security,801,"Spark tests in Jenkins are failing nightly. They've been failing since August 29th. This command:. ```; gcloud auth activate-service-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:810,Security,Secur,SecureClassLoader,810,"Spark tests in Jenkins are failing nightly. They've been failing since August 29th. This command:. ```; gcloud auth activate-service-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:840,Security,Secur,SecureClassLoader,840,"Spark tests in Jenkins are failing nightly. They've been failing since August 29th. This command:. ```; gcloud auth activate-service-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:961,Security,access,access,961,"Spark tests in Jenkins are failing nightly. They've been failing since August 29th. This command:. ```; gcloud auth activate-service-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:1123,Security,secur,security,1123,"rvice-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:1132,Security,Access,AccessController,1132," gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializerInst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:6,Testability,test,tests,6,"Spark tests in Jenkins are failing nightly. They've been failing since August 29th. This command:. ```; gcloud auth activate-service-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:168,Testability,test,test,168,"Spark tests in Jenkins are failing nightly. They've been failing since August 29th. This command:. ```; gcloud auth activate-service-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:217,Testability,test,testservice,217,"Spark tests in Jenkins are failing nightly. They've been failing since August 29th. This command:. ```; gcloud auth activate-service-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:255,Testability,test,test,255,"Spark tests in Jenkins are failing nightly. They've been failing since August 29th. This command:. ```; gcloud auth activate-service-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:670,Testability,Log,Logging,670,"Spark tests in Jenkins are failing nightly. They've been failing since August 29th. This command:. ```; gcloud auth activate-service-account gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/issues/2183:5370,Testability,Log,Logging,5370,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183
https://github.com/broadinstitute/gatk/pull/2186:20,Performance,optimiz,optimizations,20,A handful of simple optimizations for VariantRecalibrator:; - Preallocate arrays when the size is known; - Eliminate unnecessary boxing of doubles; - Lift some loop invariants with unnecessary allocations (this eliminates millions of array allocations on the full SNP test used by GATK3). The current GATK4 (multi-variant walker) implementation is about 3% faster than GATK3 without these; these bring it to about 6% faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2186
https://github.com/broadinstitute/gatk/pull/2186:268,Testability,test,test,268,A handful of simple optimizations for VariantRecalibrator:; - Preallocate arrays when the size is known; - Eliminate unnecessary boxing of doubles; - Lift some loop invariants with unnecessary allocations (this eliminates millions of array allocations on the full SNP test used by GATK3). The current GATK4 (multi-variant walker) implementation is about 3% faster than GATK3 without these; these bring it to about 6% faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2186
https://github.com/broadinstitute/gatk/pull/2186:13,Usability,simpl,simple,13,A handful of simple optimizations for VariantRecalibrator:; - Preallocate arrays when the size is known; - Eliminate unnecessary boxing of doubles; - Lift some loop invariants with unnecessary allocations (this eliminates millions of array allocations on the full SNP test used by GATK3). The current GATK4 (multi-variant walker) implementation is about 3% faster than GATK3 without these; these bring it to about 6% faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2186
https://github.com/broadinstitute/gatk/pull/2191:12,Availability,error,error,12,"The current error handling swallows the detailed part of the error string that gets reported to the user, i.e., without this change you see:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: 20:10000000-10010000. With this change:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The genome loc coordinates 10000000-10010000 exceed the contig size (200000). I originally removed the entire try/catch block, but there is downstream code that throws IllegalArgumentException, which can benefit from being decorated with the context info.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2191
https://github.com/broadinstitute/gatk/pull/2191:61,Availability,error,error,61,"The current error handling swallows the detailed part of the error string that gets reported to the user, i.e., without this change you see:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: 20:10000000-10010000. With this change:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The genome loc coordinates 10000000-10010000 exceed the contig size (200000). I originally removed the entire try/catch block, but there is downstream code that throws IllegalArgumentException, which can benefit from being decorated with the context info.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2191
https://github.com/broadinstitute/gatk/pull/2191:149,Availability,ERROR,ERROR,149,"The current error handling swallows the detailed part of the error string that gets reported to the user, i.e., without this change you see:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: 20:10000000-10010000. With this change:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The genome loc coordinates 10000000-10010000 exceed the contig size (200000). I originally removed the entire try/catch block, but there is downstream code that throws IllegalArgumentException, which can benefit from being decorated with the context info.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2191
https://github.com/broadinstitute/gatk/pull/2191:291,Availability,ERROR,ERROR,291,"The current error handling swallows the detailed part of the error string that gets reported to the user, i.e., without this change you see:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: 20:10000000-10010000. With this change:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The genome loc coordinates 10000000-10010000 exceed the contig size (200000). I originally removed the entire try/catch block, but there is downstream code that throws IllegalArgumentException, which can benefit from being decorated with the context info.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2191
https://github.com/broadinstitute/gatk/pull/2191:529,Availability,down,downstream,529,"The current error handling swallows the detailed part of the error string that gets reported to the user, i.e., without this change you see:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: 20:10000000-10010000. With this change:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The genome loc coordinates 10000000-10010000 exceed the contig size (200000). I originally removed the entire try/catch block, but there is downstream code that throws IllegalArgumentException, which can benefit from being decorated with the context info.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2191
https://github.com/broadinstitute/gatk/issues/2192:198,Energy Efficiency,reduce,reduce,198,"The idea is to partition the reference in the same way as reads, then do a `zipPartitions` to join them together. Then only the relevant part of the reference is sent to each executor, which should reduce memory usage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2192
https://github.com/broadinstitute/gatk/issues/2193:666,Integrability,depend,depends,666,@ronlevine commented on [Thu Sep 29 2016](https://github.com/broadinstitute/gatk-protected/issues/725). @ronlevine commented on [Tue Jul 19 2016](https://github.com/broadinstitute/gatk/issues/2017). Port https://github.com/broadinstitute/gsa-unstable/pull/1417. ---. @ronlevine commented on [Wed Jul 20 2016](https://github.com/broadinstitute/gatk/issues/2017#issuecomment-233990695). Need to port `GenotypeGVCFs` before testing the proposed changes in `GenotypingEngine.java`. ---. @ronlevine commented on [Thu Sep 29 2016](https://github.com/broadinstitute/gatk/issues/2017#issuecomment-250468170). @droazen Should this be a gatk-protected ticket instead since it depends on GenotypeGVCFs?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2193
https://github.com/broadinstitute/gatk/issues/2193:421,Testability,test,testing,421,@ronlevine commented on [Thu Sep 29 2016](https://github.com/broadinstitute/gatk-protected/issues/725). @ronlevine commented on [Tue Jul 19 2016](https://github.com/broadinstitute/gatk/issues/2017). Port https://github.com/broadinstitute/gsa-unstable/pull/1417. ---. @ronlevine commented on [Wed Jul 20 2016](https://github.com/broadinstitute/gatk/issues/2017#issuecomment-233990695). Need to port `GenotypeGVCFs` before testing the proposed changes in `GenotypingEngine.java`. ---. @ronlevine commented on [Thu Sep 29 2016](https://github.com/broadinstitute/gatk/issues/2017#issuecomment-250468170). @droazen Should this be a gatk-protected ticket instead since it depends on GenotypeGVCFs?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2193
https://github.com/broadinstitute/gatk/pull/2195:198,Testability,test,tests,198,"Following the same implementation as `CountingReadFilter`, this is a way of counting the `VariantContext` that were filtered. This is useful for report the status of `VariantFilter`s. There are not tests yet because I would like to have the ok on this before spending time on implement them. I think that the community could benefit from using this, and even some tools from the GATK itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195
https://github.com/broadinstitute/gatk/issues/2196:51,Deployability,pipeline,pipeline,51,"Currently in the inversion calling stage of the SV pipeline, it is assumed that insertion and homology around identified breakpoints don't co-exist. But in theory it is possible. Take an example of an inversion identified by a 5-to-3 contig.; The contig is made from; 1. reference sequence **ref1** `CACTGCCACTGGTCCTTCAACAGATTGATTCTAATTAGCCAATCAAAGACAA`; 2. homology on reference **H** `GGATCCA`; 3. reverse complement of a reference sequence (already RCed) **ref2'**: `CAATGTCAATACAAATGGGGACTAACTATTGTTTTACTTCCCT`; This contig would generate two alignment records: first mapped to 21:27374100 with CIGAR `59M43S` (from compactifying 52M7M43S), second mapped to 21:27374657 with CIGAR `50M52H` (from compactifying 43M7M52H).; Current convention is to give the homologous sequence to the second alignment record to incorporate ambiguity in locating breakpoints. Now if there's an insertion in the sample, say `AAAA`, after the homology. i.e. ```; ref1 + H + I + ref2'; ```. where a prime indicates reverse complement. Then the contig would still be mapped to the same locations on reference, but with CIGARs `59M47S` and `43M4I7M52H` (the aligner would very likely generate a `43M63H`).; The identified breakpoint should still incorporate the ambiguity introduced by the homology. ```; ref2 + I' + H' + ref1'; ```. Our caller currently doesn't handle such cases. Artificially created FASTA records from real reference sequences are attached.; [SV_HomAndIns_Ticket_data.zip](https://github.com/broadinstitute/gatk/files/509267/SV_HomAndIns_Ticket_data.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2196
https://github.com/broadinstitute/gatk/issues/2199:1157,Availability,error,error,1157,"Hi,. I ran into this issue when running VariantRecalibrator in GATK4. I used the same command as recommended in the GATK3 best practice, but seems the command can't recognize the track information of the resources:. **java -jar ~/Softwares/gatk/build/libs/gatk-package-4.alpha.2-49-g491f7f2-SNAPSHOT-local.jar VariantRecalibrator \; -R ~/Resources/genome_b38/genome.fa \; -V ${op}/DUKE_6954540_200400624.vcf.gz \; -resource hapmap,known=false,training=true,truth=true,prior=15.0 ~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz \; -resource omni,known=false,training=true,truth=true,prior=12.0 \; ~/Resources/genome_b38/1000G_omni2.5.b38.vcf.gz \; -resource 1000G,known=false,training=true,truth=false,prior=10.0 \; ~/Resources/genome_b38/tmp/1000G_phase3.snps.high_confidence.b38.rh.vcf.gz \; -resource dbsnp,known=true,training=false,truth=false,prior=2.0 \; ~/Resources/genome_b38/dbsnp_142.b38.vcf.gz \; -an DP \; -an QD \; -an FS \; -an MQRankSum \; -an ReadPosRankSum \; -mode SNP \; -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 \; -O DUKE_6954540_200400624.snp.output.recal \; -tranchesFile DUKE_6954540_200400624.snp.output.tranches** . The error I got is:. **""A USER ERROR has occurred: Invalid command line: Invalid argument '~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz'"".** . It seems the command treat **hapmap,known=false,training=true,truth=true,prior=15.0** and **~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz** as different arguments. And if I only provided ""**~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz**"" to '**-resource**', the job shut down immediately. Is this a bug? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199
https://github.com/broadinstitute/gatk/issues/2199:1184,Availability,ERROR,ERROR,1184,"Hi,. I ran into this issue when running VariantRecalibrator in GATK4. I used the same command as recommended in the GATK3 best practice, but seems the command can't recognize the track information of the resources:. **java -jar ~/Softwares/gatk/build/libs/gatk-package-4.alpha.2-49-g491f7f2-SNAPSHOT-local.jar VariantRecalibrator \; -R ~/Resources/genome_b38/genome.fa \; -V ${op}/DUKE_6954540_200400624.vcf.gz \; -resource hapmap,known=false,training=true,truth=true,prior=15.0 ~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz \; -resource omni,known=false,training=true,truth=true,prior=12.0 \; ~/Resources/genome_b38/1000G_omni2.5.b38.vcf.gz \; -resource 1000G,known=false,training=true,truth=false,prior=10.0 \; ~/Resources/genome_b38/tmp/1000G_phase3.snps.high_confidence.b38.rh.vcf.gz \; -resource dbsnp,known=true,training=false,truth=false,prior=2.0 \; ~/Resources/genome_b38/dbsnp_142.b38.vcf.gz \; -an DP \; -an QD \; -an FS \; -an MQRankSum \; -an ReadPosRankSum \; -mode SNP \; -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 \; -O DUKE_6954540_200400624.snp.output.recal \; -tranchesFile DUKE_6954540_200400624.snp.output.tranches** . The error I got is:. **""A USER ERROR has occurred: Invalid command line: Invalid argument '~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz'"".** . It seems the command treat **hapmap,known=false,training=true,truth=true,prior=15.0** and **~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz** as different arguments. And if I only provided ""**~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz**"" to '**-resource**', the job shut down immediately. Is this a bug? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199
https://github.com/broadinstitute/gatk/issues/2199:1566,Availability,down,down,1566,"Hi,. I ran into this issue when running VariantRecalibrator in GATK4. I used the same command as recommended in the GATK3 best practice, but seems the command can't recognize the track information of the resources:. **java -jar ~/Softwares/gatk/build/libs/gatk-package-4.alpha.2-49-g491f7f2-SNAPSHOT-local.jar VariantRecalibrator \; -R ~/Resources/genome_b38/genome.fa \; -V ${op}/DUKE_6954540_200400624.vcf.gz \; -resource hapmap,known=false,training=true,truth=true,prior=15.0 ~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz \; -resource omni,known=false,training=true,truth=true,prior=12.0 \; ~/Resources/genome_b38/1000G_omni2.5.b38.vcf.gz \; -resource 1000G,known=false,training=true,truth=false,prior=10.0 \; ~/Resources/genome_b38/tmp/1000G_phase3.snps.high_confidence.b38.rh.vcf.gz \; -resource dbsnp,known=true,training=false,truth=false,prior=2.0 \; ~/Resources/genome_b38/dbsnp_142.b38.vcf.gz \; -an DP \; -an QD \; -an FS \; -an MQRankSum \; -an ReadPosRankSum \; -mode SNP \; -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 \; -O DUKE_6954540_200400624.snp.output.recal \; -tranchesFile DUKE_6954540_200400624.snp.output.tranches** . The error I got is:. **""A USER ERROR has occurred: Invalid command line: Invalid argument '~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz'"".** . It seems the command treat **hapmap,known=false,training=true,truth=true,prior=15.0** and **~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz** as different arguments. And if I only provided ""**~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz**"" to '**-resource**', the job shut down immediately. Is this a bug? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199
https://github.com/broadinstitute/gatk/issues/2200:196,Availability,error,error,196,"Because the filename doesn't occur at the beginning of a word when you use a logical name:. --arg aname:~/dir1/... the ~ doesn't get expanded by the shell, so the command fails, and the resulting error message is confusing because we call file.getAbsolutePath() on the file object to create the exception, which prepends the current directory onto the (unexpanded, nonexistent) filename (see https://github.com/broadinstitute/gatk/issues/2199). If we can't fix the underlying problem we should try to improve the error message.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2200
https://github.com/broadinstitute/gatk/issues/2200:513,Availability,error,error,513,"Because the filename doesn't occur at the beginning of a word when you use a logical name:. --arg aname:~/dir1/... the ~ doesn't get expanded by the shell, so the command fails, and the resulting error message is confusing because we call file.getAbsolutePath() on the file object to create the exception, which prepends the current directory onto the (unexpanded, nonexistent) filename (see https://github.com/broadinstitute/gatk/issues/2199). If we can't fix the underlying problem we should try to improve the error message.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2200
https://github.com/broadinstitute/gatk/issues/2200:202,Integrability,message,message,202,"Because the filename doesn't occur at the beginning of a word when you use a logical name:. --arg aname:~/dir1/... the ~ doesn't get expanded by the shell, so the command fails, and the resulting error message is confusing because we call file.getAbsolutePath() on the file object to create the exception, which prepends the current directory onto the (unexpanded, nonexistent) filename (see https://github.com/broadinstitute/gatk/issues/2199). If we can't fix the underlying problem we should try to improve the error message.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2200
https://github.com/broadinstitute/gatk/issues/2200:519,Integrability,message,message,519,"Because the filename doesn't occur at the beginning of a word when you use a logical name:. --arg aname:~/dir1/... the ~ doesn't get expanded by the shell, so the command fails, and the resulting error message is confusing because we call file.getAbsolutePath() on the file object to create the exception, which prepends the current directory onto the (unexpanded, nonexistent) filename (see https://github.com/broadinstitute/gatk/issues/2199). If we can't fix the underlying problem we should try to improve the error message.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2200
https://github.com/broadinstitute/gatk/issues/2200:77,Testability,log,logical,77,"Because the filename doesn't occur at the beginning of a word when you use a logical name:. --arg aname:~/dir1/... the ~ doesn't get expanded by the shell, so the command fails, and the resulting error message is confusing because we call file.getAbsolutePath() on the file object to create the exception, which prepends the current directory onto the (unexpanded, nonexistent) filename (see https://github.com/broadinstitute/gatk/issues/2199). If we can't fix the underlying problem we should try to improve the error message.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2200
https://github.com/broadinstitute/gatk/issues/2203:311,Availability,error,error,311,"## Bug Report; ### Affected tool(s). MuTect2; ### Affected version(s). version 3.6; ### Description. A user reported that when he input a BAM file that has multiple sample names to MuTect2, the tool mseems to run fine but outputs an empty VCF. Because MuTect2 cannot run on multiple samples, there should be an error message that lets the user know he/she cannot have multiple samples in the BAM file.; #### Expected behavior. There should be an error message telling the user to input only single sample BAM files, and the run should fail.; #### Actual behavior. The tool runs to completion but outputs an empty VCF. [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/33336#Comment_33336). ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2203
https://github.com/broadinstitute/gatk/issues/2203:446,Availability,error,error,446,"## Bug Report; ### Affected tool(s). MuTect2; ### Affected version(s). version 3.6; ### Description. A user reported that when he input a BAM file that has multiple sample names to MuTect2, the tool mseems to run fine but outputs an empty VCF. Because MuTect2 cannot run on multiple samples, there should be an error message that lets the user know he/she cannot have multiple samples in the BAM file.; #### Expected behavior. There should be an error message telling the user to input only single sample BAM files, and the run should fail.; #### Actual behavior. The tool runs to completion but outputs an empty VCF. [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/33336#Comment_33336). ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2203
https://github.com/broadinstitute/gatk/issues/2203:317,Integrability,message,message,317,"## Bug Report; ### Affected tool(s). MuTect2; ### Affected version(s). version 3.6; ### Description. A user reported that when he input a BAM file that has multiple sample names to MuTect2, the tool mseems to run fine but outputs an empty VCF. Because MuTect2 cannot run on multiple samples, there should be an error message that lets the user know he/she cannot have multiple samples in the BAM file.; #### Expected behavior. There should be an error message telling the user to input only single sample BAM files, and the run should fail.; #### Actual behavior. The tool runs to completion but outputs an empty VCF. [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/33336#Comment_33336). ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2203
https://github.com/broadinstitute/gatk/issues/2203:452,Integrability,message,message,452,"## Bug Report; ### Affected tool(s). MuTect2; ### Affected version(s). version 3.6; ### Description. A user reported that when he input a BAM file that has multiple sample names to MuTect2, the tool mseems to run fine but outputs an empty VCF. Because MuTect2 cannot run on multiple samples, there should be an error message that lets the user know he/she cannot have multiple samples in the BAM file.; #### Expected behavior. There should be an error message telling the user to input only single sample BAM files, and the run should fail.; #### Actual behavior. The tool runs to completion but outputs an empty VCF. [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/33336#Comment_33336). ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2203
https://github.com/broadinstitute/gatk/pull/2204:7,Deployability,patch,patch,7,Simple patch to allow passing specific tool classes to `Main.instanceMain` instead of whole packages to solve #2140. This will allow clients that want their own command line with their tools to include only `IndexFeatureFile` for their own codecs and/or bundle tools like `CreateSequenceDictionary` to pre-process input files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204
https://github.com/broadinstitute/gatk/pull/2204:0,Usability,Simpl,Simple,0,Simple patch to allow passing specific tool classes to `Main.instanceMain` instead of whole packages to solve #2140. This will allow clients that want their own command line with their tools to include only `IndexFeatureFile` for their own codecs and/or bundle tools like `CreateSequenceDictionary` to pre-process input files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204
https://github.com/broadinstitute/gatk/pull/2205:0,Testability,Test,Tests,0,Tests and fix for https://github.com/broadinstitute/gatk/issues/1674.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2205
https://github.com/broadinstitute/gatk/issues/2212:53,Performance,load,loaded,53,"We likely want the mechanism to be a properties file loaded from the classpath, so that we can build jars that use the Picard arg parser by default.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2212
https://github.com/broadinstitute/gatk/issues/2214:114,Availability,error,error,114,"This is due to`GATKVAriantContextUtils::makeGenotypeCall` using `GenotypeLikelihoods.getAlleles`, which throws an error because you have to explicitly tell `GenotypeLikelihoods` to initialize ploidies != 2. We can fix this by explicitly initializing but it seems that `GenotypeLikelihoodCalculators` has the same functionality but more robust.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2214
https://github.com/broadinstitute/gatk/issues/2214:336,Availability,robust,robust,336,"This is due to`GATKVAriantContextUtils::makeGenotypeCall` using `GenotypeLikelihoods.getAlleles`, which throws an error because you have to explicitly tell `GenotypeLikelihoods` to initialize ploidies != 2. We can fix this by explicitly initializing but it seems that `GenotypeLikelihoodCalculators` has the same functionality but more robust.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2214
https://github.com/broadinstitute/gatk/issues/2216:5904,Availability,down,down,5904,"broadinstitute.hellbender.engine.datasources.ReferenceMultiSource, org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource@78eac8f2); - element of array (index: 1); - array (class [Ljava.lang.Object;, size 2); - field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;); - object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases, functionalInterfaceMethod=org/apache/spark/api/java/function/PairFlatMapFunction.call:(Ljava/lang/Object;)Ljava/lang/Iterable;, implementation=invokeStatic org/broadinstitute/hellbender/engine/spark/ShuffleJoinReadsWithRefBases.lambda$addBases$cff38836$1:(Lorg/broadinstitute/hellbender/utils/SerializableFunction;Lorg/broadinstitute/hellbender/engine/datasources/ReferenceMultiSource;Lscala/Tuple2;)Ljava/lang/Iterable;, instantiatedMethodType=(Lscala/Tuple2;)Ljava/lang/Iterable;, numCaptured=2]); - writeReplace data (class: java.lang.invoke.SerializedLambda); - object (class org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases$$Lambda$74/1217660878, org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases$$Lambda$74/1217660878@663d7f24); - field (class: org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1, name: f$5, type: interface org.apache.spark.api.java.function.PairFlatMapFunction); - object (class org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1, <function1>); at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101); at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301); ... 30 more; 16/10/17 16:03:47 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216
https://github.com/broadinstitute/gatk/issues/2216:2574,Deployability,deploy,deploy,2574,"unTool(BaseRecalibratorSpark.java:86); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216
https://github.com/broadinstitute/gatk/issues/2216:2611,Deployability,deploy,deploy,2611,".broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216
https://github.com/broadinstitute/gatk/issues/2216:2683,Deployability,deploy,deploy,2683,"e(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, org.broadinstitute.hellbender.engine.spark.datasources.ReferenceT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216
https://github.com/broadinstitute/gatk/issues/2216:2759,Deployability,deploy,deploy,2759,"rkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource@3c82e6f4); - field (class: org.broadinstitute.hellbender.engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216
https://github.com/broadinstitute/gatk/issues/2216:2830,Deployability,deploy,deploy,2830,"oadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource@3c82e6f4); - field (class: org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource, name: referenceSource, type: interfac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216
https://github.com/broadinstitute/gatk/issues/2216:2899,Deployability,deploy,deploy,2899,"eProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource@3c82e6f4); - field (class: org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource, name: referenceSource, type: interface org.broadinstitute.hellbender.engine.datasources.ReferenceSource); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216
https://github.com/broadinstitute/gatk/issues/2216:3826,Integrability,interface,interface,3826,"); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource@3c82e6f4); - field (class: org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource, name: referenceSource, type: interface org.broadinstitute.hellbender.engine.datasources.ReferenceSource); - object (class org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource, org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource@78eac8f2); - element of array (index: 1); - array (class [Ljava.lang.Object;, size 2); - field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;); - object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases, functionalInterfaceMethod=org/apache/spark/api/java/function/PairFlatMapFunction.call:(Ljava/lang/Object;)Ljava/lang/Iterable;, implementation=invokeStatic org/broadinstitute/hellbender/engine/spark/ShuffleJoinReadsWithRefBases.lambda$addBases$cff38836$1:(Lorg/broadinstitute/hellbender/utils/SerializableFunction;Lorg/broadinstitute/hellbender/engine/datasources/ReferenceMultiSource;Lscala/Tu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216
https://github.com/broadinstitute/gatk/issues/2216:5282,Integrability,interface,interface,5282,"broadinstitute.hellbender.engine.datasources.ReferenceMultiSource, org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource@78eac8f2); - element of array (index: 1); - array (class [Ljava.lang.Object;, size 2); - field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;); - object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases, functionalInterfaceMethod=org/apache/spark/api/java/function/PairFlatMapFunction.call:(Ljava/lang/Object;)Ljava/lang/Iterable;, implementation=invokeStatic org/broadinstitute/hellbender/engine/spark/ShuffleJoinReadsWithRefBases.lambda$addBases$cff38836$1:(Lorg/broadinstitute/hellbender/utils/SerializableFunction;Lorg/broadinstitute/hellbender/engine/datasources/ReferenceMultiSource;Lscala/Tuple2;)Ljava/lang/Iterable;, instantiatedMethodType=(Lscala/Tuple2;)Ljava/lang/Iterable;, numCaptured=2]); - writeReplace data (class: java.lang.invoke.SerializedLambda); - object (class org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases$$Lambda$74/1217660878, org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases$$Lambda$74/1217660878@663d7f24); - field (class: org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1, name: f$5, type: interface org.apache.spark.api.java.function.PairFlatMapFunction); - object (class org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1, <function1>); at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101); at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301); ... 30 more; 16/10/17 16:03:47 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216
https://github.com/broadinstitute/gatk/pull/2218:27,Deployability,integrat,integration,27,- Move command line parser integration of read filters up to GATKTool (https://github.com/broadinstitute/gatk/issues/2175); - Added fromList method to ReadFilter and CountingReadFilter (https://github.com/broadinstitute/gatk/issues/2198); - Minor change/rationalization of naming and implementation of BQSR filter methods to match the rest of the framework; - Made a small change to the implementation of the base read filter class to improve clarity/testability; - Opportunistic removal of extraneous imports in unrelated classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2218
https://github.com/broadinstitute/gatk/pull/2218:27,Integrability,integrat,integration,27,- Move command line parser integration of read filters up to GATKTool (https://github.com/broadinstitute/gatk/issues/2175); - Added fromList method to ReadFilter and CountingReadFilter (https://github.com/broadinstitute/gatk/issues/2198); - Minor change/rationalization of naming and implementation of BQSR filter methods to match the rest of the framework; - Made a small change to the implementation of the base read filter class to improve clarity/testability; - Opportunistic removal of extraneous imports in unrelated classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2218
https://github.com/broadinstitute/gatk/pull/2218:451,Testability,test,testability,451,- Move command line parser integration of read filters up to GATKTool (https://github.com/broadinstitute/gatk/issues/2175); - Added fromList method to ReadFilter and CountingReadFilter (https://github.com/broadinstitute/gatk/issues/2198); - Minor change/rationalization of naming and implementation of BQSR filter methods to match the rest of the framework; - Made a small change to the implementation of the base read filter class to improve clarity/testability; - Opportunistic removal of extraneous imports in unrelated classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2218
https://github.com/broadinstitute/gatk/issues/2219:17,Availability,error,error,17,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:3319,Availability,down,down,3319,"c.uk/ena/cram/md5/%s; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.REFERENCE_FASTA : null; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; > > 15:10:22.794 INFO PrintReadsSpark - Deflater IntelDeflater; > > 15:10:22.794 INFO PrintReadsSpark - Initializing engine; > > 15:10:22.794 INFO PrintReadsSpark - Done initializing engine; > > 15:10:23.180 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; > > 15:10:25.800 INFO PrintReadsSpark - Shutting down engine; > > [October 18, 2016 3:10:25 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.05 minutes.; > > Runtime.totalMemory()=467140608; > > org.broadinstitute.hellbender.exceptions.GATKException: unable to write bam: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:252); > > at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); > > at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:566,Deployability,install,install,566,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:644,Deployability,install,install,644,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:934,Deployability,install,install,934,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:1166,Deployability,pipeline,pipelines,1166,"ellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 1; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CREATE_MD5 : false; > > 15:10:22.79",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:3412,Deployability,pipeline,pipelines,3412,"ZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.REFERENCE_FASTA : null; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; > > 15:10:22.794 INFO PrintReadsSpark - Deflater IntelDeflater; > > 15:10:22.794 INFO PrintReadsSpark - Initializing engine; > > 15:10:22.794 INFO PrintReadsSpark - Done initializing engine; > > 15:10:23.180 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; > > 15:10:25.800 INFO PrintReadsSpark - Shutting down engine; > > [October 18, 2016 3:10:25 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.05 minutes.; > > Runtime.totalMemory()=467140608; > > org.broadinstitute.hellbender.exceptions.GATKException: unable to write bam: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:252); > > at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); > > at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:3842,Deployability,pipeline,pipelines,3842,"ECIMAL; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; > > 15:10:22.794 INFO PrintReadsSpark - Deflater IntelDeflater; > > 15:10:22.794 INFO PrintReadsSpark - Initializing engine; > > 15:10:22.794 INFO PrintReadsSpark - Done initializing engine; > > 15:10:23.180 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; > > 15:10:25.800 INFO PrintReadsSpark - Shutting down engine; > > [October 18, 2016 3:10:25 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.05 minutes.; > > Runtime.totalMemory()=467140608; > > org.broadinstitute.hellbender.exceptions.GATKException: unable to write bam: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:252); > > at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); > > at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); > > at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); > > at org.broadinstitute.hellbender.Main.main(Main.java:92)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:514,Integrability,wrap,wrapper,514,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:854,Performance,load,load,854,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:1062,Performance,load,loaded,1062,"reated for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 1; > > 15:10:22.793 INFO PrintReadsSpar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:3177,Performance,load,load,3177,"rintReadsSpark - Defaults.CUSTOM_READER_FACTORY :; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.REFERENCE_FASTA : null; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; > > 15:10:22.794 INFO PrintReadsSpark - Deflater IntelDeflater; > > 15:10:22.794 INFO PrintReadsSpark - Initializing engine; > > 15:10:22.794 INFO PrintReadsSpark - Done initializing engine; > > 15:10:23.180 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; > > 15:10:25.800 INFO PrintReadsSpark - Shutting down engine; > > [October 18, 2016 3:10:25 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.05 minutes.; > > Runtime.totalMemory()=467140608; > > org.broadinstitute.hellbender.exceptions.GATKException: unable to write bam: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:252); > > at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); > > at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:62,Testability,test,testing,62,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:412,Testability,test,tests,412,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:459,Testability,test,tests,459,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:708,Testability,test,tests,708,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:766,Testability,test,tests,766,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:1224,Testability,test,tests,1224,"e read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 1; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CREATE_MD5 : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CUSTOM_RE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2219:1294,Testability,test,tests,1294,"hub.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 1; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CREATE_MD5 : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CUSTOM_READER_FACTORY :; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.EBI_REFERENC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219
https://github.com/broadinstitute/gatk/issues/2221:114,Availability,ERROR,ERROR,114,## Feature request; ### Tool(s) involved. VariantRecalibrator; ### Description. A user reported he gets an `##### ERROR MESSAGE: No data found.` from VariantRecalibrator when running with -an MQ twice. There should be a check for this and an error message letting the user know he/she has specified the same annotation twice. . [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/33592#Comment_33592). ---,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2221
https://github.com/broadinstitute/gatk/issues/2221:242,Availability,error,error,242,## Feature request; ### Tool(s) involved. VariantRecalibrator; ### Description. A user reported he gets an `##### ERROR MESSAGE: No data found.` from VariantRecalibrator when running with -an MQ twice. There should be a check for this and an error message letting the user know he/she has specified the same annotation twice. . [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/33592#Comment_33592). ---,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2221
https://github.com/broadinstitute/gatk/issues/2221:120,Integrability,MESSAGE,MESSAGE,120,## Feature request; ### Tool(s) involved. VariantRecalibrator; ### Description. A user reported he gets an `##### ERROR MESSAGE: No data found.` from VariantRecalibrator when running with -an MQ twice. There should be a check for this and an error message letting the user know he/she has specified the same annotation twice. . [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/33592#Comment_33592). ---,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2221
https://github.com/broadinstitute/gatk/issues/2221:248,Integrability,message,message,248,## Feature request; ### Tool(s) involved. VariantRecalibrator; ### Description. A user reported he gets an `##### ERROR MESSAGE: No data found.` from VariantRecalibrator when running with -an MQ twice. There should be a check for this and an error message letting the user know he/she has specified the same annotation twice. . [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/33592#Comment_33592). ---,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2221
https://github.com/broadinstitute/gatk/issues/2223:50,Availability,error,error,50,"Depends on #2222, which should be done first. The error message should point users to the tool that allows them to add a sequence dictionary to their VCF.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2223
https://github.com/broadinstitute/gatk/issues/2223:0,Integrability,Depend,Depends,0,"Depends on #2222, which should be done first. The error message should point users to the tool that allows them to add a sequence dictionary to their VCF.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2223
https://github.com/broadinstitute/gatk/issues/2223:56,Integrability,message,message,56,"Depends on #2222, which should be done first. The error message should point users to the tool that allows them to add a sequence dictionary to their VCF.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2223
https://github.com/broadinstitute/gatk/pull/2224:295,Performance,perform,performance,295,"As a result, every walker that takes a BAM as input can now read it from; the cloud directly. Sample use: ./gatk-launch CountReads -I; ""gs://.../CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam""; (...); Tool returned:; 7275701. Note that this CL doesn't add a prefetcher or multithreading, so the; performance is inferior to that of the NIO proof of concept which was; using both. cc: @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224
https://github.com/broadinstitute/gatk/issues/2225:215,Availability,error,error,215,"While using a custom tool that validates arguments (e.g., certain range for an `Integer` parameter), a way of handling it is override `CommandLineProgram.customCommandLineValidation()` and return an String with the error found. Nevertheless, if parsing the arguments throws a `UserException.CommandLineException`, the error is printed with a concrete format (`printDecoratedUserExceptionMessage`) after the usage. This is different from the custom validation, which is printed without any decoration and before the usage. Although this behavior could be desirable, I expect that if my custom validation thrown an `UserException.CommandLineException` it is printed in the same way as other exceptions, and the exception is re-thrown in the same way (for testing purposes, for instance). But the current behaviour just exit without any error printed because the exception is catched in `Main`. A very minor change is include in the `try` block the custom validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2225
https://github.com/broadinstitute/gatk/issues/2225:318,Availability,error,error,318,"While using a custom tool that validates arguments (e.g., certain range for an `Integer` parameter), a way of handling it is override `CommandLineProgram.customCommandLineValidation()` and return an String with the error found. Nevertheless, if parsing the arguments throws a `UserException.CommandLineException`, the error is printed with a concrete format (`printDecoratedUserExceptionMessage`) after the usage. This is different from the custom validation, which is printed without any decoration and before the usage. Although this behavior could be desirable, I expect that if my custom validation thrown an `UserException.CommandLineException` it is printed in the same way as other exceptions, and the exception is re-thrown in the same way (for testing purposes, for instance). But the current behaviour just exit without any error printed because the exception is catched in `Main`. A very minor change is include in the `try` block the custom validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2225
https://github.com/broadinstitute/gatk/issues/2225:834,Availability,error,error,834,"While using a custom tool that validates arguments (e.g., certain range for an `Integer` parameter), a way of handling it is override `CommandLineProgram.customCommandLineValidation()` and return an String with the error found. Nevertheless, if parsing the arguments throws a `UserException.CommandLineException`, the error is printed with a concrete format (`printDecoratedUserExceptionMessage`) after the usage. This is different from the custom validation, which is printed without any decoration and before the usage. Although this behavior could be desirable, I expect that if my custom validation thrown an `UserException.CommandLineException` it is printed in the same way as other exceptions, and the exception is re-thrown in the same way (for testing purposes, for instance). But the current behaviour just exit without any error printed because the exception is catched in `Main`. A very minor change is include in the `try` block the custom validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2225
https://github.com/broadinstitute/gatk/issues/2225:31,Security,validat,validates,31,"While using a custom tool that validates arguments (e.g., certain range for an `Integer` parameter), a way of handling it is override `CommandLineProgram.customCommandLineValidation()` and return an String with the error found. Nevertheless, if parsing the arguments throws a `UserException.CommandLineException`, the error is printed with a concrete format (`printDecoratedUserExceptionMessage`) after the usage. This is different from the custom validation, which is printed without any decoration and before the usage. Although this behavior could be desirable, I expect that if my custom validation thrown an `UserException.CommandLineException` it is printed in the same way as other exceptions, and the exception is re-thrown in the same way (for testing purposes, for instance). But the current behaviour just exit without any error printed because the exception is catched in `Main`. A very minor change is include in the `try` block the custom validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2225
https://github.com/broadinstitute/gatk/issues/2225:448,Security,validat,validation,448,"While using a custom tool that validates arguments (e.g., certain range for an `Integer` parameter), a way of handling it is override `CommandLineProgram.customCommandLineValidation()` and return an String with the error found. Nevertheless, if parsing the arguments throws a `UserException.CommandLineException`, the error is printed with a concrete format (`printDecoratedUserExceptionMessage`) after the usage. This is different from the custom validation, which is printed without any decoration and before the usage. Although this behavior could be desirable, I expect that if my custom validation thrown an `UserException.CommandLineException` it is printed in the same way as other exceptions, and the exception is re-thrown in the same way (for testing purposes, for instance). But the current behaviour just exit without any error printed because the exception is catched in `Main`. A very minor change is include in the `try` block the custom validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2225
https://github.com/broadinstitute/gatk/issues/2225:592,Security,validat,validation,592,"While using a custom tool that validates arguments (e.g., certain range for an `Integer` parameter), a way of handling it is override `CommandLineProgram.customCommandLineValidation()` and return an String with the error found. Nevertheless, if parsing the arguments throws a `UserException.CommandLineException`, the error is printed with a concrete format (`printDecoratedUserExceptionMessage`) after the usage. This is different from the custom validation, which is printed without any decoration and before the usage. Although this behavior could be desirable, I expect that if my custom validation thrown an `UserException.CommandLineException` it is printed in the same way as other exceptions, and the exception is re-thrown in the same way (for testing purposes, for instance). But the current behaviour just exit without any error printed because the exception is catched in `Main`. A very minor change is include in the `try` block the custom validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2225
https://github.com/broadinstitute/gatk/issues/2225:953,Security,validat,validation,953,"While using a custom tool that validates arguments (e.g., certain range for an `Integer` parameter), a way of handling it is override `CommandLineProgram.customCommandLineValidation()` and return an String with the error found. Nevertheless, if parsing the arguments throws a `UserException.CommandLineException`, the error is printed with a concrete format (`printDecoratedUserExceptionMessage`) after the usage. This is different from the custom validation, which is printed without any decoration and before the usage. Although this behavior could be desirable, I expect that if my custom validation thrown an `UserException.CommandLineException` it is printed in the same way as other exceptions, and the exception is re-thrown in the same way (for testing purposes, for instance). But the current behaviour just exit without any error printed because the exception is catched in `Main`. A very minor change is include in the `try` block the custom validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2225
https://github.com/broadinstitute/gatk/issues/2225:753,Testability,test,testing,753,"While using a custom tool that validates arguments (e.g., certain range for an `Integer` parameter), a way of handling it is override `CommandLineProgram.customCommandLineValidation()` and return an String with the error found. Nevertheless, if parsing the arguments throws a `UserException.CommandLineException`, the error is printed with a concrete format (`printDecoratedUserExceptionMessage`) after the usage. This is different from the custom validation, which is printed without any decoration and before the usage. Although this behavior could be desirable, I expect that if my custom validation thrown an `UserException.CommandLineException` it is printed in the same way as other exceptions, and the exception is re-thrown in the same way (for testing purposes, for instance). But the current behaviour just exit without any error printed because the exception is catched in `Main`. A very minor change is include in the `try` block the custom validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2225
https://github.com/broadinstitute/gatk/pull/2226:17,Security,validat,validation,17,- Include custom validation in try-catch block to handle custom `UserException.CommandLineException` (solves #2225); - Unchanged behaviour for the rest of the tools.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226
https://github.com/broadinstitute/gatk/pull/2227:61,Energy Efficiency,reduce,reduce,61,this will enable use of composite gradle builds which should reduce development pain,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2227
https://github.com/broadinstitute/gatk/pull/2229:293,Availability,error,error,293,"The quality score sum (QSS) format field reports the sum of base qualities for ref and alt allele, and its correct format field count type is 'R' (see 1.2.2 in the [vcf spec](http://samtools.github.io/hts-specs/VCFv4.2.pdf)). We used to use the wrong count type of 1 and that caused a parsing error in the dream challenge evaluation script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2229
https://github.com/broadinstitute/gatk/issues/2233:133,Availability,down,down,133,"## Feature request; ### Tool(s) involved. Engine; ### Description. An ENUM with several levels of minimalism. Main use case: keeping down the lines of SQ. As put by @eitanbanks . > One thing that option would do would be to _not_ print out all of the reference contigs. While it's not terrible in b37, you should know that hg38 has thousands upon thousands of contigs and it's such a pain to have them all in the header. Simplest way to do this imho (and I would advocate making it the default GATK behavior): write to VCF header SQ lines representing only the contigs for which we have calls. More discussion needed to define exactly what options should distinguished.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2233
https://github.com/broadinstitute/gatk/issues/2233:421,Usability,Simpl,Simplest,421,"## Feature request; ### Tool(s) involved. Engine; ### Description. An ENUM with several levels of minimalism. Main use case: keeping down the lines of SQ. As put by @eitanbanks . > One thing that option would do would be to _not_ print out all of the reference contigs. While it's not terrible in b37, you should know that hg38 has thousands upon thousands of contigs and it's such a pain to have them all in the header. Simplest way to do this imho (and I would advocate making it the default GATK behavior): write to VCF header SQ lines representing only the contigs for which we have calls. More discussion needed to define exactly what options should distinguished.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2233
https://github.com/broadinstitute/gatk/pull/2235:120,Testability,test,test,120,This fixes pathologies that @ldgauthier noticed in a subset of ExAC. @vruano can you take a look? Note that the changed test files only change VQSLOD here and there by 0.03 or so and the new values are probably more accurate.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2235
https://github.com/broadinstitute/gatk/pull/2237:693,Testability,test,tests,693,"@cwhelan @tedsharpe please review. There are two tools here:. 1) PathSeqKmerSpark - builds kmer library as a hopscotch set from a given reference (done); 2) PathSeqFilterSpark - preprocessing / human read filtering tool (work in progress, currently has read preprocessing steps implemented). Made modifications to the SV kmer classes: . 1) SVKmerSmall class uses only 1 long (up to 31 bases), largely copied from SVKmer; 2) SVKmer and SVKmerSmall are both derived from SVKmerBase; 3) SVKmerizer uses generics so it can work with either type, moved some static functions to SVUtils. There are a bunch of new read filters and transformers for the filtering steps, including symmetric DUST. Unit tests included.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2237
https://github.com/broadinstitute/gatk/issues/2238:224,Energy Efficiency,adapt,adaptor,224,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238
https://github.com/broadinstitute/gatk/issues/2238:570,Energy Efficiency,adapt,adapter,570,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238
https://github.com/broadinstitute/gatk/issues/2238:732,Energy Efficiency,adapt,adapter,732,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238
https://github.com/broadinstitute/gatk/issues/2238:570,Integrability,adapter,adapter,570,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238
https://github.com/broadinstitute/gatk/issues/2238:732,Integrability,adapter,adapter,732,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238
https://github.com/broadinstitute/gatk/issues/2238:224,Modifiability,adapt,adaptor,224,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238
https://github.com/broadinstitute/gatk/issues/2238:570,Modifiability,adapt,adapter,570,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238
https://github.com/broadinstitute/gatk/issues/2238:732,Modifiability,adapt,adapter,732,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238
https://github.com/broadinstitute/gatk/pull/2239:461,Availability,failure,failures,461,"Two annotations, allele depth and total depth, consider whether reads are informative relative to the alleles in the output `VariantContext`, which is in general a subset of the alleles contains in the `ReadLikelihoods`. In PR #2185 I overlooked this and forgot to subset the likelihoods' alleles to those of the vc, which was the previous behavior (see the diff from that PR for the two annotations in this PR). This PR duplicates the old behavior. This fixes failures in the HaplotypeCaller integration tests introduced by the previous PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239
https://github.com/broadinstitute/gatk/pull/2239:493,Deployability,integrat,integration,493,"Two annotations, allele depth and total depth, consider whether reads are informative relative to the alleles in the output `VariantContext`, which is in general a subset of the alleles contains in the `ReadLikelihoods`. In PR #2185 I overlooked this and forgot to subset the likelihoods' alleles to those of the vc, which was the previous behavior (see the diff from that PR for the two annotations in this PR). This PR duplicates the old behavior. This fixes failures in the HaplotypeCaller integration tests introduced by the previous PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239
https://github.com/broadinstitute/gatk/pull/2239:493,Integrability,integrat,integration,493,"Two annotations, allele depth and total depth, consider whether reads are informative relative to the alleles in the output `VariantContext`, which is in general a subset of the alleles contains in the `ReadLikelihoods`. In PR #2185 I overlooked this and forgot to subset the likelihoods' alleles to those of the vc, which was the previous behavior (see the diff from that PR for the two annotations in this PR). This PR duplicates the old behavior. This fixes failures in the HaplotypeCaller integration tests introduced by the previous PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239
https://github.com/broadinstitute/gatk/pull/2239:505,Testability,test,tests,505,"Two annotations, allele depth and total depth, consider whether reads are informative relative to the alleles in the output `VariantContext`, which is in general a subset of the alleles contains in the `ReadLikelihoods`. In PR #2185 I overlooked this and forgot to subset the likelihoods' alleles to those of the vc, which was the previous behavior (see the diff from that PR for the two annotations in this PR). This PR duplicates the old behavior. This fixes failures in the HaplotypeCaller integration tests introduced by the previous PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239
https://github.com/broadinstitute/gatk/pull/2244:2,Deployability,Update,Updates,2,* Updates htsjdk to 2.7.0; * Use `Index.write(File)` for write the index independently of type; * Removed unused imports,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2244
https://github.com/broadinstitute/gatk/pull/2246:587,Modifiability,extend,extends,587,"* No change in the actual behaviour of GATK; * Several protected methods for customize some parameters of the toolkit:; - `String getCommandLineName()`: returns the name from the command line toolkit; - `void handleResult(Object)`: handle the output of the tool; * `main(String[] args)` code moved to a non-static final method (`mainEntry(String[])`) called inside the static one. This allow that the changes from override the customization methods could be apply with custom instances. With this changes a custom main could be simplify a lot. As an example:. ```java; public class Main extends org.broadinstitute.hellbender.Main {. @Override; protected String getCommandLineName() { return ""MyCustomName""; }. @Override; protected List<String> getPackageList() {; return Arrays.asList(""org.custom.tools"");; }. @Override; protected List<Class<? extends CommandLineProgram>> getClassList() {; return Arrays.asList(CreateSequenceDictionary.class, IndexFeatureFile.class);; }. public static void main(final String[] args) {; new Main().mainEntry(args);; }. }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2246
https://github.com/broadinstitute/gatk/pull/2246:844,Modifiability,extend,extends,844,"* No change in the actual behaviour of GATK; * Several protected methods for customize some parameters of the toolkit:; - `String getCommandLineName()`: returns the name from the command line toolkit; - `void handleResult(Object)`: handle the output of the tool; * `main(String[] args)` code moved to a non-static final method (`mainEntry(String[])`) called inside the static one. This allow that the changes from override the customization methods could be apply with custom instances. With this changes a custom main could be simplify a lot. As an example:. ```java; public class Main extends org.broadinstitute.hellbender.Main {. @Override; protected String getCommandLineName() { return ""MyCustomName""; }. @Override; protected List<String> getPackageList() {; return Arrays.asList(""org.custom.tools"");; }. @Override; protected List<Class<? extends CommandLineProgram>> getClassList() {; return Arrays.asList(CreateSequenceDictionary.class, IndexFeatureFile.class);; }. public static void main(final String[] args) {; new Main().mainEntry(args);; }. }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2246
https://github.com/broadinstitute/gatk/pull/2246:528,Usability,simpl,simplify,528,"* No change in the actual behaviour of GATK; * Several protected methods for customize some parameters of the toolkit:; - `String getCommandLineName()`: returns the name from the command line toolkit; - `void handleResult(Object)`: handle the output of the tool; * `main(String[] args)` code moved to a non-static final method (`mainEntry(String[])`) called inside the static one. This allow that the changes from override the customization methods could be apply with custom instances. With this changes a custom main could be simplify a lot. As an example:. ```java; public class Main extends org.broadinstitute.hellbender.Main {. @Override; protected String getCommandLineName() { return ""MyCustomName""; }. @Override; protected List<String> getPackageList() {; return Arrays.asList(""org.custom.tools"");; }. @Override; protected List<Class<? extends CommandLineProgram>> getClassList() {; return Arrays.asList(CreateSequenceDictionary.class, IndexFeatureFile.class);; }. public static void main(final String[] args) {; new Main().mainEntry(args);; }. }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2246
https://github.com/broadinstitute/gatk/pull/2247:2,Deployability,Update,Update,2,"* Update htsjdk to 2.7.0; * Remove usage of deprecated `SAMRecordUtil`; * Remove usage of deprecated `IndexFactory.writeIndex(index, indexFile)`; * Cleaning `IndexFeatureFile`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2247
https://github.com/broadinstitute/gatk/pull/2248:80,Security,authenticat,authentication,80,they seem to have changed the gcloud cli to use --key-file instead of; --key in authentication. this was causing travis to fail on the cloud tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2248
https://github.com/broadinstitute/gatk/pull/2248:141,Testability,test,tests,141,they seem to have changed the gcloud cli to use --key-file instead of; --key in authentication. this was causing travis to fail on the cloud tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2248
https://github.com/broadinstitute/gatk/pull/2249:348,Testability,test,test,348,"`FragmentCollection.create(ReadPileup)` assumes a `ReadPileup` sorted by start position and thus it throws an exception (#2245) while trying to fix a multi-sample `ReadPileup` constructed with an stratified one (like in LIBS). For solving the issue, I just implemented a simple fix from the constructor (sorting the stratified pielup) and a simple test for check if #2245 is solved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2249
https://github.com/broadinstitute/gatk/pull/2249:271,Usability,simpl,simple,271,"`FragmentCollection.create(ReadPileup)` assumes a `ReadPileup` sorted by start position and thus it throws an exception (#2245) while trying to fix a multi-sample `ReadPileup` constructed with an stratified one (like in LIBS). For solving the issue, I just implemented a simple fix from the constructor (sorting the stratified pielup) and a simple test for check if #2245 is solved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2249
https://github.com/broadinstitute/gatk/pull/2249:341,Usability,simpl,simple,341,"`FragmentCollection.create(ReadPileup)` assumes a `ReadPileup` sorted by start position and thus it throws an exception (#2245) while trying to fix a multi-sample `ReadPileup` constructed with an stratified one (like in LIBS). For solving the issue, I just implemented a simple fix from the constructor (sorting the stratified pielup) and a simple test for check if #2245 is solved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2249
https://github.com/broadinstitute/gatk/issues/2250:170,Performance,perform,performance,170,"It seems like the default shard padding of 1000 used by clients of the new `SparkSharder` is likely to be larger than we need, and reducing it could have a big effect on performance (particularly now that we overestimate the read extent of each partition by roughly the amount of padding). Let's profile and see what the performance impact of decreasing the shard padding is on real data, and come up with a default value more tailored to typical read lengths.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2250
https://github.com/broadinstitute/gatk/issues/2250:321,Performance,perform,performance,321,"It seems like the default shard padding of 1000 used by clients of the new `SparkSharder` is likely to be larger than we need, and reducing it could have a big effect on performance (particularly now that we overestimate the read extent of each partition by roughly the amount of padding). Let's profile and see what the performance impact of decreasing the shard padding is on real data, and come up with a default value more tailored to typical read lengths.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2250
https://github.com/broadinstitute/gatk/issues/2251:185,Availability,down,downsampling,185,"We need to make sure the `SparkSharder` can handle coverage spikes. We should profile to see how it performs in regions of high coverage, and potentially come up with a partition-level downsampling strategy that could discard partitions with extraneous coverage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2251
https://github.com/broadinstitute/gatk/issues/2251:100,Performance,perform,performs,100,"We need to make sure the `SparkSharder` can handle coverage spikes. We should profile to see how it performs in regions of high coverage, and potentially come up with a partition-level downsampling strategy that could discard partitions with extraneous coverage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2251
https://github.com/broadinstitute/gatk/issues/2252:91,Performance,perform,performance,91,"We should test with a large interval list, like a whole-exome interval list, and make sure performance doesn't get killed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2252
https://github.com/broadinstitute/gatk/issues/2252:10,Testability,test,test,10,"We should test with a large interval list, like a whole-exome interval list, and make sure performance doesn't get killed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2252
https://github.com/broadinstitute/gatk/issues/2254:229,Modifiability,config,configured,229,"We probably can't remove `ReferenceWindowFunction`, as it's being used by tools in gatk-protected. How can we guarantee that `JoinStrategy.OVERLAPS_PARTITIONER` will provide at least as much reference context as requested by the configured `ReferenceWindowFunction`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2254
https://github.com/broadinstitute/gatk/issues/2255:125,Performance,perform,performs,125,"First, question: what must we do to convince ourselves to pull the plug on the old QUAL? In every test so far, the new model performs equally well on biallelics and better on multiallelics. We will eliminate about 6000 lines of code, many future bugs, and GATK workshop slides that nobody wants to present. Assuming that we are soon convinced, the ticket is pretty simple: delete every implementation of `AFCalculator` and replace anything that calls for an abstract `AFCalculator` with the concrete class `AlleleFrequencyCalculator`. Then clean up hacky parts of `AlleleFrequencyCalculator` that were put in to implement `AFCalculator`. Finally, delete all the ancillary classes like `StateTracker` that comprised the `AFCalculator`'s military-industrial complex.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255
https://github.com/broadinstitute/gatk/issues/2255:98,Testability,test,test,98,"First, question: what must we do to convince ourselves to pull the plug on the old QUAL? In every test so far, the new model performs equally well on biallelics and better on multiallelics. We will eliminate about 6000 lines of code, many future bugs, and GATK workshop slides that nobody wants to present. Assuming that we are soon convinced, the ticket is pretty simple: delete every implementation of `AFCalculator` and replace anything that calls for an abstract `AFCalculator` with the concrete class `AlleleFrequencyCalculator`. Then clean up hacky parts of `AlleleFrequencyCalculator` that were put in to implement `AFCalculator`. Finally, delete all the ancillary classes like `StateTracker` that comprised the `AFCalculator`'s military-industrial complex.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255
https://github.com/broadinstitute/gatk/issues/2255:365,Usability,simpl,simple,365,"First, question: what must we do to convince ourselves to pull the plug on the old QUAL? In every test so far, the new model performs equally well on biallelics and better on multiallelics. We will eliminate about 6000 lines of code, many future bugs, and GATK workshop slides that nobody wants to present. Assuming that we are soon convinced, the ticket is pretty simple: delete every implementation of `AFCalculator` and replace anything that calls for an abstract `AFCalculator` with the concrete class `AlleleFrequencyCalculator`. Then clean up hacky parts of `AlleleFrequencyCalculator` that were put in to implement `AFCalculator`. Finally, delete all the ancillary classes like `StateTracker` that comprised the `AFCalculator`'s military-industrial complex.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255
https://github.com/broadinstitute/gatk/pull/2258:0,Modifiability,Refactor,Refactoring,0,"Refactoring of the structs and utilities involved calling simple inversions.; This helps preparing for calling simple insertions and deletions in SV.; Most changes are simple changes, no changes are made to the algorithm itself.; A simple fix of orginal code in `AlignmentRegion` was put in.; Major re-engineering of `getVariantContextForBreakpointAlleleAlignmentList()` in caller was done and explained in the temporary comment that will be removed after review is done. @cwhelan would you please review? Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258
https://github.com/broadinstitute/gatk/pull/2258:58,Usability,simpl,simple,58,"Refactoring of the structs and utilities involved calling simple inversions.; This helps preparing for calling simple insertions and deletions in SV.; Most changes are simple changes, no changes are made to the algorithm itself.; A simple fix of orginal code in `AlignmentRegion` was put in.; Major re-engineering of `getVariantContextForBreakpointAlleleAlignmentList()` in caller was done and explained in the temporary comment that will be removed after review is done. @cwhelan would you please review? Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258
https://github.com/broadinstitute/gatk/pull/2258:111,Usability,simpl,simple,111,"Refactoring of the structs and utilities involved calling simple inversions.; This helps preparing for calling simple insertions and deletions in SV.; Most changes are simple changes, no changes are made to the algorithm itself.; A simple fix of orginal code in `AlignmentRegion` was put in.; Major re-engineering of `getVariantContextForBreakpointAlleleAlignmentList()` in caller was done and explained in the temporary comment that will be removed after review is done. @cwhelan would you please review? Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258
https://github.com/broadinstitute/gatk/pull/2258:168,Usability,simpl,simple,168,"Refactoring of the structs and utilities involved calling simple inversions.; This helps preparing for calling simple insertions and deletions in SV.; Most changes are simple changes, no changes are made to the algorithm itself.; A simple fix of orginal code in `AlignmentRegion` was put in.; Major re-engineering of `getVariantContextForBreakpointAlleleAlignmentList()` in caller was done and explained in the temporary comment that will be removed after review is done. @cwhelan would you please review? Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258
https://github.com/broadinstitute/gatk/pull/2258:232,Usability,simpl,simple,232,"Refactoring of the structs and utilities involved calling simple inversions.; This helps preparing for calling simple insertions and deletions in SV.; Most changes are simple changes, no changes are made to the algorithm itself.; A simple fix of orginal code in `AlignmentRegion` was put in.; Major re-engineering of `getVariantContextForBreakpointAlleleAlignmentList()` in caller was done and explained in the temporary comment that will be removed after review is done. @cwhelan would you please review? Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258
https://github.com/broadinstitute/gatk/pull/2259:0,Deployability,Update,Update,0,"Update to GKL 0.3.1 which provides the following features compared to GKL 0.1.2 currently used by GATK:. 1. Improved performance for Level 1 compression.; 1. OpenMP AVX PairHMM with fallback to non-OpenMP support logic in GATK.; 1. Individual shared library objects for each native binding.; 1. Flush-to-zero get/set support. Updated `VectorPairHMMUnitTest` to test all supported implementations of `VectorLoglessPairHMM` and skip the test if no implementations are supported. **Note**; The default number of threads used by OpenMP AVX PairHMM is set to 1, and the command line argument to change the number of threads is not connected yet (see #1946). So, the OpenMP and non-OpenMP PairHMM currently have the same performance. Resolves #1819; First step for #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259
https://github.com/broadinstitute/gatk/pull/2259:326,Deployability,Update,Updated,326,"Update to GKL 0.3.1 which provides the following features compared to GKL 0.1.2 currently used by GATK:. 1. Improved performance for Level 1 compression.; 1. OpenMP AVX PairHMM with fallback to non-OpenMP support logic in GATK.; 1. Individual shared library objects for each native binding.; 1. Flush-to-zero get/set support. Updated `VectorPairHMMUnitTest` to test all supported implementations of `VectorLoglessPairHMM` and skip the test if no implementations are supported. **Note**; The default number of threads used by OpenMP AVX PairHMM is set to 1, and the command line argument to change the number of threads is not connected yet (see #1946). So, the OpenMP and non-OpenMP PairHMM currently have the same performance. Resolves #1819; First step for #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259
https://github.com/broadinstitute/gatk/pull/2259:117,Performance,perform,performance,117,"Update to GKL 0.3.1 which provides the following features compared to GKL 0.1.2 currently used by GATK:. 1. Improved performance for Level 1 compression.; 1. OpenMP AVX PairHMM with fallback to non-OpenMP support logic in GATK.; 1. Individual shared library objects for each native binding.; 1. Flush-to-zero get/set support. Updated `VectorPairHMMUnitTest` to test all supported implementations of `VectorLoglessPairHMM` and skip the test if no implementations are supported. **Note**; The default number of threads used by OpenMP AVX PairHMM is set to 1, and the command line argument to change the number of threads is not connected yet (see #1946). So, the OpenMP and non-OpenMP PairHMM currently have the same performance. Resolves #1819; First step for #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259
https://github.com/broadinstitute/gatk/pull/2259:715,Performance,perform,performance,715,"Update to GKL 0.3.1 which provides the following features compared to GKL 0.1.2 currently used by GATK:. 1. Improved performance for Level 1 compression.; 1. OpenMP AVX PairHMM with fallback to non-OpenMP support logic in GATK.; 1. Individual shared library objects for each native binding.; 1. Flush-to-zero get/set support. Updated `VectorPairHMMUnitTest` to test all supported implementations of `VectorLoglessPairHMM` and skip the test if no implementations are supported. **Note**; The default number of threads used by OpenMP AVX PairHMM is set to 1, and the command line argument to change the number of threads is not connected yet (see #1946). So, the OpenMP and non-OpenMP PairHMM currently have the same performance. Resolves #1819; First step for #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259
https://github.com/broadinstitute/gatk/pull/2259:213,Testability,log,logic,213,"Update to GKL 0.3.1 which provides the following features compared to GKL 0.1.2 currently used by GATK:. 1. Improved performance for Level 1 compression.; 1. OpenMP AVX PairHMM with fallback to non-OpenMP support logic in GATK.; 1. Individual shared library objects for each native binding.; 1. Flush-to-zero get/set support. Updated `VectorPairHMMUnitTest` to test all supported implementations of `VectorLoglessPairHMM` and skip the test if no implementations are supported. **Note**; The default number of threads used by OpenMP AVX PairHMM is set to 1, and the command line argument to change the number of threads is not connected yet (see #1946). So, the OpenMP and non-OpenMP PairHMM currently have the same performance. Resolves #1819; First step for #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259
https://github.com/broadinstitute/gatk/pull/2259:361,Testability,test,test,361,"Update to GKL 0.3.1 which provides the following features compared to GKL 0.1.2 currently used by GATK:. 1. Improved performance for Level 1 compression.; 1. OpenMP AVX PairHMM with fallback to non-OpenMP support logic in GATK.; 1. Individual shared library objects for each native binding.; 1. Flush-to-zero get/set support. Updated `VectorPairHMMUnitTest` to test all supported implementations of `VectorLoglessPairHMM` and skip the test if no implementations are supported. **Note**; The default number of threads used by OpenMP AVX PairHMM is set to 1, and the command line argument to change the number of threads is not connected yet (see #1946). So, the OpenMP and non-OpenMP PairHMM currently have the same performance. Resolves #1819; First step for #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259
https://github.com/broadinstitute/gatk/pull/2259:435,Testability,test,test,435,"Update to GKL 0.3.1 which provides the following features compared to GKL 0.1.2 currently used by GATK:. 1. Improved performance for Level 1 compression.; 1. OpenMP AVX PairHMM with fallback to non-OpenMP support logic in GATK.; 1. Individual shared library objects for each native binding.; 1. Flush-to-zero get/set support. Updated `VectorPairHMMUnitTest` to test all supported implementations of `VectorLoglessPairHMM` and skip the test if no implementations are supported. **Note**; The default number of threads used by OpenMP AVX PairHMM is set to 1, and the command line argument to change the number of threads is not connected yet (see #1946). So, the OpenMP and non-OpenMP PairHMM currently have the same performance. Resolves #1819; First step for #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259
https://github.com/broadinstitute/gatk/pull/2260:198,Deployability,patch,patch,198,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260
https://github.com/broadinstitute/gatk/pull/2260:255,Modifiability,variab,variable,255,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260
https://github.com/broadinstitute/gatk/pull/2260:303,Modifiability,refactor,refactored,303,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260
https://github.com/broadinstitute/gatk/pull/2260:318,Modifiability,variab,variable,318,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260
https://github.com/broadinstitute/gatk/pull/2260:151,Security,access,access,151,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260
https://github.com/broadinstitute/gatk/pull/2260:179,Usability,simpl,simple,179,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260
https://github.com/broadinstitute/gatk/pull/2261:7,Deployability,patch,patch,7,Simple patch to improve the `Main` usage in the same direction as previous PRs to finer control by API user:. * Added `handleNonUserException(final Exception exception)` to handle custom exceptions.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2261
https://github.com/broadinstitute/gatk/pull/2261:0,Usability,Simpl,Simple,0,Simple patch to improve the `Main` usage in the same direction as previous PRs to finer control by API user:. * Added `handleNonUserException(final Exception exception)` to handle custom exceptions.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2261
https://github.com/broadinstitute/gatk/pull/2262:67,Testability,test,test,67,removing MainTest.printReadsVersionTest because it was causing the test; suite to exit early. it wasn't a very good test to begin with so it's not much of a loss,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2262
https://github.com/broadinstitute/gatk/pull/2262:116,Testability,test,test,116,removing MainTest.printReadsVersionTest because it was causing the test; suite to exit early. it wasn't a very good test to begin with so it's not much of a loss,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2262
https://github.com/broadinstitute/gatk/pull/2263:200,Deployability,deploy,deploy-mode,200,2 changes to gatk-launch; - gcloud now requires arguments to the spark job to be separated from arguments to gcloud by `--`; - `--sparkMaster yarn-client` has been replaced with `--sparkMaster yarn --deploy-mode client`; this only requires the sparkMaster to be changed in gatk-launch gcs because the client mode is set by gcloud,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2263
https://github.com/broadinstitute/gatk/issues/2265:408,Availability,error,error,408,"@chandrans commented on [Mon May 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382). ## Feature request. A user has reported ASEReadCounter counts homozygous sites as having potential ASE. This should not be the case, as only heterozygous sites should be counted. The user should input a VCF sites file with only heterozygous sites, but sometimes that doesn't happen, so there should be an error message when he or she does not. ; ### Tool(s) involved. ASEReadCounter; ### Description. There needs to be an error message telling the user he/she needs to subset the sites VCF to only heterozygous sites. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382#issuecomment-260490723). May want to move this to GATK4, waiting for @meganshand to opine. . ---. @meganshand commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382#issuecomment-260648100). Yeah, let's move it to GATK4 and incorporate porting it in the first place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2265
https://github.com/broadinstitute/gatk/issues/2265:525,Availability,error,error,525,"@chandrans commented on [Mon May 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382). ## Feature request. A user has reported ASEReadCounter counts homozygous sites as having potential ASE. This should not be the case, as only heterozygous sites should be counted. The user should input a VCF sites file with only heterozygous sites, but sometimes that doesn't happen, so there should be an error message when he or she does not. ; ### Tool(s) involved. ASEReadCounter; ### Description. There needs to be an error message telling the user he/she needs to subset the sites VCF to only heterozygous sites. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382#issuecomment-260490723). May want to move this to GATK4, waiting for @meganshand to opine. . ---. @meganshand commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382#issuecomment-260648100). Yeah, let's move it to GATK4 and incorporate porting it in the first place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2265
https://github.com/broadinstitute/gatk/issues/2265:414,Integrability,message,message,414,"@chandrans commented on [Mon May 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382). ## Feature request. A user has reported ASEReadCounter counts homozygous sites as having potential ASE. This should not be the case, as only heterozygous sites should be counted. The user should input a VCF sites file with only heterozygous sites, but sometimes that doesn't happen, so there should be an error message when he or she does not. ; ### Tool(s) involved. ASEReadCounter; ### Description. There needs to be an error message telling the user he/she needs to subset the sites VCF to only heterozygous sites. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382#issuecomment-260490723). May want to move this to GATK4, waiting for @meganshand to opine. . ---. @meganshand commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382#issuecomment-260648100). Yeah, let's move it to GATK4 and incorporate porting it in the first place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2265
https://github.com/broadinstitute/gatk/issues/2265:531,Integrability,message,message,531,"@chandrans commented on [Mon May 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382). ## Feature request. A user has reported ASEReadCounter counts homozygous sites as having potential ASE. This should not be the case, as only heterozygous sites should be counted. The user should input a VCF sites file with only heterozygous sites, but sometimes that doesn't happen, so there should be an error message when he or she does not. ; ### Tool(s) involved. ASEReadCounter; ### Description. There needs to be an error message telling the user he/she needs to subset the sites VCF to only heterozygous sites. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382#issuecomment-260490723). May want to move this to GATK4, waiting for @meganshand to opine. . ---. @meganshand commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1382#issuecomment-260648100). Yeah, let's move it to GATK4 and incorporate porting it in the first place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2265
https://github.com/broadinstitute/gatk/issues/2268:154,Availability,echo,echo,154,"+ blaunch -no-wait -z hpcgenomicn24 /spark-1.6.2-bin-hadoop2.6//bin/spark-class org.apache.spark.deploy.worker.Worker spark://hpcgenomicn24:6311 -c 16; + echo --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:11794,Availability,ERROR,ERROR,11794,":25:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 71.1 KB, free 331.9 KB); 16/11/16 23:25:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.0 KB, free 357.9 KB); 16/11/16 23:25:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:40833 (size: 26.0 KB, free: 1247.2 MB); 16/11/16 23:25:11 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006; 16/11/16 23:25:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:273); 16/11/16 23:25:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks; 16/11/16 23:25:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,NODE_LOCAL, 1951 bytes); 16/11/16 23:25:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1); 16/11/16 23:25:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks; 16/11/16 23:25:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms; 16/11/16 23:25:11 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1); java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrRead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:13222,Availability,ERROR,ERROR,13222,"$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:16211,Availability,ERROR,ERROR,16211,"fun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/23766",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:16992,Availability,failure,failure,16992,"ava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:17049,Availability,failure,failure,17049,"Executor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:25840,Availability,down,down,25840,ly(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkUI: Stopped Spark web UI at http://172.32.65.22:4040; 16/11/16 23:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/11/16 23:25:11 INFO MemoryStore: MemoryStore cleared; 16/11/16 23:25:11 INFO BlockManager: BlockManager stopped; 16/11/16 23:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/11/16 23:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/11/16 23:25:11 INFO SparkContext: Successfully stopped SparkContext; 16/11/16 23:25:11 INFO ShutdownHookManager: Shutdown hook called; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8/httpd-7370bdc2-eb41-46d1-9512-2f387b972cac; 16/11/16 23:25:11 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; + /spark-1.6.2-bin-hadoop2.6//sbin/stop-master.sh,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:97,Deployability,deploy,deploy,97,"+ blaunch -no-wait -z hpcgenomicn24 /spark-1.6.2-bin-hadoop2.6//bin/spark-class org.apache.spark.deploy.worker.Worker spark://hpcgenomicn24:6311 -c 16; + echo --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:1510,Deployability,pipeline,pipelines,1510," spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17; Version: Version:4.alpha.2-98-g8fa5092-SNAPSHOT; 23:25:07.556 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 23:25:07.556 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 5; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREATE_MD5 : false; 23:25:07.556 INFO PrintReadsSpark ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:16839,Deployability,pipeline,pipelines,16839,"66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:21943,Deployability,pipeline,pipelines,21943,rk.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:985); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:985); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:800); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.saveAsShardedHadoopFiles(ReadsSparkSink.java:202); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:229); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:23107,Deployability,deploy,deploy,23107,roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:23144,Deployability,deploy,deploy,23144,mmandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:23217,Deployability,deploy,deploy,23217,.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:23294,Deployability,deploy,deploy,23294,ram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:23366,Deployability,deploy,deploy,23366,ram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:23436,Deployability,deploy,deploy,23436,oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:12809,Energy Efficiency,schedul,scheduler,12809,"ption in task 0.0 in stage 1.0 (TID 1); java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:12881,Energy Efficiency,schedul,scheduler,12881,"rg.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:14289,Energy Efficiency,schedul,scheduler,14289,"[Executor task launch worker-0,5,main]; java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkContext: Invoking stop() from shutdown hook; 16/11/16 23:25:11 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:14361,Energy Efficiency,schedul,scheduler,14361,"rg.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkContext: Invoking stop() from shutdown hook; 16/11/16 23:25:11 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.sca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:15798,Energy Efficiency,schedul,scheduler,15798,"sk 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:15870,Energy Efficiency,schedul,scheduler,15870,"rg.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18060,Energy Efficiency,schedul,scheduler,18060,"sk 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18132,Energy Efficiency,schedul,scheduler,18132,rg.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSchedu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18496,Energy Efficiency,schedul,scheduler,18496,1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18536,Energy Efficiency,schedul,scheduler,18536,ark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18635,Energy Efficiency,schedul,scheduler,18635, org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18733,Energy Efficiency,schedul,scheduler,18733,d.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18987,Energy Efficiency,schedul,scheduler,18987,.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:19068,Energy Efficiency,schedul,scheduler,19068,ultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922); 	at org.apache.spark.rdd.PairRDDFunctions$$an,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:19174,Energy Efficiency,schedul,scheduler,19174,he.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.Pair,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:19324,Energy Efficiency,schedul,scheduler,19324,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIH,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:19413,Energy Efficiency,schedul,scheduler,19413,va.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.RDDOperationS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:19511,Energy Efficiency,schedul,scheduler,19511,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScop,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:19607,Energy Efficiency,schedul,scheduler,19607,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); 	at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:19772,Energy Efficiency,schedul,scheduler,19772,.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:24446,Energy Efficiency,schedul,scheduler,24446,mit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkUI: Stopped Spark web UI at http://172.32.65.22:4040; 16/11/16 23:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/11/16 23:25:11 INFO MemoryStore: MemoryStore cleared; 16/11/16 23:25:11 INFO BlockManager: BlockManager stopped; 16/11/16 23:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/11/16 23:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/11/16 23:25:11 INFO SparkContext: Successfully stopped SparkContext; 16/11/16 23:25:11 INFO ShutdownHookManager: Shutdown hook c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:24518,Energy Efficiency,schedul,scheduler,24518,rg.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkUI: Stopped Spark web UI at http://172.32.65.22:4040; 16/11/16 23:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/11/16 23:25:11 INFO MemoryStore: MemoryStore cleared; 16/11/16 23:25:11 INFO BlockManager: BlockManager stopped; 16/11/16 23:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/11/16 23:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/11/16 23:25:11 INFO SparkContext: Successfully stopped SparkContext; 16/11/16 23:25:11 INFO ShutdownHookManager: Shutdown hook called; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /g,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:1240,Performance,load,load,1240,"parkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17; Version: Version:4.alpha.2-98-g8fa5092-SNAPSHOT; 23:25:07.556 INFO PrintReadsSpark - Defaults.B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:1408,Performance,load,loaded,1408,"/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17; Version: Version:4.alpha.2-98-g8fa5092-SNAPSHOT; 23:25:07.556 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 23:25:07.556 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 5; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:3600,Performance,load,load,3600,s.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 23:25:07.557 INFO PrintReadsSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:25:07.557 INFO PrintReadsSpark - Defaults.REFERENCE_FASTA : null; 23:25:07.557 INFO PrintReadsSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:25:07.557 INFO PrintReadsSpark - Deflater IntelDeflater; 23:25:07.557 INFO PrintReadsSpark - Initializing engine; 23:25:07.557 INFO PrintReadsSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 16/11/16 23:25:07 INFO SparkContext: Running Spark version 1.6.2; 16/11/16 23:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 16/11/16 23:25:07 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 16/11/16 23:25:07 INFO SecurityManager: Changing view acls to: root; 16/11/16 23:25:07 INFO SecurityManager: Changing modify acls to: root; 16/11/16 23:25:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root); 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriver' on port 53746.; 16/11/16 23:25:08 INFO Slf4jLogger: Slf4jLogger started; 16/11/16 23:25:08 INFO Remoting: Starting remoting; 16/11/16 23:25:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.32.65.22:33197]; 16/11/16 23:25:08 INFO Utils: Successfully started s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:9702,Performance,load,loader,9702,"cheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at ReadsSparkSink.java:236); 16/11/16 23:25:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks; 16/11/16 23:25:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2246 bytes); 16/11/16 23:25:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0); 16/11/16 23:25:10 INFO Executor: Fetching http://172.32.65.22:42517/jars/gatk4onspark.jar with timestamp 1479327909214; 16/11/16 23:25:10 INFO Utils: Fetching http://172.32.65.22:42517/jars/gatk4onspark.jar to /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8/userFiles-0cb55499-32ca-46c0-990a-357d578c76c6/fetchFileTemp1824632764181339560.tmp; 16/11/16 23:25:10 INFO Executor: Adding file:/gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8/userFiles-0cb55499-32ca-46c0-990a-357d578c76c6/gatk4onspark.jar to class loader; 16/11/16 23:25:11 INFO NewHadoopRDD: Input split: file:/gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam:1632-3770875903; 16/11/16 23:25:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver; 16/11/16 23:25:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 544 ms on localhost (1/1); 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO DAGScheduler: ShuffleMapStage 0 (mapToPair at ReadsSparkSink.java:236) finished in 0.562 s; 16/11/16 23:25:11 INFO DAGScheduler: looking for newly runnable stages; 16/11/16 23:25:11 INFO DAGScheduler: running: Set(); 16/11/16 23:25:11 INFO DAGScheduler: waiting: Set(ResultStage 1); 16/11/16 23:25:11 INFO DAGScheduler: failed: Set(); 16/11/16 23:25:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:273), which has no missing parents; 16/11/16 23:25:11 INFO MemoryStore: Block broadcast_3 stor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:13005,Performance,concurren,concurrent,13005,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:13090,Performance,concurren,concurrent,13090,"ke.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:14485,Performance,concurren,concurrent,14485,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkContext: Invoking stop() from shutdown hook; 16/11/16 23:25:11 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:14570,Performance,concurren,concurrent,14570,"ke.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkContext: Invoking stop() from shutdown hook; 16/11/16 23:25:11 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:15994,Performance,concurren,concurrent,15994,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:16079,Performance,concurren,concurrent,16079,"ke.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18256,Performance,concurren,concurrent,18256,/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18341,Performance,concurren,concurrent,18341,ke.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:24642,Performance,concurren,concurrent,24642,/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkUI: Stopped Spark web UI at http://172.32.65.22:4040; 16/11/16 23:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/11/16 23:25:11 INFO MemoryStore: MemoryStore cleared; 16/11/16 23:25:11 INFO BlockManager: BlockManager stopped; 16/11/16 23:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/11/16 23:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/11/16 23:25:11 INFO SparkContext: Successfully stopped SparkContext; 16/11/16 23:25:11 INFO ShutdownHookManager: Shutdown hook called; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting direc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:24727,Performance,concurren,concurrent,24727,ke.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkUI: Stopped Spark web UI at http://172.32.65.22:4040; 16/11/16 23:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/11/16 23:25:11 INFO MemoryStore: MemoryStore cleared; 16/11/16 23:25:11 INFO BlockManager: BlockManager stopped; 16/11/16 23:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/11/16 23:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/11/16 23:25:11 INFO SparkContext: Successfully stopped SparkContext; 16/11/16 23:25:11 INFO ShutdownHookManager: Shutdown hook called; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8/httpd-7370bd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:16269,Safety,abort,aborting,16269,"park.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:16971,Safety,abort,aborted,16971,"ava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18667,Safety,abort,abortStage,18667,MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:18765,Safety,abort,abortStage,18765,ckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:19010,Safety,abort,abortStage,19010,(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:3916,Security,Secur,SecurityManager,3916,- Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:25:07.557 INFO PrintReadsSpark - Deflater IntelDeflater; 23:25:07.557 INFO PrintReadsSpark - Initializing engine; 23:25:07.557 INFO PrintReadsSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 16/11/16 23:25:07 INFO SparkContext: Running Spark version 1.6.2; 16/11/16 23:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 16/11/16 23:25:07 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 16/11/16 23:25:07 INFO SecurityManager: Changing view acls to: root; 16/11/16 23:25:07 INFO SecurityManager: Changing modify acls to: root; 16/11/16 23:25:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root); 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriver' on port 53746.; 16/11/16 23:25:08 INFO Slf4jLogger: Slf4jLogger started; 16/11/16 23:25:08 INFO Remoting: Starting remoting; 16/11/16 23:25:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.32.65.22:33197]; 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 33197.; 16/11/16 23:25:08 INFO SparkEnv: Registering MapOutputTracker; 16/11/16 23:25:08 INFO SparkEnv: Registering BlockManagerMaster; 16/11/16 23:25:08 INFO DiskBlockManager: Created local directory at /gpfs/ngsdata/sparkcache/blockmgr-1cb5e4dd-5f50-43a8-b1ca-3d5df9aa6f1c; 16/11/16 23:25:08 INFO MemoryStore: MemoryStore started with capacity 1247.3 MB; 16/11/16 23:25:08 INFO SparkEnv: Registering OutputCommitCoordinator; 16/11/16 23:25:08 INFO Utils: Succ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:3985,Security,Secur,SecurityManager,3985,- Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:25:07.557 INFO PrintReadsSpark - Deflater IntelDeflater; 23:25:07.557 INFO PrintReadsSpark - Initializing engine; 23:25:07.557 INFO PrintReadsSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 16/11/16 23:25:07 INFO SparkContext: Running Spark version 1.6.2; 16/11/16 23:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 16/11/16 23:25:07 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 16/11/16 23:25:07 INFO SecurityManager: Changing view acls to: root; 16/11/16 23:25:07 INFO SecurityManager: Changing modify acls to: root; 16/11/16 23:25:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root); 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriver' on port 53746.; 16/11/16 23:25:08 INFO Slf4jLogger: Slf4jLogger started; 16/11/16 23:25:08 INFO Remoting: Starting remoting; 16/11/16 23:25:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.32.65.22:33197]; 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 33197.; 16/11/16 23:25:08 INFO SparkEnv: Registering MapOutputTracker; 16/11/16 23:25:08 INFO SparkEnv: Registering BlockManagerMaster; 16/11/16 23:25:08 INFO DiskBlockManager: Created local directory at /gpfs/ngsdata/sparkcache/blockmgr-1cb5e4dd-5f50-43a8-b1ca-3d5df9aa6f1c; 16/11/16 23:25:08 INFO MemoryStore: MemoryStore started with capacity 1247.3 MB; 16/11/16 23:25:08 INFO SparkEnv: Registering OutputCommitCoordinator; 16/11/16 23:25:08 INFO Utils: Succ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:4056,Security,Secur,SecurityManager,4056,- Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:25:07.557 INFO PrintReadsSpark - Deflater IntelDeflater; 23:25:07.557 INFO PrintReadsSpark - Initializing engine; 23:25:07.557 INFO PrintReadsSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 16/11/16 23:25:07 INFO SparkContext: Running Spark version 1.6.2; 16/11/16 23:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 16/11/16 23:25:07 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 16/11/16 23:25:07 INFO SecurityManager: Changing view acls to: root; 16/11/16 23:25:07 INFO SecurityManager: Changing modify acls to: root; 16/11/16 23:25:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root); 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriver' on port 53746.; 16/11/16 23:25:08 INFO Slf4jLogger: Slf4jLogger started; 16/11/16 23:25:08 INFO Remoting: Starting remoting; 16/11/16 23:25:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.32.65.22:33197]; 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 33197.; 16/11/16 23:25:08 INFO SparkEnv: Registering MapOutputTracker; 16/11/16 23:25:08 INFO SparkEnv: Registering BlockManagerMaster; 16/11/16 23:25:08 INFO DiskBlockManager: Created local directory at /gpfs/ngsdata/sparkcache/blockmgr-1cb5e4dd-5f50-43a8-b1ca-3d5df9aa6f1c; 16/11/16 23:25:08 INFO MemoryStore: MemoryStore started with capacity 1247.3 MB; 16/11/16 23:25:08 INFO SparkEnv: Registering OutputCommitCoordinator; 16/11/16 23:25:08 INFO Utils: Succ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:4073,Security,Secur,SecurityManager,4073,- Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:25:07.557 INFO PrintReadsSpark - Deflater IntelDeflater; 23:25:07.557 INFO PrintReadsSpark - Initializing engine; 23:25:07.557 INFO PrintReadsSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 16/11/16 23:25:07 INFO SparkContext: Running Spark version 1.6.2; 16/11/16 23:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 16/11/16 23:25:07 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 16/11/16 23:25:07 INFO SecurityManager: Changing view acls to: root; 16/11/16 23:25:07 INFO SecurityManager: Changing modify acls to: root; 16/11/16 23:25:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root); 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriver' on port 53746.; 16/11/16 23:25:08 INFO Slf4jLogger: Slf4jLogger started; 16/11/16 23:25:08 INFO Remoting: Starting remoting; 16/11/16 23:25:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.32.65.22:33197]; 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 33197.; 16/11/16 23:25:08 INFO SparkEnv: Registering MapOutputTracker; 16/11/16 23:25:08 INFO SparkEnv: Registering BlockManagerMaster; 16/11/16 23:25:08 INFO DiskBlockManager: Created local directory at /gpfs/ngsdata/sparkcache/blockmgr-1cb5e4dd-5f50-43a8-b1ca-3d5df9aa6f1c; 16/11/16 23:25:08 INFO MemoryStore: MemoryStore started with capacity 1247.3 MB; 16/11/16 23:25:08 INFO SparkEnv: Registering OutputCommitCoordinator; 16/11/16 23:25:08 INFO Utils: Succ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:4090,Security,authenticat,authentication,4090,- Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:25:07.557 INFO PrintReadsSpark - Deflater IntelDeflater; 23:25:07.557 INFO PrintReadsSpark - Initializing engine; 23:25:07.557 INFO PrintReadsSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 16/11/16 23:25:07 INFO SparkContext: Running Spark version 1.6.2; 16/11/16 23:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 16/11/16 23:25:07 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 16/11/16 23:25:07 INFO SecurityManager: Changing view acls to: root; 16/11/16 23:25:07 INFO SecurityManager: Changing modify acls to: root; 16/11/16 23:25:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root); 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriver' on port 53746.; 16/11/16 23:25:08 INFO Slf4jLogger: Slf4jLogger started; 16/11/16 23:25:08 INFO Remoting: Starting remoting; 16/11/16 23:25:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.32.65.22:33197]; 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 33197.; 16/11/16 23:25:08 INFO SparkEnv: Registering MapOutputTracker; 16/11/16 23:25:08 INFO SparkEnv: Registering BlockManagerMaster; 16/11/16 23:25:08 INFO DiskBlockManager: Created local directory at /gpfs/ngsdata/sparkcache/blockmgr-1cb5e4dd-5f50-43a8-b1ca-3d5df9aa6f1c; 16/11/16 23:25:08 INFO MemoryStore: MemoryStore started with capacity 1247.3 MB; 16/11/16 23:25:08 INFO SparkEnv: Registering OutputCommitCoordinator; 16/11/16 23:25:08 INFO Utils: Succ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:398,Testability,test,test,398,"+ blaunch -no-wait -z hpcgenomicn24 /spark-1.6.2-bin-hadoop2.6//bin/spark-class org.apache.spark.deploy.worker.Worker spark://hpcgenomicn24:6311 -c 16; + echo --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:470,Testability,test,test,470,"+ blaunch -no-wait -z hpcgenomicn24 /spark-1.6.2-bin-hadoop2.6//bin/spark-class org.apache.spark.deploy.worker.Worker spark://hpcgenomicn24:6311 -c 16; + echo --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:716,Testability,test,test,716,"+ blaunch -no-wait -z hpcgenomicn24 /spark-1.6.2-bin-hadoop2.6//bin/spark-class org.apache.spark.deploy.worker.Worker spark://hpcgenomicn24:6311 -c 16; + echo --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:788,Testability,test,test,788,"+ blaunch -no-wait -z hpcgenomicn24 /spark-1.6.2-bin-hadoop2.6//bin/spark-class org.apache.spark.deploy.worker.Worker spark://hpcgenomicn24:6311 -c 16; + echo --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:1117,Testability,test,test,1117,"y.worker.Worker spark://hpcgenomicn24:6311 -c 16; + echo --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:1189,Testability,test,test,1189,"mory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17; Version: Version:4.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:1565,Testability,test,test,1565,"/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17; Version: Version:4.alpha.2-98-g8fa5092-SNAPSHOT; 23:25:07.556 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 23:25:07.556 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 5; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREATE_MD5 : false; 23:25:07.556 INFO PrintReadsSpark - Defaults.CUSTOM_READER_FACTORY : ; 23:25:07.556 INFO PrintR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:1608,Testability,test,test,1608,"/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17; Version: Version:4.alpha.2-98-g8fa5092-SNAPSHOT; 23:25:07.556 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 23:25:07.556 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 5; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREATE_MD5 : false; 23:25:07.556 INFO PrintReadsSpark - Defaults.CUSTOM_READER_FACTORY : ; 23:25:07.556 INFO PrintR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:9794,Testability,test,test,9794,"cheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at ReadsSparkSink.java:236); 16/11/16 23:25:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks; 16/11/16 23:25:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2246 bytes); 16/11/16 23:25:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0); 16/11/16 23:25:10 INFO Executor: Fetching http://172.32.65.22:42517/jars/gatk4onspark.jar with timestamp 1479327909214; 16/11/16 23:25:10 INFO Utils: Fetching http://172.32.65.22:42517/jars/gatk4onspark.jar to /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8/userFiles-0cb55499-32ca-46c0-990a-357d578c76c6/fetchFileTemp1824632764181339560.tmp; 16/11/16 23:25:10 INFO Executor: Adding file:/gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8/userFiles-0cb55499-32ca-46c0-990a-357d578c76c6/gatk4onspark.jar to class loader; 16/11/16 23:25:11 INFO NewHadoopRDD: Input split: file:/gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam:1632-3770875903; 16/11/16 23:25:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver; 16/11/16 23:25:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 544 ms on localhost (1/1); 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO DAGScheduler: ShuffleMapStage 0 (mapToPair at ReadsSparkSink.java:236) finished in 0.562 s; 16/11/16 23:25:11 INFO DAGScheduler: looking for newly runnable stages; 16/11/16 23:25:11 INFO DAGScheduler: running: Set(); 16/11/16 23:25:11 INFO DAGScheduler: waiting: Set(ResultStage 1); 16/11/16 23:25:11 INFO DAGScheduler: failed: Set(); 16/11/16 23:25:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:273), which has no missing parents; 16/11/16 23:25:11 INFO MemoryStore: Block broadcast_3 stor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2268:25067,Usability,clear,cleared,25067,ly(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkUI: Stopped Spark web UI at http://172.32.65.22:4040; 16/11/16 23:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/11/16 23:25:11 INFO MemoryStore: MemoryStore cleared; 16/11/16 23:25:11 INFO BlockManager: BlockManager stopped; 16/11/16 23:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/11/16 23:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/11/16 23:25:11 INFO SparkContext: Successfully stopped SparkContext; 16/11/16 23:25:11 INFO ShutdownHookManager: Shutdown hook called; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8/httpd-7370bdc2-eb41-46d1-9512-2f387b972cac; 16/11/16 23:25:11 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; + /spark-1.6.2-bin-hadoop2.6//sbin/stop-master.sh,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268
https://github.com/broadinstitute/gatk/issues/2269:1699,Energy Efficiency,monitor,monitorThreadEfficiency,1699,rging=ALL interval_padding=0 reference_sequence=human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=dbsnp_138.b37.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/home/unix/droazen/bundle_snippets/dbsnp_138.b37.20.21.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false keepOriginalDP=false mendelianViolation=false invertMendelianViolation=false mendelianViolationQualThreshold=0.0 selec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269
https://github.com/broadinstitute/gatk/issues/2269:1117,Performance,perform,performanceLog,1117," to port this code and hook up to our tools via the standardized output writer creation methods. The VCF header lines look like this:. ```; ##GATKCommandLine.SelectVariants=<ID=SelectVariants,Version=3.4-46-gbc02625,Date=""Wed Aug 19 10:29:53 EDT 2015"",Epoch=1439994593766,CommandLineOptions=""analysis_type=SelectVariants input_file=[] showFullBamList=false read_buffer_size=null phone_home=AWS gatk_key=null tag=NA read_filter=[] disable_read_filter=[] intervals=[20:10000000-10250000, 21:10000000-10250000] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=dbsnp_138.b37.vcf) discordance=(RodBind",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269
https://github.com/broadinstitute/gatk/issues/2269:1385,Safety,unsafe,unsafe,1385,rging=ALL interval_padding=0 reference_sequence=human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=dbsnp_138.b37.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/home/unix/droazen/bundle_snippets/dbsnp_138.b37.20.21.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false keepOriginalDP=false mendelianViolation=false invertMendelianViolation=false mendelianViolationQualThreshold=0.0 selec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269
https://github.com/broadinstitute/gatk/issues/2269:3429,Security,Validat,Validated,3429,"ons=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false keepOriginalDP=false mendelianViolation=false invertMendelianViolation=false mendelianViolationQualThreshold=0.0 select_random_fraction=0.0 remove_fraction_genotypes=0.0 selectTypeToInclude=[] selectTypeToExclude=[] keepIDs=null excludeIDs=null fullyDecode=false justRead=false maxIndelSize=2147483647 minIndelSize=0 maxFilteredGenotypes=2147483647 minFilteredGenotypes=0 maxFractionFilteredGenotypes=1.0 minFractionFilteredGenotypes=0.0 setFilteredGtToNocall=false ALLOW_NONOVERLAPPING_COMMAND_LINE_SAMPLES=false filter_reads_with_N_cigar=false filter_mismatching_base_and_quals=false filter_bases_not_stored=false"">; ```. The BAM header lines look something like this (may have changed slightly in more recent versions):. ```; @PG	ID:GATK IndelRealigner	VN:1.2-573-g821421d	CL:knownAlleles=[(RodBinding name=knownAlleles source=/humgen/gsa-hpprojects/GATK/data/Comparisons/Validated/dbSNP/dbsnp_132_b37.leftAligned.vcf), (RodBinding name=knownAlleles2 source=/humgen/1kg/processing/official_release/phase1/projectConsensus/ALL.wgs.projectConsensus_v2b.20101123.snps.sites.vcf), (RodBinding name=knownAlleles3 source=/humgen/gsa-hpprojects/GATK/data/Comparisons/Unvalidated/AFR+EUR+ASN+1KG.dindel_august_release_merged_pilot1.20110126.sites.vcf)] LODThresholdForCleaning=5.0 consensusDeterminationModel=USE_READS entropyThreshold=0.15 maxReadsInMemory=150000 maxIsizeForMovement=3000 maxPositionalMoveAllowed=200 maxConsensuses=30 maxReadsForConsensuses=120 maxReadsForRealignment=20000 noOriginalAlignmentTags=false nWayOut=null generate_nWayOut_md5s=false check_early=false noPGTag=false keepPGTags=false indelsFileForDebugging=null statisticsFileForDebugging=null SNPsFileForDebugging=null; ```. We don't need to replicate the formatting of these -- we can just have the raw command line as a String for both headers, along with ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269
https://github.com/broadinstitute/gatk/issues/2269:1570,Usability,simpl,simplifyBAM,1570,rging=ALL interval_padding=0 reference_sequence=human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=dbsnp_138.b37.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/home/unix/droazen/bundle_snippets/dbsnp_138.b37.20.21.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false keepOriginalDP=false mendelianViolation=false invertMendelianViolation=false mendelianViolationQualThreshold=0.0 selec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269
https://github.com/broadinstitute/gatk/pull/2271:57,Testability,log,logger,57,kryo uses com.esotericsoft.MinLog which is a yet another logger; making LoggingUtils.setLoggingLevel() set MinLog's level,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2271
https://github.com/broadinstitute/gatk/pull/2271:72,Testability,Log,LoggingUtils,72,kryo uses com.esotericsoft.MinLog which is a yet another logger; making LoggingUtils.setLoggingLevel() set MinLog's level,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2271
https://github.com/broadinstitute/gatk/pull/2272:154,Testability,test,test,154,this seems to fix an issue with serializing Genotypes in haplotype caller; updating kryo-serializers:0.37 -> 0.41 just in case. I couldn't come up with a test case that fails when run locally. It only seems to fail when run with spark-submit.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2272
https://github.com/broadinstitute/gatk/issues/2276:197,Testability,test,test,197,"GATK4 in some cases outputs the first and last tranches twice in its tranche file and this leads to an extra line in the output file as well. This was reported as part of the tieout, i.e. for this test case:. gatk-launch VariantRecalibrator \; --variant /seq/dax/ExAc2/Exome/v2/sites_only_combined/chr1.sites_only.unfiltered.vcf.gz \; --resource mills,known=false,training=true,truth=true,prior=12:/seq/references/Homo_sapiens_assembly19/; --resource axiom,known=false,training=true,truth=false,prior=10:/seq/references/Homo_sapiens_assembly19; --resource dbsnp138,known=true,training=false,truth=false,prior=2:/seq/references/Homo_sapiens_assembly; -an QD -an MQRankSum -an ReadPosRankSum -an FS -an SOR \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tr; -trustAllPolymorphic \; --output results/recal4_indel_exome.vcf \; -mode INDEL \; -tranchesFile results/recal4_indel_exome.txt \; --gatk3Compatibility",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2276
https://github.com/broadinstitute/gatk/issues/2277:5,Deployability,release,release,5,This release potentially fixes a bug that the SV team (specifically @SHuang-Broad) encountered.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2277
https://github.com/broadinstitute/gatk/issues/2281:37,Performance,perform,performance,37,So that we can take advantage of the performance improvements when reading bams from Spark.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2281
https://github.com/broadinstitute/gatk/pull/2282:485,Deployability,Update,Updated,485,"When I was trying to use user exceptions in a consistent way independently of the constructor (mostly related with files), I found very weird behaviour with the messages. Here I try to fix some of the things that I was struggling with:. * Support for path in constructors for `CouldNotReadInputFile`, `CouldNotCreateOutputFile`, `MalformedFile` and `MalformedBAM`, in addition to some missing constructors to have the same structure for all of them (with `File` and/or `String`).; * ~~Updated javadoc in `CommandLineException`, including extending classes to make clear that in the GATK framework is not printed out if it is thrown out of parameter validation.~~ __Edited__: this is not longer required, because `CommandLineException` is decoupled from `UserException` through barclay.; * Added a TODO into the `MalformedBAM` constructor that includes a `GATKRead` that is not used.; * __Edited__: added final to constructors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282
https://github.com/broadinstitute/gatk/pull/2282:161,Integrability,message,messages,161,"When I was trying to use user exceptions in a consistent way independently of the constructor (mostly related with files), I found very weird behaviour with the messages. Here I try to fix some of the things that I was struggling with:. * Support for path in constructors for `CouldNotReadInputFile`, `CouldNotCreateOutputFile`, `MalformedFile` and `MalformedBAM`, in addition to some missing constructors to have the same structure for all of them (with `File` and/or `String`).; * ~~Updated javadoc in `CommandLineException`, including extending classes to make clear that in the GATK framework is not printed out if it is thrown out of parameter validation.~~ __Edited__: this is not longer required, because `CommandLineException` is decoupled from `UserException` through barclay.; * Added a TODO into the `MalformedBAM` constructor that includes a `GATKRead` that is not used.; * __Edited__: added final to constructors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282
https://github.com/broadinstitute/gatk/pull/2282:538,Modifiability,extend,extending,538,"When I was trying to use user exceptions in a consistent way independently of the constructor (mostly related with files), I found very weird behaviour with the messages. Here I try to fix some of the things that I was struggling with:. * Support for path in constructors for `CouldNotReadInputFile`, `CouldNotCreateOutputFile`, `MalformedFile` and `MalformedBAM`, in addition to some missing constructors to have the same structure for all of them (with `File` and/or `String`).; * ~~Updated javadoc in `CommandLineException`, including extending classes to make clear that in the GATK framework is not printed out if it is thrown out of parameter validation.~~ __Edited__: this is not longer required, because `CommandLineException` is decoupled from `UserException` through barclay.; * Added a TODO into the `MalformedBAM` constructor that includes a `GATKRead` that is not used.; * __Edited__: added final to constructors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282
https://github.com/broadinstitute/gatk/pull/2282:649,Security,validat,validation,649,"When I was trying to use user exceptions in a consistent way independently of the constructor (mostly related with files), I found very weird behaviour with the messages. Here I try to fix some of the things that I was struggling with:. * Support for path in constructors for `CouldNotReadInputFile`, `CouldNotCreateOutputFile`, `MalformedFile` and `MalformedBAM`, in addition to some missing constructors to have the same structure for all of them (with `File` and/or `String`).; * ~~Updated javadoc in `CommandLineException`, including extending classes to make clear that in the GATK framework is not printed out if it is thrown out of parameter validation.~~ __Edited__: this is not longer required, because `CommandLineException` is decoupled from `UserException` through barclay.; * Added a TODO into the `MalformedBAM` constructor that includes a `GATKRead` that is not used.; * __Edited__: added final to constructors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282
https://github.com/broadinstitute/gatk/pull/2282:564,Usability,clear,clear,564,"When I was trying to use user exceptions in a consistent way independently of the constructor (mostly related with files), I found very weird behaviour with the messages. Here I try to fix some of the things that I was struggling with:. * Support for path in constructors for `CouldNotReadInputFile`, `CouldNotCreateOutputFile`, `MalformedFile` and `MalformedBAM`, in addition to some missing constructors to have the same structure for all of them (with `File` and/or `String`).; * ~~Updated javadoc in `CommandLineException`, including extending classes to make clear that in the GATK framework is not printed out if it is thrown out of parameter validation.~~ __Edited__: this is not longer required, because `CommandLineException` is decoupled from `UserException` through barclay.; * Added a TODO into the `MalformedBAM` constructor that includes a `GATKRead` that is not used.; * __Edited__: added final to constructors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282
https://github.com/broadinstitute/gatk/pull/2283:12,Deployability,patch,patch,12,Very simple patch to extract arguments that may be useful for other toolkits into `StandardArgumentDefinitions` and some in-class changes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2283
https://github.com/broadinstitute/gatk/pull/2283:5,Usability,simpl,simple,5,Very simple patch to extract arguments that may be useful for other toolkits into `StandardArgumentDefinitions` and some in-class changes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2283
https://github.com/broadinstitute/gatk/pull/2285:39,Testability,test,test,39,A PR into a branch for myself to check test coverage (cannot be into the master because of the Spark bug).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2285
https://github.com/broadinstitute/gatk/pull/2286:11,Testability,test,test,11,Add a VQSR test that includes user-specified tranches (inspired by https://github.com/broadinstitute/gatk/issues/2276).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2286
https://github.com/broadinstitute/gatk/issues/2287:88,Availability,error,error,88,"@vdauwera reports that the following `ApplyBQSRSpark` command fails with an NIO-related error on dataproc:. ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $APIKEY_ORTMP \; -- \; --sparkRunner GCS \; --cluster gvda-test-bqsr \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 20g; ```. ```; java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; at java.nio.file.Paths.get(Paths.java:147); at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:30); at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:231); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2655,Availability,down,down,2655,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2773,Availability,down,down,2773,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2824,Availability,ERROR,ERROR,2824,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2925,Availability,ERROR,ERROR,2925,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:619,Deployability,install,installed,619,"@vdauwera reports that the following `ApplyBQSRSpark` command fails with an NIO-related error on dataproc:. ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $APIKEY_ORTMP \; -- \; --sparkRunner GCS \; --cluster gvda-test-bqsr \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 20g; ```. ```; java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; at java.nio.file.Paths.get(Paths.java:147); at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:30); at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:231); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2199,Deployability,deploy,deploy,2199,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2236,Deployability,deploy,deploy,2236,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2308,Deployability,deploy,deploy,2308,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2384,Deployability,deploy,deploy,2384,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2455,Deployability,deploy,deploy,2455,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:2524,Deployability,deploy,deploy,2524,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:169,Testability,test,test,169,"@vdauwera reports that the following `ApplyBQSRSpark` command fails with an NIO-related error on dataproc:. ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $APIKEY_ORTMP \; -- \; --sparkRunner GCS \; --cluster gvda-test-bqsr \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 20g; ```. ```; java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; at java.nio.file.Paths.get(Paths.java:147); at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:30); at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:231); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:184,Testability,benchmark,benchmark,184,"@vdauwera reports that the following `ApplyBQSRSpark` command fails with an NIO-related error on dataproc:. ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $APIKEY_ORTMP \; -- \; --sparkRunner GCS \; --cluster gvda-test-bqsr \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 20g; ```. ```; java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; at java.nio.file.Paths.get(Paths.java:147); at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:30); at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:231); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:303,Testability,TEST,TEST,303,"@vdauwera reports that the following `ApplyBQSRSpark` command fails with an NIO-related error on dataproc:. ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $APIKEY_ORTMP \; -- \; --sparkRunner GCS \; --cluster gvda-test-bqsr \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 20g; ```. ```; java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; at java.nio.file.Paths.get(Paths.java:147); at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:30); at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:231); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:361,Testability,TEST,TEST,361,"@vdauwera reports that the following `ApplyBQSRSpark` command fails with an NIO-related error on dataproc:. ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $APIKEY_ORTMP \; -- \; --sparkRunner GCS \; --cluster gvda-test-bqsr \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 20g; ```. ```; java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; at java.nio.file.Paths.get(Paths.java:147); at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:30); at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:231); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2287:468,Testability,test,test-bqsr,468,"@vdauwera reports that the following `ApplyBQSRSpark` command fails with an NIO-related error on dataproc:. ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $APIKEY_ORTMP \; -- \; --sparkRunner GCS \; --cluster gvda-test-bqsr \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 20g; ```. ```; java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; at java.nio.file.Paths.get(Paths.java:147); at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:30); at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:231); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287
https://github.com/broadinstitute/gatk/issues/2288:167,Availability,failure,failures,167,"Currently we have Spark performance tests on gatk-jenkins which fail intermittently due to normal variance in runtimes. As a result, we don't always closely look into failures. We need Spark correctness tests in jenkins that are separate from the performance tests and that fail only when there's an actual regression -- and when these fail, it should always trigger a timely investigation by an engineer into what's gone wrong.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288
https://github.com/broadinstitute/gatk/issues/2288:24,Performance,perform,performance,24,"Currently we have Spark performance tests on gatk-jenkins which fail intermittently due to normal variance in runtimes. As a result, we don't always closely look into failures. We need Spark correctness tests in jenkins that are separate from the performance tests and that fail only when there's an actual regression -- and when these fail, it should always trigger a timely investigation by an engineer into what's gone wrong.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288
https://github.com/broadinstitute/gatk/issues/2288:247,Performance,perform,performance,247,"Currently we have Spark performance tests on gatk-jenkins which fail intermittently due to normal variance in runtimes. As a result, we don't always closely look into failures. We need Spark correctness tests in jenkins that are separate from the performance tests and that fail only when there's an actual regression -- and when these fail, it should always trigger a timely investigation by an engineer into what's gone wrong.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288
https://github.com/broadinstitute/gatk/issues/2288:36,Testability,test,tests,36,"Currently we have Spark performance tests on gatk-jenkins which fail intermittently due to normal variance in runtimes. As a result, we don't always closely look into failures. We need Spark correctness tests in jenkins that are separate from the performance tests and that fail only when there's an actual regression -- and when these fail, it should always trigger a timely investigation by an engineer into what's gone wrong.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288
https://github.com/broadinstitute/gatk/issues/2288:203,Testability,test,tests,203,"Currently we have Spark performance tests on gatk-jenkins which fail intermittently due to normal variance in runtimes. As a result, we don't always closely look into failures. We need Spark correctness tests in jenkins that are separate from the performance tests and that fail only when there's an actual regression -- and when these fail, it should always trigger a timely investigation by an engineer into what's gone wrong.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288
https://github.com/broadinstitute/gatk/issues/2288:259,Testability,test,tests,259,"Currently we have Spark performance tests on gatk-jenkins which fail intermittently due to normal variance in runtimes. As a result, we don't always closely look into failures. We need Spark correctness tests in jenkins that are separate from the performance tests and that fail only when there's an actual regression -- and when these fail, it should always trigger a timely investigation by an engineer into what's gone wrong.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288
https://github.com/broadinstitute/gatk/issues/2289:377,Availability,Error,Error,377,"Spark tests on gatk-jenkins are currently failing:. Command:. ```; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; Error:. 16/11/29 16:21:01 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:403,Availability,ERROR,ERROR,403,"Spark tests on gatk-jenkins are currently failing:. Command:. ```; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; Error:. 16/11/29 16:21:01 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:440,Availability,Error,Error,440,"Spark tests on gatk-jenkins are currently failing:. Command:. ```; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; Error:. 16/11/29 16:21:01 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:2430,Availability,ERROR,ERROR,2430,instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:4791,Availability,down,down,4791,"t org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:6934,Availability,ERROR,ERROR,6934,.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [42925293-731b-47bb-8e5e-7f375d9c3490] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:7035,Availability,ERROR,ERROR,7035,.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [42925293-731b-47bb-8e5e-7f375d9c3490] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:2039,Deployability,deploy,deploy,2039,roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:2076,Deployability,deploy,deploy,2076,mmandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSpa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:2149,Deployability,deploy,deploy,2149,.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:2226,Deployability,deploy,deploy,2226,ram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContex,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:2298,Deployability,deploy,deploy,2298,ram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.Spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:2368,Deployability,deploy,deploy,2368,oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.b,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:4369,Deployability,deploy,deploy,4369,"roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:4406,Deployability,deploy,deploy,4406,"mmandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:4479,Deployability,deploy,deploy,4479,".broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:4556,Deployability,deploy,deploy,4556,"ram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:4628,Deployability,deploy,deploy,4628,"ram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:4698,Deployability,deploy,deploy,4698,"oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:6561,Deployability,deploy,deploy,6561,.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [42925293-731b-47bb-8e5e-7f375d9c3490] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:6598,Deployability,deploy,deploy,6598,.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [42925293-731b-47bb-8e5e-7f375d9c3490] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:6671,Deployability,deploy,deploy,6671,.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [42925293-731b-47bb-8e5e-7f375d9c3490] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:6748,Deployability,deploy,deploy,6748,.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [42925293-731b-47bb-8e5e-7f375d9c3490] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:6820,Deployability,deploy,deploy,6820,.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [42925293-731b-47bb-8e5e-7f375d9c3490] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:6890,Deployability,deploy,deploy,6890,.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [42925293-731b-47bb-8e5e-7f375d9c3490] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2289:6,Testability,test,tests,6,"Spark tests on gatk-jenkins are currently failing:. Command:. ```; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; Error:. 16/11/29 16:21:01 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289
https://github.com/broadinstitute/gatk/issues/2290:21,Testability,test,tests,21,We should try to add tests for:. - mixed-mode (SNP and Indel); - a test that uses more than one input file (this could be the same as the mixed mode test); - a test that uses an aggregate input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2290
https://github.com/broadinstitute/gatk/issues/2290:67,Testability,test,test,67,We should try to add tests for:. - mixed-mode (SNP and Indel); - a test that uses more than one input file (this could be the same as the mixed mode test); - a test that uses an aggregate input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2290
https://github.com/broadinstitute/gatk/issues/2290:149,Testability,test,test,149,We should try to add tests for:. - mixed-mode (SNP and Indel); - a test that uses more than one input file (this could be the same as the mixed mode test); - a test that uses an aggregate input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2290
https://github.com/broadinstitute/gatk/issues/2290:160,Testability,test,test,160,We should try to add tests for:. - mixed-mode (SNP and Indel); - a test that uses more than one input file (this could be the same as the mixed mode test); - a test that uses an aggregate input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2290
https://github.com/broadinstitute/gatk/issues/2292:15,Testability,Test,Test,15,"Fisher's Exact Test fails for the following two unit tests (that should be added to the data provider in `FisherExactTestUnitTest.java`):. ```; tests.add(new Object[]{0, 0, 0, 3, 1.0});; tests.add(new Object[]{9, 0, 0, 0, 1.0});; ```. R's implementation results in 1.0 for both of these tables, but the current GATK implementation results in 0.0 for both of them. They are examples of an edge case when the observed table is the only possibility given the marginal distributions (so the p-value should be 1).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2292
https://github.com/broadinstitute/gatk/issues/2292:53,Testability,test,tests,53,"Fisher's Exact Test fails for the following two unit tests (that should be added to the data provider in `FisherExactTestUnitTest.java`):. ```; tests.add(new Object[]{0, 0, 0, 3, 1.0});; tests.add(new Object[]{9, 0, 0, 0, 1.0});; ```. R's implementation results in 1.0 for both of these tables, but the current GATK implementation results in 0.0 for both of them. They are examples of an edge case when the observed table is the only possibility given the marginal distributions (so the p-value should be 1).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2292
https://github.com/broadinstitute/gatk/issues/2292:144,Testability,test,tests,144,"Fisher's Exact Test fails for the following two unit tests (that should be added to the data provider in `FisherExactTestUnitTest.java`):. ```; tests.add(new Object[]{0, 0, 0, 3, 1.0});; tests.add(new Object[]{9, 0, 0, 0, 1.0});; ```. R's implementation results in 1.0 for both of these tables, but the current GATK implementation results in 0.0 for both of them. They are examples of an edge case when the observed table is the only possibility given the marginal distributions (so the p-value should be 1).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2292
https://github.com/broadinstitute/gatk/issues/2292:187,Testability,test,tests,187,"Fisher's Exact Test fails for the following two unit tests (that should be added to the data provider in `FisherExactTestUnitTest.java`):. ```; tests.add(new Object[]{0, 0, 0, 3, 1.0});; tests.add(new Object[]{9, 0, 0, 0, 1.0});; ```. R's implementation results in 1.0 for both of these tables, but the current GATK implementation results in 0.0 for both of them. They are examples of an edge case when the observed table is the only possibility given the marginal distributions (so the p-value should be 1).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2292
https://github.com/broadinstitute/gatk/issues/2295:135,Integrability,depend,depending,135,"## Bug Report. ### Affected tool(s); Mutect2. ### Affected version(s); 3.6. ### Description ; The AF field of MuTect2 is unpredictable depending on the input interval. For example, if I run with `-L chr7:140452736-140453536`, I get this at one site:; `chr7	140453136	.	A	T	.	PASS	ECNT=1;HCNT=27;MAX_ED=.;MIN_ED=.;NLOD=239.30;TLOD=243.81	GT:AD:AF:ALT_F1R2:ALT_F2R1:FOXOG:QSS:REF_F1R2:REF_F2R1	0/1:1489,145:0.049:69:76:0.524:45565,4764:771:718	0/0:1094,0:0.00:0:0:.:34322,0:549:545`; But, if I run with `-L chr7:140452836-140453436`, I get this at the same site:; `chr7	140453136	.	A	T	.	PASS	ECNT=1;HCNT=15;MAX_ED=.;MIN_ED=.;NLOD=239.98;TLOD=245.96	GT:AD:AF:ALT_F1R2:ALT_F2R1:FOXOG:QSS:REF_F1R2:REF_F2R1	0/1:1489,144:0.092:69:75:0.521:45567,4730:770:719	0/0:1097,0:0.00:0:0:.:34389,0:551:546`; Notice the major difference in AF between the two runs. Side note, the AF does not equal the alt count / (alt count + ref count)?. #### Steps to reproduce; Test files here:; `/humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF`. Command for interval size of 600:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T MuTect2 -R /humgen/gsa-hpprojects/GATK/bundle/2.8/hg19/ucsc.hg19.fasta -I:normal /humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF/normal_chr7.12.bam -I:tumor /humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF/tumor_chr7.12.bam -o Sheila.MuTect2.vcf -L chr7:140452836-140453436. Command for interval size of 800:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T MuTect2 -R /humgen/gsa-hpprojects/GATK/bundle/2.8/hg19/ucsc.hg19.fasta -I:normal /humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF/normal_chr7.12.bam -I:tumor /humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF/tumor_chr7.12.bam -o Sheila.MuTect2.400.vcf -L chr7:140452736-140453536`. ----. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/33762#Comment_33762",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2295
https://github.com/broadinstitute/gatk/issues/2295:949,Testability,Test,Test,949,"## Bug Report. ### Affected tool(s); Mutect2. ### Affected version(s); 3.6. ### Description ; The AF field of MuTect2 is unpredictable depending on the input interval. For example, if I run with `-L chr7:140452736-140453536`, I get this at one site:; `chr7	140453136	.	A	T	.	PASS	ECNT=1;HCNT=27;MAX_ED=.;MIN_ED=.;NLOD=239.30;TLOD=243.81	GT:AD:AF:ALT_F1R2:ALT_F2R1:FOXOG:QSS:REF_F1R2:REF_F2R1	0/1:1489,145:0.049:69:76:0.524:45565,4764:771:718	0/0:1094,0:0.00:0:0:.:34322,0:549:545`; But, if I run with `-L chr7:140452836-140453436`, I get this at the same site:; `chr7	140453136	.	A	T	.	PASS	ECNT=1;HCNT=15;MAX_ED=.;MIN_ED=.;NLOD=239.98;TLOD=245.96	GT:AD:AF:ALT_F1R2:ALT_F2R1:FOXOG:QSS:REF_F1R2:REF_F2R1	0/1:1489,144:0.092:69:75:0.521:45567,4730:770:719	0/0:1097,0:0.00:0:0:.:34389,0:551:546`; Notice the major difference in AF between the two runs. Side note, the AF does not equal the alt count / (alt count + ref count)?. #### Steps to reproduce; Test files here:; `/humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF`. Command for interval size of 600:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T MuTect2 -R /humgen/gsa-hpprojects/GATK/bundle/2.8/hg19/ucsc.hg19.fasta -I:normal /humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF/normal_chr7.12.bam -I:tumor /humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF/tumor_chr7.12.bam -o Sheila.MuTect2.vcf -L chr7:140452836-140453436. Command for interval size of 800:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T MuTect2 -R /humgen/gsa-hpprojects/GATK/bundle/2.8/hg19/ucsc.hg19.fasta -I:normal /humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF/normal_chr7.12.bam -I:tumor /humgen/gsa-scr1/schandra/micknudsen_MuTect2AF/bugreport_AF/tumor_chr7.12.bam -o Sheila.MuTect2.400.vcf -L chr7:140452736-140453536`. ----. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/33762#Comment_33762",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2295
https://github.com/broadinstitute/gatk/issues/2296:53,Availability,error,error,53,"Running GATK4 BwaSpark encounter the following fatal error message:; `[M::mem_sam_pe] Paired reads have different names: ""206B4ABXX100825:7:66:2632:21260"", ""206B4ABXX100825:7:66:2632:31752""`. Script: ; `$GATK_LAUNCH BwaSpark -I $unsorted_bam_hdfs -O $sorted_bam_hdfs -t 10 --disableSequenceDictionaryValidation true -R $ref_hdfs -K 10000000 -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-cores 10 --executor-memory 40g`. $unsorted_bam_hdfs is a file generated by FastqToBam, and copied to HDFS. ; spark 2.0 is used. . The original Fastq files are perfectly fine, and we have been using it for all our tests using previous versions, including 3.6. I also manually checked the generated name-sorted BAM file generated by FastToBam, and the neighboring lines are perfectly paired as well. . What I suspect is that chunk is cut inside a pair, and thus not just this one, all subsequent lines are all error'ed out. To confirm this, I ran the job with different -K and -bps options, and the error will occur at different locations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2296
https://github.com/broadinstitute/gatk/issues/2296:920,Availability,error,error,920,"Running GATK4 BwaSpark encounter the following fatal error message:; `[M::mem_sam_pe] Paired reads have different names: ""206B4ABXX100825:7:66:2632:21260"", ""206B4ABXX100825:7:66:2632:31752""`. Script: ; `$GATK_LAUNCH BwaSpark -I $unsorted_bam_hdfs -O $sorted_bam_hdfs -t 10 --disableSequenceDictionaryValidation true -R $ref_hdfs -K 10000000 -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-cores 10 --executor-memory 40g`. $unsorted_bam_hdfs is a file generated by FastqToBam, and copied to HDFS. ; spark 2.0 is used. . The original Fastq files are perfectly fine, and we have been using it for all our tests using previous versions, including 3.6. I also manually checked the generated name-sorted BAM file generated by FastToBam, and the neighboring lines are perfectly paired as well. . What I suspect is that chunk is cut inside a pair, and thus not just this one, all subsequent lines are all error'ed out. To confirm this, I ran the job with different -K and -bps options, and the error will occur at different locations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2296
https://github.com/broadinstitute/gatk/issues/2296:1009,Availability,error,error,1009,"Running GATK4 BwaSpark encounter the following fatal error message:; `[M::mem_sam_pe] Paired reads have different names: ""206B4ABXX100825:7:66:2632:21260"", ""206B4ABXX100825:7:66:2632:31752""`. Script: ; `$GATK_LAUNCH BwaSpark -I $unsorted_bam_hdfs -O $sorted_bam_hdfs -t 10 --disableSequenceDictionaryValidation true -R $ref_hdfs -K 10000000 -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-cores 10 --executor-memory 40g`. $unsorted_bam_hdfs is a file generated by FastqToBam, and copied to HDFS. ; spark 2.0 is used. . The original Fastq files are perfectly fine, and we have been using it for all our tests using previous versions, including 3.6. I also manually checked the generated name-sorted BAM file generated by FastToBam, and the neighboring lines are perfectly paired as well. . What I suspect is that chunk is cut inside a pair, and thus not just this one, all subsequent lines are all error'ed out. To confirm this, I ran the job with different -K and -bps options, and the error will occur at different locations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2296
https://github.com/broadinstitute/gatk/issues/2296:59,Integrability,message,message,59,"Running GATK4 BwaSpark encounter the following fatal error message:; `[M::mem_sam_pe] Paired reads have different names: ""206B4ABXX100825:7:66:2632:21260"", ""206B4ABXX100825:7:66:2632:31752""`. Script: ; `$GATK_LAUNCH BwaSpark -I $unsorted_bam_hdfs -O $sorted_bam_hdfs -t 10 --disableSequenceDictionaryValidation true -R $ref_hdfs -K 10000000 -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-cores 10 --executor-memory 40g`. $unsorted_bam_hdfs is a file generated by FastqToBam, and copied to HDFS. ; spark 2.0 is used. . The original Fastq files are perfectly fine, and we have been using it for all our tests using previous versions, including 3.6. I also manually checked the generated name-sorted BAM file generated by FastToBam, and the neighboring lines are perfectly paired as well. . What I suspect is that chunk is cut inside a pair, and thus not just this one, all subsequent lines are all error'ed out. To confirm this, I ran the job with different -K and -bps options, and the error will occur at different locations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2296
https://github.com/broadinstitute/gatk/issues/2296:625,Testability,test,tests,625,"Running GATK4 BwaSpark encounter the following fatal error message:; `[M::mem_sam_pe] Paired reads have different names: ""206B4ABXX100825:7:66:2632:21260"", ""206B4ABXX100825:7:66:2632:31752""`. Script: ; `$GATK_LAUNCH BwaSpark -I $unsorted_bam_hdfs -O $sorted_bam_hdfs -t 10 --disableSequenceDictionaryValidation true -R $ref_hdfs -K 10000000 -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-cores 10 --executor-memory 40g`. $unsorted_bam_hdfs is a file generated by FastqToBam, and copied to HDFS. ; spark 2.0 is used. . The original Fastq files are perfectly fine, and we have been using it for all our tests using previous versions, including 3.6. I also manually checked the generated name-sorted BAM file generated by FastToBam, and the neighboring lines are perfectly paired as well. . What I suspect is that chunk is cut inside a pair, and thus not just this one, all subsequent lines are all error'ed out. To confirm this, I ran the job with different -K and -bps options, and the error will occur at different locations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2296
https://github.com/broadinstitute/gatk/issues/2297:167,Availability,down,downstream,167,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297
https://github.com/broadinstitute/gatk/issues/2297:97,Deployability,configurat,configuration,97,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297
https://github.com/broadinstitute/gatk/issues/2297:223,Deployability,configurat,configuration,223,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297
https://github.com/broadinstitute/gatk/issues/2297:281,Deployability,configurat,configuration,281,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297
https://github.com/broadinstitute/gatk/issues/2297:97,Modifiability,config,configuration,97,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297
https://github.com/broadinstitute/gatk/issues/2297:223,Modifiability,config,configuration,223,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297
https://github.com/broadinstitute/gatk/issues/2297:281,Modifiability,config,configuration,281,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297
https://github.com/broadinstitute/gatk/issues/2297:66,Usability,clear,clear,66,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297
https://github.com/broadinstitute/gatk/issues/2298:38,Testability,test,tests,38,"We need some short-running regression tests that run on dataproc in travis, to supplement the large-scale tests in gatk-jenkins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298
https://github.com/broadinstitute/gatk/issues/2298:106,Testability,test,tests,106,"We need some short-running regression tests that run on dataproc in travis, to supplement the large-scale tests in gatk-jenkins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298
https://github.com/broadinstitute/gatk/issues/2299:1291,Availability,down,down,1291,"EVEL : 5; 11:43:32.131 INFO SortSam - Defaults.CREATE_INDEX : false; 11:43:32.131 INFO SortSam - Defaults.CREATE_MD5 : false; 11:43:32.131 INFO SortSam - Defaults.CUSTOM_READER_FACTORY : ; 11:43:32.131 INFO SortSam - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 11:43:32.131 INFO SortSam - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:43:32.131 INFO SortSam - Defaults.REFERENCE_FASTA : null; 11:43:32.131 INFO SortSam - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:43:32.131 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:43:32.131 INFO SortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299
https://github.com/broadinstitute/gatk/issues/2299:1818,Integrability,wrap,wrapTempOutputStream,1818,"SE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:43:32.131 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:43:32.131 INFO SortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299
https://github.com/broadinstitute/gatk/issues/2299:1540,Performance,Load,LoadSnappy,1540,"I_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 11:43:32.131 INFO SortSam - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:43:32.131 INFO SortSam - Defaults.REFERENCE_FASTA : null; 11:43:32.131 INFO SortSam - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:43:32.131 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:43:32.131 INFO SortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299
https://github.com/broadinstitute/gatk/issues/2299:2845,Performance,Load,LoadSnappy,2845,"ortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299
https://github.com/broadinstitute/gatk/issues/2299:2947,Performance,load,loadClass,2947,"ortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299
https://github.com/broadinstitute/gatk/issues/2299:3017,Performance,load,loadClass,3017,"ortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299
https://github.com/broadinstitute/gatk/issues/2299:3073,Performance,load,loadClass,3073,"ortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299
https://github.com/broadinstitute/gatk/issues/2300:1291,Availability,down,down,1291,"EVEL : 5; 11:43:32.131 INFO SortSam - Defaults.CREATE_INDEX : false; 11:43:32.131 INFO SortSam - Defaults.CREATE_MD5 : false; 11:43:32.131 INFO SortSam - Defaults.CUSTOM_READER_FACTORY : ; 11:43:32.131 INFO SortSam - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 11:43:32.131 INFO SortSam - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:43:32.131 INFO SortSam - Defaults.REFERENCE_FASTA : null; 11:43:32.131 INFO SortSam - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:43:32.131 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:43:32.131 INFO SortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300
https://github.com/broadinstitute/gatk/issues/2300:1818,Integrability,wrap,wrapTempOutputStream,1818,"SE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:43:32.131 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:43:32.131 INFO SortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300
https://github.com/broadinstitute/gatk/issues/2300:1540,Performance,Load,LoadSnappy,1540,"I_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 11:43:32.131 INFO SortSam - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:43:32.131 INFO SortSam - Defaults.REFERENCE_FASTA : null; 11:43:32.131 INFO SortSam - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:43:32.131 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:43:32.131 INFO SortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300
https://github.com/broadinstitute/gatk/issues/2300:2845,Performance,Load,LoadSnappy,2845,"ortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300
https://github.com/broadinstitute/gatk/issues/2300:2947,Performance,load,loadClass,2947,"ortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300
https://github.com/broadinstitute/gatk/issues/2300:3017,Performance,load,loadClass,3017,"ortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300
https://github.com/broadinstitute/gatk/issues/2300:3073,Performance,load,loadClass,3073,"ortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300
https://github.com/broadinstitute/gatk/issues/2302:55,Deployability,upgrade,upgrades,55,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:86,Energy Efficiency,Power,PowerPC,86,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:1146,Energy Efficiency,Power,PowerPC,1146,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:494,Performance,load,load,494,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:844,Performance,load,load,844,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:906,Performance,load,loaded,906,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:1003,Performance,load,load,1003,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:1125,Performance,load,load,1125,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:372,Testability,log,logger,372,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:457,Testability,log,logger,457,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:752,Testability,log,logger,752,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:875,Testability,log,logger,875,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2302:980,Testability,log,logger,980,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302
https://github.com/broadinstitute/gatk/issues/2303:17,Testability,test,test,17,"GATK3 has such a test, we should either port it or write a new one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2303
https://github.com/broadinstitute/gatk/pull/2307:29,Testability,Test,Test,29,"Even though the Fisher Exact Test here is based on the R implementation, there was an edge case when the hypergeometric distribution we're getting the probabilities from only has one possible value. The difference in resulting p-value was coming from the fact that the apache Math3 HypergeometricDistribution behaves differently at this edge case than dhyper in R. . Adresses #2292",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307
https://github.com/broadinstitute/gatk/issues/2309:132,Energy Efficiency,efficient,efficient,132,"In the previous version of GATK, a multi-sample pileup (already stratified by sample) was handled with a different class to be more efficient while performing operations by sample or re-splitting. This was done in a very complicated way, using [`PileupElementTracker`](https://github.com/broadgsa/gatk/blob/0b73e380436aaa5a41fb3aab97ab651207669f47/public/gatk-utils/src/main/java/org/broadinstitute/gatk/utils/pileup/PileupElementTracker.java) with different implementations of a `ReadBackedPileup` interface. Instead of using a `List<PileupElement>` internally, `ReadPileup` could have an `ElementTracker` field, that could implement splitting, sorting by position and other operations to improve the efficiency. This may solve issues like https://github.com/broadinstitute/gatk/issues/2245. In addition, it may improve the performance for a `LocusWalker` that needs the reads by sample, because the `LocusIteratorByState` already split by sample name the reads and pass them with a map.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2309
https://github.com/broadinstitute/gatk/issues/2309:499,Integrability,interface,interface,499,"In the previous version of GATK, a multi-sample pileup (already stratified by sample) was handled with a different class to be more efficient while performing operations by sample or re-splitting. This was done in a very complicated way, using [`PileupElementTracker`](https://github.com/broadgsa/gatk/blob/0b73e380436aaa5a41fb3aab97ab651207669f47/public/gatk-utils/src/main/java/org/broadinstitute/gatk/utils/pileup/PileupElementTracker.java) with different implementations of a `ReadBackedPileup` interface. Instead of using a `List<PileupElement>` internally, `ReadPileup` could have an `ElementTracker` field, that could implement splitting, sorting by position and other operations to improve the efficiency. This may solve issues like https://github.com/broadinstitute/gatk/issues/2245. In addition, it may improve the performance for a `LocusWalker` that needs the reads by sample, because the `LocusIteratorByState` already split by sample name the reads and pass them with a map.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2309
https://github.com/broadinstitute/gatk/issues/2309:148,Performance,perform,performing,148,"In the previous version of GATK, a multi-sample pileup (already stratified by sample) was handled with a different class to be more efficient while performing operations by sample or re-splitting. This was done in a very complicated way, using [`PileupElementTracker`](https://github.com/broadgsa/gatk/blob/0b73e380436aaa5a41fb3aab97ab651207669f47/public/gatk-utils/src/main/java/org/broadinstitute/gatk/utils/pileup/PileupElementTracker.java) with different implementations of a `ReadBackedPileup` interface. Instead of using a `List<PileupElement>` internally, `ReadPileup` could have an `ElementTracker` field, that could implement splitting, sorting by position and other operations to improve the efficiency. This may solve issues like https://github.com/broadinstitute/gatk/issues/2245. In addition, it may improve the performance for a `LocusWalker` that needs the reads by sample, because the `LocusIteratorByState` already split by sample name the reads and pass them with a map.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2309
https://github.com/broadinstitute/gatk/issues/2309:825,Performance,perform,performance,825,"In the previous version of GATK, a multi-sample pileup (already stratified by sample) was handled with a different class to be more efficient while performing operations by sample or re-splitting. This was done in a very complicated way, using [`PileupElementTracker`](https://github.com/broadgsa/gatk/blob/0b73e380436aaa5a41fb3aab97ab651207669f47/public/gatk-utils/src/main/java/org/broadinstitute/gatk/utils/pileup/PileupElementTracker.java) with different implementations of a `ReadBackedPileup` interface. Instead of using a `List<PileupElement>` internally, `ReadPileup` could have an `ElementTracker` field, that could implement splitting, sorting by position and other operations to improve the efficiency. This may solve issues like https://github.com/broadinstitute/gatk/issues/2245. In addition, it may improve the performance for a `LocusWalker` that needs the reads by sample, because the `LocusIteratorByState` already split by sample name the reads and pass them with a map.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2309
https://github.com/broadinstitute/gatk/issues/2310:337,Deployability,Integrat,IntegrationTestSpec,337,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:372,Deployability,Integrat,IntegrationTestSpec,372,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:337,Integrability,Integrat,IntegrationTestSpec,337,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:372,Integrability,Integrat,IntegrationTestSpec,372,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:29,Testability,test,test,29,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:143,Testability,test,test,143,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:257,Testability,Test,Test,257,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:275,Testability,test,testExampleFeatureWalkerWithIntervals,275,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:357,Testability,test,testSpec,357,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:764,Testability,test,testSpec,764,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:786,Testability,test,testExampleIntervalWalker,786,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2310:1057,Testability,test,test,1057,"://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G]; 	Overlapping variant at 1:285-285. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:286-286. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310
https://github.com/broadinstitute/gatk/issues/2311:580,Deployability,update,updated,580,"I'm using the `ReadLikelihoods` in a test and I'm setting the likelihoods to negative values. Nevertheless, when I'm trying to get the best alleles for each read using the `bestAlleles()` method, it turns out to return the allele where I haven't set any likelihood (by default, 0). I think that the bug is in the private method [`searchBestAllele`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/genotyper/ReadLikelihoods.java#L438), were if the candidate likelihood is __bigger than__ the best likelihood, the best allele is updated. I apologize in advance if this is not a bug, but I would like to know if I should use negative likelihoods as in the removed `PerReadAlleleLikelihoodMap`, or positive ones, as suggest the current implementation for getting the best alleles. Thank you in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2311
https://github.com/broadinstitute/gatk/issues/2311:37,Testability,test,test,37,"I'm using the `ReadLikelihoods` in a test and I'm setting the likelihoods to negative values. Nevertheless, when I'm trying to get the best alleles for each read using the `bestAlleles()` method, it turns out to return the allele where I haven't set any likelihood (by default, 0). I think that the bug is in the private method [`searchBestAllele`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/genotyper/ReadLikelihoods.java#L438), were if the candidate likelihood is __bigger than__ the best likelihood, the best allele is updated. I apologize in advance if this is not a bug, but I would like to know if I should use negative likelihoods as in the removed `PerReadAlleleLikelihoodMap`, or positive ones, as suggest the current implementation for getting the best alleles. Thank you in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2311
https://github.com/broadinstitute/gatk/issues/2312:1813,Deployability,install,installedProviders,1813,"on;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!fou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1877,Deployability,install,installedProviders,1877,".file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1900,Deployability,install,installedProviders,1900,".file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1941,Deployability,install,installedProviders,1941,"rayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1998,Deployability,install,installedProviders,1998,"l.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it'",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:3082,Integrability,synchroniz,synchronization,3082,"NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use the same synchronization conditions as the actual `FileSystemProvider` loading, in order to not have any race conditions, I wanted to get feedback on this approach before putting effort into doing it correctly though. @tomwhite @jean-philippe-martin What do you think?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1146,Modifiability,extend,extends,1146," the user class path in order to get around the fact that our classes aren't in the system class loader. Here's an example program I wrote that can do it on the driver. . ``` java; package org.broadinstitute.hellbender.tools;. import org.apache.spark.api.java.JavaSparkContext;; import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;; import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;; import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;. import java.io.IOException;; import java.lang.reflect.Field;; import java.net.URI;; import java.net.URISyntaxException;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<File",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:89,Performance,load,load,89,"So I have a potential solution the problem of having `FileSystemProviders` providers not load correctly on spark. We can reflectively load the filesystem providers from the user class path in order to get around the fact that our classes aren't in the system class loader. Here's an example program I wrote that can do it on the driver. . ``` java; package org.broadinstitute.hellbender.tools;. import org.apache.spark.api.java.JavaSparkContext;; import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;; import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;; import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;. import java.io.IOException;; import java.lang.reflect.Field;; import java.net.URI;; import java.net.URISyntaxException;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; ins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:134,Performance,load,load,134,"So I have a potential solution the problem of having `FileSystemProviders` providers not load correctly on spark. We can reflectively load the filesystem providers from the user class path in order to get around the fact that our classes aren't in the system class loader. Here's an example program I wrote that can do it on the driver. . ``` java; package org.broadinstitute.hellbender.tools;. import org.apache.spark.api.java.JavaSparkContext;; import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;; import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;; import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;. import java.io.IOException;; import java.lang.reflect.Field;; import java.net.URI;; import java.net.URISyntaxException;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; ins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:265,Performance,load,loader,265,"So I have a potential solution the problem of having `FileSystemProviders` providers not load correctly on spark. We can reflectively load the filesystem providers from the user class path in order to get around the fact that our classes aren't in the system class loader. Here's an example program I wrote that can do it on the driver. . ``` java; package org.broadinstitute.hellbender.tools;. import org.apache.spark.api.java.JavaSparkContext;; import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;; import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;; import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;. import java.io.IOException;; import java.lang.reflect.Field;; import java.net.URI;; import java.net.URISyntaxException;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; ins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1970,Performance,load,loadInstalledProviders,1970,"l.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it'",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:2185,Performance,load,loadInstalledProviders,2185," long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use the same synchronization conditions as the actual `FileSystemProvider` loading, in order to not have any race condition",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:2338,Performance,load,load,2338,"NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use the same synchronization conditions as the actual `FileSystemProvider` loading, in order to not have any race conditions, I wanted to get feedback on this approach before putting effort into doing it correctly though. @tomwhite @jean-philippe-martin What do you think?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:3144,Performance,load,loading,3144,"NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use the same synchronization conditions as the actual `FileSystemProvider` loading, in order to not have any race conditions, I wanted to get feedback on this approach before putting effort into doing it correctly though. @tomwhite @jean-philippe-martin What do you think?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:3178,Performance,race condition,race conditions,3178,"NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use the same synchronization conditions as the actual `FileSystemProvider` loading, in order to not have any race conditions, I wanted to get feedback on this approach before putting effort into doing it correctly though. @tomwhite @jean-philippe-martin What do you think?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1047,Testability,test,test,1047,"temProviders` providers not load correctly on spark. We can reflectively load the filesystem providers from the user class path in order to get around the fact that our classes aren't in the system class loader. Here's an example program I wrote that can do it on the driver. . ``` java; package org.broadinstitute.hellbender.tools;. import org.apache.spark.api.java.JavaSparkContext;; import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;; import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;; import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;. import java.io.IOException;; import java.lang.reflect.Field;; import java.net.URI;; import java.net.URISyntaxException;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from File",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1072,Testability,test,testthing,1072,"temProviders` providers not load correctly on spark. We can reflectively load the filesystem providers from the user class path in order to get around the fact that our classes aren't in the system class loader. Here's an example program I wrote that can do it on the driver. . ``` java; package org.broadinstitute.hellbender.tools;. import org.apache.spark.api.java.JavaSparkContext;; import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;; import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;; import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;. import java.io.IOException;; import java.lang.reflect.Field;; import java.net.URI;; import java.net.URISyntaxException;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from File",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1138,Testability,Test,TestGCS,1138," the user class path in order to get around the fact that our classes aren't in the system class loader. Here's an example program I wrote that can do it on the driver. . ``` java; package org.broadinstitute.hellbender.tools;. import org.apache.spark.api.java.JavaSparkContext;; import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;; import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;; import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;. import java.io.IOException;; import java.lang.reflect.Field;; import java.net.URI;; import java.net.URISyntaxException;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<File",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1499,Testability,test,test,1499,"ndLineProgramProperties;; import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;; import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;. import java.io.IOException;; import java.lang.reflect.Field;; import java.net.URI;; import java.net.URISyntaxException;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; Strin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:1525,Testability,test,tests,1525,"rg.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;; import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;. import java.io.IOException;; import java.lang.reflect.Field;; import java.net.URI;; import java.net.URISyntaxException;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:2093,Testability,Test,TestGCS,2093,"""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:2369,Testability,Test,TestGCS,2369,"NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use the same synchronization conditions as the actual `FileSystemProvider` loading, in order to not have any race conditions, I wanted to get feedback on this approach before putting effort into doing it correctly though. @tomwhite @jean-philippe-martin What do you think?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2312:3211,Usability,feedback,feedback,3211,"NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use the same synchronization conditions as the actual `FileSystemProvider` loading, in order to not have any race conditions, I wanted to get feedback on this approach before putting effort into doing it correctly though. @tomwhite @jean-philippe-martin What do you think?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312
https://github.com/broadinstitute/gatk/issues/2313:16,Performance,perform,performance,16,"@tomwhite found performance issues with large (eg., exome target) interval lists on Spark in the course of our quarterly Spark evaluation. Specifically this test case was very slow:. ```; ./gatk-launch CountReadsSpark ; -I hdfs:///WGS-G94982-NA12878.bam ; -L Broad.human.exome.b37.interval_list; -- --sparkRunner GCS --cluster YOUR_CLUSTER --num-executors NUM_EXECUTORS --executor-cores 4 --executor-memory 16g; ```. Where the bam was `gs://hellbender/q4_spark_eval/WGS-G94982-NA12878.bam` pre-staged into HDFS and the interval list was a localized copy of `gs://hellbender/q4_spark_eval/Broad.human.exome.b37.interval_list`. A proposed fix is https://github.com/HadoopGenomics/Hadoop-BAM/pull/121",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2313
https://github.com/broadinstitute/gatk/issues/2313:157,Testability,test,test,157,"@tomwhite found performance issues with large (eg., exome target) interval lists on Spark in the course of our quarterly Spark evaluation. Specifically this test case was very slow:. ```; ./gatk-launch CountReadsSpark ; -I hdfs:///WGS-G94982-NA12878.bam ; -L Broad.human.exome.b37.interval_list; -- --sparkRunner GCS --cluster YOUR_CLUSTER --num-executors NUM_EXECUTORS --executor-cores 4 --executor-memory 16g; ```. Where the bam was `gs://hellbender/q4_spark_eval/WGS-G94982-NA12878.bam` pre-staged into HDFS and the interval list was a localized copy of `gs://hellbender/q4_spark_eval/Broad.human.exome.b37.interval_list`. A proposed fix is https://github.com/HadoopGenomics/Hadoop-BAM/pull/121",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2313
https://github.com/broadinstitute/gatk/issues/2315:256,Availability,error,error,256,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:376,Availability,error,error,376,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:971,Availability,error,error,971,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:114,Deployability,update,update,114,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:224,Integrability,depend,dependency,224,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:391,Safety,detect,detected,391,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:285,Testability,test,tests,285,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:1077,Testability,log,log,1077,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:1344,Testability,log,log,1344,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:1365,Testability,log,log,1365,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2315:1441,Testability,log,log,1441,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315
https://github.com/broadinstitute/gatk/issues/2316:281,Availability,avail,available,281,"If gatk-launch is run so that it invokes a local jar directly without the generated launch script, it fails to properly pass system properties. This will cause confusing bugs and performance issues. It was discovered in due to #2300. This effects all users using our packaged jars available on the website since they don't include the gradle generated wrapper script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316
https://github.com/broadinstitute/gatk/issues/2316:352,Integrability,wrap,wrapper,352,"If gatk-launch is run so that it invokes a local jar directly without the generated launch script, it fails to properly pass system properties. This will cause confusing bugs and performance issues. It was discovered in due to #2300. This effects all users using our packaged jars available on the website since they don't include the gradle generated wrapper script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316
https://github.com/broadinstitute/gatk/issues/2316:179,Performance,perform,performance,179,"If gatk-launch is run so that it invokes a local jar directly without the generated launch script, it fails to properly pass system properties. This will cause confusing bugs and performance issues. It was discovered in due to #2300. This effects all users using our packaged jars available on the website since they don't include the gradle generated wrapper script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316
https://github.com/broadinstitute/gatk/pull/2318:213,Availability,down,downstream,213,"Implement https://github.com/broadinstitute/gatk/issues/2147. ```; Change ReadPosRankSumTest.isUsableRead to take deletions into account. Previously, reads were skipped because the variant location was considered downstream from the read. ; ```; The same fix was done to `AS_ReadPosRankSumTest`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2318
https://github.com/broadinstitute/gatk/pull/2319:73,Availability,down,downstream,73,"Reverting the update to gkl 0.3.1 for now because we've encountered some downstream errors that are making it impossible to update gatk-protected. This reverts commit 9e3c6e3d7370c503d2a57be0c662fb1016d8b764, reversing; changes made to 767974906e91c90079cefa4512b463138ca09f68.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2319
https://github.com/broadinstitute/gatk/pull/2319:84,Availability,error,errors,84,"Reverting the update to gkl 0.3.1 for now because we've encountered some downstream errors that are making it impossible to update gatk-protected. This reverts commit 9e3c6e3d7370c503d2a57be0c662fb1016d8b764, reversing; changes made to 767974906e91c90079cefa4512b463138ca09f68.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2319
https://github.com/broadinstitute/gatk/pull/2319:14,Deployability,update,update,14,"Reverting the update to gkl 0.3.1 for now because we've encountered some downstream errors that are making it impossible to update gatk-protected. This reverts commit 9e3c6e3d7370c503d2a57be0c662fb1016d8b764, reversing; changes made to 767974906e91c90079cefa4512b463138ca09f68.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2319
https://github.com/broadinstitute/gatk/pull/2319:124,Deployability,update,update,124,"Reverting the update to gkl 0.3.1 for now because we've encountered some downstream errors that are making it impossible to update gatk-protected. This reverts commit 9e3c6e3d7370c503d2a57be0c662fb1016d8b764, reversing; changes made to 767974906e91c90079cefa4512b463138ca09f68.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2319
https://github.com/broadinstitute/gatk/pull/2320:234,Deployability,update,updated,234,"What this PR does: ; * implemented alignment breaker based on CIGAR gaps; ; * calls insertion, deletion and naive tandem repeat annotation in addition to inversion; ; * some further refactoring of code related to sv caller; ; * tests updated accordingly. The PR is into another branch but NOT `master` because it is based on a continuation effort, which is already reviewed in #2258 , therefore has all the changes already made there (a little Spark 2 phobia caused this derail; once changes are reviewed, will see if tool is runnable and ""pipelineable"" with previous stages in the SV pipeline under the new Spark version). @cwhelan please review.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2320
https://github.com/broadinstitute/gatk/pull/2320:540,Deployability,pipeline,pipelineable,540,"What this PR does: ; * implemented alignment breaker based on CIGAR gaps; ; * calls insertion, deletion and naive tandem repeat annotation in addition to inversion; ; * some further refactoring of code related to sv caller; ; * tests updated accordingly. The PR is into another branch but NOT `master` because it is based on a continuation effort, which is already reviewed in #2258 , therefore has all the changes already made there (a little Spark 2 phobia caused this derail; once changes are reviewed, will see if tool is runnable and ""pipelineable"" with previous stages in the SV pipeline under the new Spark version). @cwhelan please review.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2320
https://github.com/broadinstitute/gatk/pull/2320:585,Deployability,pipeline,pipeline,585,"What this PR does: ; * implemented alignment breaker based on CIGAR gaps; ; * calls insertion, deletion and naive tandem repeat annotation in addition to inversion; ; * some further refactoring of code related to sv caller; ; * tests updated accordingly. The PR is into another branch but NOT `master` because it is based on a continuation effort, which is already reviewed in #2258 , therefore has all the changes already made there (a little Spark 2 phobia caused this derail; once changes are reviewed, will see if tool is runnable and ""pipelineable"" with previous stages in the SV pipeline under the new Spark version). @cwhelan please review.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2320
https://github.com/broadinstitute/gatk/pull/2320:182,Modifiability,refactor,refactoring,182,"What this PR does: ; * implemented alignment breaker based on CIGAR gaps; ; * calls insertion, deletion and naive tandem repeat annotation in addition to inversion; ; * some further refactoring of code related to sv caller; ; * tests updated accordingly. The PR is into another branch but NOT `master` because it is based on a continuation effort, which is already reviewed in #2258 , therefore has all the changes already made there (a little Spark 2 phobia caused this derail; once changes are reviewed, will see if tool is runnable and ""pipelineable"" with previous stages in the SV pipeline under the new Spark version). @cwhelan please review.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2320
https://github.com/broadinstitute/gatk/pull/2320:228,Testability,test,tests,228,"What this PR does: ; * implemented alignment breaker based on CIGAR gaps; ; * calls insertion, deletion and naive tandem repeat annotation in addition to inversion; ; * some further refactoring of code related to sv caller; ; * tests updated accordingly. The PR is into another branch but NOT `master` because it is based on a continuation effort, which is already reviewed in #2258 , therefore has all the changes already made there (a little Spark 2 phobia caused this derail; once changes are reviewed, will see if tool is runnable and ""pipelineable"" with previous stages in the SV pipeline under the new Spark version). @cwhelan please review.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2320
https://github.com/broadinstitute/gatk/pull/2321:408,Performance,perform,performance,408,"This PR includes the following improvements for `ReadPileup` by using a new class for store `PileupElement` (`PileupElementTracker`):. * If sorting by alignment start is necessary, maintain a sorted `PileupElementTracker`; * If splitting by sample is already done, keep the stratified `PileupElementTracker`. Because pre-sorting and stratification is implemented in `LocusIteratorByState`, this will improve performance in `LocusWalker` and other tools using it. Other methods, `ReadPileup.fixOverlaps()` will benefit for these improvements. This closes https://github.com/broadinstitute/gatk/issues/2309 and https://github.com/broadinstitute/gatk/issues/2245 (although the latest was already fixed, but it is rather inefficient).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321
https://github.com/broadinstitute/gatk/pull/2322:130,Deployability,configurat,configuration,130,Very simple implementation of #2297 using a custom `GATKConf` class to allow both promatically (`GATKConfBuilder`) and by commons-configuration API (constructor). Only includes:. * Packages/Classes to include in the CLP on startup.; * Packages to look for codecs on startup.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322
https://github.com/broadinstitute/gatk/pull/2322:130,Modifiability,config,configuration,130,Very simple implementation of #2297 using a custom `GATKConf` class to allow both promatically (`GATKConfBuilder`) and by commons-configuration API (constructor). Only includes:. * Packages/Classes to include in the CLP on startup.; * Packages to look for codecs on startup.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322
https://github.com/broadinstitute/gatk/pull/2322:5,Usability,simpl,simple,5,Very simple implementation of #2297 using a custom `GATKConf` class to allow both promatically (`GATKConfBuilder`) and by commons-configuration API (constructor). Only includes:. * Packages/Classes to include in the CLP on startup.; * Packages to look for codecs on startup.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322
https://github.com/broadinstitute/gatk/issues/2324:82,Availability,error,error,82,"Providing a bad value for an argument is the definition of a ""user exception"" (an error that is the user's fault). If we can't make it a `UserException` directly because of the gatk/barclay split, then we should at least wrap these exceptions in `UserException` at the top level catch, and ensure that a full error message gets printed for the user for these exceptions. . Currently I think `CommandLineException` can be silently caught and ignored with no output due to a bug in `Main.mainEntry()`:. ```; protected final void mainEntry(final String[] args) {; try {; final Object result = instanceMain(args);; handleResult(result);; System.exit(0);; } catch (final CommandLineException e){; //the usage has already been printed so don't print it here.; if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; System.exit(COMMANDLINE_EXCEPTION_EXIT_VALUE);; } catch (final UserException e){; CommandLineProgram.printDecoratedUserExceptionMessage(System.err, e);. if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; ```. We should fix this as part of this ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324
https://github.com/broadinstitute/gatk/issues/2324:107,Availability,fault,fault,107,"Providing a bad value for an argument is the definition of a ""user exception"" (an error that is the user's fault). If we can't make it a `UserException` directly because of the gatk/barclay split, then we should at least wrap these exceptions in `UserException` at the top level catch, and ensure that a full error message gets printed for the user for these exceptions. . Currently I think `CommandLineException` can be silently caught and ignored with no output due to a bug in `Main.mainEntry()`:. ```; protected final void mainEntry(final String[] args) {; try {; final Object result = instanceMain(args);; handleResult(result);; System.exit(0);; } catch (final CommandLineException e){; //the usage has already been printed so don't print it here.; if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; System.exit(COMMANDLINE_EXCEPTION_EXIT_VALUE);; } catch (final UserException e){; CommandLineProgram.printDecoratedUserExceptionMessage(System.err, e);. if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; ```. We should fix this as part of this ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324
https://github.com/broadinstitute/gatk/issues/2324:309,Availability,error,error,309,"Providing a bad value for an argument is the definition of a ""user exception"" (an error that is the user's fault). If we can't make it a `UserException` directly because of the gatk/barclay split, then we should at least wrap these exceptions in `UserException` at the top level catch, and ensure that a full error message gets printed for the user for these exceptions. . Currently I think `CommandLineException` can be silently caught and ignored with no output due to a bug in `Main.mainEntry()`:. ```; protected final void mainEntry(final String[] args) {; try {; final Object result = instanceMain(args);; handleResult(result);; System.exit(0);; } catch (final CommandLineException e){; //the usage has already been printed so don't print it here.; if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; System.exit(COMMANDLINE_EXCEPTION_EXIT_VALUE);; } catch (final UserException e){; CommandLineProgram.printDecoratedUserExceptionMessage(System.err, e);. if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; ```. We should fix this as part of this ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324
https://github.com/broadinstitute/gatk/issues/2324:221,Integrability,wrap,wrap,221,"Providing a bad value for an argument is the definition of a ""user exception"" (an error that is the user's fault). If we can't make it a `UserException` directly because of the gatk/barclay split, then we should at least wrap these exceptions in `UserException` at the top level catch, and ensure that a full error message gets printed for the user for these exceptions. . Currently I think `CommandLineException` can be silently caught and ignored with no output due to a bug in `Main.mainEntry()`:. ```; protected final void mainEntry(final String[] args) {; try {; final Object result = instanceMain(args);; handleResult(result);; System.exit(0);; } catch (final CommandLineException e){; //the usage has already been printed so don't print it here.; if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; System.exit(COMMANDLINE_EXCEPTION_EXIT_VALUE);; } catch (final UserException e){; CommandLineProgram.printDecoratedUserExceptionMessage(System.err, e);. if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; ```. We should fix this as part of this ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324
https://github.com/broadinstitute/gatk/issues/2324:315,Integrability,message,message,315,"Providing a bad value for an argument is the definition of a ""user exception"" (an error that is the user's fault). If we can't make it a `UserException` directly because of the gatk/barclay split, then we should at least wrap these exceptions in `UserException` at the top level catch, and ensure that a full error message gets printed for the user for these exceptions. . Currently I think `CommandLineException` can be silently caught and ignored with no output due to a bug in `Main.mainEntry()`:. ```; protected final void mainEntry(final String[] args) {; try {; final Object result = instanceMain(args);; handleResult(result);; System.exit(0);; } catch (final CommandLineException e){; //the usage has already been printed so don't print it here.; if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; System.exit(COMMANDLINE_EXCEPTION_EXIT_VALUE);; } catch (final UserException e){; CommandLineProgram.printDecoratedUserExceptionMessage(System.err, e);. if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; ```. We should fix this as part of this ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324
https://github.com/broadinstitute/gatk/pull/2326:100,Testability,test,tests,100,"Implements #2193.; This is a port of https://github.com/broadinstitute/gsa-unstable/pull/1417. Unit tests were added since the GATK3 version was tested with GenotypeGVCFs, which is in `gatk-protected`. There is a new internal class, `GenomeLocation` is used instead of `GenomeLoc` to store the deletions. This is because `GenomeLocParser`, which is used to create `GenomeLoc` is not passed into `GenotypingEngine` in GATK4. I could not find a clean way to add it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326
https://github.com/broadinstitute/gatk/pull/2326:145,Testability,test,tested,145,"Implements #2193.; This is a port of https://github.com/broadinstitute/gsa-unstable/pull/1417. Unit tests were added since the GATK3 version was tested with GenotypeGVCFs, which is in `gatk-protected`. There is a new internal class, `GenomeLocation` is used instead of `GenomeLoc` to store the deletions. This is because `GenomeLocParser`, which is used to create `GenomeLoc` is not passed into `GenotypingEngine` in GATK4. I could not find a clean way to add it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326
https://github.com/broadinstitute/gatk/pull/2327:189,Deployability,update,updates,189,"Adds support for online documentation generation through Barclay (https://github.com/broadinstitute/gatk/issues/2211) via the gatkDoc gradle task. The first commit contains only annotation updates (to target selected classes as documentation targets). A more complete audit/update pass will need to be done; but we need some for now in order to be able to exercise the doc generation process. The second commit contains that actual code and templates for documentation, and the final one upgrades to a Barclay snapshot that has the necessary dependent classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327
https://github.com/broadinstitute/gatk/pull/2327:274,Deployability,update,update,274,"Adds support for online documentation generation through Barclay (https://github.com/broadinstitute/gatk/issues/2211) via the gatkDoc gradle task. The first commit contains only annotation updates (to target selected classes as documentation targets). A more complete audit/update pass will need to be done; but we need some for now in order to be able to exercise the doc generation process. The second commit contains that actual code and templates for documentation, and the final one upgrades to a Barclay snapshot that has the necessary dependent classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327
https://github.com/broadinstitute/gatk/pull/2327:488,Deployability,upgrade,upgrades,488,"Adds support for online documentation generation through Barclay (https://github.com/broadinstitute/gatk/issues/2211) via the gatkDoc gradle task. The first commit contains only annotation updates (to target selected classes as documentation targets). A more complete audit/update pass will need to be done; but we need some for now in order to be able to exercise the doc generation process. The second commit contains that actual code and templates for documentation, and the final one upgrades to a Barclay snapshot that has the necessary dependent classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327
https://github.com/broadinstitute/gatk/pull/2327:542,Integrability,depend,dependent,542,"Adds support for online documentation generation through Barclay (https://github.com/broadinstitute/gatk/issues/2211) via the gatkDoc gradle task. The first commit contains only annotation updates (to target selected classes as documentation targets). A more complete audit/update pass will need to be done; but we need some for now in order to be able to exercise the doc generation process. The second commit contains that actual code and templates for documentation, and the final one upgrades to a Barclay snapshot that has the necessary dependent classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327
https://github.com/broadinstitute/gatk/pull/2327:268,Security,audit,audit,268,"Adds support for online documentation generation through Barclay (https://github.com/broadinstitute/gatk/issues/2211) via the gatkDoc gradle task. The first commit contains only annotation updates (to target selected classes as documentation targets). A more complete audit/update pass will need to be done; but we need some for now in order to be able to exercise the doc generation process. The second commit contains that actual code and templates for documentation, and the final one upgrades to a Barclay snapshot that has the necessary dependent classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327
https://github.com/broadinstitute/gatk/pull/2328:32,Integrability,depend,dependency,32,No longer needed due to Barclay dependency.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2328
https://github.com/broadinstitute/gatk/pull/2331:235,Deployability,release,released,235,"via htsjdk's new wrapper feature.; Also provide a command-line switch to tune or disable it if necessary. A test with CountReads on a ~900MB input shows a 40MB buffer; gives over 5x speedup. DO NOT SUBMIT until htsjsk's new version is released; that incorporates the [wrapper feature](https://github.com/samtools/htsjdk/pull/775).; Then, update the build file before submitting. Sample run:. $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=0; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 2.82 minutes.; $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=40; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 0.49 minutes. cc: @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2331
https://github.com/broadinstitute/gatk/pull/2331:338,Deployability,update,update,338,"via htsjdk's new wrapper feature.; Also provide a command-line switch to tune or disable it if necessary. A test with CountReads on a ~900MB input shows a 40MB buffer; gives over 5x speedup. DO NOT SUBMIT until htsjsk's new version is released; that incorporates the [wrapper feature](https://github.com/samtools/htsjdk/pull/775).; Then, update the build file before submitting. Sample run:. $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=0; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 2.82 minutes.; $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=40; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 0.49 minutes. cc: @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2331
https://github.com/broadinstitute/gatk/pull/2331:17,Integrability,wrap,wrapper,17,"via htsjdk's new wrapper feature.; Also provide a command-line switch to tune or disable it if necessary. A test with CountReads on a ~900MB input shows a 40MB buffer; gives over 5x speedup. DO NOT SUBMIT until htsjsk's new version is released; that incorporates the [wrapper feature](https://github.com/samtools/htsjdk/pull/775).; Then, update the build file before submitting. Sample run:. $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=0; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 2.82 minutes.; $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=40; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 0.49 minutes. cc: @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2331
https://github.com/broadinstitute/gatk/pull/2331:268,Integrability,wrap,wrapper,268,"via htsjdk's new wrapper feature.; Also provide a command-line switch to tune or disable it if necessary. A test with CountReads on a ~900MB input shows a 40MB buffer; gives over 5x speedup. DO NOT SUBMIT until htsjsk's new version is released; that incorporates the [wrapper feature](https://github.com/samtools/htsjdk/pull/775).; Then, update the build file before submitting. Sample run:. $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=0; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 2.82 minutes.; $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=40; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 0.49 minutes. cc: @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2331
https://github.com/broadinstitute/gatk/pull/2331:73,Performance,tune,tune,73,"via htsjdk's new wrapper feature.; Also provide a command-line switch to tune or disable it if necessary. A test with CountReads on a ~900MB input shows a 40MB buffer; gives over 5x speedup. DO NOT SUBMIT until htsjsk's new version is released; that incorporates the [wrapper feature](https://github.com/samtools/htsjdk/pull/775).; Then, update the build file before submitting. Sample run:. $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=0; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 2.82 minutes.; $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=40; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 0.49 minutes. cc: @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2331
https://github.com/broadinstitute/gatk/pull/2331:108,Testability,test,test,108,"via htsjdk's new wrapper feature.; Also provide a command-line switch to tune or disable it if necessary. A test with CountReads on a ~900MB input shows a 40MB buffer; gives over 5x speedup. DO NOT SUBMIT until htsjsk's new version is released; that incorporates the [wrapper feature](https://github.com/samtools/htsjdk/pull/775).; Then, update the build file before submitting. Sample run:. $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=0; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 2.82 minutes.; $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=40; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 0.49 minutes. cc: @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2331
https://github.com/broadinstitute/gatk/pull/2333:165,Integrability,depend,depending,165,"@tomwhite @droazen This fixes the problem I was seeing, but it's insanely slow for some reason. A 30gb file on a heavy duty cluster was taking between 30-50 minutes depending on how I sharded it. Most of that time is spent on the merging step where the master concatenates the chunks. . I think the refactoring of the writing that I've done isn't a bad thing to have. I added some extra log statements that it much easier to understand what's going on when it's slow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2333
https://github.com/broadinstitute/gatk/pull/2333:299,Modifiability,refactor,refactoring,299,"@tomwhite @droazen This fixes the problem I was seeing, but it's insanely slow for some reason. A 30gb file on a heavy duty cluster was taking between 30-50 minutes depending on how I sharded it. Most of that time is spent on the merging step where the master concatenates the chunks. . I think the refactoring of the writing that I've done isn't a bad thing to have. I added some extra log statements that it much easier to understand what's going on when it's slow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2333
https://github.com/broadinstitute/gatk/pull/2333:387,Testability,log,log,387,"@tomwhite @droazen This fixes the problem I was seeing, but it's insanely slow for some reason. A 30gb file on a heavy duty cluster was taking between 30-50 minutes depending on how I sharded it. Most of that time is spent on the merging step where the master concatenates the chunks. . I think the refactoring of the writing that I've done isn't a bad thing to have. I added some extra log statements that it much easier to understand what's going on when it's slow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2333
https://github.com/broadinstitute/gatk/issues/2334:17,Security,validat,validation,17,"We need to add a validation check to ensure that any read input file contains a header (since they're technically optional). There are code paths that assume that a header/sequence dictionary is always present (i.e., some of the SplitReads tests get NPEs if presented with one).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2334
https://github.com/broadinstitute/gatk/issues/2334:240,Testability,test,tests,240,"We need to add a validation check to ensure that any read input file contains a header (since they're technically optional). There are code paths that assume that a header/sequence dictionary is always present (i.e., some of the SplitReads tests get NPEs if presented with one).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2334
https://github.com/broadinstitute/gatk/issues/2337:282,Availability,failure,failure,282,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337
https://github.com/broadinstitute/gatk/issues/2337:293,Deployability,integrat,integration,293,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337
https://github.com/broadinstitute/gatk/issues/2337:293,Integrability,integrat,integration,293,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337
https://github.com/broadinstitute/gatk/issues/2337:356,Modifiability,config,configs,356,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337
https://github.com/broadinstitute/gatk/issues/2337:522,Modifiability,config,config,522,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337
https://github.com/broadinstitute/gatk/issues/2337:19,Testability,test,tests,19,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337
https://github.com/broadinstitute/gatk/issues/2337:64,Testability,test,test,64,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337
https://github.com/broadinstitute/gatk/issues/2337:305,Testability,test,tests,305,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337
https://github.com/broadinstitute/gatk/issues/2337:633,Testability,test,test,633,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337
https://github.com/broadinstitute/gatk/issues/2338:50,Security,audit,audit,50,"Minimally, VariantWalkerSpark, but we should also audit the other classes included in https://github.com/broadinstitute/gatk/pull/2256/files to see if readFilters are properly propagated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2338
https://github.com/broadinstitute/gatk/pull/2340:56,Availability,error,error,56,"fixes the issue where CommandLineExceptions produced no error message. added a public getUsage() method to CommandLineProgram; added a catch for these in instanceMain(), where the CommandLineProgram is in scope for printing the usage message; added a protected accessor getCommandLineParser to CommandLineProgram which guards against having an uninitialized CommandLineParser; moved the ""A USER ERROR HAS OCCURRED"" text out of the actual user exception and into the pretty printing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2340
https://github.com/broadinstitute/gatk/pull/2340:395,Availability,ERROR,ERROR,395,"fixes the issue where CommandLineExceptions produced no error message. added a public getUsage() method to CommandLineProgram; added a catch for these in instanceMain(), where the CommandLineProgram is in scope for printing the usage message; added a protected accessor getCommandLineParser to CommandLineProgram which guards against having an uninitialized CommandLineParser; moved the ""A USER ERROR HAS OCCURRED"" text out of the actual user exception and into the pretty printing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2340
https://github.com/broadinstitute/gatk/pull/2340:62,Integrability,message,message,62,"fixes the issue where CommandLineExceptions produced no error message. added a public getUsage() method to CommandLineProgram; added a catch for these in instanceMain(), where the CommandLineProgram is in scope for printing the usage message; added a protected accessor getCommandLineParser to CommandLineProgram which guards against having an uninitialized CommandLineParser; moved the ""A USER ERROR HAS OCCURRED"" text out of the actual user exception and into the pretty printing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2340
https://github.com/broadinstitute/gatk/pull/2340:234,Integrability,message,message,234,"fixes the issue where CommandLineExceptions produced no error message. added a public getUsage() method to CommandLineProgram; added a catch for these in instanceMain(), where the CommandLineProgram is in scope for printing the usage message; added a protected accessor getCommandLineParser to CommandLineProgram which guards against having an uninitialized CommandLineParser; moved the ""A USER ERROR HAS OCCURRED"" text out of the actual user exception and into the pretty printing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2340
https://github.com/broadinstitute/gatk/pull/2340:261,Security,access,accessor,261,"fixes the issue where CommandLineExceptions produced no error message. added a public getUsage() method to CommandLineProgram; added a catch for these in instanceMain(), where the CommandLineProgram is in scope for printing the usage message; added a protected accessor getCommandLineParser to CommandLineProgram which guards against having an uninitialized CommandLineParser; moved the ""A USER ERROR HAS OCCURRED"" text out of the actual user exception and into the pretty printing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2340
https://github.com/broadinstitute/gatk/pull/2343:122,Testability,test,tests,122,Found in https://github.com/broadinstitute/gatk/pull/2195:. * `getSummaryLineForLevel` fix for indentLevel = 0 (including tests); * `resetFilterCount` now is propagated to lhs/rhs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2343
https://github.com/broadinstitute/gatk/pull/2344:34,Performance,load,loaded,34,"This change allows features to be loaded using a NIO file, rather than just a local file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2344
https://github.com/broadinstitute/gatk/issues/2345:186,Energy Efficiency,reduce,reduce,186,"In the classic GATK, walkers had the option to be multi-thread in two different ways:. * `NanoSchedulable` for thread-safe `map()` calls.; * `TreeReducible` for thread-safe `map()` and `reduce()` calls. Because now the new framework's walkers have only one `apply()` function, maybe the previous design is not applicable. Nevertheless, it will be useful to implement a way to allows a tool to apply the function in a multi-thread way. Is there any plan to implement something similar in GATK4?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345
https://github.com/broadinstitute/gatk/issues/2345:50,Performance,multi-thread,multi-thread,50,"In the classic GATK, walkers had the option to be multi-thread in two different ways:. * `NanoSchedulable` for thread-safe `map()` calls.; * `TreeReducible` for thread-safe `map()` and `reduce()` calls. Because now the new framework's walkers have only one `apply()` function, maybe the previous design is not applicable. Nevertheless, it will be useful to implement a way to allows a tool to apply the function in a multi-thread way. Is there any plan to implement something similar in GATK4?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345
https://github.com/broadinstitute/gatk/issues/2345:417,Performance,multi-thread,multi-thread,417,"In the classic GATK, walkers had the option to be multi-thread in two different ways:. * `NanoSchedulable` for thread-safe `map()` calls.; * `TreeReducible` for thread-safe `map()` and `reduce()` calls. Because now the new framework's walkers have only one `apply()` function, maybe the previous design is not applicable. Nevertheless, it will be useful to implement a way to allows a tool to apply the function in a multi-thread way. Is there any plan to implement something similar in GATK4?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345
https://github.com/broadinstitute/gatk/issues/2345:118,Safety,safe,safe,118,"In the classic GATK, walkers had the option to be multi-thread in two different ways:. * `NanoSchedulable` for thread-safe `map()` calls.; * `TreeReducible` for thread-safe `map()` and `reduce()` calls. Because now the new framework's walkers have only one `apply()` function, maybe the previous design is not applicable. Nevertheless, it will be useful to implement a way to allows a tool to apply the function in a multi-thread way. Is there any plan to implement something similar in GATK4?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345
https://github.com/broadinstitute/gatk/issues/2345:168,Safety,safe,safe,168,"In the classic GATK, walkers had the option to be multi-thread in two different ways:. * `NanoSchedulable` for thread-safe `map()` calls.; * `TreeReducible` for thread-safe `map()` and `reduce()` calls. Because now the new framework's walkers have only one `apply()` function, maybe the previous design is not applicable. Nevertheless, it will be useful to implement a way to allows a tool to apply the function in a multi-thread way. Is there any plan to implement something similar in GATK4?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345
https://github.com/broadinstitute/gatk/issues/2349:59,Availability,reliab,reliably,59,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:678,Availability,failure,failure,678,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:739,Availability,failure,failure,739,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3460,Deployability,Integrat,IntegrationTestSpec,3460,nonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invok,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3501,Deployability,Integrat,IntegrationTestSpec,3501,anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3577,Deployability,Integrat,IntegrationTestSpec,3577,k.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3618,Deployability,Integrat,IntegrationTestSpec,3618,; 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:2568,Energy Efficiency,schedul,scheduler,2568,pache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.t,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:2640,Energy Efficiency,schedul,scheduler,2640,irRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:19,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3460,Integrability,Integrat,IntegrationTestSpec,3460,nonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invok,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3501,Integrability,Integrat,IntegrationTestSpec,3501,anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3577,Integrability,Integrat,IntegrationTestSpec,3577,k.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3618,Integrability,Integrat,IntegrationTestSpec,3618,; 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:6925,Integrability,Message,MessageHub,6925,sks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:6948,Integrability,Message,MessageHub,6948,sks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:586,Performance,Concurren,ConcurrentModificationException,586,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:812,Performance,Concurren,ConcurrentModificationException,812,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:2764,Performance,concurren,concurrent,2764,ext(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegio,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:2849,Performance,concurren,concurrent,2849,oopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.Nat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:6994,Performance,concurren,concurrent,6994,sks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:7102,Performance,concurren,concurrent,7102,sks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:7187,Performance,concurren,concurrent,7187,sks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:7272,Performance,concurren,concurrent,7272,sks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:657,Safety,abort,aborted,657,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:21,Testability,test,testExampleAssemblyRegionWalker,21,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:90,Testability,test,tests,90,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:135,Testability,test,test,135,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:250,Testability,test,test,250,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:319,Testability,test,test,319,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:560,Testability,test,test,560,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:2998,Testability,test,test,2998,rRDDFunctions.scala:1203); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMetho,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3150,Testability,Assert,AssertionError,3150,(PairRDDFunctions.scala:1203); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMet,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3219,Testability,test,testng,3219,.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3226,Testability,Assert,Assert,3226,.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3238,Testability,Assert,Assert,3238,l.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3263,Testability,test,testng,3263,AndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3270,Testability,Assert,Assert,3270,ureCallbacks(Utils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3291,Testability,Assert,Assert,3291,tils.scala:1348); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3317,Testability,test,testng,3317,spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.inv,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3324,Testability,Assert,Assert,3324,dd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3331,Testability,assert,assertEquals,3331,tions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoke,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3344,Testability,Assert,Assert,3344,tions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoke,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3370,Testability,test,testng,3370,et$1$$anonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.inter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3377,Testability,Assert,Assert,3377,nonfun$13.apply(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Inv,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3384,Testability,assert,assertEquals,3384,y(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3397,Testability,Assert,Assert,3397,y(PairRDDFunctions.scala:1211); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3455,Testability,test,test,3455,DFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethod,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3480,Testability,assert,assertEqualTextFiles,3480,anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3572,Testability,test,test,3572,ler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3597,Testability,assert,assertEqualTextFiles,3597,; 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:3746,Testability,test,testExampleAssemblyRegionWalker,3746,xecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSeq,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4137,Testability,test,testng,4137,java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.t,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4230,Testability,test,testng,4230,fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4294,Testability,test,testng,4294,.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4362,Testability,test,testng,4362, org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4432,Testability,test,testng,4432,te.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(Tes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4448,Testability,Test,TestMethodWorker,4448,est.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4483,Testability,Test,TestMethodWorker,4483,sertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4519,Testability,test,testng,4519,ava:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4535,Testability,Test,TestMethodWorker,4535,adinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(Suite,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4556,Testability,Test,TestMethodWorker,4556,der.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4592,Testability,test,testng,4592,c.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4599,Testability,Test,TestRunner,4599,qualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAcce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4621,Testability,Test,TestRunner,4621,egrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4651,Testability,test,testng,4651,rg.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.N,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4658,Testability,Test,TestRunner,4658,nstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMeth,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4673,Testability,Test,TestRunner,4673,bender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4703,Testability,test,testng,4703,ssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4761,Testability,test,testng,4761,yRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4827,Testability,test,testng,4827,:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.refle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4888,Testability,test,testng,4888, Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.di,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:4942,Testability,test,testng,4942,oke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5012,Testability,test,testng,5012,dAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5077,Testability,test,testng,5077,ava.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.Co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5084,Testability,Test,TestNG,5084,g.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextCl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5113,Testability,Test,TestNG,5113,nvoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5140,Testability,test,testng,5140,.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5147,Testability,Test,TestNG,5147,al.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5171,Testability,Test,TestNG,5171,ionHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5198,Testability,test,testng,5198,tionHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5205,Testability,Test,TestNG,5205,per.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$Dis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5216,Testability,Test,TestNG,5216,85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5269,Testability,test,testing,5269,r.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5277,Testability,test,testng,5277,639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5284,Testability,Test,TestNGTestClassProcessor,5284,stng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5318,Testability,Test,TestNGTestClassProcessor,5318,stMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5388,Testability,test,testing,5388,TestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5396,Testability,test,testng,5396,hods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5403,Testability,Test,TestNGTestClassProcessor,5403,a:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5433,Testability,Test,TestNGTestClassProcessor,5433,rnal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccesso,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:5502,Testability,test,testing,5502,24); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:773); 	at org.testng.TestRunner.run(TestRunner.java:623); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); 	at org.testng.SuiteRunner.run(SuiteRunner.java:259); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); 	at org.testng.TestNG.run(TestNG.java:1018); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAcces,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:6355,Testability,test,testing,6355,rg.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Th,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:6370,Testability,Test,TestWorker,6370,ternal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/issues/2349:6386,Testability,Test,TestWorker,6386,sks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349
https://github.com/broadinstitute/gatk/pull/2350:0,Performance,Optimiz,Optimization,0,"Optimization of intervals on Spark, so that specifying intervals is faster than not using intervals.; ; See https://github.com/HadoopGenomics/Hadoop-BAM/issues/91.; ; * With this change Hadoop-BAM no longer supports the; KEEP_PAIRED_READS_TOGETHER_PROPERTY property, so this is now; implemented in BwaSparkEngine (which is the only place that uses it.); * Remove use of IntervalUtils.getAllIntervalsForReference in; ReadsSparkSource, since not specifying intervals now returns all; reads.; * Add coalesce(1) to ensure that example Spark walkers produce one; output file (some failed with this change).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2350
https://github.com/broadinstitute/gatk/issues/2351:2343,Security,access,access,2343,".32-642.6.2.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14; Version: Version:null; WARNING 2017-01-20 09:07:57 SamFiles The index file /seq/fargo_picard_aggregation/C802/HSCX1550T/v3/HSCX1550T.bai was found by resolving the canonical path of a symlink: /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq/cromwell-executions/full_dl_ob_training/2c9b89c2-27ef-448a-b5af-1d090e76ada1/call-dl_ob_training/shard-25/dl_ob_training/00210f87-b7f5-4057-bb02-d3be31029996/call-CollectSequencingArtifactMetrics/inputs/seq/picard_aggregation/C802/HSCX1550T/v3/HSCX1550T.bam -> /seq/fargo_picard_aggregation/C802/HSCX1550T/v3/HSCX1550T.bam; [January 20, 2017 9:23:02 AM EST] org.broadinstitute.hellbender.tools.picard.analysis.artifacts.CollectSequencingArtifactMetrics done. Elapsed time: 15.09 minutes.; Runtime.totalMemory()=4831313920; java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.ContextAccumulator$AlignmentAccumulator.access$100(ContextAccumulator.java:113); at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.ContextAccumulator.countRecord(ContextAccumulator.java:39); at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.ArtifactCounter.countRecord(ArtifactCounter.java:82); at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.CollectSequencingArtifactMetrics.acceptRead(CollectSequencingArtifactMetrics.java:228); at org.broadinstitute.hellbender.tools.picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:114); at org.broadinstitute.hellbender.tools.picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.java:53); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2351
https://github.com/broadinstitute/gatk/pull/2352:46,Modifiability,plugin,plugins,46,To allow implementation of other `ReadFilter` plugins with the same parameter names.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2352
https://github.com/broadinstitute/gatk/pull/2354:24,Availability,error,errors,24,Fix deprecation-related errors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2354
https://github.com/broadinstitute/gatk/pull/2355:368,Integrability,depend,depending,368,"I would like to use the implementation of `GATKReadFilterPluginDescriptor` with some minor changes, but the current design of the class does not allow it. In this PR I include the following changes (divided in commits):. - Argument collection to apply to `GATKReadFilterPluginDescriptor` added on construction. This allow to modify the argument descriptions/behaviour depending on the usages.; - Make fields protected, and a couple of final keywords to methods that are not expected to be modified.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355
https://github.com/broadinstitute/gatk/issues/2356:3935,Deployability,configurat,configuration,3935,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:4007,Deployability,Configurat,Configuration,4007,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:4021,Deployability,update,updated,4021,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:3363,Integrability,Protocol,Protocol,3363,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:3935,Modifiability,config,configuration,3935,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:4007,Modifiability,Config,Configuration,4007,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:544,Performance,load,load,544,"Included is a JProfiler screenshot of where it is spending all of its time. As you can see below, it has not even run any of the ```LocusWalker.apply(...)``` code.; ```; lichtens@OncobuntuMk3:~/IdeaProjects/hellbender-protected$ java -jar build/libs/gatk-protected.jar Pileup -I ~/broad_oncotator_configs/hcc_purity/SM-74NEG.bam -O ~/broad_oncotator_configs/pileup.out -L /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list; 15:04:36.262 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/home/lichtens/IdeaProjects/hellbender-protected/build/libs/gatk-protected-all-0556d5b-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 15:04:36.344 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [January 25, 2017 3:04:36 PM EST] org.broadinstitute.hellbender.tools.walkers.qc.Pileup --output /home/lichtens/broad_oncotator_configs/pileup.out --intervals /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list --input /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam --showVerbose false --outputInsertLength false --maxDepthPerSample 0 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [January 25, 2017 3:04:36 PM EST] Executing as lichtens@OncobuntuMk3 on Linux 3.19.0-30-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_51-b16; Version: Version:0556d5b-SNAPSHOT; 15:04:36.348 INFO Pileup - Defaults.BUFFER_SIZE : 131072; 15:04:36.348 INFO Pileup - Defaults.COMP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:791,Performance,load,loaded,791,"Included is a JProfiler screenshot of where it is spending all of its time. As you can see below, it has not even run any of the ```LocusWalker.apply(...)``` code.; ```; lichtens@OncobuntuMk3:~/IdeaProjects/hellbender-protected$ java -jar build/libs/gatk-protected.jar Pileup -I ~/broad_oncotator_configs/hcc_purity/SM-74NEG.bam -O ~/broad_oncotator_configs/pileup.out -L /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list; 15:04:36.262 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/home/lichtens/IdeaProjects/hellbender-protected/build/libs/gatk-protected-all-0556d5b-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 15:04:36.344 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [January 25, 2017 3:04:36 PM EST] org.broadinstitute.hellbender.tools.walkers.qc.Pileup --output /home/lichtens/broad_oncotator_configs/pileup.out --intervals /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list --input /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam --showVerbose false --outputInsertLength false --maxDepthPerSample 0 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [January 25, 2017 3:04:36 PM EST] Executing as lichtens@OncobuntuMk3 on Linux 3.19.0-30-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_51-b16; Version: Version:0556d5b-SNAPSHOT; 15:04:36.348 INFO Pileup - Defaults.BUFFER_SIZE : 131072; 15:04:36.348 INFO Pileup - Defaults.COMP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:3437,Safety,detect,detected,3437,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:480,Testability,test,test,480,"Included is a JProfiler screenshot of where it is spending all of its time. As you can see below, it has not even run any of the ```LocusWalker.apply(...)``` code.; ```; lichtens@OncobuntuMk3:~/IdeaProjects/hellbender-protected$ java -jar build/libs/gatk-protected.jar Pileup -I ~/broad_oncotator_configs/hcc_purity/SM-74NEG.bam -O ~/broad_oncotator_configs/pileup.out -L /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list; 15:04:36.262 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/home/lichtens/IdeaProjects/hellbender-protected/build/libs/gatk-protected-all-0556d5b-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 15:04:36.344 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [January 25, 2017 3:04:36 PM EST] org.broadinstitute.hellbender.tools.walkers.qc.Pileup --output /home/lichtens/broad_oncotator_configs/pileup.out --intervals /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list --input /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam --showVerbose false --outputInsertLength false --maxDepthPerSample 0 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [January 25, 2017 3:04:36 PM EST] Executing as lichtens@OncobuntuMk3 on Linux 3.19.0-30-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_51-b16; Version: Version:0556d5b-SNAPSHOT; 15:04:36.348 INFO Pileup - Defaults.BUFFER_SIZE : 131072; 15:04:36.348 INFO Pileup - Defaults.COMP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2356:1082,Testability,test,test,1082,"an see below, it has not even run any of the ```LocusWalker.apply(...)``` code.; ```; lichtens@OncobuntuMk3:~/IdeaProjects/hellbender-protected$ java -jar build/libs/gatk-protected.jar Pileup -I ~/broad_oncotator_configs/hcc_purity/SM-74NEG.bam -O ~/broad_oncotator_configs/pileup.out -L /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list; 15:04:36.262 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/home/lichtens/IdeaProjects/hellbender-protected/build/libs/gatk-protected-all-0556d5b-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 15:04:36.344 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [January 25, 2017 3:04:36 PM EST] org.broadinstitute.hellbender.tools.walkers.qc.Pileup --output /home/lichtens/broad_oncotator_configs/pileup.out --intervals /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list --input /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam --showVerbose false --outputInsertLength false --maxDepthPerSample 0 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [January 25, 2017 3:04:36 PM EST] Executing as lichtens@OncobuntuMk3 on Linux 3.19.0-30-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_51-b16; Version: Version:0556d5b-SNAPSHOT; 15:04:36.348 INFO Pileup - Defaults.BUFFER_SIZE : 131072; 15:04:36.348 INFO Pileup - Defaults.COMPRESSION_LEVEL : 5; 15:04:36.348 INFO Pileup - Defaults.CREATE_INDEX : false; 15:04:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356
https://github.com/broadinstitute/gatk/issues/2357:43,Availability,error,error,43,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:237,Modifiability,extend,extends,237,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:315,Modifiability,plugin,plugin,315,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:383,Modifiability,plugin,plugin,383,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:504,Modifiability,extend,extends,504,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:717,Modifiability,extend,extends,717,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:14,Testability,test,test,14,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:298,Testability,Test,Test,298,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:366,Testability,Test,Test,366,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:431,Testability,Test,TestProgramGroup,431,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:477,Testability,Test,TestWithDefaultReadFilters,477,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:589,Testability,Test,TestWithDefaultReadFilters,589,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:934,Testability,Test,Test,934,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:952,Testability,test,testWithDefaultReadFiltersWithParams,952,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:1158,Testability,Test,TestWithDefaultReadFilters,1158,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2357:1592,Testability,log,logging,1592,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357
https://github.com/broadinstitute/gatk/issues/2358:295,Modifiability,plugin,plugin,295,"Because it does not make sense to disable a filter that is not enabled. The only withdraw is that if no default filter is provided, this argument does not have possible values. If #2355 is accepted, at least the part for the argument collection, this could be solved on the construction for the plugin by providing a plugin argument collection without the `--disableReadFilter` argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2358
https://github.com/broadinstitute/gatk/issues/2358:317,Modifiability,plugin,plugin,317,"Because it does not make sense to disable a filter that is not enabled. The only withdraw is that if no default filter is provided, this argument does not have possible values. If #2355 is accepted, at least the part for the argument collection, this could be solved on the construction for the plugin by providing a plugin argument collection without the `--disableReadFilter` argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2358
https://github.com/broadinstitute/gatk/issues/2361:20,Modifiability,plugin,plugin,20,"In the `ReadFilter` plugin, the `--disableAllReadFilters` option disable all read filters that are provided by the tool and by the user. From my point of view, it does not make sense to disable all the filters and provide some, but I think that the option may be a shortcut to disable all the provided ones and allow the user to set their own filters (or none, if they prefer it).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2361
https://github.com/broadinstitute/gatk/issues/2362:519,Integrability,contract,contract,519,"`GATKReadFilterPluginDescriptor.getAllInstances()` returns only the filters provided by the user, but I expect it to return the default ones. I know that they are added to the merged filter in `getMergedReadFilter`, but this does not allow to retrieve it as a list. In addition, there is no way to access the default filters provided. I suggest to move the code to merge into the same list the default and the user-provided filters to `getAllInstances()` and use that list in the `getMergedReadFilter`. ## EDITED:; The contract of `getAllInstances()` says that it should not include the default ones, so I propose a new specific method for retrieval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362
https://github.com/broadinstitute/gatk/issues/2362:298,Security,access,access,298,"`GATKReadFilterPluginDescriptor.getAllInstances()` returns only the filters provided by the user, but I expect it to return the default ones. I know that they are added to the merged filter in `getMergedReadFilter`, but this does not allow to retrieve it as a list. In addition, there is no way to access the default filters provided. I suggest to move the code to merge into the same list the default and the user-provided filters to `getAllInstances()` and use that list in the `getMergedReadFilter`. ## EDITED:; The contract of `getAllInstances()` says that it should not include the default ones, so I propose a new specific method for retrieval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362
https://github.com/broadinstitute/gatk/issues/2363:216,Testability,log,log,216,"Now if the user arguments provide the option `--disableAllReadFilters` and the `isDisableFilter` pass any of the default/user filters, it returns false. If some filter is required for the tool or a warning should be log, this does not allow to do it in the proper way. I propose to honor this option in the current implementation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2363
https://github.com/broadinstitute/gatk/pull/2364:15,Modifiability,extend,extend,15,"Solves to some extend #2362, by adding a new getter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2364
https://github.com/broadinstitute/gatk/issues/2368:48,Deployability,Configurat,Configuration,48,Ideally using a standard mechanism like Apache `Configuration`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368
https://github.com/broadinstitute/gatk/issues/2368:48,Modifiability,Config,Configuration,48,Ideally using a standard mechanism like Apache `Configuration`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368
https://github.com/broadinstitute/gatk/pull/2371:86,Usability,simpl,simple,86,"As part of my work to do a walker for sliding-window processing, I implemented a very simple argument collection for sharding intervals. As an example, I included in the `AssemblyRegionWalker`to allow users to also set the shard step in case they want to apply it. It also allows to specify a window-step to `AssemblyRegionWalker`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2371
https://github.com/broadinstitute/gatk/pull/2372:105,Performance,perform,perform,105,"The `SlidingWindowWalker` variant implementation (#1198) requires a similar class to `LocalReadShard` to perform sharding over variants. Here is a simple implementation based on the read one, which also includes:. * Move up to `Shard<T>` some methods in the read shard implementation.; * Extract a common `FilteringIterator` from `ReadFilteringIterator`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2372
https://github.com/broadinstitute/gatk/pull/2372:147,Usability,simpl,simple,147,"The `SlidingWindowWalker` variant implementation (#1198) requires a similar class to `LocalReadShard` to perform sharding over variants. Here is a simple implementation based on the read one, which also includes:. * Move up to `Shard<T>` some methods in the read shard implementation.; * Extract a common `FilteringIterator` from `ReadFilteringIterator`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2372
https://github.com/broadinstitute/gatk/pull/2374:34,Testability,test,test,34,"Rough prototype for @LeeTL1220 to test. If it satisfies your use case, we can polish this branch up, add some tests, and merge.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2374
https://github.com/broadinstitute/gatk/pull/2374:110,Testability,test,tests,110,"Rough prototype for @LeeTL1220 to test. If it satisfies your use case, we can polish this branch up, add some tests, and merge.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2374
https://github.com/broadinstitute/gatk/issues/2375:279,Availability,down,down,279,"1. Wrap `SeekablePathStream` in the tribble feature readers with a `Function<SeekableByteChannel, SeekableByteChannel>`. 2. Mirror what was done in https://github.com/broadinstitute/gatk/pull/2331 for `FeatureDataSource`: propagate cloud prefetching buffer sizes from `GATKTool` down to `FeatureDataSource`, create wrapper functions within `FeatureDataSource`, and pass down to htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2375
https://github.com/broadinstitute/gatk/issues/2375:370,Availability,down,down,370,"1. Wrap `SeekablePathStream` in the tribble feature readers with a `Function<SeekableByteChannel, SeekableByteChannel>`. 2. Mirror what was done in https://github.com/broadinstitute/gatk/pull/2331 for `FeatureDataSource`: propagate cloud prefetching buffer sizes from `GATKTool` down to `FeatureDataSource`, create wrapper functions within `FeatureDataSource`, and pass down to htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2375
https://github.com/broadinstitute/gatk/issues/2375:3,Integrability,Wrap,Wrap,3,"1. Wrap `SeekablePathStream` in the tribble feature readers with a `Function<SeekableByteChannel, SeekableByteChannel>`. 2. Mirror what was done in https://github.com/broadinstitute/gatk/pull/2331 for `FeatureDataSource`: propagate cloud prefetching buffer sizes from `GATKTool` down to `FeatureDataSource`, create wrapper functions within `FeatureDataSource`, and pass down to htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2375
https://github.com/broadinstitute/gatk/issues/2375:315,Integrability,wrap,wrapper,315,"1. Wrap `SeekablePathStream` in the tribble feature readers with a `Function<SeekableByteChannel, SeekableByteChannel>`. 2. Mirror what was done in https://github.com/broadinstitute/gatk/pull/2331 for `FeatureDataSource`: propagate cloud prefetching buffer sizes from `GATKTool` down to `FeatureDataSource`, create wrapper functions within `FeatureDataSource`, and pass down to htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2375
https://github.com/broadinstitute/gatk/issues/2377:306,Availability,error,error,306,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377
https://github.com/broadinstitute/gatk/issues/2377:338,Availability,error,error,338,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377
https://github.com/broadinstitute/gatk/issues/2377:372,Availability,error,error,372,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377
https://github.com/broadinstitute/gatk/issues/2377:490,Deployability,pipeline,pipeline,490,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377
https://github.com/broadinstitute/gatk/issues/2377:786,Deployability,pipeline,pipeline,786,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377
https://github.com/broadinstitute/gatk/issues/2377:20,Modifiability,plugin,plugin,20,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377
https://github.com/broadinstitute/gatk/issues/2377:10,Security,audit,audit,10,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377
https://github.com/broadinstitute/gatk/issues/2377:43,Testability,test,tests,43,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377
https://github.com/broadinstitute/gatk/issues/2380:97,Deployability,pipeline,pipelines,97,"- [ ] Currently [BaseRecalibrator runs with 4g of memory](https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/PairedSingleSampleWf.wdl#L535-L554) in the production pipeline. We should check that it still does if we have bam index caching on. . - [ ] Similarly, make sure [ApplyBQSR runs with 3g of memory](https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/PairedSingleSampleWf.wdl#L576-L597)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2380
https://github.com/broadinstitute/gatk/issues/2380:208,Deployability,pipeline,pipeline,208,"- [ ] Currently [BaseRecalibrator runs with 4g of memory](https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/PairedSingleSampleWf.wdl#L535-L554) in the production pipeline. We should check that it still does if we have bam index caching on. . - [ ] Similarly, make sure [ApplyBQSR runs with 3g of memory](https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/PairedSingleSampleWf.wdl#L576-L597)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2380
https://github.com/broadinstitute/gatk/issues/2380:389,Deployability,pipeline,pipelines,389,"- [ ] Currently [BaseRecalibrator runs with 4g of memory](https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/PairedSingleSampleWf.wdl#L535-L554) in the production pipeline. We should check that it still does if we have bam index caching on. . - [ ] Similarly, make sure [ApplyBQSR runs with 3g of memory](https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/PairedSingleSampleWf.wdl#L576-L597)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2380
https://github.com/broadinstitute/gatk/issues/2382:112,Availability,error,error,112,"While trying to run a tool with a CRAM input located on a hdfs attached to a dataproc cluster, we are hit by an error message described below:. ```; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:206); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:229,Availability,ERROR,ERROR,229,"While trying to run a tool with a CRAM input located on a hdfs attached to a dataproc cluster, we are hit by an error message described below:. ```; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:206); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:733,Availability,ERROR,ERROR,733,"While trying to run a tool with a CRAM input located on a hdfs attached to a dataproc cluster, we are hit by an error message described below:. ```; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:206); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:4099,Availability,ERROR,ERROR,4099,"2921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.pathExists(SparkUtils.java:100); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.setHadoopBAMConfigurationProperties(ReadsSparkSource.java:241); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:203); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [da63aa3c-e3bc-4893-9f40-42921719a343] entered state [ERROR] while waiting for [DONE].; ```. to reproduce this error, . ```bash; cd /Users/shuang/GATK/gatk. CLUSTER_NAME=""svdev-caller""; MASTER_NODE=""hdfs://svdev-caller-m:8020""; PROJECT_DIR=""user/shuang/NA12878_PCR-_30X"". ./gatk-launch FindBreakpointEvidenceSpark \; -R ""$MASTER_NODE""/reference/Homo_sapiens_assembly38.fasta \; -I ""$MASTER_NODE""/data/smallCram.cram \; -O ""$MASTER_NODE""/""$PROJECT_DIR""/fastq \; --exclusionIntervals gs://sv-data-dsde-dev/reference/GRCh38.kill.intervals \; --kmersToIgnore gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.dups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameInt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:4200,Availability,ERROR,ERROR,4200,"ot a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.pathExists(SparkUtils.java:100); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.setHadoopBAMConfigurationProperties(ReadsSparkSource.java:241); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:203); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [da63aa3c-e3bc-4893-9f40-42921719a343] entered state [ERROR] while waiting for [DONE].; ```. to reproduce this error, . ```bash; cd /Users/shuang/GATK/gatk. CLUSTER_NAME=""svdev-caller""; MASTER_NODE=""hdfs://svdev-caller-m:8020""; PROJECT_DIR=""user/shuang/NA12878_PCR-_30X"". ./gatk-launch FindBreakpointEvidenceSpark \; -R ""$MASTER_NODE""/reference/Homo_sapiens_assembly38.fasta \; -I ""$MASTER_NODE""/data/smallCram.cram \; -O ""$MASTER_NODE""/""$PROJECT_DIR""/fastq \; --exclusionIntervals gs://sv-data-dsde-dev/reference/GRCh38.kill.intervals \; --kmersToIgnore gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.dups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsForAssembly \; --maxFASTQSize 10000000 \; -- \; --sparkRunner GCS \; --clust",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:4257,Availability,error,error,4257,"System.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.pathExists(SparkUtils.java:100); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.setHadoopBAMConfigurationProperties(ReadsSparkSource.java:241); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:203); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [da63aa3c-e3bc-4893-9f40-42921719a343] entered state [ERROR] while waiting for [DONE].; ```. to reproduce this error, . ```bash; cd /Users/shuang/GATK/gatk. CLUSTER_NAME=""svdev-caller""; MASTER_NODE=""hdfs://svdev-caller-m:8020""; PROJECT_DIR=""user/shuang/NA12878_PCR-_30X"". ./gatk-launch FindBreakpointEvidenceSpark \; -R ""$MASTER_NODE""/reference/Homo_sapiens_assembly38.fasta \; -I ""$MASTER_NODE""/data/smallCram.cram \; -O ""$MASTER_NODE""/""$PROJECT_DIR""/fastq \; --exclusionIntervals gs://sv-data-dsde-dev/reference/GRCh38.kill.intervals \; --kmersToIgnore gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.dups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsForAssembly \; --maxFASTQSize 10000000 \; -- \; --sparkRunner GCS \; --cluster svdev-caller; ```. ========================. On the other hand, w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:5268,Availability,error,error,5268,"ER_NAME=""svdev-caller""; MASTER_NODE=""hdfs://svdev-caller-m:8020""; PROJECT_DIR=""user/shuang/NA12878_PCR-_30X"". ./gatk-launch FindBreakpointEvidenceSpark \; -R ""$MASTER_NODE""/reference/Homo_sapiens_assembly38.fasta \; -I ""$MASTER_NODE""/data/smallCram.cram \; -O ""$MASTER_NODE""/""$PROJECT_DIR""/fastq \; --exclusionIntervals gs://sv-data-dsde-dev/reference/GRCh38.kill.intervals \; --kmersToIgnore gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.dups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsForAssembly \; --maxFASTQSize 10000000 \; -- \; --sparkRunner GCS \; --cluster svdev-caller; ```. ========================. On the other hand, we see a similar error if the input is changed to the same file but stored in a google bucket (although the cited cause is different):. ```; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://sv-data-dsde-dev/test_data/smallCram.cram; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from gs://sv-data-dsde-dev/test_data/smallCram.cram; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.Spa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:5472,Availability,ERROR,ERROR,5472,".fasta \; -I ""$MASTER_NODE""/data/smallCram.cram \; -O ""$MASTER_NODE""/""$PROJECT_DIR""/fastq \; --exclusionIntervals gs://sv-data-dsde-dev/reference/GRCh38.kill.intervals \; --kmersToIgnore gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.dups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsForAssembly \; --maxFASTQSize 10000000 \; -- \; --sparkRunner GCS \; --cluster svdev-caller; ```. ========================. On the other hand, we see a similar error if the input is changed to the same file but stored in a google bucket (although the cited cause is different):. ```; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://sv-data-dsde-dev/test_data/smallCram.cram; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from gs://sv-data-dsde-dev/test_data/smallCram.cram; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.Com",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:5723,Availability,ERROR,ERROR,5723,"ups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsForAssembly \; --maxFASTQSize 10000000 \; -- \; --sparkRunner GCS \; --cluster svdev-caller; ```. ========================. On the other hand, we see a similar error if the input is changed to the same file but stored in a google bucket (although the cited cause is different):. ```; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://sv-data-dsde-dev/test_data/smallCram.cram; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from gs://sv-data-dsde-dev/test_data/smallCram.cram; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:8060,Availability,ERROR,ERROR,8060,"e(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist.; 	at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:8161,Availability,ERROR,ERROR,8161,"invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist.; 	at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:8394,Availability,ERROR,ERROR,8394,"ache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist.; 	at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:8672,Availability,ERROR,ERROR,8672,"nputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist.; 	at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.Com",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:10710,Availability,ERROR,ERROR,10710,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:10811,Availability,ERROR,ERROR,10811,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:2529,Deployability,deploy,deploy,2529,roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResol,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:2566,Deployability,deploy,deploy,2566,mmandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:2639,Deployability,deploy,deploy,2639,.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(Di,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:2716,Deployability,deploy,deploy,2716,ram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(Fi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:2788,Deployability,deploy,deploy,2788,ram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.Spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:2858,Deployability,deploy,deploy,2858,oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.pathExists(SparkUtils.java:100); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:7265,Deployability,deploy,deploy,7265,"roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:7302,Deployability,deploy,deploy,7302,"mmandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:7375,Deployability,deploy,deploy,7375,".broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; *****************************************************************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:7452,Deployability,deploy,deploy,7452,"ram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:7524,Deployability,deploy,deploy,7524,"ram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. *********",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:7594,Deployability,deploy,deploy,7594,"oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:10337,Deployability,deploy,deploy,10337,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:10374,Deployability,deploy,deploy,10374,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:10447,Deployability,deploy,deploy,10447,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:10524,Deployability,deploy,deploy,10524,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:10596,Deployability,deploy,deploy,10596,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:10666,Deployability,deploy,deploy,10666,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2382:118,Integrability,message,message,118,"While trying to run a tool with a CRAM input located on a hdfs attached to a dataproc cluster, we are hit by an error message described below:. ```; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:206); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382
https://github.com/broadinstitute/gatk/issues/2383:133,Deployability,integrat,integrate,133,"For some tools that I'm developing, I want to keep as much as possible the nomenclature for some common parameters in Picard/GATK to integrate my custom tools into a framework that is consistent regarding the argument names. Because of that, I'm having several issues with the `SplitReads`arguments while using splitting by these parameters in my toolkit:. - The current `SplitReads` argument names are in capital, but the rest of the framework use a camel case formatting.; - The short names clash with the `AddOrReplaceReadGroups` from Picard. Because I have a tool that adds read groups with the possibility of splitting in a similar way as `SplitReads` I would like to integrate these parameters in the same tool and I can't with the current implementation. I propose to change the `SplitReads` names to a camel case format different from Picard and pointing out that they are related with splitting.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2383
https://github.com/broadinstitute/gatk/issues/2383:673,Deployability,integrat,integrate,673,"For some tools that I'm developing, I want to keep as much as possible the nomenclature for some common parameters in Picard/GATK to integrate my custom tools into a framework that is consistent regarding the argument names. Because of that, I'm having several issues with the `SplitReads`arguments while using splitting by these parameters in my toolkit:. - The current `SplitReads` argument names are in capital, but the rest of the framework use a camel case formatting.; - The short names clash with the `AddOrReplaceReadGroups` from Picard. Because I have a tool that adds read groups with the possibility of splitting in a similar way as `SplitReads` I would like to integrate these parameters in the same tool and I can't with the current implementation. I propose to change the `SplitReads` names to a camel case format different from Picard and pointing out that they are related with splitting.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2383
https://github.com/broadinstitute/gatk/issues/2383:133,Integrability,integrat,integrate,133,"For some tools that I'm developing, I want to keep as much as possible the nomenclature for some common parameters in Picard/GATK to integrate my custom tools into a framework that is consistent regarding the argument names. Because of that, I'm having several issues with the `SplitReads`arguments while using splitting by these parameters in my toolkit:. - The current `SplitReads` argument names are in capital, but the rest of the framework use a camel case formatting.; - The short names clash with the `AddOrReplaceReadGroups` from Picard. Because I have a tool that adds read groups with the possibility of splitting in a similar way as `SplitReads` I would like to integrate these parameters in the same tool and I can't with the current implementation. I propose to change the `SplitReads` names to a camel case format different from Picard and pointing out that they are related with splitting.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2383
https://github.com/broadinstitute/gatk/issues/2383:673,Integrability,integrat,integrate,673,"For some tools that I'm developing, I want to keep as much as possible the nomenclature for some common parameters in Picard/GATK to integrate my custom tools into a framework that is consistent regarding the argument names. Because of that, I'm having several issues with the `SplitReads`arguments while using splitting by these parameters in my toolkit:. - The current `SplitReads` argument names are in capital, but the rest of the framework use a camel case formatting.; - The short names clash with the `AddOrReplaceReadGroups` from Picard. Because I have a tool that adds read groups with the possibility of splitting in a similar way as `SplitReads` I would like to integrate these parameters in the same tool and I can't with the current implementation. I propose to change the `SplitReads` names to a camel case format different from Picard and pointing out that they are related with splitting.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2383
https://github.com/broadinstitute/gatk/pull/2384:26,Safety,avoid,avoid,26,"As described in #2383, to avoid clash with Picard arguments for add read groups and point out that they are meant to split.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2384
https://github.com/broadinstitute/gatk/pull/2387:160,Testability,test,tests,160,- add missing null check for reads a whole block past EOF; - correct handling for seek just past EOF + read; - fix reuse of caching buffer when near EOF; - add tests that reproduce(d) the problems,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2387
https://github.com/broadinstitute/gatk/pull/2388:589,Performance,load,loads,589,"Awaiting the Barclay snapshot with broadinstitute/barclay#33, which is ready, but I'll sanity check again in AM before I merge. NOTE: All of the GATK tests have passed locally with this branch. However, I had to make one temporary change because SelectVariants has a feature that clashes with the collection list file feature in https://github.com/broadinstitute/barclay/pull/28. SelectVariants currently has two arguments that are defined as `List<File>`, that are each intended to take a list of file names, each of which in turn contains a list of sample names. SelectVariants manually loads all of the samples from all of the files in the list, and creates a list of unique sample names. With the https://github.com/broadinstitute/barclay/pull/28, the CLP now loads the list directly, and hands SelectVariants a list of sample names rather than the list of file names, which breaks one test. I think both features are working as intended, but collide when a .list file is used. I temporarily renamed the test file to not have a .list extension (so it won't trigger the CLP file loading), but we'll have to decide how to properly reconcile these two features.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388
https://github.com/broadinstitute/gatk/pull/2388:764,Performance,load,loads,764,"Awaiting the Barclay snapshot with broadinstitute/barclay#33, which is ready, but I'll sanity check again in AM before I merge. NOTE: All of the GATK tests have passed locally with this branch. However, I had to make one temporary change because SelectVariants has a feature that clashes with the collection list file feature in https://github.com/broadinstitute/barclay/pull/28. SelectVariants currently has two arguments that are defined as `List<File>`, that are each intended to take a list of file names, each of which in turn contains a list of sample names. SelectVariants manually loads all of the samples from all of the files in the list, and creates a list of unique sample names. With the https://github.com/broadinstitute/barclay/pull/28, the CLP now loads the list directly, and hands SelectVariants a list of sample names rather than the list of file names, which breaks one test. I think both features are working as intended, but collide when a .list file is used. I temporarily renamed the test file to not have a .list extension (so it won't trigger the CLP file loading), but we'll have to decide how to properly reconcile these two features.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388
https://github.com/broadinstitute/gatk/pull/2388:1082,Performance,load,loading,1082,"Awaiting the Barclay snapshot with broadinstitute/barclay#33, which is ready, but I'll sanity check again in AM before I merge. NOTE: All of the GATK tests have passed locally with this branch. However, I had to make one temporary change because SelectVariants has a feature that clashes with the collection list file feature in https://github.com/broadinstitute/barclay/pull/28. SelectVariants currently has two arguments that are defined as `List<File>`, that are each intended to take a list of file names, each of which in turn contains a list of sample names. SelectVariants manually loads all of the samples from all of the files in the list, and creates a list of unique sample names. With the https://github.com/broadinstitute/barclay/pull/28, the CLP now loads the list directly, and hands SelectVariants a list of sample names rather than the list of file names, which breaks one test. I think both features are working as intended, but collide when a .list file is used. I temporarily renamed the test file to not have a .list extension (so it won't trigger the CLP file loading), but we'll have to decide how to properly reconcile these two features.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388
https://github.com/broadinstitute/gatk/pull/2388:87,Safety,sanity check,sanity check,87,"Awaiting the Barclay snapshot with broadinstitute/barclay#33, which is ready, but I'll sanity check again in AM before I merge. NOTE: All of the GATK tests have passed locally with this branch. However, I had to make one temporary change because SelectVariants has a feature that clashes with the collection list file feature in https://github.com/broadinstitute/barclay/pull/28. SelectVariants currently has two arguments that are defined as `List<File>`, that are each intended to take a list of file names, each of which in turn contains a list of sample names. SelectVariants manually loads all of the samples from all of the files in the list, and creates a list of unique sample names. With the https://github.com/broadinstitute/barclay/pull/28, the CLP now loads the list directly, and hands SelectVariants a list of sample names rather than the list of file names, which breaks one test. I think both features are working as intended, but collide when a .list file is used. I temporarily renamed the test file to not have a .list extension (so it won't trigger the CLP file loading), but we'll have to decide how to properly reconcile these two features.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388
https://github.com/broadinstitute/gatk/pull/2388:150,Testability,test,tests,150,"Awaiting the Barclay snapshot with broadinstitute/barclay#33, which is ready, but I'll sanity check again in AM before I merge. NOTE: All of the GATK tests have passed locally with this branch. However, I had to make one temporary change because SelectVariants has a feature that clashes with the collection list file feature in https://github.com/broadinstitute/barclay/pull/28. SelectVariants currently has two arguments that are defined as `List<File>`, that are each intended to take a list of file names, each of which in turn contains a list of sample names. SelectVariants manually loads all of the samples from all of the files in the list, and creates a list of unique sample names. With the https://github.com/broadinstitute/barclay/pull/28, the CLP now loads the list directly, and hands SelectVariants a list of sample names rather than the list of file names, which breaks one test. I think both features are working as intended, but collide when a .list file is used. I temporarily renamed the test file to not have a .list extension (so it won't trigger the CLP file loading), but we'll have to decide how to properly reconcile these two features.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388
https://github.com/broadinstitute/gatk/pull/2388:890,Testability,test,test,890,"Awaiting the Barclay snapshot with broadinstitute/barclay#33, which is ready, but I'll sanity check again in AM before I merge. NOTE: All of the GATK tests have passed locally with this branch. However, I had to make one temporary change because SelectVariants has a feature that clashes with the collection list file feature in https://github.com/broadinstitute/barclay/pull/28. SelectVariants currently has two arguments that are defined as `List<File>`, that are each intended to take a list of file names, each of which in turn contains a list of sample names. SelectVariants manually loads all of the samples from all of the files in the list, and creates a list of unique sample names. With the https://github.com/broadinstitute/barclay/pull/28, the CLP now loads the list directly, and hands SelectVariants a list of sample names rather than the list of file names, which breaks one test. I think both features are working as intended, but collide when a .list file is used. I temporarily renamed the test file to not have a .list extension (so it won't trigger the CLP file loading), but we'll have to decide how to properly reconcile these two features.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388
https://github.com/broadinstitute/gatk/pull/2388:1008,Testability,test,test,1008,"Awaiting the Barclay snapshot with broadinstitute/barclay#33, which is ready, but I'll sanity check again in AM before I merge. NOTE: All of the GATK tests have passed locally with this branch. However, I had to make one temporary change because SelectVariants has a feature that clashes with the collection list file feature in https://github.com/broadinstitute/barclay/pull/28. SelectVariants currently has two arguments that are defined as `List<File>`, that are each intended to take a list of file names, each of which in turn contains a list of sample names. SelectVariants manually loads all of the samples from all of the files in the list, and creates a list of unique sample names. With the https://github.com/broadinstitute/barclay/pull/28, the CLP now loads the list directly, and hands SelectVariants a list of sample names rather than the list of file names, which breaks one test. I think both features are working as intended, but collide when a .list file is used. I temporarily renamed the test file to not have a .list extension (so it won't trigger the CLP file loading), but we'll have to decide how to properly reconcile these two features.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388
https://github.com/broadinstitute/gatk/pull/2389:88,Deployability,configurat,configuration,88,"GenomicsDBImport lets users writes variants to GenomicsDB. The inputs are a loader JSON configuration file, callsets JSON file containing sample names and corresponding stream names and a stream JSON file containing files names of the streams. Note: This code uses GenomicsDB v0.4.0. Please check whether Maven central has the updated version first. @kgururaj , please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389
https://github.com/broadinstitute/gatk/pull/2389:327,Deployability,update,updated,327,"GenomicsDBImport lets users writes variants to GenomicsDB. The inputs are a loader JSON configuration file, callsets JSON file containing sample names and corresponding stream names and a stream JSON file containing files names of the streams. Note: This code uses GenomicsDB v0.4.0. Please check whether Maven central has the updated version first. @kgururaj , please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389
https://github.com/broadinstitute/gatk/pull/2389:88,Modifiability,config,configuration,88,"GenomicsDBImport lets users writes variants to GenomicsDB. The inputs are a loader JSON configuration file, callsets JSON file containing sample names and corresponding stream names and a stream JSON file containing files names of the streams. Note: This code uses GenomicsDB v0.4.0. Please check whether Maven central has the updated version first. @kgururaj , please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389
https://github.com/broadinstitute/gatk/pull/2389:76,Performance,load,loader,76,"GenomicsDBImport lets users writes variants to GenomicsDB. The inputs are a loader JSON configuration file, callsets JSON file containing sample names and corresponding stream names and a stream JSON file containing files names of the streams. Note: This code uses GenomicsDB v0.4.0. Please check whether Maven central has the updated version first. @kgururaj , please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389
https://github.com/broadinstitute/gatk/issues/2390:688,Performance,load,loads,688,"The collection list file feature implemented in broadinstitute/barclay#28 can break tools that already have a `Collection<Files>` argument that is used to manually from populate another collection, if the argument value supplied is a "".list"" file. The CLP feature checks to see if an argument value for a collection has a "".list"" extension, and if so, populates the argument field with the contents of the file, rather than with the filename. SelectVariants has two such arguments (""sample_file"" and ""exclude_sample_file"") that are typed as `Set<File>`, and are each intended to contain a list of file names, each of which in turn contains a list of sample names. SelectVariants manually loads all of the samples from all of the files in the set, and creates a list of unique sample names. With the broadinstitute/barclay#28, the CLP populates the file set directly with the contents of the file if the argument value ends in the extension "".list"". SelectVariants will fail when this happens, because it then tries to treat the sample names in the collection as file names. One possible solution would be to change the CLP to not implement this functionality if the type parameter of the Collection is File.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2390
https://github.com/broadinstitute/gatk/pull/2391:65,Availability,down,down,65,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391
https://github.com/broadinstitute/gatk/pull/2391:250,Modifiability,refactor,refactor,250,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391
https://github.com/broadinstitute/gatk/pull/2391:41,Performance,perform,performance,41,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391
https://github.com/broadinstitute/gatk/pull/2391:70,Performance,perform,performance,70,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391
https://github.com/broadinstitute/gatk/pull/2391:296,Safety,avoid,avoid,296,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391
https://github.com/broadinstitute/gatk/pull/2391:332,Safety,risk,risks,332,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391
https://github.com/broadinstitute/gatk/pull/2391:151,Testability,test,test,151,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391
https://github.com/broadinstitute/gatk/issues/2394:29,Security,access,access,29,"There currently is no way to access files in a GCS bucket that is not public. I will need this feature in the near future to analyze sensitive data. . Just discussed this with @lbergelson, and it sounds like this is something that is not a ton of work but has just been on the back-burner for a while.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394
https://github.com/broadinstitute/gatk/issues/2395:65,Security,validat,validation,65,"SamAssertionUtils.samsEqualStringent tries to short-circuit file validation by first doing an md5 compare. Most of the time when we run the test suite (in the current tests 171 out of 193), the short-circuiting fails we do the more expensive read by read test. All of these have one or more that fail md5 check (some are probably crams, which IIRC places the file name in the cram header):. MarkDuplicatesIntegrationTest; AddOrReplaceReadGroupsIntegrationTest; ApplyBQSRIntegrationTest; ApplyBQSRSparkIntegrationTest; BQSRPipelineSparkIntegrationTest; BwaSparkIntegrationTest; GatherBamFilesIntegrationTest; HaplotypeBAMWriterUnitTest; PrintReadsIntegrationTest; ReadsPipelineSparkIntegrationTest; SamAssertionUtilsUnitTest; SamFormatConverterIntegrationTest; SortReadFileSparkIntegrationTest; SortSamIntegrationTest; SplitNCigarReadsIntegrationTest; ClipReadsIntegrationTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2395
https://github.com/broadinstitute/gatk/issues/2395:140,Testability,test,test,140,"SamAssertionUtils.samsEqualStringent tries to short-circuit file validation by first doing an md5 compare. Most of the time when we run the test suite (in the current tests 171 out of 193), the short-circuiting fails we do the more expensive read by read test. All of these have one or more that fail md5 check (some are probably crams, which IIRC places the file name in the cram header):. MarkDuplicatesIntegrationTest; AddOrReplaceReadGroupsIntegrationTest; ApplyBQSRIntegrationTest; ApplyBQSRSparkIntegrationTest; BQSRPipelineSparkIntegrationTest; BwaSparkIntegrationTest; GatherBamFilesIntegrationTest; HaplotypeBAMWriterUnitTest; PrintReadsIntegrationTest; ReadsPipelineSparkIntegrationTest; SamAssertionUtilsUnitTest; SamFormatConverterIntegrationTest; SortReadFileSparkIntegrationTest; SortSamIntegrationTest; SplitNCigarReadsIntegrationTest; ClipReadsIntegrationTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2395
https://github.com/broadinstitute/gatk/issues/2395:167,Testability,test,tests,167,"SamAssertionUtils.samsEqualStringent tries to short-circuit file validation by first doing an md5 compare. Most of the time when we run the test suite (in the current tests 171 out of 193), the short-circuiting fails we do the more expensive read by read test. All of these have one or more that fail md5 check (some are probably crams, which IIRC places the file name in the cram header):. MarkDuplicatesIntegrationTest; AddOrReplaceReadGroupsIntegrationTest; ApplyBQSRIntegrationTest; ApplyBQSRSparkIntegrationTest; BQSRPipelineSparkIntegrationTest; BwaSparkIntegrationTest; GatherBamFilesIntegrationTest; HaplotypeBAMWriterUnitTest; PrintReadsIntegrationTest; ReadsPipelineSparkIntegrationTest; SamAssertionUtilsUnitTest; SamFormatConverterIntegrationTest; SortReadFileSparkIntegrationTest; SortSamIntegrationTest; SplitNCigarReadsIntegrationTest; ClipReadsIntegrationTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2395
https://github.com/broadinstitute/gatk/issues/2395:255,Testability,test,test,255,"SamAssertionUtils.samsEqualStringent tries to short-circuit file validation by first doing an md5 compare. Most of the time when we run the test suite (in the current tests 171 out of 193), the short-circuiting fails we do the more expensive read by read test. All of these have one or more that fail md5 check (some are probably crams, which IIRC places the file name in the cram header):. MarkDuplicatesIntegrationTest; AddOrReplaceReadGroupsIntegrationTest; ApplyBQSRIntegrationTest; ApplyBQSRSparkIntegrationTest; BQSRPipelineSparkIntegrationTest; BwaSparkIntegrationTest; GatherBamFilesIntegrationTest; HaplotypeBAMWriterUnitTest; PrintReadsIntegrationTest; ReadsPipelineSparkIntegrationTest; SamAssertionUtilsUnitTest; SamFormatConverterIntegrationTest; SortReadFileSparkIntegrationTest; SortSamIntegrationTest; SplitNCigarReadsIntegrationTest; ClipReadsIntegrationTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2395
https://github.com/broadinstitute/gatk/pull/2399:29,Deployability,integrat,integration,29,"tests will now run as cloud, integration, and unit on travis; this reduces our wallclock time from 30ish -> 20ish minutes. cleaned up some wierdness in the way things were specified as well",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2399
https://github.com/broadinstitute/gatk/pull/2399:67,Energy Efficiency,reduce,reduces,67,"tests will now run as cloud, integration, and unit on travis; this reduces our wallclock time from 30ish -> 20ish minutes. cleaned up some wierdness in the way things were specified as well",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2399
https://github.com/broadinstitute/gatk/pull/2399:29,Integrability,integrat,integration,29,"tests will now run as cloud, integration, and unit on travis; this reduces our wallclock time from 30ish -> 20ish minutes. cleaned up some wierdness in the way things were specified as well",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2399
https://github.com/broadinstitute/gatk/pull/2399:0,Testability,test,tests,0,"tests will now run as cloud, integration, and unit on travis; this reduces our wallclock time from 30ish -> 20ish minutes. cleaned up some wierdness in the way things were specified as well",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2399
https://github.com/broadinstitute/gatk/pull/2401:119,Integrability,message,message,119,- Disable default read filter with arguments does not longer blows up but log a warning (fixes #2357); - Improved help message for read filter (fixes #2358 and #2398); - `--disableAllReadFilters` changed to `--disableToolDefaultReadFilters` and `isDisabledFilter()` honor this behavior (fixes #2361 and #2363); - Add test for exercise the path of logging a warning when disabling a not enabled filter (as proposed in #2377); - Disabling a non-existent read filter throws a `CommandLineException` (fixes #2397),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401
https://github.com/broadinstitute/gatk/pull/2401:74,Testability,log,log,74,- Disable default read filter with arguments does not longer blows up but log a warning (fixes #2357); - Improved help message for read filter (fixes #2358 and #2398); - `--disableAllReadFilters` changed to `--disableToolDefaultReadFilters` and `isDisabledFilter()` honor this behavior (fixes #2361 and #2363); - Add test for exercise the path of logging a warning when disabling a not enabled filter (as proposed in #2377); - Disabling a non-existent read filter throws a `CommandLineException` (fixes #2397),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401
https://github.com/broadinstitute/gatk/pull/2401:317,Testability,test,test,317,- Disable default read filter with arguments does not longer blows up but log a warning (fixes #2357); - Improved help message for read filter (fixes #2358 and #2398); - `--disableAllReadFilters` changed to `--disableToolDefaultReadFilters` and `isDisabledFilter()` honor this behavior (fixes #2361 and #2363); - Add test for exercise the path of logging a warning when disabling a not enabled filter (as proposed in #2377); - Disabling a non-existent read filter throws a `CommandLineException` (fixes #2397),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401
https://github.com/broadinstitute/gatk/pull/2401:347,Testability,log,logging,347,- Disable default read filter with arguments does not longer blows up but log a warning (fixes #2357); - Improved help message for read filter (fixes #2358 and #2398); - `--disableAllReadFilters` changed to `--disableToolDefaultReadFilters` and `isDisabledFilter()` honor this behavior (fixes #2361 and #2363); - Add test for exercise the path of logging a warning when disabling a not enabled filter (as proposed in #2377); - Disabling a non-existent read filter throws a `CommandLineException` (fixes #2397),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401
https://github.com/broadinstitute/gatk/issues/2402:27,Security,authenticat,authentication,27,"NIO uses the gcloud system authentication, so we shouldn't need this anymore.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2402
https://github.com/broadinstitute/gatk/pull/2403:194,Testability,test,test,194,This would publish master-SNAPSHOT on every commit to master. It would give people a single moving snapshot to build off of if they wanted to be on the bleeding edge. Fixes #1995 . It's hard to test that it pushes only on commits to master without committing to master... but I think it should work.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2403
https://github.com/broadinstitute/gatk/pull/2404:245,Integrability,Depend,Depending,245,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:332,Integrability,depend,dependsOn,332,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:361,Integrability,depend,dependency,361,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:424,Modifiability,variab,variable,424,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:101,Performance,load,loading,101,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:22,Safety,abort,aborts,22,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:11,Testability,test,test,11,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:181,Testability,test,tests,181,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:224,Testability,test,test,224,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:284,Testability,test,tests,284,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/pull/2404:379,Testability,test,tests,379,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404
https://github.com/broadinstitute/gatk/issues/2409:311,Testability,test,tested,311,"Now that we have good NIO support for reading BAMs from GCS via ReadWalkers, we should look at CRAM as well and evaluate what changes need to be made in htsjdk/GATK to allow NIO to work for CRAMs. It's a separate code path in `SAMReaderFactory`, so I suspect it will not work immediately (though I have not yet tested).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2409
https://github.com/broadinstitute/gatk/issues/2412:1128,Testability,test,test,1128,"### SelectVariants could expand on their types (INDEL, SNP, MIXED, MNP, SYMBOLIC, NO_VARIATION) to include more types as follows:. - whether insertion or deletion as requested by user shis [here](http://gatkforums.broadinstitute.org/gatk/discussion/8988/how-to-select-insertion-or-deletion-from-a-indels-vcf-file#latest); - by star `*` allele as requested by user nkobmoo [here](http://gatkforums.broadinstitute.org/gatk/discussion/8977/how-to-filter-out-the-snps-with-deletion-allel#latest); - by AD values as requested by numerous users over the year I've been here and most recently by CanH [here](http://gatkforums.broadinstitute.org/gatk/discussion/9032/selectvariants-according-to-ad-values#latest). Also beware of something buggy described by user zzq [here](http://gatkforums.broadinstitute.org/gatk/discussion/8966/select-snp-and-reference-monomorphic-sites-from-a-large-callset#latest). I've asked them to reproduce the results using the latest version of GATK. It appears that the SYMBOLIC type, which I think must refer to the star allele, is not being recognized with v3.5-0-g36282e4. I don't have any such data to test this out with, otherwise, I'd report more on this issue. #### Current workarounds; Users are currently using AWK or JEXL to select their variants. JEXL documentation is [here](http://gatkforums.broadinstitute.org/gatk/discussion/1255/using-jexl-to-apply-hard-filters-or-select-variants-based-on-annotation-values).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2412
https://github.com/broadinstitute/gatk/issues/2415:494,Availability,error,error,494,"@kcibul reports the following issue while trying to use `SelectVariants` to read a VCF from GCS using the latest master:. ```; I was trying this out today, and ran into some problems. I built GATK4 from the latest master and ran:. VCF=gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/e11051c9-27ce-4ff1-ba70-de73bf11f312/PairedEndSingleSampleWorkflow/326f24b8-9158-4120-8df3-7fc4b928589c/call-GatherVCFs/S1-1.g.vcf.gz; ; ./gatk-launch SelectVariants -V $VCF -L 1:1000-2000 -O foo.vcf; and I get an error about token being expired (see below). However, I can run. gsutil cat $VCF | zcat | head; without a problem, so I don't think my gcloud credentials are the problem. com.google.cloud.storage.StorageException: 400 Bad Request; {; ""error"" : ""invalid_grant"",; ""error_description"" : ""Token has been expired or revoked.""; }; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:202); 	at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:348); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:531); 	at java.nio.file.Files.exists(Files.java:2385); 	at org.broadinstitute.hellbender.utils.io.IOUtils.assertFileIsReadable(IOUtils.java:567); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:248); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:209); 	at org.broadinstitute.hellbender.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:54); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415
https://github.com/broadinstitute/gatk/issues/2415:729,Availability,error,error,729,"@kcibul reports the following issue while trying to use `SelectVariants` to read a VCF from GCS using the latest master:. ```; I was trying this out today, and ran into some problems. I built GATK4 from the latest master and ran:. VCF=gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/e11051c9-27ce-4ff1-ba70-de73bf11f312/PairedEndSingleSampleWorkflow/326f24b8-9158-4120-8df3-7fc4b928589c/call-GatherVCFs/S1-1.g.vcf.gz; ; ./gatk-launch SelectVariants -V $VCF -L 1:1000-2000 -O foo.vcf; and I get an error about token being expired (see below). However, I can run. gsutil cat $VCF | zcat | head; without a problem, so I don't think my gcloud credentials are the problem. com.google.cloud.storage.StorageException: 400 Bad Request; {; ""error"" : ""invalid_grant"",; ""error_description"" : ""Token has been expired or revoked.""; }; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:202); 	at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:348); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:531); 	at java.nio.file.Files.exists(Files.java:2385); 	at org.broadinstitute.hellbender.utils.io.IOUtils.assertFileIsReadable(IOUtils.java:567); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:248); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:209); 	at org.broadinstitute.hellbender.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:54); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415
https://github.com/broadinstitute/gatk/issues/2415:2805,Availability,error,error,2805,"e.hellbender.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:54); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:41); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:432); 	at org.broadinstitute.hellbender.engine.VariantWalker.onStartup(VariantWalker.java:43); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); Caused by: shaded.cloud-nio.com.google.api.client.http.HttpResponseException: 400 Bad Request; {; ""error"" : ""invalid_grant"",; ""error_description"" : ""Token has been expired or revoked.""; }; 	at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1061); 	at shaded.cloud-nio.com.google.auth.oauth2.UserCredentials.refreshAccessToken(UserCredentials.java:121); 	at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:97); 	at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:74); 	at shaded.cloud-nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:65); 	at com.google.cloud.HttpServiceOptions$1.initialize(HttpServiceOptions.java:233); 	at shaded.cloud-nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415
https://github.com/broadinstitute/gatk/issues/2415:1564,Testability,assert,assertFileIsReadable,1564,"cat | head; without a problem, so I don't think my gcloud credentials are the problem. com.google.cloud.storage.StorageException: 400 Bad Request; {; ""error"" : ""invalid_grant"",; ""error_description"" : ""Token has been expired or revoked.""; }; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:202); 	at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:348); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:531); 	at java.nio.file.Files.exists(Files.java:2385); 	at org.broadinstitute.hellbender.utils.io.IOUtils.assertFileIsReadable(IOUtils.java:567); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:248); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:209); 	at org.broadinstitute.hellbender.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:54); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:41); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:432); 	at org.broadinstitute.hellbender.engine.VariantWalker.onStartup(VariantWalker.java:43); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415
https://github.com/broadinstitute/gatk/pull/2417:231,Availability,ERROR,ERROR,231,"*before*: GATK crashes with a stack trace. The stack trace contains useful; info, but:; * it's hard to read; * it doesn't include the name of the file we cannot access. *now*, a better message that addresses both issues:; > A USER ERROR has occurred: Couldn't read file gs://(...). Error was:; > 401: Anonymous users does not have storage.objects.get access to object (...). Additional information (including a stack trace) is displayed if the; user specifies `--verbosity=DEBUG`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417
https://github.com/broadinstitute/gatk/pull/2417:282,Availability,Error,Error,282,"*before*: GATK crashes with a stack trace. The stack trace contains useful; info, but:; * it's hard to read; * it doesn't include the name of the file we cannot access. *now*, a better message that addresses both issues:; > A USER ERROR has occurred: Couldn't read file gs://(...). Error was:; > 401: Anonymous users does not have storage.objects.get access to object (...). Additional information (including a stack trace) is displayed if the; user specifies `--verbosity=DEBUG`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417
https://github.com/broadinstitute/gatk/pull/2417:185,Integrability,message,message,185,"*before*: GATK crashes with a stack trace. The stack trace contains useful; info, but:; * it's hard to read; * it doesn't include the name of the file we cannot access. *now*, a better message that addresses both issues:; > A USER ERROR has occurred: Couldn't read file gs://(...). Error was:; > 401: Anonymous users does not have storage.objects.get access to object (...). Additional information (including a stack trace) is displayed if the; user specifies `--verbosity=DEBUG`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417
https://github.com/broadinstitute/gatk/pull/2417:161,Security,access,access,161,"*before*: GATK crashes with a stack trace. The stack trace contains useful; info, but:; * it's hard to read; * it doesn't include the name of the file we cannot access. *now*, a better message that addresses both issues:; > A USER ERROR has occurred: Couldn't read file gs://(...). Error was:; > 401: Anonymous users does not have storage.objects.get access to object (...). Additional information (including a stack trace) is displayed if the; user specifies `--verbosity=DEBUG`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417
https://github.com/broadinstitute/gatk/pull/2417:351,Security,access,access,351,"*before*: GATK crashes with a stack trace. The stack trace contains useful; info, but:; * it's hard to read; * it doesn't include the name of the file we cannot access. *now*, a better message that addresses both issues:; > A USER ERROR has occurred: Couldn't read file gs://(...). Error was:; > 401: Anonymous users does not have storage.objects.get access to object (...). Additional information (including a stack trace) is displayed if the; user specifies `--verbosity=DEBUG`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417
https://github.com/broadinstitute/gatk/issues/2418:90,Testability,test,test,90,"From @droazen on https://github.com/broadinstitute/gatk/pull/2350. > Would be good to add test cases with: 1. secondary alignments in addition to primary alignments 2. supplementary alignments in addition to primary alignments, and possibly 3. the case of more than two reads in a fragment (in which case both ""first of pair"" and ""second of pair"" can be set).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2418
https://github.com/broadinstitute/gatk/issues/2422:287,Availability,Error,Error,287,"Writing output via NIO is currently broken, as reported by @jean-philippe-martin:. ```; /home/jpmartin/build/install/gatk/bin/gatk PrintReads -I inputs/CEUTrio.HiSeq.WEx.b37.NA12892.bam -L 10:1000000-2000000 -O gs://jpmartin/staging/out_tmp.bam; htsjdk.samtools.util.RuntimeIOException: Error opening file: /home/jpmartin/gs:/jpmartin/staging/out_tmp.bam; at htsjdk.samtools.SAMFileWriterFactory.makeBAMWriter(SAMFileWriterFactory.java:246); ```. Now that the read support is in good shape, let's get the writing end working -- should hopefully be an easy fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2422
https://github.com/broadinstitute/gatk/issues/2422:109,Deployability,install,install,109,"Writing output via NIO is currently broken, as reported by @jean-philippe-martin:. ```; /home/jpmartin/build/install/gatk/bin/gatk PrintReads -I inputs/CEUTrio.HiSeq.WEx.b37.NA12892.bam -L 10:1000000-2000000 -O gs://jpmartin/staging/out_tmp.bam; htsjdk.samtools.util.RuntimeIOException: Error opening file: /home/jpmartin/gs:/jpmartin/staging/out_tmp.bam; at htsjdk.samtools.SAMFileWriterFactory.makeBAMWriter(SAMFileWriterFactory.java:246); ```. Now that the read support is in good shape, let's get the writing end working -- should hopefully be an easy fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2422
https://github.com/broadinstitute/gatk/pull/2423:0,Deployability,Update,Update,0,"Update GKL to 0.4.1 and HTSJDK to 2.9.1. GKL 0.4.1 provides the following improvements over GKL 0.1.2 (currently used by GATK4):; - Improved performance and compression ratio for level-1 compression; - Improved performance for compression levels > 1; - Decompression acceleration with `IntelInflater`. The `IntelInflater` is enabled by default and can be disabled with the option `--use_jdk_inflater`. Resolves #2421, #2315, #2302",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423
https://github.com/broadinstitute/gatk/pull/2423:141,Performance,perform,performance,141,"Update GKL to 0.4.1 and HTSJDK to 2.9.1. GKL 0.4.1 provides the following improvements over GKL 0.1.2 (currently used by GATK4):; - Improved performance and compression ratio for level-1 compression; - Improved performance for compression levels > 1; - Decompression acceleration with `IntelInflater`. The `IntelInflater` is enabled by default and can be disabled with the option `--use_jdk_inflater`. Resolves #2421, #2315, #2302",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423
https://github.com/broadinstitute/gatk/pull/2423:211,Performance,perform,performance,211,"Update GKL to 0.4.1 and HTSJDK to 2.9.1. GKL 0.4.1 provides the following improvements over GKL 0.1.2 (currently used by GATK4):; - Improved performance and compression ratio for level-1 compression; - Improved performance for compression levels > 1; - Decompression acceleration with `IntelInflater`. The `IntelInflater` is enabled by default and can be disabled with the option `--use_jdk_inflater`. Resolves #2421, #2315, #2302",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423
https://github.com/broadinstitute/gatk/issues/2430:307,Availability,error,error,307,"From https://github.com/broadinstitute/dsde-docs/issues/1517.; For `ApplyBQSR`, if the input covariates table file for base quality score recalibration has a read group in the quality score (RecalTable1) or covariates (RecalTable2) report that is not in read group report (RecalTable0), give an informative error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2430
https://github.com/broadinstitute/gatk/pull/2431:38,Integrability,message,message,38,Implements #2430.; Made the exception message informative.; Created a test where the input covariates table file has a read group in the quality score (RecalTable1) or covariates (RecalTable2) report that is not in read group report (RecalTable0).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2431
https://github.com/broadinstitute/gatk/pull/2431:70,Testability,test,test,70,Implements #2430.; Made the exception message informative.; Created a test where the input covariates table file has a read group in the quality score (RecalTable1) or covariates (RecalTable2) report that is not in read group report (RecalTable0).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2431
https://github.com/broadinstitute/gatk/issues/2432:572,Performance,perform,perform,572,"@chandrans commented on [Wed Sep 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1476). ## Feature request; ### Tool(s) involved. PrintReads; ### Description. Right now, we have a flag that outputs reads with a MQ greater than the set threshold. Users may want to only output reads that have a MQ below a certain threshold. There should be a flag (something like -maximumMappingQuality) that only emits reads that have less than the set Mapping Quality. . For example, a user needs this to compare low mapQ reads from different samples, blast search and/or perform de novo assembly. ---. This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/gatk/discussion/comment/32943#Comment_32943) . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1476#issuecomment-260498511). Might end up being a feature request for bidirectional read filtering functionality in GATK4. . ---. @chandrans commented on [Tue Feb 28 2017](https://github.com/broadinstitute/gsa-unstable/issues/1476#issuecomment-283141618). Okay. Should I keep this open or close it/move to GATK4?. ---. @vdauwera commented on [Tue Feb 28 2017](https://github.com/broadinstitute/gsa-unstable/issues/1476#issuecomment-283150007). Yeah move to 4 please",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2432
https://github.com/broadinstitute/gatk/pull/2435:94,Deployability,pipeline,pipeline,94,"@cwhelan @tedsharpe @vruano @mwalker174 , I've organized the scripts for running the whole sv pipeline as it exists right now. There used to be a PR (if I recall correctly) but since that's outdated, why not an up-to-date one. Here's how to run it. 1. To create the cluster. ```; ./create_cluster.sh broad-dsde-methods sv-methods-1 broad-dsde-methods/sv; ```; where the 1st argument is the project name, 2nd argument is the cluster name, and the 3rd name is the place where input data lives. 2. To run the whole pipeline. ```; ./svCall.sh /Users/shuang/GATK/gatk sv-methods-1 /user/shuang/NA12878_PCR-_30X; ```; where the 1st argument is the location of my GATK directory and the 3rd argument is the location of all outputs on the cluster. So change them as necessary. The different stages called by the master script `svCall.sh` are (in order) `scanBam.sh` -> `assembly.sh` -> `alignAssembly.sh` -> `callVariants.sh`, which all take the same arguments. 3. To delete the whole cluster. ```; ./delete_cluster.sh sv-methods-1; ```; This avoids having to wait for the web-based Console's confirmation. One thing to note though, is that I've copied everything to a bucket at; ```; gs://broad-dsde-methods/sv/; ```; under a different project. We used to be developing under the project ""broad-dsde-dev"", but we are asked to move to project ""broad-dsde-methods"". So to run these scripts, you might need to switch to a different project via. ```; gcloud config set project broad-dsde-methods; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435
https://github.com/broadinstitute/gatk/pull/2435:512,Deployability,pipeline,pipeline,512,"@cwhelan @tedsharpe @vruano @mwalker174 , I've organized the scripts for running the whole sv pipeline as it exists right now. There used to be a PR (if I recall correctly) but since that's outdated, why not an up-to-date one. Here's how to run it. 1. To create the cluster. ```; ./create_cluster.sh broad-dsde-methods sv-methods-1 broad-dsde-methods/sv; ```; where the 1st argument is the project name, 2nd argument is the cluster name, and the 3rd name is the place where input data lives. 2. To run the whole pipeline. ```; ./svCall.sh /Users/shuang/GATK/gatk sv-methods-1 /user/shuang/NA12878_PCR-_30X; ```; where the 1st argument is the location of my GATK directory and the 3rd argument is the location of all outputs on the cluster. So change them as necessary. The different stages called by the master script `svCall.sh` are (in order) `scanBam.sh` -> `assembly.sh` -> `alignAssembly.sh` -> `callVariants.sh`, which all take the same arguments. 3. To delete the whole cluster. ```; ./delete_cluster.sh sv-methods-1; ```; This avoids having to wait for the web-based Console's confirmation. One thing to note though, is that I've copied everything to a bucket at; ```; gs://broad-dsde-methods/sv/; ```; under a different project. We used to be developing under the project ""broad-dsde-dev"", but we are asked to move to project ""broad-dsde-methods"". So to run these scripts, you might need to switch to a different project via. ```; gcloud config set project broad-dsde-methods; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435
https://github.com/broadinstitute/gatk/pull/2435:1447,Modifiability,config,config,1447,"@cwhelan @tedsharpe @vruano @mwalker174 , I've organized the scripts for running the whole sv pipeline as it exists right now. There used to be a PR (if I recall correctly) but since that's outdated, why not an up-to-date one. Here's how to run it. 1. To create the cluster. ```; ./create_cluster.sh broad-dsde-methods sv-methods-1 broad-dsde-methods/sv; ```; where the 1st argument is the project name, 2nd argument is the cluster name, and the 3rd name is the place where input data lives. 2. To run the whole pipeline. ```; ./svCall.sh /Users/shuang/GATK/gatk sv-methods-1 /user/shuang/NA12878_PCR-_30X; ```; where the 1st argument is the location of my GATK directory and the 3rd argument is the location of all outputs on the cluster. So change them as necessary. The different stages called by the master script `svCall.sh` are (in order) `scanBam.sh` -> `assembly.sh` -> `alignAssembly.sh` -> `callVariants.sh`, which all take the same arguments. 3. To delete the whole cluster. ```; ./delete_cluster.sh sv-methods-1; ```; This avoids having to wait for the web-based Console's confirmation. One thing to note though, is that I've copied everything to a bucket at; ```; gs://broad-dsde-methods/sv/; ```; under a different project. We used to be developing under the project ""broad-dsde-dev"", but we are asked to move to project ""broad-dsde-methods"". So to run these scripts, you might need to switch to a different project via. ```; gcloud config set project broad-dsde-methods; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435
https://github.com/broadinstitute/gatk/pull/2435:1035,Safety,avoid,avoids,1035,"@cwhelan @tedsharpe @vruano @mwalker174 , I've organized the scripts for running the whole sv pipeline as it exists right now. There used to be a PR (if I recall correctly) but since that's outdated, why not an up-to-date one. Here's how to run it. 1. To create the cluster. ```; ./create_cluster.sh broad-dsde-methods sv-methods-1 broad-dsde-methods/sv; ```; where the 1st argument is the project name, 2nd argument is the cluster name, and the 3rd name is the place where input data lives. 2. To run the whole pipeline. ```; ./svCall.sh /Users/shuang/GATK/gatk sv-methods-1 /user/shuang/NA12878_PCR-_30X; ```; where the 1st argument is the location of my GATK directory and the 3rd argument is the location of all outputs on the cluster. So change them as necessary. The different stages called by the master script `svCall.sh` are (in order) `scanBam.sh` -> `assembly.sh` -> `alignAssembly.sh` -> `callVariants.sh`, which all take the same arguments. 3. To delete the whole cluster. ```; ./delete_cluster.sh sv-methods-1; ```; This avoids having to wait for the web-based Console's confirmation. One thing to note though, is that I've copied everything to a bucket at; ```; gs://broad-dsde-methods/sv/; ```; under a different project. We used to be developing under the project ""broad-dsde-dev"", but we are asked to move to project ""broad-dsde-methods"". So to run these scripts, you might need to switch to a different project via. ```; gcloud config set project broad-dsde-methods; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435
https://github.com/broadinstitute/gatk/issues/2436:101,Availability,error,error,101,"When reading BAM or VCF files using the NIO code, sometimes it fails with a 503 ""server unavailable"" error. In this case we should retry instead of giving up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2436
https://github.com/broadinstitute/gatk/issues/2437:24,Deployability,release,release,24,"GATK4 depends on a beta release of Protocol Buffer 3.0.0, specifically ""3.0.0-beta-1"" because other versions break the tests depending on hadoop.dfs.minicluster.utils. Eventually, we must move up to a stable version of protocol buffers i.e. either .3.0.0 or higher.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2437
https://github.com/broadinstitute/gatk/issues/2437:6,Integrability,depend,depends,6,"GATK4 depends on a beta release of Protocol Buffer 3.0.0, specifically ""3.0.0-beta-1"" because other versions break the tests depending on hadoop.dfs.minicluster.utils. Eventually, we must move up to a stable version of protocol buffers i.e. either .3.0.0 or higher.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2437
https://github.com/broadinstitute/gatk/issues/2437:35,Integrability,Protocol,Protocol,35,"GATK4 depends on a beta release of Protocol Buffer 3.0.0, specifically ""3.0.0-beta-1"" because other versions break the tests depending on hadoop.dfs.minicluster.utils. Eventually, we must move up to a stable version of protocol buffers i.e. either .3.0.0 or higher.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2437
https://github.com/broadinstitute/gatk/issues/2437:125,Integrability,depend,depending,125,"GATK4 depends on a beta release of Protocol Buffer 3.0.0, specifically ""3.0.0-beta-1"" because other versions break the tests depending on hadoop.dfs.minicluster.utils. Eventually, we must move up to a stable version of protocol buffers i.e. either .3.0.0 or higher.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2437
https://github.com/broadinstitute/gatk/issues/2437:219,Integrability,protocol,protocol,219,"GATK4 depends on a beta release of Protocol Buffer 3.0.0, specifically ""3.0.0-beta-1"" because other versions break the tests depending on hadoop.dfs.minicluster.utils. Eventually, we must move up to a stable version of protocol buffers i.e. either .3.0.0 or higher.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2437
https://github.com/broadinstitute/gatk/issues/2437:119,Testability,test,tests,119,"GATK4 depends on a beta release of Protocol Buffer 3.0.0, specifically ""3.0.0-beta-1"" because other versions break the tests depending on hadoop.dfs.minicluster.utils. Eventually, we must move up to a stable version of protocol buffers i.e. either .3.0.0 or higher.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2437
https://github.com/broadinstitute/gatk/issues/2439:62,Modifiability,extend,extending,62,"As noted by @mbabadi, it would be useful to allow TableReader extending classes to process comments present in the input.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2439
https://github.com/broadinstitute/gatk/issues/2442:344,Availability,robust,robust,344,"`ReadsSparkSource.putPairsInSamePartition()` is currently rather naive -- it assumes that in queryname-sorted data, read pairs are always sorted with the first of pair first and the second of pair second, that there are no secondary or supplementary alignments, and that there are always two reads per fragment. We should make this method more robust:. 1. It should not assume that each pair is sorted with the first of pair first and the second of pair second. This is not an invariant of queryname sort order, which requires only that reads with the same name be together. 2. It should work in the presence of secondary and supplementary alignments. 3. It should work even if there are more than two reads per fragment (which is the case with some sequencing technologies). All of these points could be resolved by simply iterating until the read name changes when peeking into each partition. Once we make that change, we should add good unit tests to cover the above 3 cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2442
https://github.com/broadinstitute/gatk/issues/2442:946,Testability,test,tests,946,"`ReadsSparkSource.putPairsInSamePartition()` is currently rather naive -- it assumes that in queryname-sorted data, read pairs are always sorted with the first of pair first and the second of pair second, that there are no secondary or supplementary alignments, and that there are always two reads per fragment. We should make this method more robust:. 1. It should not assume that each pair is sorted with the first of pair first and the second of pair second. This is not an invariant of queryname sort order, which requires only that reads with the same name be together. 2. It should work in the presence of secondary and supplementary alignments. 3. It should work even if there are more than two reads per fragment (which is the case with some sequencing technologies). All of these points could be resolved by simply iterating until the read name changes when peeking into each partition. Once we make that change, we should add good unit tests to cover the above 3 cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2442
https://github.com/broadinstitute/gatk/issues/2442:817,Usability,simpl,simply,817,"`ReadsSparkSource.putPairsInSamePartition()` is currently rather naive -- it assumes that in queryname-sorted data, read pairs are always sorted with the first of pair first and the second of pair second, that there are no secondary or supplementary alignments, and that there are always two reads per fragment. We should make this method more robust:. 1. It should not assume that each pair is sorted with the first of pair first and the second of pair second. This is not an invariant of queryname sort order, which requires only that reads with the same name be together. 2. It should work in the presence of secondary and supplementary alignments. 3. It should work even if there are more than two reads per fragment (which is the case with some sequencing technologies). All of these points could be resolved by simply iterating until the read name changes when peeking into each partition. Once we make that change, we should add good unit tests to cover the above 3 cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2442
https://github.com/broadinstitute/gatk/issues/2443:154,Integrability,message,message,154,The fact that you can print stack traces on UserException is not very discoverable. We should probably include instructions to do so in the UserException message itself. We might want to write the stack trace to a file so that people don't have to rerun the program to get it as well. . It's also weird that it's set through an environment variable instead of as an argument. (Although it may be difficult to implement as an argument since it has to be set correctly even if argument parsing fails. @cmnbroad Any thoughts on that? ),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443
https://github.com/broadinstitute/gatk/issues/2443:340,Modifiability,variab,variable,340,The fact that you can print stack traces on UserException is not very discoverable. We should probably include instructions to do so in the UserException message itself. We might want to write the stack trace to a file so that people don't have to rerun the program to get it as well. . It's also weird that it's set through an environment variable instead of as an argument. (Although it may be difficult to implement as an argument since it has to be set correctly even if argument parsing fails. @cmnbroad Any thoughts on that? ),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443
https://github.com/broadinstitute/gatk/issues/2446:117,Security,certificate,certificate,117,https://gatk-jenkins.broadinstitute.org/ is currently giving a warning when you try to visit it. We need to fix it's certificate so that it doesn't give a scary warning. It says it expired several days ago. @davidbernick Could you look into this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2446
https://github.com/broadinstitute/gatk/issues/2449:225,Availability,ERROR,ERROR,225,Our [spark tests on jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) are failing with:; ```; Runtime.totalMemory()=554696704; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:189); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getParallelReads(ReadsSparkSource.java:93); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getUnfilteredReads(GATKSparkTool.java:238); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:2776,Availability,ERROR,ERROR,2776,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:2877,Availability,ERROR,ERROR,2877,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:2232,Deployability,deploy,deploy,2232,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:2269,Deployability,deploy,deploy,2269,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:2342,Deployability,deploy,deploy,2342,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:2419,Deployability,deploy,deploy,2419,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:2491,Deployability,deploy,deploy,2491,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:2561,Deployability,deploy,deploy,2561,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:74,Performance,Perform,Performance,74,Our [spark tests on jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) are failing with:; ```; Runtime.totalMemory()=554696704; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:189); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getParallelReads(ReadsSparkSource.java:93); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getUnfilteredReads(GATKSparkTool.java:238); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:11,Testability,test,tests,11,Our [spark tests on jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) are failing with:; ```; Runtime.totalMemory()=554696704; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:189); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getParallelReads(ReadsSparkSource.java:93); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getUnfilteredReads(GATKSparkTool.java:238); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:292,Testability,test,test-jenkins,292,Our [spark tests on jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) are failing with:; ```; Runtime.totalMemory()=554696704; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:189); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getParallelReads(ReadsSparkSource.java:93); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getUnfilteredReads(GATKSparkTool.java:238); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2449:545,Testability,test,test-jenkins,545,Our [spark tests on jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) are failing with:; ```; Runtime.totalMemory()=554696704; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:189); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getParallelReads(ReadsSparkSource.java:93); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getUnfilteredReads(GATKSparkTool.java:238); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449
https://github.com/broadinstitute/gatk/issues/2451:375,Performance,load,load,375,BwaMemIntegrationTest fails on gsa5 and crashes the test suite. . Bwa produces ```[bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!``` and then exits which kills the test suite in a gross way. @SHuang-Broad points out that this often indicates that the version of bwa that was used to generate the index has a difference from the version that is used to load the index. It's unclear what's happening here because that same test with the same files passes on travis and on mac.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2451
https://github.com/broadinstitute/gatk/issues/2451:146,Safety,Abort,Abort,146,BwaMemIntegrationTest fails on gsa5 and crashes the test suite. . Bwa produces ```[bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!``` and then exits which kills the test suite in a gross way. @SHuang-Broad points out that this often indicates that the version of bwa that was used to generate the index has a difference from the version that is used to load the index. It's unclear what's happening here because that same test with the same files passes on travis and on mac.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2451
https://github.com/broadinstitute/gatk/issues/2451:52,Testability,test,test,52,BwaMemIntegrationTest fails on gsa5 and crashes the test suite. . Bwa produces ```[bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!``` and then exits which kills the test suite in a gross way. @SHuang-Broad points out that this often indicates that the version of bwa that was used to generate the index has a difference from the version that is used to load the index. It's unclear what's happening here because that same test with the same files passes on travis and on mac.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2451
https://github.com/broadinstitute/gatk/issues/2451:187,Testability,test,test,187,BwaMemIntegrationTest fails on gsa5 and crashes the test suite. . Bwa produces ```[bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!``` and then exits which kills the test suite in a gross way. @SHuang-Broad points out that this often indicates that the version of bwa that was used to generate the index has a difference from the version that is used to load the index. It's unclear what's happening here because that same test with the same files passes on travis and on mac.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2451
https://github.com/broadinstitute/gatk/issues/2451:444,Testability,test,test,444,BwaMemIntegrationTest fails on gsa5 and crashes the test suite. . Bwa produces ```[bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!``` and then exits which kills the test suite in a gross way. @SHuang-Broad points out that this often indicates that the version of bwa that was used to generate the index has a difference from the version that is used to load the index. It's unclear what's happening here because that same test with the same files passes on travis and on mac.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2451
https://github.com/broadinstitute/gatk/issues/2454:24,Deployability,update,update,24,"Please, I would like to update my GATK dependency to the latest master. Nevertheless, the Barclay version included contains a bug that is solved in https://github.com/broadinstitute/barclay/commit/87c3fa228fa96d11b7177f8bfdfca4801c83a068, but a change in the usage method of Barclay before the fix to print hidden arguments broke the GATK command line class. Could it be possible to update to the latest Barclay master, please?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2454
https://github.com/broadinstitute/gatk/issues/2454:383,Deployability,update,update,383,"Please, I would like to update my GATK dependency to the latest master. Nevertheless, the Barclay version included contains a bug that is solved in https://github.com/broadinstitute/barclay/commit/87c3fa228fa96d11b7177f8bfdfca4801c83a068, but a change in the usage method of Barclay before the fix to print hidden arguments broke the GATK command line class. Could it be possible to update to the latest Barclay master, please?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2454
https://github.com/broadinstitute/gatk/issues/2454:39,Integrability,depend,dependency,39,"Please, I would like to update my GATK dependency to the latest master. Nevertheless, the Barclay version included contains a bug that is solved in https://github.com/broadinstitute/barclay/commit/87c3fa228fa96d11b7177f8bfdfca4801c83a068, but a change in the usage method of Barclay before the fix to print hidden arguments broke the GATK command line class. Could it be possible to update to the latest Barclay master, please?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2454
https://github.com/broadinstitute/gatk/pull/2456:124,Performance,perform,performing,124,"@cwhelan @tedsharpe @vruano please review.; It is intended for copying the right data for the ""correctly""-named cluster for performing intended analysis.; Cluster provisioning time is now ~25 minutes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2456
https://github.com/broadinstitute/gatk/issues/2457:123,Deployability,deploy,deployment,123,Then gatk-protected and gatk docker images can specify this one in ``USING``. See gatk-protected scripts/docker for both a deployment script and a Dockerfile that (subset) can be used for a base image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2457
https://github.com/broadinstitute/gatk/issues/2462:28,Testability,test,test,28,@droazen @lbergelson - Need test input gVCF files and golden output for the GenomicsDB importer test code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2462
https://github.com/broadinstitute/gatk/issues/2462:96,Testability,test,test,96,@droazen @lbergelson - Need test input gVCF files and golden output for the GenomicsDB importer test code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2462
https://github.com/broadinstitute/gatk/issues/2464:1073,Performance,scalab,scalable,1073,"This feature request stems from discussion at the end of <https://github.com/broadinstitute/gsa-unstable/issues/1547> for CombineVariants. If the field as a whole (and its tools) does not have a deterministic logic in ordering ALT alleles in multiallelic VCF records of mixed types (@vdauwera says AFAIK there is no order), then I think it will be useful to have an option when combining sites of different types, e.g. INDEL and SNP, to be able to represent the ALT alleles according to a user-provided file. ALT alleles not present in the user-provided file would become a separate VCF record. I ask for this feature because (outside) tools like Beagle (identity-by-descent calculations) require identical representation of ALT alleles to its resource file. Records that do not match in CHROM, POS, REF and ALT are ignored by the tool. I have used an outside tool to merge records vertically so each site is represented once. It wasn't clear to me how the ALT alleles were ordered so I manually reviewed the 80-some multiallelic sites of mixed type after the merge. Not a scalable solution.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2464
https://github.com/broadinstitute/gatk/issues/2464:209,Testability,log,logic,209,"This feature request stems from discussion at the end of <https://github.com/broadinstitute/gsa-unstable/issues/1547> for CombineVariants. If the field as a whole (and its tools) does not have a deterministic logic in ordering ALT alleles in multiallelic VCF records of mixed types (@vdauwera says AFAIK there is no order), then I think it will be useful to have an option when combining sites of different types, e.g. INDEL and SNP, to be able to represent the ALT alleles according to a user-provided file. ALT alleles not present in the user-provided file would become a separate VCF record. I ask for this feature because (outside) tools like Beagle (identity-by-descent calculations) require identical representation of ALT alleles to its resource file. Records that do not match in CHROM, POS, REF and ALT are ignored by the tool. I have used an outside tool to merge records vertically so each site is represented once. It wasn't clear to me how the ALT alleles were ordered so I manually reviewed the 80-some multiallelic sites of mixed type after the merge. Not a scalable solution.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2464
https://github.com/broadinstitute/gatk/issues/2464:937,Usability,clear,clear,937,"This feature request stems from discussion at the end of <https://github.com/broadinstitute/gsa-unstable/issues/1547> for CombineVariants. If the field as a whole (and its tools) does not have a deterministic logic in ordering ALT alleles in multiallelic VCF records of mixed types (@vdauwera says AFAIK there is no order), then I think it will be useful to have an option when combining sites of different types, e.g. INDEL and SNP, to be able to represent the ALT alleles according to a user-provided file. ALT alleles not present in the user-provided file would become a separate VCF record. I ask for this feature because (outside) tools like Beagle (identity-by-descent calculations) require identical representation of ALT alleles to its resource file. Records that do not match in CHROM, POS, REF and ALT are ignored by the tool. I have used an outside tool to merge records vertically so each site is represented once. It wasn't clear to me how the ALT alleles were ordered so I manually reviewed the 80-some multiallelic sites of mixed type after the merge. Not a scalable solution.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2464
https://github.com/broadinstitute/gatk/issues/2466:486,Integrability,rout,routines,486,"In GATK3 `GATKVariantContextUtils` line 852 we have:. ```; if ( newLikelihoods == null || (originalVC.getAttributeAsInt(VCFConstants.DEPTH_KEY, 0) == 0 && likelihoodsAreUninformative(newLikelihoods) )) {; gb.noPL();; } else {; gb.PL(newLikelihoods);; }; ```. The equivalent block of code in GATK4 is in `AlleleSubsettingUtils` line 71 (since `GATKVariantContextUtils.createGenotypesWithSubsettedLikelihoods()` was extracted to a separate class in GATK4 along with all allele subsetting routines). ```; final boolean useNewLikelihoods = newLikelihoods != null && GATKVariantContextUtils.isInformative(newLikelihoods);; final GenotypeBuilder gb = useNewLikelihoods ? new GenotypeBuilder(g).PL(newLikelihoods) : new GenotypeBuilder(g).noPL();; ```. Note that the GATK4 version of this line lacks the `(originalVC.getAttributeAsInt(VCFConstants.DEPTH_KEY, 0) == 0` check present in GATK3. This check was added to GATK3 by @davidbenjamin as part of his ""Genotype GVCF emits QUAL for ref sites"" PR (https://github.com/broadinstitute/gsa-unstable/pull/1346), but appears to have never made it to GATK4.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2466
https://github.com/broadinstitute/gatk/pull/2467:200,Testability,test,test,200,"Currently the interval arguments are ignored in `FeatureWalker` (#2310) because the initialization of intervals is done after the ones for features. This PR includes the following changes:; - Failing test (as described in #2310); - Fix by moving the code for set the intervals for traversal to `onStartup()`, as in the case of the reads in `ReadWalker`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2467
https://github.com/broadinstitute/gatk/issues/2484:49,Availability,error,errors,49,@cmnbroad reports that he's seeing out-of-memory errors when running the test suite locally. He says it could be a recently-introduced regression -- we should narrow it down to a single commit.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2484
https://github.com/broadinstitute/gatk/issues/2484:169,Availability,down,down,169,@cmnbroad reports that he's seeing out-of-memory errors when running the test suite locally. He says it could be a recently-introduced regression -- we should narrow it down to a single commit.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2484
https://github.com/broadinstitute/gatk/issues/2484:73,Testability,test,test,73,@cmnbroad reports that he's seeing out-of-memory errors when running the test suite locally. He says it could be a recently-introduced regression -- we should narrow it down to a single commit.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2484
https://github.com/broadinstitute/gatk/issues/2485:1863,Availability,robust,robust,1863,"typeGVCFs. The set of sample calls in a pair should be derived from WGS and WEx data for the same sample.; * </p>; ```. Does it _have_ to be WGS + WEx? Could it be WGS + WGS or WEx + Wex for example?. ```; * <h3>Output</h3>; * <p>; * A combined VCF with combined calls for each pair of samples specified and de-uniquified sample names.; * </p>; *; * <h3>Examples</h3>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * -o output.vcf; * </pre>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * --uniquified_sample_name NA12878.variant \; * --uniquified_sample_name NA12878.variant2; * -o output.vcf; * </pre>; ```. I don't get what's the difference between the first and second example. . In any case I'm not going to push this through now in light of all the TODOs:. ```; /*TODO: when this tool is moved into protected the following will have to be addressed:; * Do more robust error checking on sample name de-uniquification -- right now checks for pairs of <sampleName>.variantX and <sampleName>.variantY but should be extended to allow tagged VCF input into GenotypeGVCFs, which will produce names like <sampleName>.RODtagName; * Move sample name uniqufication/de-uniquification to SampleListUtils.java; * Check to make sure all genotype attributes are preserved after merge, e.g. allele phasing and genotype filters; * Generalize for all ploidies?; * Change GenotypeGVCFs --uniquifySamples argument from hidden (maybe still keep @advanced?); *; */; ```. ---. @ldgauthier commented on [Mon Nov 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-159059807). 1) Could be any two sets of data, I just did WGS + WEx for GTEx; 2) I think the first example probably should have two -V entries (that have the same sample names) compared with the second that has one input -V that has already uniquified samples. ---. @vdauwera commented on [Mo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:1870,Availability,error,error,1870,"typeGVCFs. The set of sample calls in a pair should be derived from WGS and WEx data for the same sample.; * </p>; ```. Does it _have_ to be WGS + WEx? Could it be WGS + WGS or WEx + Wex for example?. ```; * <h3>Output</h3>; * <p>; * A combined VCF with combined calls for each pair of samples specified and de-uniquified sample names.; * </p>; *; * <h3>Examples</h3>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * -o output.vcf; * </pre>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * --uniquified_sample_name NA12878.variant \; * --uniquified_sample_name NA12878.variant2; * -o output.vcf; * </pre>; ```. I don't get what's the difference between the first and second example. . In any case I'm not going to push this through now in light of all the TODOs:. ```; /*TODO: when this tool is moved into protected the following will have to be addressed:; * Do more robust error checking on sample name de-uniquification -- right now checks for pairs of <sampleName>.variantX and <sampleName>.variantY but should be extended to allow tagged VCF input into GenotypeGVCFs, which will produce names like <sampleName>.RODtagName; * Move sample name uniqufication/de-uniquification to SampleListUtils.java; * Check to make sure all genotype attributes are preserved after merge, e.g. allele phasing and genotype filters; * Generalize for all ploidies?; * Change GenotypeGVCFs --uniquifySamples argument from hidden (maybe still keep @advanced?); *; */; ```. ---. @ldgauthier commented on [Mon Nov 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-159059807). 1) Could be any two sets of data, I just did WGS + WEx for GTEx; 2) I think the first example probably should have two -V entries (that have the same sample names) compared with the second that has one input -V that has already uniquified samples. ---. @vdauwera commented on [Mo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:4750,Availability,avail,available,4750,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:4110,Deployability,integrat,integrated,4110,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:4203,Deployability,release,release,4203,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:4467,Deployability,release,release,4467,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:4539,Deployability,integrat,integrates,4539,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:4644,Deployability,release,release,4644,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:4110,Integrability,integrat,integrated,4110,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:4539,Integrability,integrat,integrates,4539,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:2013,Modifiability,extend,extended,2013,"r example?. ```; * <h3>Output</h3>; * <p>; * A combined VCF with combined calls for each pair of samples specified and de-uniquified sample names.; * </p>; *; * <h3>Examples</h3>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * -o output.vcf; * </pre>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * --uniquified_sample_name NA12878.variant \; * --uniquified_sample_name NA12878.variant2; * -o output.vcf; * </pre>; ```. I don't get what's the difference between the first and second example. . In any case I'm not going to push this through now in light of all the TODOs:. ```; /*TODO: when this tool is moved into protected the following will have to be addressed:; * Do more robust error checking on sample name de-uniquification -- right now checks for pairs of <sampleName>.variantX and <sampleName>.variantY but should be extended to allow tagged VCF input into GenotypeGVCFs, which will produce names like <sampleName>.RODtagName; * Move sample name uniqufication/de-uniquification to SampleListUtils.java; * Check to make sure all genotype attributes are preserved after merge, e.g. allele phasing and genotype filters; * Generalize for all ploidies?; * Change GenotypeGVCFs --uniquifySamples argument from hidden (maybe still keep @advanced?); *; */; ```. ---. @ldgauthier commented on [Mon Nov 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-159059807). 1) Could be any two sets of data, I just did WGS + WEx for GTEx; 2) I think the first example probably should have two -V entries (that have the same sample names) compared with the second that has one input -V that has already uniquified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:4102,Performance,perform,perform,4102,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:247,Safety,avoid,avoid,247,"@vdauwera commented on [Fri Nov 06 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208). Following on https://github.com/broadinstitute/dsde-docs/issues/308. This walker enables combining callsets originating from the same sample, to avoid having to re-run the variant calling. . ---. @vdauwera commented on [Fri Nov 06 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-154586008). When this gets done we should notify the user in the original issue ticket. ---. @vdauwera commented on [Fri Nov 20 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-158551620). Hey @ldgauthier, I'm looking at the docs for this tool and I'm not clear on the acceptable scope of application based on the usage instructions:. ```; <h3>Input</h3>; * <p>; * A VCF containing pairs of samples, as uniquified by GenotypeGVCFs. The set of sample calls in a pair should be derived from WGS and WEx data for the same sample.; * </p>; ```. Does it _have_ to be WGS + WEx? Could it be WGS + WGS or WEx + Wex for example?. ```; * <h3>Output</h3>; * <p>; * A combined VCF with combined calls for each pair of samples specified and de-uniquified sample names.; * </p>; *; * <h3>Examples</h3>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * -o output.vcf; * </pre>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * --uniquified_sample_name NA12878.variant \; * --uniquified_sample_name NA12878.variant2; * -o output.vcf; * </pre>; ```. I don't get what's the difference between the first and second example. . In any case I'm not going to push this through now in light of all the TODOs:. ```; /*TODO: when this tool is moved into protected the following will have to be addressed:; * Do more robust error checking on sample name de-uniquification -- right now checks for pairs of <sampleName>.variantX and <sampleName>.variantY bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2485:696,Usability,clear,clear,696,"@vdauwera commented on [Fri Nov 06 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208). Following on https://github.com/broadinstitute/dsde-docs/issues/308. This walker enables combining callsets originating from the same sample, to avoid having to re-run the variant calling. . ---. @vdauwera commented on [Fri Nov 06 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-154586008). When this gets done we should notify the user in the original issue ticket. ---. @vdauwera commented on [Fri Nov 20 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-158551620). Hey @ldgauthier, I'm looking at the docs for this tool and I'm not clear on the acceptable scope of application based on the usage instructions:. ```; <h3>Input</h3>; * <p>; * A VCF containing pairs of samples, as uniquified by GenotypeGVCFs. The set of sample calls in a pair should be derived from WGS and WEx data for the same sample.; * </p>; ```. Does it _have_ to be WGS + WEx? Could it be WGS + WGS or WEx + Wex for example?. ```; * <h3>Output</h3>; * <p>; * A combined VCF with combined calls for each pair of samples specified and de-uniquified sample names.; * </p>; *; * <h3>Examples</h3>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * -o output.vcf; * </pre>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * --uniquified_sample_name NA12878.variant \; * --uniquified_sample_name NA12878.variant2; * -o output.vcf; * </pre>; ```. I don't get what's the difference between the first and second example. . In any case I'm not going to push this through now in light of all the TODOs:. ```; /*TODO: when this tool is moved into protected the following will have to be addressed:; * Do more robust error checking on sample name de-uniquification -- right now checks for pairs of <sampleName>.variantX and <sampleName>.variantY bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485
https://github.com/broadinstitute/gatk/issues/2486:72,Integrability,depend,dependency,72,Would be very helpful for projects like htsjdk that can't have a direct dependency on `google-cloud-java`!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2486
https://github.com/broadinstitute/gatk/issues/2489:1026,Deployability,INTEGRAT,INTEGRATION,1026,"## Feature request. Since CombineVariants will not be ported, we need equivalent functionality to its ability to annotate ""set"", ie which callset(s) a site is present in. Here is an excerpt from a tutorial that describes this functionality in action:. ----. To find out which set each variant belongs to, we can use CombineVariants. CombineVariants has a way to annotate each site with which set the site belongs to. For example, if a site is in GIAB and failed hard filtering but passed VQSR, CombineVariants will annotate the site with set=G-filterInH-V. The ""filterIn"" flag before the filtering method tells us the site failed the filtering method, hence it was ""filtered"" in the set. java -jar GenomeAnalysisTK.jar \; 	-T CombineVariants \; 	-R ref/human_g1k_b37_20.fasta \; 	-V:G truth_dataset/NA12878.GIAB.vcf \; 	-V:H vcfs/NA12878.hard.filtered.vcf \; 	-V:V vcfs/NA12878.VQSR.filtered.vcf \; 	-o sandbox/NA12878.Combined.vcf . The set-annotated VCF looks like this:. ````; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT INTEGRATION NA12878; 20 61795 rs4814683 G T 2034.16 PASS AC=2;AF=0.500;AN=4;(...);set=Intersection​ ​ GT:AD:ADALL:DP :GQ:PL 0/1:218,205:172,169:769:99 0/1:30,30:.:60:99:1003,0,1027; ````. In this record, ""set=Intersection​"" indicates this record was present and unfiltered in all callsets considered. Here is a key of all the possible combinations for this 3-way venn:. | Meaning | Annotation |; |:-|:-|; | In GIAB only | G |; | In GIAB and failed VQSR only | G-H-filterInV |; | In GIAB and failed both hard filtering and VQSR | G-filterInH-filterInV |; | In GIAB and failed hard filtering only | G-filterInH-V |; | In GIAB and passed both hard filtering and VQSR | Intersection |; | Not in GIAB and failed VQSR only | H-filterInV |; | Not in GIAB and failed both hard filtering and VQSR | FilteredInAll |; | Not in GIAB and failed hard filtering only | filterInH-V |; | Not in GIAB and passed both hard filtering and VQSR | H-V |",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2489
https://github.com/broadinstitute/gatk/issues/2489:1026,Integrability,INTEGRAT,INTEGRATION,1026,"## Feature request. Since CombineVariants will not be ported, we need equivalent functionality to its ability to annotate ""set"", ie which callset(s) a site is present in. Here is an excerpt from a tutorial that describes this functionality in action:. ----. To find out which set each variant belongs to, we can use CombineVariants. CombineVariants has a way to annotate each site with which set the site belongs to. For example, if a site is in GIAB and failed hard filtering but passed VQSR, CombineVariants will annotate the site with set=G-filterInH-V. The ""filterIn"" flag before the filtering method tells us the site failed the filtering method, hence it was ""filtered"" in the set. java -jar GenomeAnalysisTK.jar \; 	-T CombineVariants \; 	-R ref/human_g1k_b37_20.fasta \; 	-V:G truth_dataset/NA12878.GIAB.vcf \; 	-V:H vcfs/NA12878.hard.filtered.vcf \; 	-V:V vcfs/NA12878.VQSR.filtered.vcf \; 	-o sandbox/NA12878.Combined.vcf . The set-annotated VCF looks like this:. ````; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT INTEGRATION NA12878; 20 61795 rs4814683 G T 2034.16 PASS AC=2;AF=0.500;AN=4;(...);set=Intersection​ ​ GT:AD:ADALL:DP :GQ:PL 0/1:218,205:172,169:769:99 0/1:30,30:.:60:99:1003,0,1027; ````. In this record, ""set=Intersection​"" indicates this record was present and unfiltered in all callsets considered. Here is a key of all the possible combinations for this 3-way venn:. | Meaning | Annotation |; |:-|:-|; | In GIAB only | G |; | In GIAB and failed VQSR only | G-H-filterInV |; | In GIAB and failed both hard filtering and VQSR | G-filterInH-filterInV |; | In GIAB and failed hard filtering only | G-filterInH-V |; | In GIAB and passed both hard filtering and VQSR | Intersection |; | Not in GIAB and failed VQSR only | H-filterInV |; | Not in GIAB and failed both hard filtering and VQSR | FilteredInAll |; | Not in GIAB and failed hard filtering only | filterInH-V |; | Not in GIAB and passed both hard filtering and VQSR | H-V |",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2489
https://github.com/broadinstitute/gatk/issues/2489:903,Modifiability,sandbox,sandbox,903,"## Feature request. Since CombineVariants will not be ported, we need equivalent functionality to its ability to annotate ""set"", ie which callset(s) a site is present in. Here is an excerpt from a tutorial that describes this functionality in action:. ----. To find out which set each variant belongs to, we can use CombineVariants. CombineVariants has a way to annotate each site with which set the site belongs to. For example, if a site is in GIAB and failed hard filtering but passed VQSR, CombineVariants will annotate the site with set=G-filterInH-V. The ""filterIn"" flag before the filtering method tells us the site failed the filtering method, hence it was ""filtered"" in the set. java -jar GenomeAnalysisTK.jar \; 	-T CombineVariants \; 	-R ref/human_g1k_b37_20.fasta \; 	-V:G truth_dataset/NA12878.GIAB.vcf \; 	-V:H vcfs/NA12878.hard.filtered.vcf \; 	-V:V vcfs/NA12878.VQSR.filtered.vcf \; 	-o sandbox/NA12878.Combined.vcf . The set-annotated VCF looks like this:. ````; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT INTEGRATION NA12878; 20 61795 rs4814683 G T 2034.16 PASS AC=2;AF=0.500;AN=4;(...);set=Intersection​ ​ GT:AD:ADALL:DP :GQ:PL 0/1:218,205:172,169:769:99 0/1:30,30:.:60:99:1003,0,1027; ````. In this record, ""set=Intersection​"" indicates this record was present and unfiltered in all callsets considered. Here is a key of all the possible combinations for this 3-way venn:. | Meaning | Annotation |; |:-|:-|; | In GIAB only | G |; | In GIAB and failed VQSR only | G-H-filterInV |; | In GIAB and failed both hard filtering and VQSR | G-filterInH-filterInV |; | In GIAB and failed hard filtering only | G-filterInH-V |; | In GIAB and passed both hard filtering and VQSR | Intersection |; | Not in GIAB and failed VQSR only | H-filterInV |; | Not in GIAB and failed both hard filtering and VQSR | FilteredInAll |; | Not in GIAB and failed hard filtering only | filterInH-V |; | Not in GIAB and passed both hard filtering and VQSR | H-V |",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2489
https://github.com/broadinstitute/gatk/issues/2489:903,Testability,sandbox,sandbox,903,"## Feature request. Since CombineVariants will not be ported, we need equivalent functionality to its ability to annotate ""set"", ie which callset(s) a site is present in. Here is an excerpt from a tutorial that describes this functionality in action:. ----. To find out which set each variant belongs to, we can use CombineVariants. CombineVariants has a way to annotate each site with which set the site belongs to. For example, if a site is in GIAB and failed hard filtering but passed VQSR, CombineVariants will annotate the site with set=G-filterInH-V. The ""filterIn"" flag before the filtering method tells us the site failed the filtering method, hence it was ""filtered"" in the set. java -jar GenomeAnalysisTK.jar \; 	-T CombineVariants \; 	-R ref/human_g1k_b37_20.fasta \; 	-V:G truth_dataset/NA12878.GIAB.vcf \; 	-V:H vcfs/NA12878.hard.filtered.vcf \; 	-V:V vcfs/NA12878.VQSR.filtered.vcf \; 	-o sandbox/NA12878.Combined.vcf . The set-annotated VCF looks like this:. ````; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT INTEGRATION NA12878; 20 61795 rs4814683 G T 2034.16 PASS AC=2;AF=0.500;AN=4;(...);set=Intersection​ ​ GT:AD:ADALL:DP :GQ:PL 0/1:218,205:172,169:769:99 0/1:30,30:.:60:99:1003,0,1027; ````. In this record, ""set=Intersection​"" indicates this record was present and unfiltered in all callsets considered. Here is a key of all the possible combinations for this 3-way venn:. | Meaning | Annotation |; |:-|:-|; | In GIAB only | G |; | In GIAB and failed VQSR only | G-H-filterInV |; | In GIAB and failed both hard filtering and VQSR | G-filterInH-filterInV |; | In GIAB and failed hard filtering only | G-filterInH-V |; | In GIAB and passed both hard filtering and VQSR | Intersection |; | Not in GIAB and failed VQSR only | H-filterInV |; | Not in GIAB and failed both hard filtering and VQSR | FilteredInAll |; | Not in GIAB and failed hard filtering only | filterInH-V |; | Not in GIAB and passed both hard filtering and VQSR | H-V |",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2489
https://github.com/broadinstitute/gatk/issues/2490:138,Availability,failure,failure,138,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490
https://github.com/broadinstitute/gatk/issues/2490:103,Testability,test,tests,103,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490
https://github.com/broadinstitute/gatk/issues/2490:201,Testability,Test,Test,201,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490
https://github.com/broadinstitute/gatk/issues/2490:207,Testability,Test,Test,207,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490
https://github.com/broadinstitute/gatk/issues/2490:219,Testability,test,testWritingToFileURL,219,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490
https://github.com/broadinstitute/gatk/issues/2490:269,Testability,test,test,269,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490
https://github.com/broadinstitute/gatk/issues/2490:530,Testability,test,test,530,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490
https://github.com/broadinstitute/gatk/issues/2490:641,Testability,test,test,641,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490
https://github.com/broadinstitute/gatk/issues/2493:69,Integrability,interface,interface,69,"Some annotation base classes don't implement the `Annotation` marker interface. (i.e. `VariantAnnotation`) This was the case in gatk3 as well, but it's unclear why. It may be a historical artifact rather than a design decision, in which case it should be corrected. If it's important to have the marker interface then all the annotations should implement them. If it isn't, then we should probably remove it or move abstract functions into the interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2493
https://github.com/broadinstitute/gatk/issues/2493:303,Integrability,interface,interface,303,"Some annotation base classes don't implement the `Annotation` marker interface. (i.e. `VariantAnnotation`) This was the case in gatk3 as well, but it's unclear why. It may be a historical artifact rather than a design decision, in which case it should be corrected. If it's important to have the marker interface then all the annotations should implement them. If it isn't, then we should probably remove it or move abstract functions into the interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2493
https://github.com/broadinstitute/gatk/issues/2493:444,Integrability,interface,interface,444,"Some annotation base classes don't implement the `Annotation` marker interface. (i.e. `VariantAnnotation`) This was the case in gatk3 as well, but it's unclear why. It may be a historical artifact rather than a design decision, in which case it should be corrected. If it's important to have the marker interface then all the annotations should implement them. If it isn't, then we should probably remove it or move abstract functions into the interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2493
https://github.com/broadinstitute/gatk/issues/2496:217,Deployability,release,release,217,"See #2488 for context. In short, the internal pathways for authentication changed, breaking some tests. We're pushing forward anyways but need to remember to re-enable the code & tests once we can (should be the next release).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2496
https://github.com/broadinstitute/gatk/issues/2496:59,Security,authenticat,authentication,59,"See #2488 for context. In short, the internal pathways for authentication changed, breaking some tests. We're pushing forward anyways but need to remember to re-enable the code & tests once we can (should be the next release).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2496
https://github.com/broadinstitute/gatk/issues/2496:97,Testability,test,tests,97,"See #2488 for context. In short, the internal pathways for authentication changed, breaking some tests. We're pushing forward anyways but need to remember to re-enable the code & tests once we can (should be the next release).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2496
https://github.com/broadinstitute/gatk/issues/2496:179,Testability,test,tests,179,"See #2488 for context. In short, the internal pathways for authentication changed, breaking some tests. We're pushing forward anyways but need to remember to re-enable the code & tests once we can (should be the next release).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2496
https://github.com/broadinstitute/gatk/issues/2498:203,Deployability,release,release,203,"@ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508). ## Bug Report. ### Affected tool(s); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version -- at least back to 3.1; - [x] Latest development/master branch as of Nov. 10, 2016; ### Description ; Addition of a rare SNP to a common deletion changes the representation of the deletion.; ![image](https://cloud.githubusercontent.com/assets/6578548/20180442/825d4a14-a728-11e6-9caa-bfad9e20378d.png); Top bam is the BWA-aligned bam for the sample with deletion and SNP, middle bam is the BWA-aligned bam for the sample with just the deletion, bottom bam is the bamout from calling the two together (artificial haplotypes in the top readgroup). The haplotype with the 9bp deletion stays the same, but the haplotype with the SNP turns into two separate deletions -- one of 1bp and one of 8bp. #### Steps to reproduce; Data where this was reported is sensitive, but I added a unit test that reproduces the problem using the haplotype sequences from the data (SWPairwiseAlignmentUnitTest::testLongHapDeletionNearSNP in the branch ldg_SWparamExamples). #### Expected behavior; CIGARs for the deletion haplotype sequences with and without the SNP should be the same. #### Actual behavior; CIGARs for the deletion haplotype sequences with and without the SNP have different deletions. Specifically, in this case, the SNP causes the deletion to be represented as two separate deletions seemingly to avoid the mismatch penalty. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-259727960). I did find that using SWParameterSet.STANDARD_NGS as the parameters instead of CigarUtils.NEW_SW_PARAMETERS will resolve this particular case, but a more comprehensive analysis would be required before we make the change since this will impact a lot of other events too. ---. @ldgauthier commented on [Thu Nov 10 2016](https://g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498
https://github.com/broadinstitute/gatk/issues/2498:2677,Performance,optimiz,optimization,2677,"1508#issuecomment-259727960). I did find that using SWParameterSet.STANDARD_NGS as the parameters instead of CigarUtils.NEW_SW_PARAMETERS will resolve this particular case, but a more comprehensive analysis would be required before we make the change since this will impact a lot of other events too. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-259744146). And just to provide a little more context, this is a problem in the clinical context because there's often an analysis step where potential causal variants are filtered out if they are seen in a normal reference panel (like ExAC) at more than some threshold allele frequency. If the representation doesn't match, then that step gets a lot more complicated than the way it's currently done. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-287818162). I head our Intel friends are working on a new SW implementation (or maybe ""just"" optimization?). . @droazen is this relevant to the Intel SW work? And should any/all improvements along these lines be done in GATK4 going forward?. ---. @droazen commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-287885846). @vdauwera Yes, Intel is working on a native implementation of `SmithWaterman` at our request -- it should materialize this quarter. Any changes/fixes to the Java implementation are relevant to Intel's work, since that is the implementation whose output they will have to match :) Of course, we'd certainly like such improvements to be implemented in GATK4 -- whether they are backported to GATK3 as well is up to you. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-287889598). @ldgauthier Any objections to this getting tackled in GATK4?. ---. @ldgauthier commented on [Mon Mar 20 2017](https://github.com/broadinstitu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498
https://github.com/broadinstitute/gatk/issues/2498:1513,Safety,avoid,avoid,1513,"8/20180442/825d4a14-a728-11e6-9caa-bfad9e20378d.png); Top bam is the BWA-aligned bam for the sample with deletion and SNP, middle bam is the BWA-aligned bam for the sample with just the deletion, bottom bam is the bamout from calling the two together (artificial haplotypes in the top readgroup). The haplotype with the 9bp deletion stays the same, but the haplotype with the SNP turns into two separate deletions -- one of 1bp and one of 8bp. #### Steps to reproduce; Data where this was reported is sensitive, but I added a unit test that reproduces the problem using the haplotype sequences from the data (SWPairwiseAlignmentUnitTest::testLongHapDeletionNearSNP in the branch ldg_SWparamExamples). #### Expected behavior; CIGARs for the deletion haplotype sequences with and without the SNP should be the same. #### Actual behavior; CIGARs for the deletion haplotype sequences with and without the SNP have different deletions. Specifically, in this case, the SNP causes the deletion to be represented as two separate deletions seemingly to avoid the mismatch penalty. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-259727960). I did find that using SWParameterSet.STANDARD_NGS as the parameters instead of CigarUtils.NEW_SW_PARAMETERS will resolve this particular case, but a more comprehensive analysis would be required before we make the change since this will impact a lot of other events too. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-259744146). And just to provide a little more context, this is a problem in the clinical context because there's often an analysis step where potential causal variants are filtered out if they are seen in a normal reference panel (like ExAC) at more than some threshold allele frequency. If the representation doesn't match, then that step gets a lot more complicated than the way it's currently done. ---.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498
https://github.com/broadinstitute/gatk/issues/2498:1000,Testability,test,test,1000,"6](https://github.com/broadinstitute/gsa-unstable/issues/1508). ## Bug Report. ### Affected tool(s); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version -- at least back to 3.1; - [x] Latest development/master branch as of Nov. 10, 2016; ### Description ; Addition of a rare SNP to a common deletion changes the representation of the deletion.; ![image](https://cloud.githubusercontent.com/assets/6578548/20180442/825d4a14-a728-11e6-9caa-bfad9e20378d.png); Top bam is the BWA-aligned bam for the sample with deletion and SNP, middle bam is the BWA-aligned bam for the sample with just the deletion, bottom bam is the bamout from calling the two together (artificial haplotypes in the top readgroup). The haplotype with the 9bp deletion stays the same, but the haplotype with the SNP turns into two separate deletions -- one of 1bp and one of 8bp. #### Steps to reproduce; Data where this was reported is sensitive, but I added a unit test that reproduces the problem using the haplotype sequences from the data (SWPairwiseAlignmentUnitTest::testLongHapDeletionNearSNP in the branch ldg_SWparamExamples). #### Expected behavior; CIGARs for the deletion haplotype sequences with and without the SNP should be the same. #### Actual behavior; CIGARs for the deletion haplotype sequences with and without the SNP have different deletions. Specifically, in this case, the SNP causes the deletion to be represented as two separate deletions seemingly to avoid the mismatch penalty. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-259727960). I did find that using SWParameterSet.STANDARD_NGS as the parameters instead of CigarUtils.NEW_SW_PARAMETERS will resolve this particular case, but a more comprehensive analysis would be required before we make the change since this will impact a lot of other events too. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/is",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498
https://github.com/broadinstitute/gatk/issues/2498:1107,Testability,test,testLongHapDeletionNearSNP,1107,"6](https://github.com/broadinstitute/gsa-unstable/issues/1508). ## Bug Report. ### Affected tool(s); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version -- at least back to 3.1; - [x] Latest development/master branch as of Nov. 10, 2016; ### Description ; Addition of a rare SNP to a common deletion changes the representation of the deletion.; ![image](https://cloud.githubusercontent.com/assets/6578548/20180442/825d4a14-a728-11e6-9caa-bfad9e20378d.png); Top bam is the BWA-aligned bam for the sample with deletion and SNP, middle bam is the BWA-aligned bam for the sample with just the deletion, bottom bam is the bamout from calling the two together (artificial haplotypes in the top readgroup). The haplotype with the 9bp deletion stays the same, but the haplotype with the SNP turns into two separate deletions -- one of 1bp and one of 8bp. #### Steps to reproduce; Data where this was reported is sensitive, but I added a unit test that reproduces the problem using the haplotype sequences from the data (SWPairwiseAlignmentUnitTest::testLongHapDeletionNearSNP in the branch ldg_SWparamExamples). #### Expected behavior; CIGARs for the deletion haplotype sequences with and without the SNP should be the same. #### Actual behavior; CIGARs for the deletion haplotype sequences with and without the SNP have different deletions. Specifically, in this case, the SNP causes the deletion to be represented as two separate deletions seemingly to avoid the mismatch penalty. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-259727960). I did find that using SWParameterSet.STANDARD_NGS as the parameters instead of CigarUtils.NEW_SW_PARAMETERS will resolve this particular case, but a more comprehensive analysis would be required before we make the change since this will impact a lot of other events too. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/is",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498
https://github.com/broadinstitute/gatk/pull/2500:45,Integrability,wrap,wrapper,45,previously system properties were set by the wrapper script but not by gatk-launch when running with a local jar; now both local jar and wrapper script get the same properties; more properties are now explicitely set for spark tools as well; fixes #2316,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2500
https://github.com/broadinstitute/gatk/pull/2500:137,Integrability,wrap,wrapper,137,previously system properties were set by the wrapper script but not by gatk-launch when running with a local jar; now both local jar and wrapper script get the same properties; more properties are now explicitely set for spark tools as well; fixes #2316,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2500
https://github.com/broadinstitute/gatk/issues/2503:1219,Deployability,deploy,deploy,1219,TKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.Read,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503
https://github.com/broadinstitute/gatk/issues/2503:1256,Deployability,deploy,deploy,1256,3); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503
https://github.com/broadinstitute/gatk/issues/2503:1329,Deployability,deploy,deploy,1329,neProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.hellbender.engine.spark.datasources.Rea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503
https://github.com/broadinstitute/gatk/issues/2503:1406,Deployability,deploy,deploy,1406,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:119); 	at org.broadinstitute.hellb,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503
https://github.com/broadinstitute/gatk/issues/2503:1478,Deployability,deploy,deploy,1478,; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:119); 	at org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark.runTool(BwaSpark.java:49); 	... 17 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503
https://github.com/broadinstitute/gatk/issues/2503:1548,Deployability,deploy,deploy,1548,; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:119); 	at org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark.runTool(BwaSpark.java:49); 	... 17 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503
https://github.com/broadinstitute/gatk/issues/2504:119,Deployability,integrat,integration,119,This or these tool would be use to use piece meal bam files that we can use for local (laptop/desktop) development and integration tests. The input would be SV called variant VCF with and two intervals lists. The first one would indicate for what variants in input we want to gather the evidence reads and the second the supported reference interval in the output (anything outside that interval will be changed to unmapped). The second interval list must include the first interval list since otherwise it would not make any sense.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2504
https://github.com/broadinstitute/gatk/issues/2504:119,Integrability,integrat,integration,119,This or these tool would be use to use piece meal bam files that we can use for local (laptop/desktop) development and integration tests. The input would be SV called variant VCF with and two intervals lists. The first one would indicate for what variants in input we want to gather the evidence reads and the second the supported reference interval in the output (anything outside that interval will be changed to unmapped). The second interval list must include the first interval list since otherwise it would not make any sense.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2504
https://github.com/broadinstitute/gatk/issues/2504:131,Testability,test,tests,131,This or these tool would be use to use piece meal bam files that we can use for local (laptop/desktop) development and integration tests. The input would be SV called variant VCF with and two intervals lists. The first one would indicate for what variants in input we want to gather the evidence reads and the second the supported reference interval in the output (anything outside that interval will be changed to unmapped). The second interval list must include the first interval list since otherwise it would not make any sense.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2504
https://github.com/broadinstitute/gatk/issues/2507:7806,Availability,down,down,7806," @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLPwS6I5nu9TQiw4BFqRojmTiL0aks5rnq_OgaJpZM4FaLwX>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:8244,Availability,down,down,8244," @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLPwS6I5nu9TQiw4BFqRojmTiL0aks5rnq_OgaJpZM4FaLwX>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:4514,Deployability,update,update,4514,"h/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:4596,Deployability,integrat,integration,4596,"h/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:7627,Deployability,release,release,7627," the info field annotations. ---. @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLPwS6I5nu9TQiw4BFqRojmT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:8049,Deployability,pipeline,pipelines,8049," @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLPwS6I5nu9TQiw4BFqRojmTiL0aks5rnq_OgaJpZM4FaLwX>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:4596,Integrability,integrat,integration,4596,"h/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:135,Security,Validat,ValidateVariants,135,"MIGRATED FROM GATK3. @ldgauthier commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053). Currently ValidateVariants relies on genotypes to transitively check that each alt allele occurs in at least one sample and that the AC adds up. However, this can fail on sites-only files because there are no genotypes. We should use the definition of the info annotations in the header to check how many entries each should have.; ### Outline; - Add a new validation type for info-field counts to enum and to switch statement; - Grab info headers from input VCF with something like GATKVCFUtils.getVCFHeadersFromRods(getToolkit(), variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; - In the map() function, for each info header line, call on each VCFInfoHeaderLine getCount(vc) to get the expected number of info annotation entries; - Compare the expected number with a count based on vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some additional parsing because it returns an Object; - (Bonus points if you use the isFixedCount() and getCount() functions on the VCF info header line to simplify annotations that aren't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:482,Security,validat,validation,482,"MIGRATED FROM GATK3. @ldgauthier commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053). Currently ValidateVariants relies on genotypes to transitively check that each alt allele occurs in at least one sample and that the AC adds up. However, this can fail on sites-only files because there are no genotypes. We should use the definition of the info annotations in the header to check how many entries each should have.; ### Outline; - Add a new validation type for info-field counts to enum and to switch statement; - Grab info headers from input VCF with something like GATKVCFUtils.getVCFHeadersFromRods(getToolkit(), variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; - In the map() function, for each info header line, call on each VCFInfoHeaderLine getCount(vc) to get the expected number of info annotation entries; - Compare the expected number with a count based on vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some additional parsing because it returns an Object; - (Bonus points if you use the isFixedCount() and getCount() functions on the VCF info header line to simplify annotations that aren't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:1332,Security,validat,validation,1332,"genotypes. We should use the definition of the info annotations in the header to check how many entries each should have.; ### Outline; - Add a new validation type for info-field counts to enum and to switch statement; - Grab info headers from input VCF with something like GATKVCFUtils.getVCFHeadersFromRods(getToolkit(), variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; - In the map() function, for each info header line, call on each VCFInfoHeaderLine getCount(vc) to get the expected number of info annotation entries; - Compare the expected number with a count based on vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some additional parsing because it returns an Object; - (Bonus points if you use the isFixedCount() and getCount() functions on the VCF info header line to simplify annotations that aren't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very appropriate to validate in htsjdk. On Thu, Jul 16, 2015 at 4:05 PM, ldgauthier notifications@github.com; wrote:. > Currently ValidateVariants relies on genotypes to transitively check that; > each alt allele occurs in at least one sample and that the AC adds up.; > However, this can fail on sites-only files because there are no geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:1443,Security,validat,validator,1443,"ne; - Add a new validation type for info-field counts to enum and to switch statement; - Grab info headers from input VCF with something like GATKVCFUtils.getVCFHeadersFromRods(getToolkit(), variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; - In the map() function, for each info header line, call on each VCFInfoHeaderLine getCount(vc) to get the expected number of info annotation entries; - Compare the expected number with a count based on vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some additional parsing because it returns an Object; - (Bonus points if you use the isFixedCount() and getCount() functions on the VCF info header line to simplify annotations that aren't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very appropriate to validate in htsjdk. On Thu, Jul 16, 2015 at 4:05 PM, ldgauthier notifications@github.com; wrote:. > Currently ValidateVariants relies on genotypes to transitively check that; > each alt allele occurs in at least one sample and that the AC adds up.; > However, this can fail on sites-only files because there are no genotypes.; > We should use the definition of the info annotations in the header to check; > how many entries each should have.; > Outlin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:1753,Security,validat,validation,1753,"ch info header line, call on each VCFInfoHeaderLine getCount(vc) to get the expected number of info annotation entries; - Compare the expected number with a count based on vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some additional parsing because it returns an Object; - (Bonus points if you use the isFixedCount() and getCount() functions on the VCF info header line to simplify annotations that aren't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very appropriate to validate in htsjdk. On Thu, Jul 16, 2015 at 4:05 PM, ldgauthier notifications@github.com; wrote:. > Currently ValidateVariants relies on genotypes to transitively check that; > each alt allele occurs in at least one sample and that the AC adds up.; > However, this can fail on sites-only files because there are no genotypes.; > We should use the definition of the info annotations in the header to check; > how many entries each should have.; > Outline; > - Add a new validation type for info-field counts to enum and to; > switch statement; > - Grab info headers from input VCF with something like; > GATKVCFUtils.getVCFHeadersFromRods(getToolkit(),; > variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; > - In the map() f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:2015,Security,validat,validate,2015,"ome additional parsing because it returns an Object; - (Bonus points if you use the isFixedCount() and getCount() functions on the VCF info header line to simplify annotations that aren't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very appropriate to validate in htsjdk. On Thu, Jul 16, 2015 at 4:05 PM, ldgauthier notifications@github.com; wrote:. > Currently ValidateVariants relies on genotypes to transitively check that; > each alt allele occurs in at least one sample and that the AC adds up.; > However, this can fail on sites-only files because there are no genotypes.; > We should use the definition of the info annotations in the header to check; > how many entries each should have.; > Outline; > - Add a new validation type for info-field counts to enum and to; > switch statement; > - Grab info headers from input VCF with something like; > GATKVCFUtils.getVCFHeadersFromRods(getToolkit(),; > variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; > - In the map() function, for each info header line, call on each; > VCFInfoHeaderLine getCount(vc) to get the expected number of info; > annotation entries; > - Compare the expected number with a count based on; > vc.getAttribute(currentVCFinfoHeaderLine.getID",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:2125,Security,Validat,ValidateVariants,2125,"en't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very appropriate to validate in htsjdk. On Thu, Jul 16, 2015 at 4:05 PM, ldgauthier notifications@github.com; wrote:. > Currently ValidateVariants relies on genotypes to transitively check that; > each alt allele occurs in at least one sample and that the AC adds up.; > However, this can fail on sites-only files because there are no genotypes.; > We should use the definition of the info annotations in the header to check; > how many entries each should have.; > Outline; > - Add a new validation type for info-field counts to enum and to; > switch statement; > - Grab info headers from input VCF with something like; > GATKVCFUtils.getVCFHeadersFromRods(getToolkit(),; > variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; > - In the map() function, for each info header line, call on each; > VCFInfoHeaderLine getCount(vc) to get the expected number of info; > annotation entries; > - Compare the expected number with a count based on; > vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some; > additional parsing because it returns an Object; > - (Bonus points if you use the isFixedCount() and getCount() functions; > on the VCF info header li",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:2484,Security,validat,validation,2484,"O field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very appropriate to validate in htsjdk. On Thu, Jul 16, 2015 at 4:05 PM, ldgauthier notifications@github.com; wrote:. > Currently ValidateVariants relies on genotypes to transitively check that; > each alt allele occurs in at least one sample and that the AC adds up.; > However, this can fail on sites-only files because there are no genotypes.; > We should use the definition of the info annotations in the header to check; > how many entries each should have.; > Outline; > - Add a new validation type for info-field counts to enum and to; > switch statement; > - Grab info headers from input VCF with something like; > GATKVCFUtils.getVCFHeadersFromRods(getToolkit(),; > variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; > - In the map() function, for each info header line, call on each; > VCFInfoHeaderLine getCount(vc) to get the expected number of info; > annotation entries; > - Compare the expected number with a count based on; > vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some; > additional parsing because it returns an Object; > - (Bonus points if you use the isFixedCount() and getCount() functions; > on the VCF info header line to simplify annotations that aren't according; > to the number of alt alleles); > ; > Test data; > ; > /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:3379,Security,validat,validation,3379,"info annotations in the header to check; > how many entries each should have.; > Outline; > - Add a new validation type for info-field counts to enum and to; > switch statement; > - Grab info headers from input VCF with something like; > GATKVCFUtils.getVCFHeadersFromRods(getToolkit(),; > variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; > - In the map() function, for each info header line, call on each; > VCFInfoHeaderLine getCount(vc) to get the expected number of info; > annotation entries; > - Compare the expected number with a count based on; > vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some; > additional parsing because it returns an Object; > - (Bonus points if you use the isFixedCount() and getCount() functions; > on the VCF info header line to simplify annotations that aren't according; > to the number of alt alleles); > ; > Test data; > ; > /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for valid",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:3494,Security,validat,validator,3494,"e for info-field counts to enum and to; > switch statement; > - Grab info headers from input VCF with something like; > GATKVCFUtils.getVCFHeadersFromRods(getToolkit(),; > variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; > - In the map() function, for each info header line, call on each; > VCFInfoHeaderLine getCount(vc) to get the expected number of info; > annotation entries; > - Compare the expected number with a count based on; > vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some; > additional parsing because it returns an Object; > - (Bonus points if you use the isFixedCount() and getCount() functions; > on the VCF info header line to simplify annotations that aren't according; > to the number of alt alleles); > ; > Test data; > ; > /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After chang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:3815,Security,validat,validation,3815,"fo header line, call on each; > VCFInfoHeaderLine getCount(vc) to get the expected number of info; > annotation entries; > - Compare the expected number with a count based on; > vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some; > additional parsing because it returns an Object; > - (Bonus points if you use the isFixedCount() and getCount() functions; > on the VCF info header line to simplify annotations that aren't according; > to the number of alt alleles); > ; > Test data; > ; > /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:4375,Security,validat,validateInfoFieldCounts,4375,"uthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:4521,Security,Validat,ValidateVariants,4521,"h/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:5394,Security,validat,validation,5394,"derLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613997). Agreed. ---. @ronlevine commented on [Thu Nov 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262635204). The change should be a lot simpler than proposed. The code can validate the number of alleles before it checks for the presence of genotypes in [VariantContext#validateChromosomeCounts](https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/variant/variantcontext/VariantContext.java#L1236). . ---. @ldgauthier commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263277972). Sorry, I needed to refresh my memory. I actually don't want to bypass AC validation for variants without genotypes, but I think you already figured that out. My proposal was more general, but you're rig",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:5843,Security,validat,validate,5843,"adinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613997). Agreed. ---. @ronlevine commented on [Thu Nov 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262635204). The change should be a lot simpler than proposed. The code can validate the number of alleles before it checks for the presence of genotypes in [VariantContext#validateChromosomeCounts](https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/variant/variantcontext/VariantContext.java#L1236). . ---. @ldgauthier commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263277972). Sorry, I needed to refresh my memory. I actually don't want to bypass AC validation for variants without genotypes, but I think you already figured that out. My proposal was more general, but you're right -- AC and AF should always have the same count as alt alleles and we don't need to check the header for that. When this came up (a year and a half ago!) we were thinking about validating all the info field annotations. ---. @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ld",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:5940,Security,validat,validateChromosomeCounts,5940,"adinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613997). Agreed. ---. @ronlevine commented on [Thu Nov 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262635204). The change should be a lot simpler than proposed. The code can validate the number of alleles before it checks for the presence of genotypes in [VariantContext#validateChromosomeCounts](https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/variant/variantcontext/VariantContext.java#L1236). . ---. @ldgauthier commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263277972). Sorry, I needed to refresh my memory. I actually don't want to bypass AC validation for variants without genotypes, but I think you already figured that out. My proposal was more general, but you're right -- AC and AF should always have the same count as alt alleles and we don't need to check the header for that. When this came up (a year and a half ago!) we were thinking about validating all the info field annotations. ---. @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ld",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:6292,Security,validat,validation,6292,"mment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613997). Agreed. ---. @ronlevine commented on [Thu Nov 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262635204). The change should be a lot simpler than proposed. The code can validate the number of alleles before it checks for the presence of genotypes in [VariantContext#validateChromosomeCounts](https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/variant/variantcontext/VariantContext.java#L1236). . ---. @ldgauthier commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263277972). Sorry, I needed to refresh my memory. I actually don't want to bypass AC validation for variants without genotypes, but I think you already figured that out. My proposal was more general, but you're right -- AC and AF should always have the same count as alt alleles and we don't need to check the header for that. When this came up (a year and a half ago!) we were thinking about validating all the info field annotations. ---. @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` val",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:6600,Security,validat,validating,6600,"ute/gsa-unstable/issues/1053#issuecomment-262613997). Agreed. ---. @ronlevine commented on [Thu Nov 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262635204). The change should be a lot simpler than proposed. The code can validate the number of alleles before it checks for the presence of genotypes in [VariantContext#validateChromosomeCounts](https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/variant/variantcontext/VariantContext.java#L1236). . ---. @ldgauthier commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263277972). Sorry, I needed to refresh my memory. I actually don't want to bypass AC validation for variants without genotypes, but I think you already figured that out. My proposal was more general, but you're right -- AC and AF should always have the same count as alt alleles and we don't need to check the header for that. When this came up (a year and a half ago!) we were thinking about validating all the info field annotations. ---. @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:7293,Security,validat,validating,7293,"ion for variants without genotypes, but I think you already figured that out. My proposal was more general, but you're right -- AC and AF should always have the same count as alt alleles and we don't need to check the header for that. When this came up (a year and a half ago!) we were thinking about validating all the info field annotations. ---. @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:7832,Security,Validat,ValidateVariants,7832," @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLPwS6I5nu9TQiw4BFqRojmTiL0aks5rnq_OgaJpZM4FaLwX>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:8270,Security,Validat,ValidateVariants,8270," @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLPwS6I5nu9TQiw4BFqRojmTiL0aks5rnq_OgaJpZM4FaLwX>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:1237,Testability,Test,Test,1237,"/issues/1053). Currently ValidateVariants relies on genotypes to transitively check that each alt allele occurs in at least one sample and that the AC adds up. However, this can fail on sites-only files because there are no genotypes. We should use the definition of the info annotations in the header to check how many entries each should have.; ### Outline; - Add a new validation type for info-field counts to enum and to switch statement; - Grab info headers from input VCF with something like GATKVCFUtils.getVCFHeadersFromRods(getToolkit(), variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; - In the map() function, for each info header line, call on each VCFInfoHeaderLine getCount(vc) to get the expected number of info annotation entries; - Compare the expected number with a count based on vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some additional parsing because it returns an Object; - (Bonus points if you use the isFixedCount() and getCount() functions on the VCF info header line to simplify annotations that aren't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very appropriate to validate in htsjdk. On Thu, Jul 16, 2015 at 4:05 PM, ldgauthier notifications@github.com; wrote:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:3276,Testability,Test,Test,3276," check that; > each alt allele occurs in at least one sample and that the AC adds up.; > However, this can fail on sites-only files because there are no genotypes.; > We should use the definition of the info annotations in the header to check; > how many entries each should have.; > Outline; > - Add a new validation type for info-field counts to enum and to; > switch statement; > - Grab info headers from input VCF with something like; > GATKVCFUtils.getVCFHeadersFromRods(getToolkit(),; > variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; > - In the map() function, for each info header line, call on each; > VCFInfoHeaderLine getCount(vc) to get the expected number of info; > annotation entries; > - Compare the expected number with a count based on; > vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some; > additional parsing because it returns an Object; > - (Bonus points if you use the isFixedCount() and getCount() functions; > on the VCF info header line to simplify annotations that aren't according; > to the number of alt alleles); > ; > Test data; > ; > /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I le",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:4447,Testability,test,test,4447,"uthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:4584,Testability,test,test,4584,"h/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:4608,Testability,test,tests,4608,"h/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:1159,Usability,simpl,simplify,1159,"/issues/1053). Currently ValidateVariants relies on genotypes to transitively check that each alt allele occurs in at least one sample and that the AC adds up. However, this can fail on sites-only files because there are no genotypes. We should use the definition of the info annotations in the header to check how many entries each should have.; ### Outline; - Add a new validation type for info-field counts to enum and to switch statement; - Grab info headers from input VCF with something like GATKVCFUtils.getVCFHeadersFromRods(getToolkit(), variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; - In the map() function, for each info header line, call on each VCFInfoHeaderLine getCount(vc) to get the expected number of info annotation entries; - Compare the expected number with a count based on vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some additional parsing because it returns an Object; - (Bonus points if you use the isFixedCount() and getCount() functions on the VCF info header line to simplify annotations that aren't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very appropriate to validate in htsjdk. On Thu, Jul 16, 2015 at 4:05 PM, ldgauthier notifications@github.com; wrote:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:3193,Usability,simpl,simplify,3193," check that; > each alt allele occurs in at least one sample and that the AC adds up.; > However, this can fail on sites-only files because there are no genotypes.; > We should use the definition of the info annotations in the header to check; > how many entries each should have.; > Outline; > - Add a new validation type for info-field counts to enum and to; > switch statement; > - Grab info headers from input VCF with something like; > GATKVCFUtils.getVCFHeadersFromRods(getToolkit(),; > variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; > - In the map() function, for each info header line, call on each; > VCFInfoHeaderLine getCount(vc) to get the expected number of info; > annotation entries; > - Compare the expected number with a count based on; > vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some; > additional parsing because it returns an Object; > - (Bonus points if you use the isFixedCount() and getCount() functions; > on the VCF info header line to simplify annotations that aren't according; > to the number of alt alleles); > ; > Test data; > ; > /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I le",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:4175,Usability,learn,learned,4175,"uthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2507:5807,Usability,simpl,simpler,5807,"ing been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613997). Agreed. ---. @ronlevine commented on [Thu Nov 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262635204). The change should be a lot simpler than proposed. The code can validate the number of alleles before it checks for the presence of genotypes in [VariantContext#validateChromosomeCounts](https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/variant/variantcontext/VariantContext.java#L1236). . ---. @ldgauthier commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263277972). Sorry, I needed to refresh my memory. I actually don't want to bypass AC validation for variants without genotypes, but I think you already figured that out. My proposal was more general, but you're right -- AC and AF should always have the same count as alt alleles and we don't need to check the header for that. When this came up (a year and a half ago!) we were thinking about validating all the info field annotations. ---. @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507
https://github.com/broadinstitute/gatk/issues/2508:921,Availability,fault,faulty,921,"MIGRATED FROM GATK3. @ldgauthier commented on [Thu Mar 12 2015](https://github.com/broadinstitute/gsa-unstable/issues/868). From Grace Tiao:. We found a large number of PASS frameshift indels that looked suspicious in our pan-cancer germline callset (~37k samples) that are driving the top genes in some of our case-control associations. These indels all have the following properties:; 1. They are flagged as PASS by VQSR and are PASS in ExAC; but examination in IGV shows that these variants are all present at extremely low allelic fractions. (These are all normal samples.); 2. These variants occur in the specific context of 7-base (and in one case, 8-base) homopolymer runs. The variant we looked at in the meeting had a very low QD score, and Eric Banks's suggestion for the short-term is to filter our callset for low QD indels (i.e., remove all indels with QD<1). The hard filter will take care of most of these faulty variants, but at least two of the suspicious indels have QD scores > 1 (see VCF columns below). In the long run, it might be beneficial to catch these cases in the VQSR indel modeling. 1 33745932 . G GC 2175.55 PASS AC=131;AF=1.743e-03;AN=75168;BaseQRankSum=-1.540e-01;CCC=75168;ClippingRankSum=0.306;DP=886479;FS=0.000;GQ_MEAN=58.06;GQ_STDDEV=13.70;HWP=1.0000;InbreedingCoeff=-0.0041;MLEAC=86;MLEAF=1.144e-03;MQ=59.62;MQ0=0;MQRankSum=0.457;NCC=23;QD=0.55;ReadPosRankSum=-2.130e-01;VQSLOD=2.00;culprit=QD; 1 40028015 . T TG 3661.09 PASS AC=143;AF=1.902e-03;AN=75202;BaseQRankSum=0.169;CCC=75202;ClippingRankSum=0.331;DP=1059106;FS=0.000;GQ_MEAN=61.43;GQ_STDDEV=19.66;HWP=1.0000;InbreedingCoeff=-0.0032;MLEAC=93;MLEAF=1.237e-03;MQ=59.79;MQ0=0;MQRankSum=0.331;NCC=6;QD=0.53;ReadPosRankSum=-1.760e-01;VQSLOD=2.03;culprit=QD; 2 220504281 . T TG 2883 PASS AC=98;AF=1.303e-03;AN=75214;BaseQRankSum=0.095;CCC=75214;ClippingRankSum=0.040;DP=1139600;FS=0.000;GQ_MEAN=68.44;GQ_STDDEV=17.59;HWP=1.0000;InbreedingCoeff=-0.0022;MLEAC=57;MLEAF=7.578e-04;MQ=59.42;MQ0=0;MQRankSum=0.401;NC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2508
https://github.com/broadinstitute/gatk/issues/2508:5128,Availability,avail,available,5128,".38;HWP=1.0000;InbreedingCoeff=-0.0030;MLEAC=80;MLEAF=1.064e-03;MQ=59.45;MQ0=0;MQRankSum=0.306;NCC=2;QD=0.50;ReadPosRankSum=-1.290e-01;VQSLOD=2.18;culprit=QD; 14 45369723 . T TG 12376.8 PASS AC=182;AF=2.420e-03;AN=75210;BaseQRankSum=-1.810e-01;CCC=75210;ClippingRankSum=0.00;DP=1041458;FS=0.000;GQ_MEAN=67.78;GQ_STDDEV=20.59;HWP=1.0000;InbreedingCoeff=-0.0033;MLEAC=121;MLEAF=1.609e-03;MQ=59.77;MQ0=0;MQRankSum=0.410;NCC=2;QD=1.24;ReadPosRankSum=-1.100e-01;VQSLOD=2.08;culprit=QD; 19 8193948 . C CG 2846.04 PASS AC=102;AF=1.356e-03;AN=75204;BaseQRankSum=0.337;CCC=75204;ClippingRankSum=0.513;DP=1255955;FS=0.000;GQ_MEAN=76.78;GQ_STDDEV=24.97;HWP=1.0000;InbreedingCoeff=-0.0017;MLEAC=65;MLEAF=8.643e-04;MQ=59.81;MQ0=0;MQRankSum=0.514;NCC=5;QD=0.69;ReadPosRankSum=-1.050e-01;VQSLOD=2.07;culprit=QD. ---. @ldgauthier commented on [Thu Mar 12 2015](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-78600061). Are the VQSR output graphs for the ExAC dataset available anywhere? I really want to know what the QD distribution and fit for the indels look like. ---. @eitanbanks commented on [Fri Mar 13 2015](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-78798219). Thanks for looking into this. It's definitely a problem. ---. @ldgauthier commented on [Tue Mar 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-82372578). A lot of the training data for ExAC INDELs are low QD; ![image](https://cloud.githubusercontent.com/assets/6578548/6687531/218c719c-cc82-11e4-8e76-5fd6e9b09f9e.png); About 35% of these training INDELs are multiallelic, compared with 47% of training INDELs with QD <= 2. QD for SNPs has much smaller variance (and a lot more data); ![image](https://cloud.githubusercontent.com/assets/6578548/6688722/0696c90a-cc8c-11e4-8b61-7c812246abb2.png); For SNPs 14% overall are multiallelic with 13% of QD <= 2.0 SNPs being multiallelic. So it's not a huge surprise that low QD indels pass because there's a fair amount",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2508
https://github.com/broadinstitute/gatk/issues/2508:8205,Deployability,update,update,8205,"a commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260457641). @ldgauthier Now that we have allele-specific VQSR, do we still need this ticket? . ---. @ldgauthier commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260462851). Yes we do. Sad but true. It's something we've been focusing on with the new VQSR model work, though. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260467426). Ah, ok. So keep it in the GATK3 repo then, for now? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260642385). Yes. I believe @lucidtronix is doing VQSR development in GATK3 and we'll port later. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505). @ldgauthier @lucidtronix Any update on this since I heard VQSR got ported to GATK4?. ---. @ldgauthier commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287905381). It's unlikely the behavior has changed. For gnomad we used hard filters to; address the problem, which is probably a good global recommendation. On Mar 20, 2017 1:35 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @lucidtronix; > <https://github.com/lucidtronix> Any update on this since I heard VQSR; > got ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLRwdezIkmt3uPqIABWLggVjRN3yks5rnrjegaJpZM4Dt4t7>; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/iss",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2508
https://github.com/broadinstitute/gatk/issues/2508:8725,Deployability,update,update,8725,"). Ah, ok. So keep it in the GATK3 repo then, for now? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260642385). Yes. I believe @lucidtronix is doing VQSR development in GATK3 and we'll port later. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505). @ldgauthier @lucidtronix Any update on this since I heard VQSR got ported to GATK4?. ---. @ldgauthier commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287905381). It's unlikely the behavior has changed. For gnomad we used hard filters to; address the problem, which is probably a good global recommendation. On Mar 20, 2017 1:35 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @lucidtronix; > <https://github.com/lucidtronix> Any update on this since I heard VQSR; > got ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLRwdezIkmt3uPqIABWLggVjRN3yks5rnrjegaJpZM4Dt4t7>; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287919609). OK, that makes sense, thanks. Do you want me to migrate the issue to GATK4? Otherwise I'll just close it out here as WONTFIX. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-288223457). Somewhere we need a record of updates to filtering best practices until we; publish a new thing, so yeah, please migrate. On Mar 20, 2017 6:36 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > OK, that makes sense, thanks. Do you want me to migra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2508
https://github.com/broadinstitute/gatk/issues/2508:9538,Deployability,update,updates,9538," @lucidtronix Any update on this since I heard VQSR got ported to GATK4?. ---. @ldgauthier commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287905381). It's unlikely the behavior has changed. For gnomad we used hard filters to; address the problem, which is probably a good global recommendation. On Mar 20, 2017 1:35 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @lucidtronix; > <https://github.com/lucidtronix> Any update on this since I heard VQSR; > got ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLRwdezIkmt3uPqIABWLggVjRN3yks5rnrjegaJpZM4Dt4t7>; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287919609). OK, that makes sense, thanks. Do you want me to migrate the issue to GATK4? Otherwise I'll just close it out here as WONTFIX. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-288223457). Somewhere we need a record of updates to filtering best practices until we; publish a new thing, so yeah, please migrate. On Mar 20, 2017 6:36 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > OK, that makes sense, thanks. Do you want me to migrate the issue to; > GATK4? Otherwise I'll just close it out here as WONTFIX.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287919609>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdIeoOhkvNSNkC_XpxSZTt-kPdFDMks5rnv9RgaJpZM4Dt4t7>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2508
https://github.com/broadinstitute/gatk/issues/2509:226,Availability,error,error,226,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:4147,Availability,down,down,4147,"engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.str",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:453,Deployability,install,install,453,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:521,Deployability,install,install,521,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:799,Deployability,install,install,799,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:232,Integrability,message,message,232,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:403,Integrability,wrap,wrapper,403,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:5174,Integrability,wrap,wrapAndCopyInto,5174,] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:664); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:717,Performance,Load,Loading,717,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:0,Security,Validat,ValidateVariants,0,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:258,Security,Validat,ValidateVariants,258,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:348,Security,Validat,ValidateVariants,348,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:365,Security,validat,validationExampleGood,365,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:543,Security,Validat,ValidateVariants,543,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:633,Security,Validat,ValidateVariants,633,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:650,Security,validat,validationExampleGood,650,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:967,Security,Validat,ValidateVariants,967,"alArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:1057,Security,Validat,ValidateVariants,1057,"alArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:1074,Security,validat,validationExampleGood,1074,"alArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:1979,Security,Validat,ValidateVariants,1979,"riant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2047,Security,Validat,ValidateVariants,2047,"/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2116,Security,Validat,ValidateVariants,2116,"cords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO Valid",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2184,Security,Validat,ValidateVariants,2184,"l_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2250,Security,Validat,ValidateVariants,2250,"ncy SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2321,Security,Validat,ValidateVariants,2321,"naryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/Va",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2438,Security,Validat,ValidateVariants,2438,"utputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2515,Security,Validat,ValidateVariants,2515,"dPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2585,Security,Validat,ValidateVariants,2585,"ing false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2664,Security,Validat,ValidateVariants,2664,"IET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resour",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2750,Security,Validat,ValidateVariants,2750,"lters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2836,Security,Validat,ValidateVariants,2836," 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2922,Security,Validat,ValidateVariants,2922,"lpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done init",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:2999,Security,Validat,ValidateVariants,2999," 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3060,Security,Validat,ValidateVariants,3060,"SSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Process",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3121,Security,Validat,ValidateVariants,3121,"ts.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3329,Security,Validat,ValidateVariants,3329,"Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants don",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3346,Security,validat,validationExampleGood,3346,"Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants don",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3541,Security,Validat,ValidateVariants,3541,"_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3558,Security,validat,validationExampleGood,3558,"_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3722,Security,Validat,ValidateVariants,3722,"nts - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3739,Security,validat,validationExampleGood,3739,"nts - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3904,Security,Validat,ValidateVariants,3904,"dateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:4119,Security,Validat,ValidateVariants,4119,"engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.str",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:4249,Security,Validat,ValidateVariants,4249,"/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:4599,Security,Validat,ValidateVariants,4599,"xUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:4622,Security,Validat,ValidateVariants,4622,"e ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:289,Testability,test,test,289,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:574,Testability,test,test,574,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:998,Testability,test,test,998,"alArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3270,Testability,test,test,3270,"Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants don",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3482,Testability,test,test,3482,"_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2509:3663,Testability,test,test,3663,"nts - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509
https://github.com/broadinstitute/gatk/issues/2514:14,Availability,failure,failure,14,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:29,Availability,error,errors,29,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:326,Availability,Error,Error,326,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:2053,Availability,Error,Error,2053,e.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegrationTest.java:33); Caused by:; java.io.IOException: Error getting access token for service account: ; at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:319); at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:149); at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:135); at shaded.cloud-nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); at com.google.cloud.HttpTransportOptions$1.initialize(HttpTransportOptions.java:149); at shaded.cloud-nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googlea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:112,Integrability,depend,dependency,112,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:4167,Integrability,protocol,protocol,4167,executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:317); ... 27 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:4245,Integrability,protocol,protocol,4245,executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:317); ... 27 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:4364,Integrability,protocol,protocol,4364,executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:317); ... 27 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:4458,Integrability,protocol,protocol,4458,executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:317); ... 27 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:4551,Integrability,protocol,protocol,4551,executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:317); ... 27 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:3952,Performance,perform,performInitialHandshake,3952,t.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:317); ... 27 more; `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:61,Security,access,access,61,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:340,Security,access,access,340,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:2067,Security,access,access,2067,e.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegrationTest.java:33); Caused by:; java.io.IOException: Error getting access token for service account: ; at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:319); at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:149); at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:135); at shaded.cloud-nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); at com.google.cloud.HttpTransportOptions$1.initialize(HttpTransportOptions.java:149); at shaded.cloud-nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googlea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:3730,Security,secur,security,3730,); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:3795,Security,secur,security,3795,AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:3855,Security,secur,security,3855,lientRequest.java:300); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.goog,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:3925,Security,secur,security,3925,leapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAcc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:4009,Security,secur,security,4009,executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:317); ... 27 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:4084,Security,secur,security,4084,executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:317); ... 27 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:9,Testability,test,test,9,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/issues/2514:191,Testability,test,test,191,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514
https://github.com/broadinstitute/gatk/pull/2515:315,Energy Efficiency,adapt,adapter,315,"The current FindBreakpointEvidence code is classifying reads pairs that overlap such that the start position of the reverse read is before the start position of the forward read as ""OutiesPair"" discordant read pair evidence. However, these are likely due to sequencing of very short inserts that causes some of the adapter to be sequenced and potentially aligned. This change requires a read pair to not be overlapping to be counted as an 'OutiesPair'. On the CHM dataset this causes the number of intervals discovered to drop from 23152 to 21633, and the number of called variants to drop from 3467 to 3366. . @tedsharpe could you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2515
https://github.com/broadinstitute/gatk/pull/2515:315,Integrability,adapter,adapter,315,"The current FindBreakpointEvidence code is classifying reads pairs that overlap such that the start position of the reverse read is before the start position of the forward read as ""OutiesPair"" discordant read pair evidence. However, these are likely due to sequencing of very short inserts that causes some of the adapter to be sequenced and potentially aligned. This change requires a read pair to not be overlapping to be counted as an 'OutiesPair'. On the CHM dataset this causes the number of intervals discovered to drop from 23152 to 21633, and the number of called variants to drop from 3467 to 3366. . @tedsharpe could you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2515
https://github.com/broadinstitute/gatk/pull/2515:315,Modifiability,adapt,adapter,315,"The current FindBreakpointEvidence code is classifying reads pairs that overlap such that the start position of the reverse read is before the start position of the forward read as ""OutiesPair"" discordant read pair evidence. However, these are likely due to sequencing of very short inserts that causes some of the adapter to be sequenced and potentially aligned. This change requires a read pair to not be overlapping to be counted as an 'OutiesPair'. On the CHM dataset this causes the number of intervals discovered to drop from 23152 to 21633, and the number of called variants to drop from 3467 to 3366. . @tedsharpe could you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2515
https://github.com/broadinstitute/gatk/issues/2516:98,Integrability,message,message,98,"It looks like when reading from cloud we sometimes hit a bug that crashes GATK with the following message:. ```; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-385875968]; at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:78); at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:140); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:121); ```. This doesn't happen every time, but clearly it should never happen at all.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2516
https://github.com/broadinstitute/gatk/issues/2516:695,Usability,clear,clearly,695,"It looks like when reading from cloud we sometimes hit a bug that crashes GATK with the following message:. ```; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-385875968]; at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:78); at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:140); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:121); ```. This doesn't happen every time, but clearly it should never happen at all.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2516
https://github.com/broadinstitute/gatk/issues/2517:123,Availability,error,error,123,We've made progress and the [Jenkins Tests](	at java.nio.file.Files.readAttributes(Files.java:1737) are failing with a new error now. . ```; com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:208); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:161); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:117); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:86); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:183); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$run,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517
https://github.com/broadinstitute/gatk/issues/2517:1941,Deployability,deploy,deploy,1941,TKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:57,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517
https://github.com/broadinstitute/gatk/issues/2517:1978,Deployability,deploy,deploy,1978,1); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Fi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517
https://github.com/broadinstitute/gatk/issues/2517:2051,Deployability,deploy,deploy,2051,neProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.isDirectory(Files.java:21,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517
https://github.com/broadinstitute/gatk/issues/2517:2128,Deployability,deploy,deploy,2128,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.isDirectory(Files.java:2192); 	at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:346); 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517
https://github.com/broadinstitute/gatk/issues/2517:2200,Deployability,deploy,deploy,2200, 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.isDirectory(Files.java:2192); 	at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:346); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsData,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517
https://github.com/broadinstitute/gatk/issues/2517:2270,Deployability,deploy,deploy,2270,mdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.isDirectory(Files.java:2192); 	at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:346); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:205); 	... 23 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517
https://github.com/broadinstitute/gatk/issues/2517:37,Testability,Test,Tests,37,We've made progress and the [Jenkins Tests](	at java.nio.file.Files.readAttributes(Files.java:1737) are failing with a new error now. . ```; com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:208); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:161); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:117); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:86); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:183); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$run,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517
https://github.com/broadinstitute/gatk/issues/2517:3092,Testability,assert,assertFileIsReadable,3092,mdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.isDirectory(Files.java:2192); 	at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:346); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:205); 	... 23 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517
https://github.com/broadinstitute/gatk/issues/2519:1390,Deployability,release,release,1390,"sa-unstable/issues/1425#issuecomment-232150240). Comparison using KB NA12878 on PCR+ PCR- data across 20:10M-25M:. <img width=""550"" alt=""screen shot 2016-07-12 at 3 15 45 pm"" src=""https://cloud.githubusercontent.com/assets/791104/16780183/9042fcda-4843-11e6-99e5-cb9b39e5dfdc.png"">. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260494000). @vruano Are you currently working on this? Or can this be moved into the GATK4 repo for future work? . ---. @vruano commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260503444). Working on it on GATK3 but I could merge it into GATK4 whenever is ready if you prefer. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260509332). Ok great, totally fine to do in 3 but please do port to 4 when it's ready. Do you have an order of magnitude sense of when it might be ready? Meaning days/weeks/months (for release scheduling purposes). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285721454). In the last methods meeting I presented the results of our first effort to improve accuracy calling in STR. As far as unfiltered single and trio calls are concerned the recommendation is to apply the new model with PCR+ data. However, for PCR- dataset one either can choose not apply any correction or to apply the new model train on PCR- data... the latter seems to have slightly F1 values however for the sake of simplicity it might just make sense no to apply any correct; either way is good. The presentation I gave can be found [here](https://drive.google.com/open?id=0Bzt9p0vCNxlHWlZVUHZfdXR5MTg). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285722791). It seems that at some point Planatir will take a look and see whether it im",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2519
https://github.com/broadinstitute/gatk/issues/2519:2596,Deployability,release,release,2596,"ra commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260494000). @vruano Are you currently working on this? Or can this be moved into the GATK4 repo for future work? . ---. @vruano commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260503444). Working on it on GATK3 but I could merge it into GATK4 whenever is ready if you prefer. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260509332). Ok great, totally fine to do in 3 but please do port to 4 when it's ready. Do you have an order of magnitude sense of when it might be ready? Meaning days/weeks/months (for release scheduling purposes). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285721454). In the last methods meeting I presented the results of our first effort to improve accuracy calling in STR. As far as unfiltered single and trio calls are concerned the recommendation is to apply the new model with PCR+ data. However, for PCR- dataset one either can choose not apply any correction or to apply the new model train on PCR- data... the latter seems to have slightly F1 values however for the sake of simplicity it might just make sense no to apply any correct; either way is good. The presentation I gave can be found [here](https://drive.google.com/open?id=0Bzt9p0vCNxlHWlZVUHZfdXR5MTg). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285722791). It seems that at some point Planatir will take a look and see whether it improves calls once filtered with VQSR. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-287822174). Is there a PR associated with this issue? Will there be a new feature to release? Need to know for release scheduling purposes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2519
https://github.com/broadinstitute/gatk/issues/2519:2622,Deployability,release,release,2622,"ra commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260494000). @vruano Are you currently working on this? Or can this be moved into the GATK4 repo for future work? . ---. @vruano commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260503444). Working on it on GATK3 but I could merge it into GATK4 whenever is ready if you prefer. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260509332). Ok great, totally fine to do in 3 but please do port to 4 when it's ready. Do you have an order of magnitude sense of when it might be ready? Meaning days/weeks/months (for release scheduling purposes). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285721454). In the last methods meeting I presented the results of our first effort to improve accuracy calling in STR. As far as unfiltered single and trio calls are concerned the recommendation is to apply the new model with PCR+ data. However, for PCR- dataset one either can choose not apply any correction or to apply the new model train on PCR- data... the latter seems to have slightly F1 values however for the sake of simplicity it might just make sense no to apply any correct; either way is good. The presentation I gave can be found [here](https://drive.google.com/open?id=0Bzt9p0vCNxlHWlZVUHZfdXR5MTg). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285722791). It seems that at some point Planatir will take a look and see whether it improves calls once filtered with VQSR. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-287822174). Is there a PR associated with this issue? Will there be a new feature to release? Need to know for release scheduling purposes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2519
https://github.com/broadinstitute/gatk/issues/2519:1398,Energy Efficiency,schedul,scheduling,1398,"sa-unstable/issues/1425#issuecomment-232150240). Comparison using KB NA12878 on PCR+ PCR- data across 20:10M-25M:. <img width=""550"" alt=""screen shot 2016-07-12 at 3 15 45 pm"" src=""https://cloud.githubusercontent.com/assets/791104/16780183/9042fcda-4843-11e6-99e5-cb9b39e5dfdc.png"">. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260494000). @vruano Are you currently working on this? Or can this be moved into the GATK4 repo for future work? . ---. @vruano commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260503444). Working on it on GATK3 but I could merge it into GATK4 whenever is ready if you prefer. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260509332). Ok great, totally fine to do in 3 but please do port to 4 when it's ready. Do you have an order of magnitude sense of when it might be ready? Meaning days/weeks/months (for release scheduling purposes). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285721454). In the last methods meeting I presented the results of our first effort to improve accuracy calling in STR. As far as unfiltered single and trio calls are concerned the recommendation is to apply the new model with PCR+ data. However, for PCR- dataset one either can choose not apply any correction or to apply the new model train on PCR- data... the latter seems to have slightly F1 values however for the sake of simplicity it might just make sense no to apply any correct; either way is good. The presentation I gave can be found [here](https://drive.google.com/open?id=0Bzt9p0vCNxlHWlZVUHZfdXR5MTg). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285722791). It seems that at some point Planatir will take a look and see whether it im",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2519
https://github.com/broadinstitute/gatk/issues/2519:2630,Energy Efficiency,schedul,scheduling,2630,"ra commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260494000). @vruano Are you currently working on this? Or can this be moved into the GATK4 repo for future work? . ---. @vruano commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260503444). Working on it on GATK3 but I could merge it into GATK4 whenever is ready if you prefer. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260509332). Ok great, totally fine to do in 3 but please do port to 4 when it's ready. Do you have an order of magnitude sense of when it might be ready? Meaning days/weeks/months (for release scheduling purposes). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285721454). In the last methods meeting I presented the results of our first effort to improve accuracy calling in STR. As far as unfiltered single and trio calls are concerned the recommendation is to apply the new model with PCR+ data. However, for PCR- dataset one either can choose not apply any correction or to apply the new model train on PCR- data... the latter seems to have slightly F1 values however for the sake of simplicity it might just make sense no to apply any correct; either way is good. The presentation I gave can be found [here](https://drive.google.com/open?id=0Bzt9p0vCNxlHWlZVUHZfdXR5MTg). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285722791). It seems that at some point Planatir will take a look and see whether it improves calls once filtered with VQSR. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-287822174). Is there a PR associated with this issue? Will there be a new feature to release? Need to know for release scheduling purposes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2519
https://github.com/broadinstitute/gatk/issues/2519:1963,Usability,simpl,simplicity,1963,"ra commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260494000). @vruano Are you currently working on this? Or can this be moved into the GATK4 repo for future work? . ---. @vruano commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260503444). Working on it on GATK3 but I could merge it into GATK4 whenever is ready if you prefer. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260509332). Ok great, totally fine to do in 3 but please do port to 4 when it's ready. Do you have an order of magnitude sense of when it might be ready? Meaning days/weeks/months (for release scheduling purposes). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285721454). In the last methods meeting I presented the results of our first effort to improve accuracy calling in STR. As far as unfiltered single and trio calls are concerned the recommendation is to apply the new model with PCR+ data. However, for PCR- dataset one either can choose not apply any correction or to apply the new model train on PCR- data... the latter seems to have slightly F1 values however for the sake of simplicity it might just make sense no to apply any correct; either way is good. The presentation I gave can be found [here](https://drive.google.com/open?id=0Bzt9p0vCNxlHWlZVUHZfdXR5MTg). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285722791). It seems that at some point Planatir will take a look and see whether it improves calls once filtered with VQSR. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-287822174). Is there a PR associated with this issue? Will there be a new feature to release? Need to know for release scheduling purposes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2519
https://github.com/broadinstitute/gatk/issues/2520:296,Deployability,release,released,296,"@ronlevine commented on [Fri Jul 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438). ### Instructions. Follow up to #1432.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. ---; ## Feature request; ### Tool(s) involved. Any tool using `IntervalUtils. intervalFileToList()` ; ### Description. Once this change is made, -1 length intervals will be validated and an exception will be thrown. ---. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927). From what I understand of the referenced thread, the ""incorrect"" interval list may always be around, so we may never be able to just blow up on it. Would it perhaps be more viable to add an option to toggle the level of stringency, ie choose in the command line whether to blow up or skip on these invalid intervals? . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260496001). @yfarjoun will want to opine on this, I think. . ---. @yfarjoun commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260513266). I hope that when we move exomes to hg38 we will correct this silly thing; and a few decades later we will no need this code (hehe). Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:1007,Deployability,toggle,toggle,1007,"6](https://github.com/broadinstitute/gsa-unstable/issues/1438). ### Instructions. Follow up to #1432.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. ---; ## Feature request; ### Tool(s) involved. Any tool using `IntervalUtils. intervalFileToList()` ; ### Description. Once this change is made, -1 length intervals will be validated and an exception will be thrown. ---. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927). From what I understand of the referenced thread, the ""incorrect"" interval list may always be around, so we may never be able to just blow up on it. Would it perhaps be more viable to add an option to toggle the level of stringency, ie choose in the command line whether to blow up or skip on these invalid intervals? . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260496001). @yfarjoun will want to opine on this, I think. . ---. @yfarjoun commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260513266). I hope that when we move exomes to hg38 we will correct this silly thing; and a few decades later we will no need this code (hehe). Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; > —; > You are receiving this because y",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:1873,Deployability,toggle,toggle,1873,"up on it. Would it perhaps be more viable to add an option to toggle the level of stringency, ie choose in the command line whether to blow up or skip on these invalid intervals? . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260496001). @yfarjoun will want to opine on this, I think. . ---. @yfarjoun commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260513266). I hope that when we move exomes to hg38 we will correct this silly thing; and a few decades later we will no need this code (hehe). Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0uvegvUmCq7_G7U2PSuTpvIYl0wQks5q-Ox0gaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118). So, would adding a toggle be acceptable? And more importantly, can we make stringent validation default, with the option to not blow up on silly exome files? Will production accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notificati",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:2493,Deployability,toggle,toggle,2493,"Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0uvegvUmCq7_G7U2PSuTpvIYl0wQks5q-Ox0gaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118). So, would adding a toggle be acceptable? And more importantly, can we make stringent validation default, with the option to not blow up on silly exome files? Will production accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:2990,Deployability,toggle,toggle,2990,"directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0uvegvUmCq7_G7U2PSuTpvIYl0wQks5q-Ox0gaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118). So, would adding a toggle be acceptable? And more importantly, can we make stringent validation default, with the option to not blow up on silly exome files? Will production accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154). Any update on this, @yfarjoun ?. ---. @yfarjoun commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287826525). I think we will only fix the interval list when we move exomes to; hg38....so, no. On Mon, Mar 20, 2017 at 12:45 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Any update on this, @yfarjoun <https://github.com/yfarjoun> ?; >; > —; > You are receiving this because you were mentioned.; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:3631,Deployability,update,update,3631,"ction accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154). Any update on this, @yfarjoun ?. ---. @yfarjoun commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287826525). I think we will only fix the interval list when we move exomes to; hg38....so, no. On Mon, Mar 20, 2017 at 12:45 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Any update on this, @yfarjoun <https://github.com/yfarjoun> ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0hMTukUGLtk1oTOse4Oj3awHf_exks5rnq1CgaJpZM4JNjE->; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287828851). OK well this workaround should really be moved to a ""validation stringency"" level decision, not a hardcoded hack. . @ronlevine Do you know i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:3975,Deployability,update,update,3975," 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154). Any update on this, @yfarjoun ?. ---. @yfarjoun commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287826525). I think we will only fix the interval list when we move exomes to; hg38....so, no. On Mon, Mar 20, 2017 at 12:45 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Any update on this, @yfarjoun <https://github.com/yfarjoun> ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0hMTukUGLtk1oTOse4Oj3awHf_exks5rnq1CgaJpZM4JNjE->; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287828851). OK well this workaround should really be moved to a ""validation stringency"" level decision, not a hardcoded hack. . @ronlevine Do you know if this hack is also present in GATK4's equivalent code?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:629,Security,validat,validated,629,"@ronlevine commented on [Fri Jul 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438). ### Instructions. Follow up to #1432.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. ---; ## Feature request; ### Tool(s) involved. Any tool using `IntervalUtils. intervalFileToList()` ; ### Description. Once this change is made, -1 length intervals will be validated and an exception will be thrown. ---. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927). From what I understand of the referenced thread, the ""incorrect"" interval list may always be around, so we may never be able to just blow up on it. Would it perhaps be more viable to add an option to toggle the level of stringency, ie choose in the command line whether to blow up or skip on these invalid intervals? . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260496001). @yfarjoun will want to opine on this, I think. . ---. @yfarjoun commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260513266). I hope that when we move exomes to hg38 we will correct this silly thing; and a few decades later we will no need this code (hehe). Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:2559,Security,validat,validation,2559,"Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0uvegvUmCq7_G7U2PSuTpvIYl0wQks5q-Ox0gaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118). So, would adding a toggle be acceptable? And more importantly, can we make stringent validation default, with the option to not blow up on silly exome files? Will production accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:3059,Security,validat,validation,3059,"directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0uvegvUmCq7_G7U2PSuTpvIYl0wQks5q-Ox0gaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118). So, would adding a toggle be acceptable? And more importantly, can we make stringent validation default, with the option to not blow up on silly exome files? Will production accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154). Any update on this, @yfarjoun ?. ---. @yfarjoun commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287826525). I think we will only fix the interval list when we move exomes to; hg38....so, no. On Mon, Mar 20, 2017 at 12:45 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Any update on this, @yfarjoun <https://github.com/yfarjoun> ?; >; > —; > You are receiving this because you were mentioned.; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:4556,Security,validat,validation,4556," 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154). Any update on this, @yfarjoun ?. ---. @yfarjoun commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287826525). I think we will only fix the interval list when we move exomes to; hg38....so, no. On Mon, Mar 20, 2017 at 12:45 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Any update on this, @yfarjoun <https://github.com/yfarjoun> ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0hMTukUGLtk1oTOse4Oj3awHf_exks5rnq1CgaJpZM4JNjE->; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287828851). OK well this workaround should really be moved to a ""validation stringency"" level decision, not a hardcoded hack. . @ronlevine Do you know if this hack is also present in GATK4's equivalent code?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2520:368,Testability,log,logger,368,"@ronlevine commented on [Fri Jul 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438). ### Instructions. Follow up to #1432.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. ---; ## Feature request; ### Tool(s) involved. Any tool using `IntervalUtils. intervalFileToList()` ; ### Description. Once this change is made, -1 length intervals will be validated and an exception will be thrown. ---. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927). From what I understand of the referenced thread, the ""incorrect"" interval list may always be around, so we may never be able to just blow up on it. Would it perhaps be more viable to add an option to toggle the level of stringency, ie choose in the command line whether to blow up or skip on these invalid intervals? . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260496001). @yfarjoun will want to opine on this, I think. . ---. @yfarjoun commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260513266). I hope that when we move exomes to hg38 we will correct this silly thing; and a few decades later we will no need this code (hehe). Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520
https://github.com/broadinstitute/gatk/issues/2522:39,Testability,test,tested,39,Currently `GenotypingEngine` is mostly tested indirectly via the `HaplotypeCaller` tests in protected. It would be good to expand `GenotypingEngineUnitTest` to include more comprehensive direct tests for this crucial class.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2522
https://github.com/broadinstitute/gatk/issues/2522:83,Testability,test,tests,83,Currently `GenotypingEngine` is mostly tested indirectly via the `HaplotypeCaller` tests in protected. It would be good to expand `GenotypingEngineUnitTest` to include more comprehensive direct tests for this crucial class.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2522
https://github.com/broadinstitute/gatk/issues/2522:194,Testability,test,tests,194,Currently `GenotypingEngine` is mostly tested indirectly via the `HaplotypeCaller` tests in protected. It would be good to expand `GenotypingEngineUnitTest` to include more comprehensive direct tests for this crucial class.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2522
https://github.com/broadinstitute/gatk/issues/2523:20,Deployability,integrat,integration,20,BwaSpark needs good integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2523
https://github.com/broadinstitute/gatk/issues/2523:20,Integrability,integrat,integration,20,BwaSpark needs good integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2523
https://github.com/broadinstitute/gatk/issues/2523:32,Testability,test,tests,32,BwaSpark needs good integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2523
https://github.com/broadinstitute/gatk/pull/2524:99,Energy Efficiency,reduce,reducers,99,this allows you to convert RAW_MQ -> MQ ; this is not a general port of the reducible annotation's reducers,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2524
https://github.com/broadinstitute/gatk/issues/2530:1301,Integrability,wrap,wrapAndCopyInto,1301,"gatk3. . ```; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; 	at java.util.ArrayList.rangeCheck(ArrayList.java:653); 	at java.util.ArrayList.get(ArrayList.java:429); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:270); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:204); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:174); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:157); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:664); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2530
https://github.com/broadinstitute/gatk/pull/2531:174,Testability,test,tests,174,"* changes to GenotypingEngine.calculateGenotypes in #2528 introduced a regression that caused IndexOutOfBoundsException; * fixes #2530 by preventing the exception and adding tests. @davidbenjamin Note that I also changed the condition from ""first alt allele is NON_REF"" to ""any alt allele is NON_REF"". This more accurately matches what the comment says, but let me know if you think it's not the right thing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2531
https://github.com/broadinstitute/gatk/issues/2532:84,Modifiability,plugin,plugin,84,"This comes from pre-Barclay code, but it is an issue when looking for tests for the plugin:. * `ReadFilterPluginUnitTest` lives in org.broadinstitute.hellbender.engine.filters while the plugin lives in `org.broadinstitute.hellbender.cmdline.GATKPlugin`; * In addition, the test is called in a different way that the plugin, which is `GATKReadFilterPluginDescriptor`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2532
https://github.com/broadinstitute/gatk/issues/2532:186,Modifiability,plugin,plugin,186,"This comes from pre-Barclay code, but it is an issue when looking for tests for the plugin:. * `ReadFilterPluginUnitTest` lives in org.broadinstitute.hellbender.engine.filters while the plugin lives in `org.broadinstitute.hellbender.cmdline.GATKPlugin`; * In addition, the test is called in a different way that the plugin, which is `GATKReadFilterPluginDescriptor`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2532
https://github.com/broadinstitute/gatk/issues/2532:316,Modifiability,plugin,plugin,316,"This comes from pre-Barclay code, but it is an issue when looking for tests for the plugin:. * `ReadFilterPluginUnitTest` lives in org.broadinstitute.hellbender.engine.filters while the plugin lives in `org.broadinstitute.hellbender.cmdline.GATKPlugin`; * In addition, the test is called in a different way that the plugin, which is `GATKReadFilterPluginDescriptor`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2532
https://github.com/broadinstitute/gatk/issues/2532:70,Testability,test,tests,70,"This comes from pre-Barclay code, but it is an issue when looking for tests for the plugin:. * `ReadFilterPluginUnitTest` lives in org.broadinstitute.hellbender.engine.filters while the plugin lives in `org.broadinstitute.hellbender.cmdline.GATKPlugin`; * In addition, the test is called in a different way that the plugin, which is `GATKReadFilterPluginDescriptor`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2532
https://github.com/broadinstitute/gatk/issues/2532:273,Testability,test,test,273,"This comes from pre-Barclay code, but it is an issue when looking for tests for the plugin:. * `ReadFilterPluginUnitTest` lives in org.broadinstitute.hellbender.engine.filters while the plugin lives in `org.broadinstitute.hellbender.cmdline.GATKPlugin`; * In addition, the test is called in a different way that the plugin, which is `GATKReadFilterPluginDescriptor`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2532
https://github.com/broadinstitute/gatk/issues/2533:107,Integrability,depend,dependency,107,"It will be nice to have the GATK testing framework in a different module and maven artifact to add it as a dependency only for testing, and do not include it in the uber-jar the TestNG dependecy (or the Mini-Cluster from Hadoop).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2533
https://github.com/broadinstitute/gatk/issues/2533:185,Integrability,depend,dependecy,185,"It will be nice to have the GATK testing framework in a different module and maven artifact to add it as a dependency only for testing, and do not include it in the uber-jar the TestNG dependecy (or the Mini-Cluster from Hadoop).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2533
https://github.com/broadinstitute/gatk/issues/2533:33,Testability,test,testing,33,"It will be nice to have the GATK testing framework in a different module and maven artifact to add it as a dependency only for testing, and do not include it in the uber-jar the TestNG dependecy (or the Mini-Cluster from Hadoop).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2533
https://github.com/broadinstitute/gatk/issues/2533:127,Testability,test,testing,127,"It will be nice to have the GATK testing framework in a different module and maven artifact to add it as a dependency only for testing, and do not include it in the uber-jar the TestNG dependecy (or the Mini-Cluster from Hadoop).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2533
https://github.com/broadinstitute/gatk/issues/2533:178,Testability,Test,TestNG,178,"It will be nice to have the GATK testing framework in a different module and maven artifact to add it as a dependency only for testing, and do not include it in the uber-jar the TestNG dependecy (or the Mini-Cluster from Hadoop).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2533
https://github.com/broadinstitute/gatk/issues/2535:277,Availability,down,down,277,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:651,Availability,error,error,651,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:1459,Availability,error,error,1459,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:1626,Availability,failure,failures,1626,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:712,Energy Efficiency,allocate,allocated,712,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:1520,Energy Efficiency,allocate,allocated,1520,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:557,Performance,Cache,Caches,557,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:218,Testability,test,test,218,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:485,Testability,test,tests,485,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:516,Testability,Test,TestNG,516,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:588,Testability,test,testng-customsuite,588,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:894,Testability,test,test,894,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:1054,Testability,test,test,1054,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:1125,Testability,test,test,1125,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:1213,Testability,test,testWritingToFileURL,1213,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:1272,Testability,test,test,1272,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2535:1604,Testability,test,tests,1604,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535
https://github.com/broadinstitute/gatk/issues/2536:16,Availability,error,error,16,Should throw an error if the client tries to write to TileDB in such a way as would corrupt an instance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2536
https://github.com/broadinstitute/gatk/issues/2539:448,Modifiability,extend,extend,448,"I'm exploring the usage of the `AssemblyRegionWalker` engine for custom tools and I realized that there is very little support for customization. One of the things that will be nice is to use a custom `ActivityProfileState.Type` for assigning custom reasons to be an active region (or non active at all). My proposal is the following:. * Abstract `ActivityProfileState.Type` with one method to get the value as a `Number`.; * Make the current enum extend this abstract class.; * Change the method to get the value to delegate into the `Number`. This will allow custom types for the activity profile, and value range for the result value (now it should be positive or null).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2539
https://github.com/broadinstitute/gatk/pull/2540:451,Deployability,integrat,integration,451,"This tool copies single large files or directories from GCS into HDFS using Spark. Spark parallelization allows each task to copy a chunk in the size of the blocks of the target HDFS system simultaneously. When copying a directory containing a 120GB WGS bam and its index, this takes approximately 1 minute on a 10 worker / 160 core cluster, as opposed to approximately 20 minutes using Hadoop distcp. This may eventually be superseded by the NIO GCS integration work if that ends up performing comparably. @lbergelson would you like to review? Or feel free to nominate someone else.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540
https://github.com/broadinstitute/gatk/pull/2540:451,Integrability,integrat,integration,451,"This tool copies single large files or directories from GCS into HDFS using Spark. Spark parallelization allows each task to copy a chunk in the size of the blocks of the target HDFS system simultaneously. When copying a directory containing a 120GB WGS bam and its index, this takes approximately 1 minute on a 10 worker / 160 core cluster, as opposed to approximately 20 minutes using Hadoop distcp. This may eventually be superseded by the NIO GCS integration work if that ends up performing comparably. @lbergelson would you like to review? Or feel free to nominate someone else.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540
https://github.com/broadinstitute/gatk/pull/2540:484,Performance,perform,performing,484,"This tool copies single large files or directories from GCS into HDFS using Spark. Spark parallelization allows each task to copy a chunk in the size of the blocks of the target HDFS system simultaneously. When copying a directory containing a 120GB WGS bam and its index, this takes approximately 1 minute on a 10 worker / 160 core cluster, as opposed to approximately 20 minutes using Hadoop distcp. This may eventually be superseded by the NIO GCS integration work if that ends up performing comparably. @lbergelson would you like to review? Or feel free to nominate someone else.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540
https://github.com/broadinstitute/gatk/issues/2541:120,Availability,down,down,120,First it does not complain that the file does not exists but when you request the getReadsHeader() it results in a deep down NPE exception. The fact that the walker requires reads does not help either.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2541
https://github.com/broadinstitute/gatk/issues/2545:5,Availability,error,error,5,"This error does not occur with every VCF but at least with the one enclosed below. It passes vcf-validator with default arguments. . To reproduce: . ```; term1$ spark-shell # start you spark local cluster in another screen; ...; term2$ cd /dsde/working/valentin/bugs/gatk-var-walker-ser; term2$ git checkout 58cb99ec ; term2$ ./gradlew sparkJar; term2$ ./gatk-launch ExampleVariantWalkerSpark -V ./in.vcf.gz -- --sparkRunner SPARK --sparkMaster local. ```. ```; The stacktrace starts with:. 17/03/29 16:44:56 INFO SparkContext: Successfully stopped SparkContext; 16:44:56.000 INFO ExampleVariantWalkerSpark - Shutting down engine; [March 29, 2017 4:44:56 PM EDT] org.broadinstitute.hellbender.tools.examples.ExampleVariantWalkerSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=576192512; java.lang.IllegalArgumentException: requirement failed: The partition coalescer passed in must be serializable.; 	at scala.Predef$.require(Predef.scala:224); 	at org.apache.spark.rdd.CoalescedRDD.<init>(CoalescedRDD.scala:84); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:466); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:445); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.coalesce(RDD.scala:445); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.coalesce(SparkSharder.java:321); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:189); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:126); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.shard(SparkSharder.java:99); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.getVariants(VariantWalkerSpark.java:129); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545
https://github.com/broadinstitute/gatk/issues/2545:618,Availability,down,down,618,"This error does not occur with every VCF but at least with the one enclosed below. It passes vcf-validator with default arguments. . To reproduce: . ```; term1$ spark-shell # start you spark local cluster in another screen; ...; term2$ cd /dsde/working/valentin/bugs/gatk-var-walker-ser; term2$ git checkout 58cb99ec ; term2$ ./gradlew sparkJar; term2$ ./gatk-launch ExampleVariantWalkerSpark -V ./in.vcf.gz -- --sparkRunner SPARK --sparkMaster local. ```. ```; The stacktrace starts with:. 17/03/29 16:44:56 INFO SparkContext: Successfully stopped SparkContext; 16:44:56.000 INFO ExampleVariantWalkerSpark - Shutting down engine; [March 29, 2017 4:44:56 PM EDT] org.broadinstitute.hellbender.tools.examples.ExampleVariantWalkerSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=576192512; java.lang.IllegalArgumentException: requirement failed: The partition coalescer passed in must be serializable.; 	at scala.Predef$.require(Predef.scala:224); 	at org.apache.spark.rdd.CoalescedRDD.<init>(CoalescedRDD.scala:84); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:466); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:445); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.coalesce(RDD.scala:445); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.coalesce(SparkSharder.java:321); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:189); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:126); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.shard(SparkSharder.java:99); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.getVariants(VariantWalkerSpark.java:129); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545
https://github.com/broadinstitute/gatk/issues/2545:97,Security,validat,validator,97,"This error does not occur with every VCF but at least with the one enclosed below. It passes vcf-validator with default arguments. . To reproduce: . ```; term1$ spark-shell # start you spark local cluster in another screen; ...; term2$ cd /dsde/working/valentin/bugs/gatk-var-walker-ser; term2$ git checkout 58cb99ec ; term2$ ./gradlew sparkJar; term2$ ./gatk-launch ExampleVariantWalkerSpark -V ./in.vcf.gz -- --sparkRunner SPARK --sparkMaster local. ```. ```; The stacktrace starts with:. 17/03/29 16:44:56 INFO SparkContext: Successfully stopped SparkContext; 16:44:56.000 INFO ExampleVariantWalkerSpark - Shutting down engine; [March 29, 2017 4:44:56 PM EDT] org.broadinstitute.hellbender.tools.examples.ExampleVariantWalkerSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=576192512; java.lang.IllegalArgumentException: requirement failed: The partition coalescer passed in must be serializable.; 	at scala.Predef$.require(Predef.scala:224); 	at org.apache.spark.rdd.CoalescedRDD.<init>(CoalescedRDD.scala:84); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:466); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:445); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.coalesce(RDD.scala:445); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.coalesce(SparkSharder.java:321); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:189); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:126); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.shard(SparkSharder.java:99); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.getVariants(VariantWalkerSpark.java:129); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545
https://github.com/broadinstitute/gatk/pull/2546:309,Testability,test,tests,309,"* changing GenotypeCounts to be doubles instead of ints, this is weird but it's how gatk3 is; * adding an additional boolean parameter to computeDiploidGenotypeCounts that controls rounding behavior; ExcessHet needs rounding on; InbreedingCoeff needs it off; * adding a new GenotypeUtilsUnitTest class; these tests are not exhaustive and leave most conditions untested; * updating InbreedingCoeffUnitTest with a test that checks that we match gatk3 output for a previously divergent case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2546
https://github.com/broadinstitute/gatk/pull/2546:412,Testability,test,test,412,"* changing GenotypeCounts to be doubles instead of ints, this is weird but it's how gatk3 is; * adding an additional boolean parameter to computeDiploidGenotypeCounts that controls rounding behavior; ExcessHet needs rounding on; InbreedingCoeff needs it off; * adding a new GenotypeUtilsUnitTest class; these tests are not exhaustive and leave most conditions untested; * updating InbreedingCoeffUnitTest with a test that checks that we match gatk3 output for a previously divergent case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2546
https://github.com/broadinstitute/gatk/pull/2547:45,Modifiability,variab,variable,45,"@droazen here you go. It turns out that this variable had no business having class scope, so I made it local to the one method that uses it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2547
https://github.com/broadinstitute/gatk/pull/2550:40,Testability,test,test,40,"@droazen easy one for you... we do have test for both of these. In fact, I think we ONLY have tests that enable these... Inspired by #2549",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2550
https://github.com/broadinstitute/gatk/pull/2550:94,Testability,test,tests,94,"@droazen easy one for you... we do have test for both of these. In fact, I think we ONLY have tests that enable these... Inspired by #2549",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2550
https://github.com/broadinstitute/gatk/issues/2551:4,Availability,error,error,4,The error `Cannot use index file with textual SAM file` was reported by `AssemblyRegionUnitTest.testCreateFromReadShard`when trying to read what was likely a corrupted large file bam on travis. This error does not accurately reflect the problem and is likely caused by a chain of fallbacks in htsjdk. It's possible that the switch from file -> path has changed the sequence of fallbacks causing this error to appear.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2551
https://github.com/broadinstitute/gatk/issues/2551:199,Availability,error,error,199,The error `Cannot use index file with textual SAM file` was reported by `AssemblyRegionUnitTest.testCreateFromReadShard`when trying to read what was likely a corrupted large file bam on travis. This error does not accurately reflect the problem and is likely caused by a chain of fallbacks in htsjdk. It's possible that the switch from file -> path has changed the sequence of fallbacks causing this error to appear.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2551
https://github.com/broadinstitute/gatk/issues/2551:400,Availability,error,error,400,The error `Cannot use index file with textual SAM file` was reported by `AssemblyRegionUnitTest.testCreateFromReadShard`when trying to read what was likely a corrupted large file bam on travis. This error does not accurately reflect the problem and is likely caused by a chain of fallbacks in htsjdk. It's possible that the switch from file -> path has changed the sequence of fallbacks causing this error to appear.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2551
https://github.com/broadinstitute/gatk/issues/2551:96,Testability,test,testCreateFromReadShard,96,The error `Cannot use index file with textual SAM file` was reported by `AssemblyRegionUnitTest.testCreateFromReadShard`when trying to read what was likely a corrupted large file bam on travis. This error does not accurately reflect the problem and is likely caused by a chain of fallbacks in htsjdk. It's possible that the switch from file -> path has changed the sequence of fallbacks causing this error to appear.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2551
https://github.com/broadinstitute/gatk/issues/2554:254,Availability,failure,failure,254,"It seems that the code in SparkSharder responsible of grouping Locatables into sharded reference intervals cannot handle large locatables that would overlap more than a couple of shards... . ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 10000, but locatable size is 1157593. Try increasing shard size and/or padding. Locatable: [VC Unknown @ 1:29721370-30878962 Q. of type=SYMBOLIC alleles=[C*, <INV>] attr={ALIGN_LENGTHS=144, ASSEMBLY_IDS=276, CONTIG_IDS=contig-5, END=30878962, HQ_MAPPINGS=1, INSERTED_SEQUENCE=AAACCAGGCCCCAGGGCCCCAGAAAGCAGGTAGTAGGGCCAAGCGAGGGCCGGGGCAGGCTAGCTCCAAGCCCACTGCAGGCCTCAGCTCTGCT, INV55=true, MAPPING_QUALITIES=60, MAX_ALIGN_LENGTH=144, SVLEN=1157592, SVTYPE=INV, TOTAL_MAPPINGS=1} GT=[]; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:232); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:212); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$7.computeNext(Iterators.java:650); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554
https://github.com/broadinstitute/gatk/issues/2554:311,Availability,failure,failure,311,"It seems that the code in SparkSharder responsible of grouping Locatables into sharded reference intervals cannot handle large locatables that would overlap more than a couple of shards... . ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 10000, but locatable size is 1157593. Try increasing shard size and/or padding. Locatable: [VC Unknown @ 1:29721370-30878962 Q. of type=SYMBOLIC alleles=[C*, <INV>] attr={ALIGN_LENGTHS=144, ASSEMBLY_IDS=276, CONTIG_IDS=contig-5, END=30878962, HQ_MAPPINGS=1, INSERTED_SEQUENCE=AAACCAGGCCCCAGGGCCCCAGAAAGCAGGTAGTAGGGCCAAGCGAGGGCCGGGGCAGGCTAGCTCCAAGCCCACTGCAGGCCTCAGCTCTGCT, INV55=true, MAPPING_QUALITIES=60, MAX_ALIGN_LENGTH=144, SVLEN=1157592, SVTYPE=INV, TOTAL_MAPPINGS=1} GT=[]; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:232); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:212); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$7.computeNext(Iterators.java:650); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554
https://github.com/broadinstitute/gatk/issues/2554:1925,Integrability,Wrap,Wrappers,1925,"It seems that the code in SparkSharder responsible of grouping Locatables into sharded reference intervals cannot handle large locatables that would overlap more than a couple of shards... . ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 10000, but locatable size is 1157593. Try increasing shard size and/or padding. Locatable: [VC Unknown @ 1:29721370-30878962 Q. of type=SYMBOLIC alleles=[C*, <INV>] attr={ALIGN_LENGTHS=144, ASSEMBLY_IDS=276, CONTIG_IDS=contig-5, END=30878962, HQ_MAPPINGS=1, INSERTED_SEQUENCE=AAACCAGGCCCCAGGGCCCCAGAAAGCAGGTAGTAGGGCCAAGCGAGGGCCGGGGCAGGCTAGCTCCAAGCCCACTGCAGGCCTCAGCTCTGCT, INV55=true, MAPPING_QUALITIES=60, MAX_ALIGN_LENGTH=144, SVLEN=1157592, SVTYPE=INV, TOTAL_MAPPINGS=1} GT=[]; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:232); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:212); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$7.computeNext(Iterators.java:650); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554
https://github.com/broadinstitute/gatk/issues/2554:1959,Integrability,Wrap,Wrappers,1959,"It seems that the code in SparkSharder responsible of grouping Locatables into sharded reference intervals cannot handle large locatables that would overlap more than a couple of shards... . ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 10000, but locatable size is 1157593. Try increasing shard size and/or padding. Locatable: [VC Unknown @ 1:29721370-30878962 Q. of type=SYMBOLIC alleles=[C*, <INV>] attr={ALIGN_LENGTHS=144, ASSEMBLY_IDS=276, CONTIG_IDS=contig-5, END=30878962, HQ_MAPPINGS=1, INSERTED_SEQUENCE=AAACCAGGCCCCAGGGCCCCAGAAAGCAGGTAGTAGGGCCAAGCGAGGGCCGGGGCAGGCTAGCTCCAAGCCCACTGCAGGCCTCAGCTCTGCT, INV55=true, MAPPING_QUALITIES=60, MAX_ALIGN_LENGTH=144, SVLEN=1157592, SVTYPE=INV, TOTAL_MAPPINGS=1} GT=[]; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:232); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:212); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$7.computeNext(Iterators.java:650); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554
https://github.com/broadinstitute/gatk/issues/2554:233,Safety,abort,aborted,233,"It seems that the code in SparkSharder responsible of grouping Locatables into sharded reference intervals cannot handle large locatables that would overlap more than a couple of shards... . ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 10000, but locatable size is 1157593. Try increasing shard size and/or padding. Locatable: [VC Unknown @ 1:29721370-30878962 Q. of type=SYMBOLIC alleles=[C*, <INV>] attr={ALIGN_LENGTHS=144, ASSEMBLY_IDS=276, CONTIG_IDS=contig-5, END=30878962, HQ_MAPPINGS=1, INSERTED_SEQUENCE=AAACCAGGCCCCAGGGCCCCAGAAAGCAGGTAGTAGGGCCAAGCGAGGGCCGGGGCAGGCTAGCTCCAAGCCCACTGCAGGCCTCAGCTCTGCT, INV55=true, MAPPING_QUALITIES=60, MAX_ALIGN_LENGTH=144, SVLEN=1157592, SVTYPE=INV, TOTAL_MAPPINGS=1} GT=[]; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:232); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:212); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$7.computeNext(Iterators.java:650); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554
https://github.com/broadinstitute/gatk/issues/2555:79,Deployability,release,released,79,We should try to keep on top of the latest spark version while we're still not released yet. . When we upgrade we should verify that #2545 is resolved.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2555
https://github.com/broadinstitute/gatk/issues/2555:103,Deployability,upgrade,upgrade,103,We should try to keep on top of the latest spark version while we're still not released yet. . When we upgrade we should verify that #2545 is resolved.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2555
https://github.com/broadinstitute/gatk/pull/2556:142,Testability,test,tested,142,"It was assuming that the `ReferenceContext`'s interval has the same start as the `VariantContext` being annotated, which is often not true. I tested this in Mutect and it seems to work correctly now. @LeeTL1220 This will improve indel specificty in M2 by 75% or so. Can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2556
https://github.com/broadinstitute/gatk/pull/2558:156,Testability,Test,Tested,156,"Add a version of `createSAMWriter` that takes in a `Path` so we can work with NIO for output. This allows PrintReads to write its output to a cloud bucket. Tested like this:; ```; $ ./gatk-launch PrintReads -I gs://$MYBUCKET/CEUTrio.HiSeq.WEx.b37.NA12892.bam -L 10:1000000-2000000 -O gs://$MYBUCKET/pr.bam; $ gsutil cp gs//$MYBUCKET/pr.bam .; $ samtools view pr.bam | md5sum; c7f41be91031bea6d28d59d40b54f304 -; ```. This MD5 matches the expected output for this command. Once we agree this is the correct way forward, I should go ahead and change all of the other tools similarly. This PR is part of addressing #2422",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558
https://github.com/broadinstitute/gatk/issues/2562:75,Performance,optimiz,optimizations,75,"@davidbenjamin has requested that we look into whether there are `PairHMM` optimizations we could do that would benefit Mutect2 specifically -- the performance profile may be different than for the `HaplotypeCaller`. . Intel has agreed to have a look, but they'll need a Mutect2 test case that they can run. @davidbenjamin would you be able to provide one?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562
https://github.com/broadinstitute/gatk/issues/2562:148,Performance,perform,performance,148,"@davidbenjamin has requested that we look into whether there are `PairHMM` optimizations we could do that would benefit Mutect2 specifically -- the performance profile may be different than for the `HaplotypeCaller`. . Intel has agreed to have a look, but they'll need a Mutect2 test case that they can run. @davidbenjamin would you be able to provide one?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562
https://github.com/broadinstitute/gatk/issues/2562:279,Testability,test,test,279,"@davidbenjamin has requested that we look into whether there are `PairHMM` optimizations we could do that would benefit Mutect2 specifically -- the performance profile may be different than for the `HaplotypeCaller`. . Intel has agreed to have a look, but they'll need a Mutect2 test case that they can run. @davidbenjamin would you be able to provide one?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562
https://github.com/broadinstitute/gatk/issues/2563:22,Deployability,integrat,integration,22,"Currently most of the integration tests for BQSR run with BAQ and indel qualities on, which is no longer the default. We should make most of the tests run with the default settings of no BAQ and no indel qualities.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2563
https://github.com/broadinstitute/gatk/issues/2563:22,Integrability,integrat,integration,22,"Currently most of the integration tests for BQSR run with BAQ and indel qualities on, which is no longer the default. We should make most of the tests run with the default settings of no BAQ and no indel qualities.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2563
https://github.com/broadinstitute/gatk/issues/2563:34,Testability,test,tests,34,"Currently most of the integration tests for BQSR run with BAQ and indel qualities on, which is no longer the default. We should make most of the tests run with the default settings of no BAQ and no indel qualities.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2563
https://github.com/broadinstitute/gatk/issues/2563:145,Testability,test,tests,145,"Currently most of the integration tests for BQSR run with BAQ and indel qualities on, which is no longer the default. We should make most of the tests run with the default settings of no BAQ and no indel qualities.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2563
https://github.com/broadinstitute/gatk/pull/2565:64,Security,authenticat,authentication,64,"Since NIO reportedly now works from Spark clients, this bit of; authentication is unnecessary. Removing it greatly simplifies; BucketUtils, and also stops users having to worry about where; to get the AuthHolder from. This work is part of #2402",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2565
https://github.com/broadinstitute/gatk/pull/2565:115,Usability,simpl,simplifies,115,"Since NIO reportedly now works from Spark clients, this bit of; authentication is unnecessary. Removing it greatly simplifies; BucketUtils, and also stops users having to worry about where; to get the AuthHolder from. This work is part of #2402",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2565
https://github.com/broadinstitute/gatk/pull/2566:177,Testability,test,tests,177,"* Bump GKL version to 0.4.3 which addresses some GC and memory leak issues.; * Remove DEBUG verbosity setting in `IntelInflaterDeflaterIntegrationTest`, which caused subsequent tests to generate huge amounts of debug info and run out of memory on some systems. Resolves #2490 and resolves #2535.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2566
https://github.com/broadinstitute/gatk/pull/2567:156,Deployability,pipeline,pipeline,156,"This PR:. * fixes a problem observed for a complex event with tandem duplicated sequence and insertion and deletion, which causes the discovery phase of SV pipeline to throw exceptions;; * the fix works by changing how tandem duplications are annotated:; * duplicated sequence is no longer provided, instead the corresponding duplication unit reference span is provided in `DUP_REPET_UNIT_REF_SPAN`,; * the duplicated units on the assembled contigs' CIGAR when aligned to reference is provided in `DUP_ASM_CTG_CIGARS`;; * adds annotation `DUP_ANNOT_FROM_OPT` when tandem repeat annotations were generated by a simple approximation procedure, which should be viewed with care;; * logs the total number of variants and different types; * updated and added tests to reflect these changes. The PR was tested to be runnable based on output from scanning the CHM-mix bam with PR #2444, which discovered the exception.; The number of variants discovered are:. For CHM-mix; ```; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 204 INVs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2775 DELs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 954 DUPs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 977 INSs; ```; And for NA12878_PCR-_30X; ```; 22:14:15.653 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4686 variants.; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 228 INV's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2719 DEL's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 835 DUP's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 904 INS's; ```; @cwhelan Could you please to review?; @tedsharpe feel free to poke around and test run it. __UPDATE__:; This is to be merged after PR #2444.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567
https://github.com/broadinstitute/gatk/pull/2567:736,Deployability,update,updated,736,"This PR:. * fixes a problem observed for a complex event with tandem duplicated sequence and insertion and deletion, which causes the discovery phase of SV pipeline to throw exceptions;; * the fix works by changing how tandem duplications are annotated:; * duplicated sequence is no longer provided, instead the corresponding duplication unit reference span is provided in `DUP_REPET_UNIT_REF_SPAN`,; * the duplicated units on the assembled contigs' CIGAR when aligned to reference is provided in `DUP_ASM_CTG_CIGARS`;; * adds annotation `DUP_ANNOT_FROM_OPT` when tandem repeat annotations were generated by a simple approximation procedure, which should be viewed with care;; * logs the total number of variants and different types; * updated and added tests to reflect these changes. The PR was tested to be runnable based on output from scanning the CHM-mix bam with PR #2444, which discovered the exception.; The number of variants discovered are:. For CHM-mix; ```; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 204 INVs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2775 DELs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 954 DUPs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 977 INSs; ```; And for NA12878_PCR-_30X; ```; 22:14:15.653 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4686 variants.; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 228 INV's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2719 DEL's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 835 DUP's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 904 INS's; ```; @cwhelan Could you please to review?; @tedsharpe feel free to poke around and test run it. __UPDATE__:; This is to be merged after PR #2444.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567
https://github.com/broadinstitute/gatk/pull/2567:679,Testability,log,logs,679,"This PR:. * fixes a problem observed for a complex event with tandem duplicated sequence and insertion and deletion, which causes the discovery phase of SV pipeline to throw exceptions;; * the fix works by changing how tandem duplications are annotated:; * duplicated sequence is no longer provided, instead the corresponding duplication unit reference span is provided in `DUP_REPET_UNIT_REF_SPAN`,; * the duplicated units on the assembled contigs' CIGAR when aligned to reference is provided in `DUP_ASM_CTG_CIGARS`;; * adds annotation `DUP_ANNOT_FROM_OPT` when tandem repeat annotations were generated by a simple approximation procedure, which should be viewed with care;; * logs the total number of variants and different types; * updated and added tests to reflect these changes. The PR was tested to be runnable based on output from scanning the CHM-mix bam with PR #2444, which discovered the exception.; The number of variants discovered are:. For CHM-mix; ```; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 204 INVs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2775 DELs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 954 DUPs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 977 INSs; ```; And for NA12878_PCR-_30X; ```; 22:14:15.653 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4686 variants.; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 228 INV's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2719 DEL's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 835 DUP's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 904 INS's; ```; @cwhelan Could you please to review?; @tedsharpe feel free to poke around and test run it. __UPDATE__:; This is to be merged after PR #2444.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567
https://github.com/broadinstitute/gatk/pull/2567:754,Testability,test,tests,754,"This PR:. * fixes a problem observed for a complex event with tandem duplicated sequence and insertion and deletion, which causes the discovery phase of SV pipeline to throw exceptions;; * the fix works by changing how tandem duplications are annotated:; * duplicated sequence is no longer provided, instead the corresponding duplication unit reference span is provided in `DUP_REPET_UNIT_REF_SPAN`,; * the duplicated units on the assembled contigs' CIGAR when aligned to reference is provided in `DUP_ASM_CTG_CIGARS`;; * adds annotation `DUP_ANNOT_FROM_OPT` when tandem repeat annotations were generated by a simple approximation procedure, which should be viewed with care;; * logs the total number of variants and different types; * updated and added tests to reflect these changes. The PR was tested to be runnable based on output from scanning the CHM-mix bam with PR #2444, which discovered the exception.; The number of variants discovered are:. For CHM-mix; ```; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 204 INVs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2775 DELs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 954 DUPs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 977 INSs; ```; And for NA12878_PCR-_30X; ```; 22:14:15.653 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4686 variants.; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 228 INV's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2719 DEL's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 835 DUP's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 904 INS's; ```; @cwhelan Could you please to review?; @tedsharpe feel free to poke around and test run it. __UPDATE__:; This is to be merged after PR #2444.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567
https://github.com/broadinstitute/gatk/pull/2567:797,Testability,test,tested,797,"This PR:. * fixes a problem observed for a complex event with tandem duplicated sequence and insertion and deletion, which causes the discovery phase of SV pipeline to throw exceptions;; * the fix works by changing how tandem duplications are annotated:; * duplicated sequence is no longer provided, instead the corresponding duplication unit reference span is provided in `DUP_REPET_UNIT_REF_SPAN`,; * the duplicated units on the assembled contigs' CIGAR when aligned to reference is provided in `DUP_ASM_CTG_CIGARS`;; * adds annotation `DUP_ANNOT_FROM_OPT` when tandem repeat annotations were generated by a simple approximation procedure, which should be viewed with care;; * logs the total number of variants and different types; * updated and added tests to reflect these changes. The PR was tested to be runnable based on output from scanning the CHM-mix bam with PR #2444, which discovered the exception.; The number of variants discovered are:. For CHM-mix; ```; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 204 INVs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2775 DELs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 954 DUPs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 977 INSs; ```; And for NA12878_PCR-_30X; ```; 22:14:15.653 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4686 variants.; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 228 INV's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2719 DEL's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 835 DUP's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 904 INS's; ```; @cwhelan Could you please to review?; @tedsharpe feel free to poke around and test run it. __UPDATE__:; This is to be merged after PR #2444.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567
https://github.com/broadinstitute/gatk/pull/2567:1860,Testability,test,test,1860,"This PR:. * fixes a problem observed for a complex event with tandem duplicated sequence and insertion and deletion, which causes the discovery phase of SV pipeline to throw exceptions;; * the fix works by changing how tandem duplications are annotated:; * duplicated sequence is no longer provided, instead the corresponding duplication unit reference span is provided in `DUP_REPET_UNIT_REF_SPAN`,; * the duplicated units on the assembled contigs' CIGAR when aligned to reference is provided in `DUP_ASM_CTG_CIGARS`;; * adds annotation `DUP_ANNOT_FROM_OPT` when tandem repeat annotations were generated by a simple approximation procedure, which should be viewed with care;; * logs the total number of variants and different types; * updated and added tests to reflect these changes. The PR was tested to be runnable based on output from scanning the CHM-mix bam with PR #2444, which discovered the exception.; The number of variants discovered are:. For CHM-mix; ```; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 204 INVs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2775 DELs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 954 DUPs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 977 INSs; ```; And for NA12878_PCR-_30X; ```; 22:14:15.653 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4686 variants.; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 228 INV's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2719 DEL's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 835 DUP's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 904 INS's; ```; @cwhelan Could you please to review?; @tedsharpe feel free to poke around and test run it. __UPDATE__:; This is to be merged after PR #2444.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567
https://github.com/broadinstitute/gatk/pull/2567:610,Usability,simpl,simple,610,"This PR:. * fixes a problem observed for a complex event with tandem duplicated sequence and insertion and deletion, which causes the discovery phase of SV pipeline to throw exceptions;; * the fix works by changing how tandem duplications are annotated:; * duplicated sequence is no longer provided, instead the corresponding duplication unit reference span is provided in `DUP_REPET_UNIT_REF_SPAN`,; * the duplicated units on the assembled contigs' CIGAR when aligned to reference is provided in `DUP_ASM_CTG_CIGARS`;; * adds annotation `DUP_ANNOT_FROM_OPT` when tandem repeat annotations were generated by a simple approximation procedure, which should be viewed with care;; * logs the total number of variants and different types; * updated and added tests to reflect these changes. The PR was tested to be runnable based on output from scanning the CHM-mix bam with PR #2444, which discovered the exception.; The number of variants discovered are:. For CHM-mix; ```; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 204 INVs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2775 DELs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 954 DUPs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 977 INSs; ```; And for NA12878_PCR-_30X; ```; 22:14:15.653 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4686 variants.; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 228 INV's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2719 DEL's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 835 DUP's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 904 INS's; ```; @cwhelan Could you please to review?; @tedsharpe feel free to poke around and test run it. __UPDATE__:; This is to be merged after PR #2444.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567
https://github.com/broadinstitute/gatk/pull/2569:492,Availability,failure,failures,492,"This branch takes the version of gatk-public that gatk-protected currently depends on (4.alpha.2-188-g7332d10) and applies @davidbenjamin 's fix to the `TandemRepeat` annotation to it. The only purpose of this PR is to cause a snapshot to be generated -- do not merge!. This is necessary to unblock @davidbenjamin 's work, because the `HaplotypeCaller` tests are failing if we update protected to the latest public head, and although we've fixed some of the issues there are some unexplained failures in the concordance tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2569
https://github.com/broadinstitute/gatk/pull/2569:377,Deployability,update,update,377,"This branch takes the version of gatk-public that gatk-protected currently depends on (4.alpha.2-188-g7332d10) and applies @davidbenjamin 's fix to the `TandemRepeat` annotation to it. The only purpose of this PR is to cause a snapshot to be generated -- do not merge!. This is necessary to unblock @davidbenjamin 's work, because the `HaplotypeCaller` tests are failing if we update protected to the latest public head, and although we've fixed some of the issues there are some unexplained failures in the concordance tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2569
https://github.com/broadinstitute/gatk/pull/2569:75,Integrability,depend,depends,75,"This branch takes the version of gatk-public that gatk-protected currently depends on (4.alpha.2-188-g7332d10) and applies @davidbenjamin 's fix to the `TandemRepeat` annotation to it. The only purpose of this PR is to cause a snapshot to be generated -- do not merge!. This is necessary to unblock @davidbenjamin 's work, because the `HaplotypeCaller` tests are failing if we update protected to the latest public head, and although we've fixed some of the issues there are some unexplained failures in the concordance tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2569
https://github.com/broadinstitute/gatk/pull/2569:353,Testability,test,tests,353,"This branch takes the version of gatk-public that gatk-protected currently depends on (4.alpha.2-188-g7332d10) and applies @davidbenjamin 's fix to the `TandemRepeat` annotation to it. The only purpose of this PR is to cause a snapshot to be generated -- do not merge!. This is necessary to unblock @davidbenjamin 's work, because the `HaplotypeCaller` tests are failing if we update protected to the latest public head, and although we've fixed some of the issues there are some unexplained failures in the concordance tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2569
https://github.com/broadinstitute/gatk/pull/2569:520,Testability,test,tests,520,"This branch takes the version of gatk-public that gatk-protected currently depends on (4.alpha.2-188-g7332d10) and applies @davidbenjamin 's fix to the `TandemRepeat` annotation to it. The only purpose of this PR is to cause a snapshot to be generated -- do not merge!. This is necessary to unblock @davidbenjamin 's work, because the `HaplotypeCaller` tests are failing if we update protected to the latest public head, and although we've fixed some of the issues there are some unexplained failures in the concordance tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2569
https://github.com/broadinstitute/gatk/issues/2572:262,Deployability,Configurat,Configuration,262,"There seems to be no obvious way to read thru the unmapped read pairs in a bam file in Spark. Looking at the code in ```ReadSparkSource#getParallelReads(String, String, List, long)``` it seems that ; perhaps it is possible by setting the appropriate property in Configuration returned by ```ctx.hadoopConfiguration()``` however there is no documentation as to what property that could be. . @droazen I assign it to you initially so that you route it to whoever might be most suited to address this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2572
https://github.com/broadinstitute/gatk/issues/2572:441,Integrability,rout,route,441,"There seems to be no obvious way to read thru the unmapped read pairs in a bam file in Spark. Looking at the code in ```ReadSparkSource#getParallelReads(String, String, List, long)``` it seems that ; perhaps it is possible by setting the appropriate property in Configuration returned by ```ctx.hadoopConfiguration()``` however there is no documentation as to what property that could be. . @droazen I assign it to you initially so that you route it to whoever might be most suited to address this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2572
https://github.com/broadinstitute/gatk/issues/2572:262,Modifiability,Config,Configuration,262,"There seems to be no obvious way to read thru the unmapped read pairs in a bam file in Spark. Looking at the code in ```ReadSparkSource#getParallelReads(String, String, List, long)``` it seems that ; perhaps it is possible by setting the appropriate property in Configuration returned by ```ctx.hadoopConfiguration()``` however there is no documentation as to what property that could be. . @droazen I assign it to you initially so that you route it to whoever might be most suited to address this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2572
https://github.com/broadinstitute/gatk/pull/2574:46,Deployability,toggle,toggle,46,"…ation (AVX vs AVX-OMP), max num threads, and toggle using double precision. The changes are based off of [the lb_connect_pairhmmargs branch](https://github.com/broadinstitute/gatk/tree/lb_connect_pairhmmargs), which, apparently, never got merged. The main difference between that branch and this fork is that the PairHMMNativeArguments object is passed all the way through to VectorLoglessPairHMM, which was not the case in the lb_connect_pairhmmargs branch. There is a corresponding [fork of gatk-protected](https://github.com/erniebrau/gatk-protected-1) which defines the new CLI arguments that talk to the changes in this PR. There should be a PR from that fork to gatk-protected, if and when this one is merged.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2574
https://github.com/broadinstitute/gatk/pull/2576:273,Deployability,install,installation,273,"This PR removes the use of hadoop distcp to copy reference and sample data into HDFS, and replaces it with the new ParallelCopyGCSDirectoryIntoHDFSSpark tool. Based on initial tests, cluster creation with reference and a 110GB sample BAM file now takes ~8 minutes with SGA installation on the worker nodes and ~5 minutes without SGA installation. @SHuang-Broad can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2576
https://github.com/broadinstitute/gatk/pull/2576:333,Deployability,install,installation,333,"This PR removes the use of hadoop distcp to copy reference and sample data into HDFS, and replaces it with the new ParallelCopyGCSDirectoryIntoHDFSSpark tool. Based on initial tests, cluster creation with reference and a 110GB sample BAM file now takes ~8 minutes with SGA installation on the worker nodes and ~5 minutes without SGA installation. @SHuang-Broad can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2576
https://github.com/broadinstitute/gatk/pull/2576:176,Testability,test,tests,176,"This PR removes the use of hadoop distcp to copy reference and sample data into HDFS, and replaces it with the new ParallelCopyGCSDirectoryIntoHDFSSpark tool. Based on initial tests, cluster creation with reference and a 110GB sample BAM file now takes ~8 minutes with SGA installation on the worker nodes and ~5 minutes without SGA installation. @SHuang-Broad can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2576
https://github.com/broadinstitute/gatk/issues/2577:43,Testability,log,log-space,43,"Many tools in our world require going from log-space to linear-space and back using log10 and exp. These are slow functions and are usually used on an array (typically of length 3) of doubles. It would be great to have a ; ```; log10OfArray(double[] array) ; ```; and; ```; expOfArray(double[] array, base); ```; functions to enable fast computation of logs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577
https://github.com/broadinstitute/gatk/issues/2577:353,Testability,log,logs,353,"Many tools in our world require going from log-space to linear-space and back using log10 and exp. These are slow functions and are usually used on an array (typically of length 3) of doubles. It would be great to have a ; ```; log10OfArray(double[] array) ; ```; and; ```; expOfArray(double[] array, base); ```; functions to enable fast computation of logs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577
https://github.com/broadinstitute/gatk/issues/2578:260,Integrability,depend,dependency,260,"@cmnbroad and I were looking yesterday at the process for building GATK4 protected with a custom htsjdk snapshot, and we found that we couldn't get the htsjdk snapshot to propagate all the way through to protected unless we excluded htsjdk from our GenomicsDB dependency:. ```; - compile 'com.intel:genomicsdb:0.3.0'; + compile ('com.intel:genomicsdb:0.3.0') {; + exclude module: 'htsjdk'; + }; ```. Without this, htsjdk gets pulled in transitively via GenomicsDB. @lbergelson @kgururaj @kdatta Can you think of any reason why we shouldn't make this change?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2578
https://github.com/broadinstitute/gatk/issues/2579:252,Availability,error,error,252,"I just pulled to the latest version and was surprised to see `gradlew clean` not work!; ```; $ ./gradlew clean; (...); Could not find org.broadinstitute:barclay:1.0.0-24-g87c3fa2-SNAPSHOT; ```. Reverting to 1.0.0-17-g30db73c-SNAPSHOT didn't work (same error).; Reverting to 1.0.0 made it fail somewhere else, with:; Could not resolve org.broadinstitute:gatk-bwamem-jni:1.0.0-rc1-SNAPSHOT. What's going on? Is there something wrong with my configuration?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579
https://github.com/broadinstitute/gatk/issues/2579:439,Deployability,configurat,configuration,439,"I just pulled to the latest version and was surprised to see `gradlew clean` not work!; ```; $ ./gradlew clean; (...); Could not find org.broadinstitute:barclay:1.0.0-24-g87c3fa2-SNAPSHOT; ```. Reverting to 1.0.0-17-g30db73c-SNAPSHOT didn't work (same error).; Reverting to 1.0.0 made it fail somewhere else, with:; Could not resolve org.broadinstitute:gatk-bwamem-jni:1.0.0-rc1-SNAPSHOT. What's going on? Is there something wrong with my configuration?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579
https://github.com/broadinstitute/gatk/issues/2579:439,Modifiability,config,configuration,439,"I just pulled to the latest version and was surprised to see `gradlew clean` not work!; ```; $ ./gradlew clean; (...); Could not find org.broadinstitute:barclay:1.0.0-24-g87c3fa2-SNAPSHOT; ```. Reverting to 1.0.0-17-g30db73c-SNAPSHOT didn't work (same error).; Reverting to 1.0.0 made it fail somewhere else, with:; Could not resolve org.broadinstitute:gatk-bwamem-jni:1.0.0-rc1-SNAPSHOT. What's going on? Is there something wrong with my configuration?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579
https://github.com/broadinstitute/gatk/pull/2581:44,Deployability,update,update,44,Also uncomment getAuthenticatedGcs test and update; code to new way of setting retry params.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2581
https://github.com/broadinstitute/gatk/pull/2581:35,Testability,test,test,35,Also uncomment getAuthenticatedGcs test and update; code to new way of setting retry params.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2581
https://github.com/broadinstitute/gatk/pull/2588:44,Integrability,depend,dependency,44,Exclude htsjdk and protobuf from genomicsdb dependency,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2588
https://github.com/broadinstitute/gatk/issues/2591:320,Deployability,pipeline,pipeline,320,"A request from @eitanbanks and @yfarjoun :. ""Yossi and I are just looking at our production processing costs and the HaplotypeCaller is the biggest culprit right now. That's because it currently requires these high memory machines. If we could somehow get it to use a max of 3 GB RAM then we'd cut 10% off of the entire pipeline. Even 6GB would be okay, but 3 would be huge.; What do you think -- will it be possible?""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2591
https://github.com/broadinstitute/gatk/issues/2592:157,Integrability,depend,dependent,157,"We should probably consider using a resolution policy of ""failOnVersionConflict"" in both gatk and gatk-protected. This will force us to manually resolve any dependent module version conflicts as they arise, rather than depending on Gradle (which will choose the newest one by default) to do the resolution for us.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2592
https://github.com/broadinstitute/gatk/issues/2592:219,Integrability,depend,depending,219,"We should probably consider using a resolution policy of ""failOnVersionConflict"" in both gatk and gatk-protected. This will force us to manually resolve any dependent module version conflicts as they arise, rather than depending on Gradle (which will choose the newest one by default) to do the resolution for us.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2592
https://github.com/broadinstitute/gatk/pull/2595:148,Deployability,pipeline,pipeline,148,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/pull/2595:316,Deployability,integrat,integration,316,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/pull/2595:316,Integrability,integrat,integration,316,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/pull/2595:1072,Integrability,rout,routes,1072,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/pull/2595:180,Modifiability,refactor,refactoring,180,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/pull/2595:807,Performance,optimiz,optimization,807,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/pull/2595:328,Testability,test,test,328,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/pull/2595:482,Testability,test,test,482,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/pull/2595:1135,Testability,test,test,1135,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/pull/2595:1402,Testability,log,logic,1402,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595
https://github.com/broadinstitute/gatk/issues/2596:131,Deployability,release,release,131,"As discussed in https://github.com/broadinstitute/picard/issues/786#issuecomment-293999711 and approved by @vdauwera , for the 4.0 release of GATK4 we are going to standardize all of our long argument names to follow the `--lowercase-with-dashes-as-separators` style, as opposed to the current inconsistent mix of camel case, underscores, and dashes. As part of this, we should have an opt-in enforcement check in barclay to ensure that this convention is obeyed. This change will be made more palatable for our users via https://github.com/broadinstitute/gatk/issues/1454, which is now targeted for 4.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596
https://github.com/broadinstitute/gatk/issues/2597:142,Deployability,release,release,142,"Some of our argument names have grossly inconsistent short vs. long names, like `-writeFullFormat,--never_trim_vcf_format_field`. For the 4.0 release we should do a pass to ensure that all short and long names are consistent with each other.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2597
https://github.com/broadinstitute/gatk/issues/2600:175,Availability,down,downstream,175,"Repeated runs of the mentioned tool results in different number of kmers and templates discovered, shown as below, which sometimes result in the number of variants discovered downstream to be slightly varying as well (so fart only the number of `DEL`s are observed to be affected). ```; 01:30:55.991 INFO FindBreakpointEvidenceSpark - Discovered 32911079 kmers.; 01:35:37.667 INFO FindBreakpointEvidenceSpark - Discovered 30639344 unique template names for assembly.; 02:03:37.992 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4910 variants.; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INV: 204; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DEL: 2777; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DUP: 954; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INS: 975. 02:09:24.685 INFO FindBreakpointEvidenceSpark - Discovered 32911147 kmers.; 02:14:21.462 INFO FindBreakpointEvidenceSpark - Discovered 30637728 unique template names for assembly.; 02:44:21.009 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4906 variants.; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INV: 204; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DEL: 2773; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DUP: 954; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INS: 975. 03:54:34.062 INFO FindBreakpointEvidenceSpark - Discovered 32911130 kmers.; 03:59:31.807 INFO FindBreakpointEvidenceSpark - Discovered 30637959 unique template names for assembly.; 04:31:05.403 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4907 variants.; 04:31:05.416 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INV: 204; 04:31:05.416 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DEL: 2774; 04:31:05.416 INFO DiscoverStructuralVa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2600
https://github.com/broadinstitute/gatk/issues/2601:63,Modifiability,refactor,refactoring,63,"The VCFHeader and header line hierarchy classes in htsjdk need refactoring to fix a number of bugs, and to support clean handling of new versions of VCF and BCF. This work is mostly done, PR is [here](https://github.com/samtools/htsjdk/pull/835).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2601
https://github.com/broadinstitute/gatk/issues/2603:8,Deployability,integrat,integration,8,Enables integration with GenomicsDB.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2603
https://github.com/broadinstitute/gatk/issues/2603:8,Integrability,integrat,integration,8,Enables integration with GenomicsDB.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2603
https://github.com/broadinstitute/gatk/pull/2605:329,Deployability,update,updated,329,"MannWhitneyU was re-written from scratch in 2016 in GATK3,; but these changes never got ported to GATK4. This new version; produces significantly different results from the version; currently in GATK4, resulting in VERY different values for the; RankSumTest annotations in HaplotypeCaller output. @meganshand informs me that the updated GATK3 version has been; validated in R, and has much better tests than the old version. This is a straightforward port of that version with minimal changes:. -Merged ""MWUnitTest"" and ""RankSumUnitTest"" from GATK3 into a single; test class MannWhitneyUUnitTest; -Ported MathUtils.binomialCoefficient() and wrote new test for it; -Updated RankSumTest class and tests as appropriate. I've confirmed that with this change, the RankSum annotations produced; by the GATK4 HaplotypeCaller closely match those produced by the GATK3; HaplotypeCaller. Resolves #2604",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605
https://github.com/broadinstitute/gatk/pull/2605:665,Deployability,Update,Updated,665,"MannWhitneyU was re-written from scratch in 2016 in GATK3,; but these changes never got ported to GATK4. This new version; produces significantly different results from the version; currently in GATK4, resulting in VERY different values for the; RankSumTest annotations in HaplotypeCaller output. @meganshand informs me that the updated GATK3 version has been; validated in R, and has much better tests than the old version. This is a straightforward port of that version with minimal changes:. -Merged ""MWUnitTest"" and ""RankSumUnitTest"" from GATK3 into a single; test class MannWhitneyUUnitTest; -Ported MathUtils.binomialCoefficient() and wrote new test for it; -Updated RankSumTest class and tests as appropriate. I've confirmed that with this change, the RankSum annotations produced; by the GATK4 HaplotypeCaller closely match those produced by the GATK3; HaplotypeCaller. Resolves #2604",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605
https://github.com/broadinstitute/gatk/pull/2605:361,Security,validat,validated,361,"MannWhitneyU was re-written from scratch in 2016 in GATK3,; but these changes never got ported to GATK4. This new version; produces significantly different results from the version; currently in GATK4, resulting in VERY different values for the; RankSumTest annotations in HaplotypeCaller output. @meganshand informs me that the updated GATK3 version has been; validated in R, and has much better tests than the old version. This is a straightforward port of that version with minimal changes:. -Merged ""MWUnitTest"" and ""RankSumUnitTest"" from GATK3 into a single; test class MannWhitneyUUnitTest; -Ported MathUtils.binomialCoefficient() and wrote new test for it; -Updated RankSumTest class and tests as appropriate. I've confirmed that with this change, the RankSum annotations produced; by the GATK4 HaplotypeCaller closely match those produced by the GATK3; HaplotypeCaller. Resolves #2604",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605
https://github.com/broadinstitute/gatk/pull/2605:397,Testability,test,tests,397,"MannWhitneyU was re-written from scratch in 2016 in GATK3,; but these changes never got ported to GATK4. This new version; produces significantly different results from the version; currently in GATK4, resulting in VERY different values for the; RankSumTest annotations in HaplotypeCaller output. @meganshand informs me that the updated GATK3 version has been; validated in R, and has much better tests than the old version. This is a straightforward port of that version with minimal changes:. -Merged ""MWUnitTest"" and ""RankSumUnitTest"" from GATK3 into a single; test class MannWhitneyUUnitTest; -Ported MathUtils.binomialCoefficient() and wrote new test for it; -Updated RankSumTest class and tests as appropriate. I've confirmed that with this change, the RankSum annotations produced; by the GATK4 HaplotypeCaller closely match those produced by the GATK3; HaplotypeCaller. Resolves #2604",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605
https://github.com/broadinstitute/gatk/pull/2605:564,Testability,test,test,564,"MannWhitneyU was re-written from scratch in 2016 in GATK3,; but these changes never got ported to GATK4. This new version; produces significantly different results from the version; currently in GATK4, resulting in VERY different values for the; RankSumTest annotations in HaplotypeCaller output. @meganshand informs me that the updated GATK3 version has been; validated in R, and has much better tests than the old version. This is a straightforward port of that version with minimal changes:. -Merged ""MWUnitTest"" and ""RankSumUnitTest"" from GATK3 into a single; test class MannWhitneyUUnitTest; -Ported MathUtils.binomialCoefficient() and wrote new test for it; -Updated RankSumTest class and tests as appropriate. I've confirmed that with this change, the RankSum annotations produced; by the GATK4 HaplotypeCaller closely match those produced by the GATK3; HaplotypeCaller. Resolves #2604",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605
https://github.com/broadinstitute/gatk/pull/2605:651,Testability,test,test,651,"MannWhitneyU was re-written from scratch in 2016 in GATK3,; but these changes never got ported to GATK4. This new version; produces significantly different results from the version; currently in GATK4, resulting in VERY different values for the; RankSumTest annotations in HaplotypeCaller output. @meganshand informs me that the updated GATK3 version has been; validated in R, and has much better tests than the old version. This is a straightforward port of that version with minimal changes:. -Merged ""MWUnitTest"" and ""RankSumUnitTest"" from GATK3 into a single; test class MannWhitneyUUnitTest; -Ported MathUtils.binomialCoefficient() and wrote new test for it; -Updated RankSumTest class and tests as appropriate. I've confirmed that with this change, the RankSum annotations produced; by the GATK4 HaplotypeCaller closely match those produced by the GATK3; HaplotypeCaller. Resolves #2604",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605
https://github.com/broadinstitute/gatk/pull/2605:695,Testability,test,tests,695,"MannWhitneyU was re-written from scratch in 2016 in GATK3,; but these changes never got ported to GATK4. This new version; produces significantly different results from the version; currently in GATK4, resulting in VERY different values for the; RankSumTest annotations in HaplotypeCaller output. @meganshand informs me that the updated GATK3 version has been; validated in R, and has much better tests than the old version. This is a straightforward port of that version with minimal changes:. -Merged ""MWUnitTest"" and ""RankSumUnitTest"" from GATK3 into a single; test class MannWhitneyUUnitTest; -Ported MathUtils.binomialCoefficient() and wrote new test for it; -Updated RankSumTest class and tests as appropriate. I've confirmed that with this change, the RankSum annotations produced; by the GATK4 HaplotypeCaller closely match those produced by the GATK3; HaplotypeCaller. Resolves #2604",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605
https://github.com/broadinstitute/gatk/issues/2608:160,Testability,test,test,160,"Once https://github.com/broadinstitute/gatk/pull/2389 is merged, we need to make a simple PR against the tool to allow it to work with NIO, and add a GCS-based test case. This should be an easy change and < 1 day of work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2608
https://github.com/broadinstitute/gatk/issues/2608:83,Usability,simpl,simple,83,"Once https://github.com/broadinstitute/gatk/pull/2389 is merged, we need to make a simple PR against the tool to allow it to work with NIO, and add a GCS-based test case. This should be an easy change and < 1 day of work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2608
https://github.com/broadinstitute/gatk/pull/2610:84,Testability,test,tests,84,Surgical changes to the VariantContextTestUtils to allow the GenomicsDB importer to tests to compare variants coming from a BCF stream with variants coming from a VCF stream.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2610
https://github.com/broadinstitute/gatk/pull/2611:377,Integrability,wrap,wrapping,377,"The version of testNG that we're using has some [inconsistencies](https://github.com/cbeust/testng/pull/790) in how nested arrays/collections are handled, and they've resolved them in newer versions by implementing changes that are not backward compatible. The AlleleFrequencyCalculatorUnitTest tests fail with newer versions of testNG that have these changes, since they were wrapping an array in a single element List (not sure if that was intentional or not). The testNG issues seem to suggest using assertEqualsDeep for [some cases](https://github.com/cbeust/testng/issues/1342) but even the newest version doesn't seem to have an overload for Collections. For AlleleFrequencyCalculatorUnitTest though, the collection wrapping was unnecessary anyway, so the array comparisons can be made directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2611
https://github.com/broadinstitute/gatk/pull/2611:722,Integrability,wrap,wrapping,722,"The version of testNG that we're using has some [inconsistencies](https://github.com/cbeust/testng/pull/790) in how nested arrays/collections are handled, and they've resolved them in newer versions by implementing changes that are not backward compatible. The AlleleFrequencyCalculatorUnitTest tests fail with newer versions of testNG that have these changes, since they were wrapping an array in a single element List (not sure if that was intentional or not). The testNG issues seem to suggest using assertEqualsDeep for [some cases](https://github.com/cbeust/testng/issues/1342) but even the newest version doesn't seem to have an overload for Collections. For AlleleFrequencyCalculatorUnitTest though, the collection wrapping was unnecessary anyway, so the array comparisons can be made directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2611
https://github.com/broadinstitute/gatk/pull/2611:15,Testability,test,testNG,15,"The version of testNG that we're using has some [inconsistencies](https://github.com/cbeust/testng/pull/790) in how nested arrays/collections are handled, and they've resolved them in newer versions by implementing changes that are not backward compatible. The AlleleFrequencyCalculatorUnitTest tests fail with newer versions of testNG that have these changes, since they were wrapping an array in a single element List (not sure if that was intentional or not). The testNG issues seem to suggest using assertEqualsDeep for [some cases](https://github.com/cbeust/testng/issues/1342) but even the newest version doesn't seem to have an overload for Collections. For AlleleFrequencyCalculatorUnitTest though, the collection wrapping was unnecessary anyway, so the array comparisons can be made directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2611
https://github.com/broadinstitute/gatk/pull/2611:92,Testability,test,testng,92,"The version of testNG that we're using has some [inconsistencies](https://github.com/cbeust/testng/pull/790) in how nested arrays/collections are handled, and they've resolved them in newer versions by implementing changes that are not backward compatible. The AlleleFrequencyCalculatorUnitTest tests fail with newer versions of testNG that have these changes, since they were wrapping an array in a single element List (not sure if that was intentional or not). The testNG issues seem to suggest using assertEqualsDeep for [some cases](https://github.com/cbeust/testng/issues/1342) but even the newest version doesn't seem to have an overload for Collections. For AlleleFrequencyCalculatorUnitTest though, the collection wrapping was unnecessary anyway, so the array comparisons can be made directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2611
https://github.com/broadinstitute/gatk/pull/2611:295,Testability,test,tests,295,"The version of testNG that we're using has some [inconsistencies](https://github.com/cbeust/testng/pull/790) in how nested arrays/collections are handled, and they've resolved them in newer versions by implementing changes that are not backward compatible. The AlleleFrequencyCalculatorUnitTest tests fail with newer versions of testNG that have these changes, since they were wrapping an array in a single element List (not sure if that was intentional or not). The testNG issues seem to suggest using assertEqualsDeep for [some cases](https://github.com/cbeust/testng/issues/1342) but even the newest version doesn't seem to have an overload for Collections. For AlleleFrequencyCalculatorUnitTest though, the collection wrapping was unnecessary anyway, so the array comparisons can be made directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2611
https://github.com/broadinstitute/gatk/pull/2611:329,Testability,test,testNG,329,"The version of testNG that we're using has some [inconsistencies](https://github.com/cbeust/testng/pull/790) in how nested arrays/collections are handled, and they've resolved them in newer versions by implementing changes that are not backward compatible. The AlleleFrequencyCalculatorUnitTest tests fail with newer versions of testNG that have these changes, since they were wrapping an array in a single element List (not sure if that was intentional or not). The testNG issues seem to suggest using assertEqualsDeep for [some cases](https://github.com/cbeust/testng/issues/1342) but even the newest version doesn't seem to have an overload for Collections. For AlleleFrequencyCalculatorUnitTest though, the collection wrapping was unnecessary anyway, so the array comparisons can be made directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2611
https://github.com/broadinstitute/gatk/pull/2611:467,Testability,test,testNG,467,"The version of testNG that we're using has some [inconsistencies](https://github.com/cbeust/testng/pull/790) in how nested arrays/collections are handled, and they've resolved them in newer versions by implementing changes that are not backward compatible. The AlleleFrequencyCalculatorUnitTest tests fail with newer versions of testNG that have these changes, since they were wrapping an array in a single element List (not sure if that was intentional or not). The testNG issues seem to suggest using assertEqualsDeep for [some cases](https://github.com/cbeust/testng/issues/1342) but even the newest version doesn't seem to have an overload for Collections. For AlleleFrequencyCalculatorUnitTest though, the collection wrapping was unnecessary anyway, so the array comparisons can be made directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2611
https://github.com/broadinstitute/gatk/pull/2611:503,Testability,assert,assertEqualsDeep,503,"The version of testNG that we're using has some [inconsistencies](https://github.com/cbeust/testng/pull/790) in how nested arrays/collections are handled, and they've resolved them in newer versions by implementing changes that are not backward compatible. The AlleleFrequencyCalculatorUnitTest tests fail with newer versions of testNG that have these changes, since they were wrapping an array in a single element List (not sure if that was intentional or not). The testNG issues seem to suggest using assertEqualsDeep for [some cases](https://github.com/cbeust/testng/issues/1342) but even the newest version doesn't seem to have an overload for Collections. For AlleleFrequencyCalculatorUnitTest though, the collection wrapping was unnecessary anyway, so the array comparisons can be made directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2611
https://github.com/broadinstitute/gatk/pull/2611:563,Testability,test,testng,563,"The version of testNG that we're using has some [inconsistencies](https://github.com/cbeust/testng/pull/790) in how nested arrays/collections are handled, and they've resolved them in newer versions by implementing changes that are not backward compatible. The AlleleFrequencyCalculatorUnitTest tests fail with newer versions of testNG that have these changes, since they were wrapping an array in a single element List (not sure if that was intentional or not). The testNG issues seem to suggest using assertEqualsDeep for [some cases](https://github.com/cbeust/testng/issues/1342) but even the newest version doesn't seem to have an overload for Collections. For AlleleFrequencyCalculatorUnitTest though, the collection wrapping was unnecessary anyway, so the array comparisons can be made directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2611
https://github.com/broadinstitute/gatk/pull/2612:61,Testability,test,test,61,Also fix an issue with git lfs and some tabix indices in src/test/resources/large/gvcfs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2612
https://github.com/broadinstitute/gatk/pull/2615:18,Availability,error,errors,18,This gives better errors in the case of sequence dictionary mismatches.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2615
https://github.com/broadinstitute/gatk/pull/2617:23,Integrability,depend,dependencies,23,And remove unnecessary dependencies introduced for GenomicsDB.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2617
https://github.com/broadinstitute/gatk/pull/2618:15,Availability,failure,failures,15,We were seeing failures on spark clusters that manifested as being; unable to find the Main class while running spark submit. The caused was the accidental introduction of a jar signature file; and key from the transitive gnu.getopt dependency. This was causing; signature validation failures since our uber jar did not match the; expected hashes. Fixed by excluding .SF and .RSA files from our jars.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618
https://github.com/broadinstitute/gatk/pull/2618:284,Availability,failure,failures,284,We were seeing failures on spark clusters that manifested as being; unable to find the Main class while running spark submit. The caused was the accidental introduction of a jar signature file; and key from the transitive gnu.getopt dependency. This was causing; signature validation failures since our uber jar did not match the; expected hashes. Fixed by excluding .SF and .RSA files from our jars.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618
https://github.com/broadinstitute/gatk/pull/2618:233,Integrability,depend,dependency,233,We were seeing failures on spark clusters that manifested as being; unable to find the Main class while running spark submit. The caused was the accidental introduction of a jar signature file; and key from the transitive gnu.getopt dependency. This was causing; signature validation failures since our uber jar did not match the; expected hashes. Fixed by excluding .SF and .RSA files from our jars.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618
https://github.com/broadinstitute/gatk/pull/2618:273,Security,validat,validation,273,We were seeing failures on spark clusters that manifested as being; unable to find the Main class while running spark submit. The caused was the accidental introduction of a jar signature file; and key from the transitive gnu.getopt dependency. This was causing; signature validation failures since our uber jar did not match the; expected hashes. Fixed by excluding .SF and .RSA files from our jars.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618
https://github.com/broadinstitute/gatk/pull/2618:340,Security,hash,hashes,340,We were seeing failures on spark clusters that manifested as being; unable to find the Main class while running spark submit. The caused was the accidental introduction of a jar signature file; and key from the transitive gnu.getopt dependency. This was causing; signature validation failures since our uber jar did not match the; expected hashes. Fixed by excluding .SF and .RSA files from our jars.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618
https://github.com/broadinstitute/gatk/pull/2621:567,Deployability,integrat,integration,567,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:872,Deployability,integrat,integration,872,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:567,Integrability,integrat,integration,567,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:872,Integrability,integrat,integration,872,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:306,Modifiability,refactor,refactored,306,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:1321,Security,access,access,1321,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:1455,Security,access,access,1455,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:579,Testability,test,tests,579,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:884,Testability,test,tests,884,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:897,Testability,test,test,897,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:936,Testability,test,tests,936,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:978,Testability,test,tests,978,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:1031,Testability,test,tested,1031,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:1209,Testability,log,logs,1209,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:929,Usability,simpl,simple,929,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/pull/2621:1415,Usability,simpl,simply,1415,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621
https://github.com/broadinstitute/gatk/issues/2622:38,Availability,error,error,38,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:73,Availability,ERROR,ERROR,73,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:191,Availability,ERROR,ERROR,191,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:256,Availability,ERROR,ERROR,256,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:336,Availability,ERROR,ERROR,336,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:457,Availability,ERROR,ERROR,457,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:518,Availability,ERROR,ERROR,518,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:636,Availability,ERROR,ERROR,636,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:701,Availability,ERROR,ERROR,701,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:781,Availability,ERROR,ERROR,781,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:902,Availability,ERROR,ERROR,902,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:44,Integrability,message,messages,44,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:1105,Integrability,depend,dependency,1105,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:174,Modifiability,variab,variable,174,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:619,Modifiability,variab,variable,619,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:239,Performance,load,loaded,239,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:381,Performance,load,loaded,381,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:684,Performance,load,loaded,684,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2622:826,Performance,load,loaded,826,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622
https://github.com/broadinstitute/gatk/issues/2625:278,Safety,avoid,avoid,278,"Currently we have to convert from `List` to array in `RankSumTest` when calling into `MannWhitneyU`:. ```; final MannWhitneyU.Result result = mannWhitneyU.test(Doubles.toArray(altQuals), Doubles.toArray(refQuals), MannWhitneyU.TestType.FIRST_DOMINATES);; ```. Ideally we should avoid this wasteful conversion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2625
https://github.com/broadinstitute/gatk/issues/2625:155,Testability,test,test,155,"Currently we have to convert from `List` to array in `RankSumTest` when calling into `MannWhitneyU`:. ```; final MannWhitneyU.Result result = mannWhitneyU.test(Doubles.toArray(altQuals), Doubles.toArray(refQuals), MannWhitneyU.TestType.FIRST_DOMINATES);; ```. Ideally we should avoid this wasteful conversion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2625
https://github.com/broadinstitute/gatk/issues/2625:227,Testability,Test,TestType,227,"Currently we have to convert from `List` to array in `RankSumTest` when calling into `MannWhitneyU`:. ```; final MannWhitneyU.Result result = mannWhitneyU.test(Doubles.toArray(altQuals), Doubles.toArray(refQuals), MannWhitneyU.TestType.FIRST_DOMINATES);; ```. Ideally we should avoid this wasteful conversion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2625
https://github.com/broadinstitute/gatk/pull/2626:373,Availability,avail,available,373,"* added a reference parameter to FeatureData source and FeatureManager methods; * genomicsDB requires a reference, previously this was being passed; through by hardcoding it in the required json files; * json files are now autogenerated by the importer tool, but the; reference wasn't being handled correctly. * updated the various walkers to pass the reference through if available. * gendb:// paths now point to the workspace directory instead of a; directory of jsons. * removed the ability to specify array, vidmap.json, and; callset.json paths in the importer tool since we now rely on the; structure and naming of the files when loading; moved some constants to GenomicsDBConstants. * updated GenomicsDBIntegration tests to use the new importer instead of a; prepackaged and very brittle set of json files. fixed a bug in GenomicsDBImporterIntegrationTests that made both tests; write to the same workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626
https://github.com/broadinstitute/gatk/pull/2626:312,Deployability,update,updated,312,"* added a reference parameter to FeatureData source and FeatureManager methods; * genomicsDB requires a reference, previously this was being passed; through by hardcoding it in the required json files; * json files are now autogenerated by the importer tool, but the; reference wasn't being handled correctly. * updated the various walkers to pass the reference through if available. * gendb:// paths now point to the workspace directory instead of a; directory of jsons. * removed the ability to specify array, vidmap.json, and; callset.json paths in the importer tool since we now rely on the; structure and naming of the files when loading; moved some constants to GenomicsDBConstants. * updated GenomicsDBIntegration tests to use the new importer instead of a; prepackaged and very brittle set of json files. fixed a bug in GenomicsDBImporterIntegrationTests that made both tests; write to the same workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626
https://github.com/broadinstitute/gatk/pull/2626:691,Deployability,update,updated,691,"* added a reference parameter to FeatureData source and FeatureManager methods; * genomicsDB requires a reference, previously this was being passed; through by hardcoding it in the required json files; * json files are now autogenerated by the importer tool, but the; reference wasn't being handled correctly. * updated the various walkers to pass the reference through if available. * gendb:// paths now point to the workspace directory instead of a; directory of jsons. * removed the ability to specify array, vidmap.json, and; callset.json paths in the importer tool since we now rely on the; structure and naming of the files when loading; moved some constants to GenomicsDBConstants. * updated GenomicsDBIntegration tests to use the new importer instead of a; prepackaged and very brittle set of json files. fixed a bug in GenomicsDBImporterIntegrationTests that made both tests; write to the same workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626
https://github.com/broadinstitute/gatk/pull/2626:635,Performance,load,loading,635,"* added a reference parameter to FeatureData source and FeatureManager methods; * genomicsDB requires a reference, previously this was being passed; through by hardcoding it in the required json files; * json files are now autogenerated by the importer tool, but the; reference wasn't being handled correctly. * updated the various walkers to pass the reference through if available. * gendb:// paths now point to the workspace directory instead of a; directory of jsons. * removed the ability to specify array, vidmap.json, and; callset.json paths in the importer tool since we now rely on the; structure and naming of the files when loading; moved some constants to GenomicsDBConstants. * updated GenomicsDBIntegration tests to use the new importer instead of a; prepackaged and very brittle set of json files. fixed a bug in GenomicsDBImporterIntegrationTests that made both tests; write to the same workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626
https://github.com/broadinstitute/gatk/pull/2626:721,Testability,test,tests,721,"* added a reference parameter to FeatureData source and FeatureManager methods; * genomicsDB requires a reference, previously this was being passed; through by hardcoding it in the required json files; * json files are now autogenerated by the importer tool, but the; reference wasn't being handled correctly. * updated the various walkers to pass the reference through if available. * gendb:// paths now point to the workspace directory instead of a; directory of jsons. * removed the ability to specify array, vidmap.json, and; callset.json paths in the importer tool since we now rely on the; structure and naming of the files when loading; moved some constants to GenomicsDBConstants. * updated GenomicsDBIntegration tests to use the new importer instead of a; prepackaged and very brittle set of json files. fixed a bug in GenomicsDBImporterIntegrationTests that made both tests; write to the same workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626
https://github.com/broadinstitute/gatk/pull/2626:878,Testability,test,tests,878,"* added a reference parameter to FeatureData source and FeatureManager methods; * genomicsDB requires a reference, previously this was being passed; through by hardcoding it in the required json files; * json files are now autogenerated by the importer tool, but the; reference wasn't being handled correctly. * updated the various walkers to pass the reference through if available. * gendb:// paths now point to the workspace directory instead of a; directory of jsons. * removed the ability to specify array, vidmap.json, and; callset.json paths in the importer tool since we now rely on the; structure and naming of the files when loading; moved some constants to GenomicsDBConstants. * updated GenomicsDBIntegration tests to use the new importer instead of a; prepackaged and very brittle set of json files. fixed a bug in GenomicsDBImporterIntegrationTests that made both tests; write to the same workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626
https://github.com/broadinstitute/gatk/pull/2631:43,Security,expose,exposed,43,@droazen I realized I should probably have exposed these methods for protected since we'd need to duplicated them in order to do broadinstitute/gatk-protected#1013. this includes the changes in #2630,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2631
https://github.com/broadinstitute/gatk/issues/2633:185,Performance,optimiz,optimize,185,"All that needs to be done here is prove that GenomicsDBImport can run on an 11k sample callset over a single interval without exploding or running out of memory. We don't need to hyper-optimize memory usage, or optimize the instance types for cost, etc. We should also probably pair it with GenotypeGVCFs in a single WDL script, to make sure that tool doesn't blow up either. Once done, we should share the settings we used with red team. Details on how to access the 11k sample set are in a Google doc that has been shared privately.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633
https://github.com/broadinstitute/gatk/issues/2633:211,Performance,optimiz,optimize,211,"All that needs to be done here is prove that GenomicsDBImport can run on an 11k sample callset over a single interval without exploding or running out of memory. We don't need to hyper-optimize memory usage, or optimize the instance types for cost, etc. We should also probably pair it with GenotypeGVCFs in a single WDL script, to make sure that tool doesn't blow up either. Once done, we should share the settings we used with red team. Details on how to access the 11k sample set are in a Google doc that has been shared privately.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633
https://github.com/broadinstitute/gatk/issues/2633:457,Security,access,access,457,"All that needs to be done here is prove that GenomicsDBImport can run on an 11k sample callset over a single interval without exploding or running out of memory. We don't need to hyper-optimize memory usage, or optimize the instance types for cost, etc. We should also probably pair it with GenotypeGVCFs in a single WDL script, to make sure that tool doesn't blow up either. Once done, we should share the settings we used with red team. Details on how to access the 11k sample set are in a Google doc that has been shared privately.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633
https://github.com/broadinstitute/gatk/pull/2634:54,Deployability,release,released,54,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634
https://github.com/broadinstitute/gatk/pull/2634:324,Deployability,configurat,configuration,324,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634
https://github.com/broadinstitute/gatk/pull/2634:71,Integrability,depend,dependence,71,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634
https://github.com/broadinstitute/gatk/pull/2634:166,Integrability,depend,dependence,166,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634
https://github.com/broadinstitute/gatk/pull/2634:338,Integrability,protocol,protocol,338,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634
https://github.com/broadinstitute/gatk/pull/2634:385,Integrability,depend,dependence,385,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634
https://github.com/broadinstitute/gatk/pull/2634:324,Modifiability,config,configuration,324,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634
https://github.com/broadinstitute/gatk/issues/2636:161,Deployability,configurat,configuration,161,"@kdatta @kgururaj It seems like we're losing rsID's in the input gvcf when we load them into genomics db. Is this deliberate to save space? Is it a bug? Is it a configuration option that isn't exposed by `GenomicsDBImport`? . I don't think it's important for production because they pass in a dbSNP at genotyping time so that can be recomputed, but it's causing issues in some of my tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2636
https://github.com/broadinstitute/gatk/issues/2636:161,Modifiability,config,configuration,161,"@kdatta @kgururaj It seems like we're losing rsID's in the input gvcf when we load them into genomics db. Is this deliberate to save space? Is it a bug? Is it a configuration option that isn't exposed by `GenomicsDBImport`? . I don't think it's important for production because they pass in a dbSNP at genotyping time so that can be recomputed, but it's causing issues in some of my tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2636
https://github.com/broadinstitute/gatk/issues/2636:78,Performance,load,load,78,"@kdatta @kgururaj It seems like we're losing rsID's in the input gvcf when we load them into genomics db. Is this deliberate to save space? Is it a bug? Is it a configuration option that isn't exposed by `GenomicsDBImport`? . I don't think it's important for production because they pass in a dbSNP at genotyping time so that can be recomputed, but it's causing issues in some of my tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2636
https://github.com/broadinstitute/gatk/issues/2636:193,Security,expose,exposed,193,"@kdatta @kgururaj It seems like we're losing rsID's in the input gvcf when we load them into genomics db. Is this deliberate to save space? Is it a bug? Is it a configuration option that isn't exposed by `GenomicsDBImport`? . I don't think it's important for production because they pass in a dbSNP at genotyping time so that can be recomputed, but it's causing issues in some of my tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2636
https://github.com/broadinstitute/gatk/issues/2636:383,Testability,test,tests,383,"@kdatta @kgururaj It seems like we're losing rsID's in the input gvcf when we load them into genomics db. Is this deliberate to save space? Is it a bug? Is it a configuration option that isn't exposed by `GenomicsDBImport`? . I don't think it's important for production because they pass in a dbSNP at genotyping time so that can be recomputed, but it's causing issues in some of my tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2636
https://github.com/broadinstitute/gatk/issues/2638:85,Testability,assert,assertCondition,85,"Currently in `GenomicsDBImportIntegrationTest` we have the following:. ```; BaseTest.assertCondition(actualVcs, expectedVcs, (a, e) -> {; // TODO: Temporary hacks to make this test pass. Must be removed later; if ( // allele order; e.getStart() != 17967343 && e.getStart() != 17966384 &&; // split block; e.getEnd() != 17981447; ) {; VariantContextTestUtils.assertVariantContextsAreEqual(a, e, Collections.emptyList());; }; });; ```. We should remove these exclusions once we're able to compare without regard to allele ordering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2638
https://github.com/broadinstitute/gatk/issues/2638:176,Testability,test,test,176,"Currently in `GenomicsDBImportIntegrationTest` we have the following:. ```; BaseTest.assertCondition(actualVcs, expectedVcs, (a, e) -> {; // TODO: Temporary hacks to make this test pass. Must be removed later; if ( // allele order; e.getStart() != 17967343 && e.getStart() != 17966384 &&; // split block; e.getEnd() != 17981447; ) {; VariantContextTestUtils.assertVariantContextsAreEqual(a, e, Collections.emptyList());; }; });; ```. We should remove these exclusions once we're able to compare without regard to allele ordering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2638
https://github.com/broadinstitute/gatk/issues/2638:358,Testability,assert,assertVariantContextsAreEqual,358,"Currently in `GenomicsDBImportIntegrationTest` we have the following:. ```; BaseTest.assertCondition(actualVcs, expectedVcs, (a, e) -> {; // TODO: Temporary hacks to make this test pass. Must be removed later; if ( // allele order; e.getStart() != 17967343 && e.getStart() != 17966384 &&; // split block; e.getEnd() != 17981447; ) {; VariantContextTestUtils.assertVariantContextsAreEqual(a, e, Collections.emptyList());; }; });; ```. We should remove these exclusions once we're able to compare without regard to allele ordering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2638
https://github.com/broadinstitute/gatk/issues/2639:0,Performance,Load,Loading,0,"Loading all the headers up-front to get the sample names and create a merged header in `GenomicsDBImport` is going to be very expensive over GCS with large numbers of samples. Let's try to find a way to avoid this (eg., could we pass in a list of samples explicitly, plus a vcf whose header to treat as canonical?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2639
https://github.com/broadinstitute/gatk/issues/2639:203,Safety,avoid,avoid,203,"Loading all the headers up-front to get the sample names and create a merged header in `GenomicsDBImport` is going to be very expensive over GCS with large numbers of samples. Let's try to find a way to avoid this (eg., could we pass in a list of samples explicitly, plus a vcf whose header to treat as canonical?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2639
https://github.com/broadinstitute/gatk/issues/2640:264,Performance,perform,performance,264,"`--cloudPrefetchBuffer` and `--cloudIndexPrefetchBuffer` default to 40 MB, which is far too large for the `GenomicsDBImport` tool, as we are allocating 80 MB of buffer per reader. This will limit the number of samples we can batch together, and degrade GenomicsDB performance as a result. Let's find the smallest values for these arguments that still give acceptable performance. We can do this by running GenomicsDBImport with, eg., 10 samples with the default settings, then repeatedly halving the cloud buffer sizes until performance degrades too much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640
https://github.com/broadinstitute/gatk/issues/2640:367,Performance,perform,performance,367,"`--cloudPrefetchBuffer` and `--cloudIndexPrefetchBuffer` default to 40 MB, which is far too large for the `GenomicsDBImport` tool, as we are allocating 80 MB of buffer per reader. This will limit the number of samples we can batch together, and degrade GenomicsDB performance as a result. Let's find the smallest values for these arguments that still give acceptable performance. We can do this by running GenomicsDBImport with, eg., 10 samples with the default settings, then repeatedly halving the cloud buffer sizes until performance degrades too much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640
https://github.com/broadinstitute/gatk/issues/2640:525,Performance,perform,performance,525,"`--cloudPrefetchBuffer` and `--cloudIndexPrefetchBuffer` default to 40 MB, which is far too large for the `GenomicsDBImport` tool, as we are allocating 80 MB of buffer per reader. This will limit the number of samples we can batch together, and degrade GenomicsDB performance as a result. Let's find the smallest values for these arguments that still give acceptable performance. We can do this by running GenomicsDBImport with, eg., 10 samples with the default settings, then repeatedly halving the cloud buffer sizes until performance degrades too much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640
https://github.com/broadinstitute/gatk/issues/2641:240,Performance,perform,performance,240,"This argument controls the number of simultaneous readers open in the tool, and is therefore a way of controlling memory use. We want to find a value for this argument that controls memory use effectively while still not harming GenomicsDB performance too much by fragmenting the database. As part of this, we should determine whether we need to run consolidation/defragmentation on GenomicsDB after running `GenomicsDB` import.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2641
https://github.com/broadinstitute/gatk/pull/2643:47,Availability,error,error,47,"Wrapping a prefetcher into another would be an error, check; for it and report it. The commit also includes a test. Also shut down the executor when the channel is closed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643
https://github.com/broadinstitute/gatk/pull/2643:126,Availability,down,down,126,"Wrapping a prefetcher into another would be an error, check; for it and report it. The commit also includes a test. Also shut down the executor when the channel is closed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643
https://github.com/broadinstitute/gatk/pull/2643:0,Integrability,Wrap,Wrapping,0,"Wrapping a prefetcher into another would be an error, check; for it and report it. The commit also includes a test. Also shut down the executor when the channel is closed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643
https://github.com/broadinstitute/gatk/pull/2643:110,Testability,test,test,110,"Wrapping a prefetcher into another would be an error, check; for it and report it. The commit also includes a test. Also shut down the executor when the channel is closed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643
https://github.com/broadinstitute/gatk/pull/2646:955,Availability,mask,masking,955,"@cwhelan @tedsharpe please review. The pipeline tools are:; 1. PathSeqFilterSpark : quality/low-complexity/host read filtering; 2. PathSeqPathogenAlignSpark : bwa-mem aligner; 3. PathSeqClassifyReadsSpark : quantifies pathogen abundance. These are supported by utilities:; 4. PathSeqKmerSpark - creates kmer library (either a Hopscotch set or Bloom filter) used by Filter tool; 5. PathSeqBuildReferenceTaxonomy - creates a file containing taxonomic information for a given reference, required by ClassifyReads. tools.spark.pathseq package:; Contains all the tools. Static helper functions were put into PS<ToolName>Utils classes, eg PSFilterUtils contains functions used by PathSeqFilterSpark. PSUtils contains mostly functions that are used by more than one tool. There are a number of other ""PS"" classes for doing Bwa, taxonomy bookkeeping, and read classification. Also has the kmer and host alignment read filters. tools.spark.sv package:; Added base masking to SVKmerShort class. tools.spark.utils package:; Hopscotch set and Bloom filter for long primitives. Each type has a ""Large"" class for sets exceeding the maximum JVM array size (~2B).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2646
https://github.com/broadinstitute/gatk/pull/2646:39,Deployability,pipeline,pipeline,39,"@cwhelan @tedsharpe please review. The pipeline tools are:; 1. PathSeqFilterSpark : quality/low-complexity/host read filtering; 2. PathSeqPathogenAlignSpark : bwa-mem aligner; 3. PathSeqClassifyReadsSpark : quantifies pathogen abundance. These are supported by utilities:; 4. PathSeqKmerSpark - creates kmer library (either a Hopscotch set or Bloom filter) used by Filter tool; 5. PathSeqBuildReferenceTaxonomy - creates a file containing taxonomic information for a given reference, required by ClassifyReads. tools.spark.pathseq package:; Contains all the tools. Static helper functions were put into PS<ToolName>Utils classes, eg PSFilterUtils contains functions used by PathSeqFilterSpark. PSUtils contains mostly functions that are used by more than one tool. There are a number of other ""PS"" classes for doing Bwa, taxonomy bookkeeping, and read classification. Also has the kmer and host alignment read filters. tools.spark.sv package:; Added base masking to SVKmerShort class. tools.spark.utils package:; Hopscotch set and Bloom filter for long primitives. Each type has a ""Large"" class for sets exceeding the maximum JVM array size (~2B).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2646
https://github.com/broadinstitute/gatk/issues/2652:155,Usability,clear,clear,155,"Currently the prefetcher threads all show up as something like ""thread-pool-1-thread-1"". It would be nice if we could give them a name so it's immediately clear in the profiler what they are. It looks like this can be done with a custom thread factory. Guava has one that should make this easy, ; `new ThreadFactoryBuilder().setNameFormat(""nio-prefetcher-thread-%d"").build()`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2652
https://github.com/broadinstitute/gatk/issues/2658:38,Availability,error,error,38,"@kcibul reports getting the following error while running `GenotypeGVCFs`:. ```; New error coming out of GenotypeGVCFs when running 1000 shards:. [11:20] ; ***********************************************************************. A USER ERROR has occurred: Bad input: Cannot calculate Root Mean Square Mapping Quality if there are 0 or less reads.; Number of reads recorded as :0; In VariantContext: [VC UG_call @ chr1:143249103 Q38.86 of type=SNP alleles=[C*, T] attr={DP=14, ExcessHet=3.0102999210357666, MLEAC=[2], MLEAF=[0.25], RAW_MQ=0.0} GT=[[09C81377 C*/C* GQ 6 DP 5 PL 0,6,90 {MIN_DP=4}],[09C83237 C*/C* GQ 3 DP 6 PL 0,3,45 {MIN_DP=6}],[09C97255 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[09C98651 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[09C99383 T/T GQ 3 PL 45,3,0 {PGT=0|1, PID=143249097_C_T, SB=[0, 0, 0, 0]}],[09C99677 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[10C100868 C*/C* GQ 3 DP 7 PL 0,3,45 {MIN_DP=4}],[10C101312 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[10C102545 ./. DP 1 PL 0,0,0 {MIN_DP=0}],[10C102782 ./. DP 0 PL 0,0,0 {MIN_DP=0}]]. [11:21] ; ```. I think this might be a bug, or at least a case of over-aggressive error checking. We have an extra check in our version of `RMSMappingQuality` that is not present in the GATK3 version:. ```; if (numOfReads <= 0){; throw new UserException.BadInput(""Cannot calculate Root Mean Square Mapping Quality if there are 0 or less reads."" +; ""\nNumber of reads recorded as :"" +numOfReads +; ""\nIn VariantContext: ""+ vc.toStringDecodeGenotypes());; }; ```. We should match the GATK3 behavior in this case, unless we can **prove** (and not merely infer) that GATK3 also explodes. @lbergelson Since you are git blamed here, can you give your thoughts on this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2658
https://github.com/broadinstitute/gatk/issues/2658:85,Availability,error,error,85,"@kcibul reports getting the following error while running `GenotypeGVCFs`:. ```; New error coming out of GenotypeGVCFs when running 1000 shards:. [11:20] ; ***********************************************************************. A USER ERROR has occurred: Bad input: Cannot calculate Root Mean Square Mapping Quality if there are 0 or less reads.; Number of reads recorded as :0; In VariantContext: [VC UG_call @ chr1:143249103 Q38.86 of type=SNP alleles=[C*, T] attr={DP=14, ExcessHet=3.0102999210357666, MLEAC=[2], MLEAF=[0.25], RAW_MQ=0.0} GT=[[09C81377 C*/C* GQ 6 DP 5 PL 0,6,90 {MIN_DP=4}],[09C83237 C*/C* GQ 3 DP 6 PL 0,3,45 {MIN_DP=6}],[09C97255 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[09C98651 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[09C99383 T/T GQ 3 PL 45,3,0 {PGT=0|1, PID=143249097_C_T, SB=[0, 0, 0, 0]}],[09C99677 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[10C100868 C*/C* GQ 3 DP 7 PL 0,3,45 {MIN_DP=4}],[10C101312 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[10C102545 ./. DP 1 PL 0,0,0 {MIN_DP=0}],[10C102782 ./. DP 0 PL 0,0,0 {MIN_DP=0}]]. [11:21] ; ```. I think this might be a bug, or at least a case of over-aggressive error checking. We have an extra check in our version of `RMSMappingQuality` that is not present in the GATK3 version:. ```; if (numOfReads <= 0){; throw new UserException.BadInput(""Cannot calculate Root Mean Square Mapping Quality if there are 0 or less reads."" +; ""\nNumber of reads recorded as :"" +numOfReads +; ""\nIn VariantContext: ""+ vc.toStringDecodeGenotypes());; }; ```. We should match the GATK3 behavior in this case, unless we can **prove** (and not merely infer) that GATK3 also explodes. @lbergelson Since you are git blamed here, can you give your thoughts on this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2658
https://github.com/broadinstitute/gatk/issues/2658:236,Availability,ERROR,ERROR,236,"@kcibul reports getting the following error while running `GenotypeGVCFs`:. ```; New error coming out of GenotypeGVCFs when running 1000 shards:. [11:20] ; ***********************************************************************. A USER ERROR has occurred: Bad input: Cannot calculate Root Mean Square Mapping Quality if there are 0 or less reads.; Number of reads recorded as :0; In VariantContext: [VC UG_call @ chr1:143249103 Q38.86 of type=SNP alleles=[C*, T] attr={DP=14, ExcessHet=3.0102999210357666, MLEAC=[2], MLEAF=[0.25], RAW_MQ=0.0} GT=[[09C81377 C*/C* GQ 6 DP 5 PL 0,6,90 {MIN_DP=4}],[09C83237 C*/C* GQ 3 DP 6 PL 0,3,45 {MIN_DP=6}],[09C97255 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[09C98651 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[09C99383 T/T GQ 3 PL 45,3,0 {PGT=0|1, PID=143249097_C_T, SB=[0, 0, 0, 0]}],[09C99677 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[10C100868 C*/C* GQ 3 DP 7 PL 0,3,45 {MIN_DP=4}],[10C101312 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[10C102545 ./. DP 1 PL 0,0,0 {MIN_DP=0}],[10C102782 ./. DP 0 PL 0,0,0 {MIN_DP=0}]]. [11:21] ; ```. I think this might be a bug, or at least a case of over-aggressive error checking. We have an extra check in our version of `RMSMappingQuality` that is not present in the GATK3 version:. ```; if (numOfReads <= 0){; throw new UserException.BadInput(""Cannot calculate Root Mean Square Mapping Quality if there are 0 or less reads."" +; ""\nNumber of reads recorded as :"" +numOfReads +; ""\nIn VariantContext: ""+ vc.toStringDecodeGenotypes());; }; ```. We should match the GATK3 behavior in this case, unless we can **prove** (and not merely infer) that GATK3 also explodes. @lbergelson Since you are git blamed here, can you give your thoughts on this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2658
https://github.com/broadinstitute/gatk/issues/2658:1095,Availability,error,error,1095,"@kcibul reports getting the following error while running `GenotypeGVCFs`:. ```; New error coming out of GenotypeGVCFs when running 1000 shards:. [11:20] ; ***********************************************************************. A USER ERROR has occurred: Bad input: Cannot calculate Root Mean Square Mapping Quality if there are 0 or less reads.; Number of reads recorded as :0; In VariantContext: [VC UG_call @ chr1:143249103 Q38.86 of type=SNP alleles=[C*, T] attr={DP=14, ExcessHet=3.0102999210357666, MLEAC=[2], MLEAF=[0.25], RAW_MQ=0.0} GT=[[09C81377 C*/C* GQ 6 DP 5 PL 0,6,90 {MIN_DP=4}],[09C83237 C*/C* GQ 3 DP 6 PL 0,3,45 {MIN_DP=6}],[09C97255 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[09C98651 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[09C99383 T/T GQ 3 PL 45,3,0 {PGT=0|1, PID=143249097_C_T, SB=[0, 0, 0, 0]}],[09C99677 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[10C100868 C*/C* GQ 3 DP 7 PL 0,3,45 {MIN_DP=4}],[10C101312 ./. DP 0 PL 0,0,0 {MIN_DP=0}],[10C102545 ./. DP 1 PL 0,0,0 {MIN_DP=0}],[10C102782 ./. DP 0 PL 0,0,0 {MIN_DP=0}]]. [11:21] ; ```. I think this might be a bug, or at least a case of over-aggressive error checking. We have an extra check in our version of `RMSMappingQuality` that is not present in the GATK3 version:. ```; if (numOfReads <= 0){; throw new UserException.BadInput(""Cannot calculate Root Mean Square Mapping Quality if there are 0 or less reads."" +; ""\nNumber of reads recorded as :"" +numOfReads +; ""\nIn VariantContext: ""+ vc.toStringDecodeGenotypes());; }; ```. We should match the GATK3 behavior in this case, unless we can **prove** (and not merely infer) that GATK3 also explodes. @lbergelson Since you are git blamed here, can you give your thoughts on this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2658
https://github.com/broadinstitute/gatk/pull/2659:194,Testability,test,test,194,"Finish porting https://github.com/broadinstitute/gsa-unstable/pull/1440.; * Changed the last band from [lastValue, Integer.MAX_VALUE) to [lastValue, MAX_GENOTYPE_QUAL) .; * Changed the expected test data and compare against the output partition (value) rather than the input (key).; * Changed GVCFBlock in a test gvcf file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2659
https://github.com/broadinstitute/gatk/pull/2659:308,Testability,test,test,308,"Finish porting https://github.com/broadinstitute/gsa-unstable/pull/1440.; * Changed the last band from [lastValue, Integer.MAX_VALUE) to [lastValue, MAX_GENOTYPE_QUAL) .; * Changed the expected test data and compare against the output partition (value) rather than the input (key).; * Changed GVCFBlock in a test gvcf file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2659
https://github.com/broadinstitute/gatk/issues/2660:239,Deployability,update,update,239,"I saw this message recently when running gcloud:. ```; WARNING: `gcloud auth login` no longer writes application default credentials.; If you need to use ADC, see:; gcloud auth application-default --help; ```. It looks like we may have to update our travis script / our instructions for running on cloud files at some point in the near future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2660
https://github.com/broadinstitute/gatk/issues/2660:11,Integrability,message,message,11,"I saw this message recently when running gcloud:. ```; WARNING: `gcloud auth login` no longer writes application default credentials.; If you need to use ADC, see:; gcloud auth application-default --help; ```. It looks like we may have to update our travis script / our instructions for running on cloud files at some point in the near future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2660
https://github.com/broadinstitute/gatk/issues/2660:77,Testability,log,login,77,"I saw this message recently when running gcloud:. ```; WARNING: `gcloud auth login` no longer writes application default credentials.; If you need to use ADC, see:; gcloud auth application-default --help; ```. It looks like we may have to update our travis script / our instructions for running on cloud files at some point in the near future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2660
https://github.com/broadinstitute/gatk/pull/2662:220,Availability,mask,masking,220,"@tedsharpe please review. - SVKmerizer takes in an integer specifying the spacing between between kmers. This is is an effective way to reduce the kmer set size without affecting sensitivity much.; - SVKmerShort - added masking function that returns a copy of the current kmer after deleting bases at the specified positions; - Reworded some error messages about kmer length; - Moved and added some hashing functions to SVUtils, which will be used in another PR for the long-typed set classes and de-duplication filter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2662
https://github.com/broadinstitute/gatk/pull/2662:342,Availability,error,error,342,"@tedsharpe please review. - SVKmerizer takes in an integer specifying the spacing between between kmers. This is is an effective way to reduce the kmer set size without affecting sensitivity much.; - SVKmerShort - added masking function that returns a copy of the current kmer after deleting bases at the specified positions; - Reworded some error messages about kmer length; - Moved and added some hashing functions to SVUtils, which will be used in another PR for the long-typed set classes and de-duplication filter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2662
https://github.com/broadinstitute/gatk/pull/2662:136,Energy Efficiency,reduce,reduce,136,"@tedsharpe please review. - SVKmerizer takes in an integer specifying the spacing between between kmers. This is is an effective way to reduce the kmer set size without affecting sensitivity much.; - SVKmerShort - added masking function that returns a copy of the current kmer after deleting bases at the specified positions; - Reworded some error messages about kmer length; - Moved and added some hashing functions to SVUtils, which will be used in another PR for the long-typed set classes and de-duplication filter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2662
https://github.com/broadinstitute/gatk/pull/2662:348,Integrability,message,messages,348,"@tedsharpe please review. - SVKmerizer takes in an integer specifying the spacing between between kmers. This is is an effective way to reduce the kmer set size without affecting sensitivity much.; - SVKmerShort - added masking function that returns a copy of the current kmer after deleting bases at the specified positions; - Reworded some error messages about kmer length; - Moved and added some hashing functions to SVUtils, which will be used in another PR for the long-typed set classes and de-duplication filter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2662
https://github.com/broadinstitute/gatk/pull/2662:399,Security,hash,hashing,399,"@tedsharpe please review. - SVKmerizer takes in an integer specifying the spacing between between kmers. This is is an effective way to reduce the kmer set size without affecting sensitivity much.; - SVKmerShort - added masking function that returns a copy of the current kmer after deleting bases at the specified positions; - Reworded some error messages about kmer length; - Moved and added some hashing functions to SVUtils, which will be used in another PR for the long-typed set classes and de-duplication filter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2662
https://github.com/broadinstitute/gatk/pull/2665:779,Modifiability,variab,variables,779,"@tedsharpe @cwhelan please review. - Adds MarkDuplicatesReadFilter (to replace MarkedOpticalDuplicateReadFilter). MarkedOpticalDuplicateReadFilter will be removed in a subsequent PR because the Filter tool currently uses it.; - Changed some types (short to int, float to double) in the DUST algorithm; - Adds HostAlignmentReadFilter for filtering sufficiently well-mapped host reads. The helper function is there to run the test on supplementary alignments. I chose not to expose this as a GATK filter because the definitions of coverage and identity used here could be different than what some users would expect. @lbergelson Addressed your comments from the other branch:; - Added docstring to AmbiguousBaseReadFilter argument; - Made filterOpticalOnly an argument; - Argument variables changed from uppercase to lowercase; - See above regarding the duplicates filters",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2665
https://github.com/broadinstitute/gatk/pull/2665:473,Security,expose,expose,473,"@tedsharpe @cwhelan please review. - Adds MarkDuplicatesReadFilter (to replace MarkedOpticalDuplicateReadFilter). MarkedOpticalDuplicateReadFilter will be removed in a subsequent PR because the Filter tool currently uses it.; - Changed some types (short to int, float to double) in the DUST algorithm; - Adds HostAlignmentReadFilter for filtering sufficiently well-mapped host reads. The helper function is there to run the test on supplementary alignments. I chose not to expose this as a GATK filter because the definitions of coverage and identity used here could be different than what some users would expect. @lbergelson Addressed your comments from the other branch:; - Added docstring to AmbiguousBaseReadFilter argument; - Made filterOpticalOnly an argument; - Argument variables changed from uppercase to lowercase; - See above regarding the duplicates filters",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2665
https://github.com/broadinstitute/gatk/pull/2665:424,Testability,test,test,424,"@tedsharpe @cwhelan please review. - Adds MarkDuplicatesReadFilter (to replace MarkedOpticalDuplicateReadFilter). MarkedOpticalDuplicateReadFilter will be removed in a subsequent PR because the Filter tool currently uses it.; - Changed some types (short to int, float to double) in the DUST algorithm; - Adds HostAlignmentReadFilter for filtering sufficiently well-mapped host reads. The helper function is there to run the test on supplementary alignments. I chose not to expose this as a GATK filter because the definitions of coverage and identity used here could be different than what some users would expect. @lbergelson Addressed your comments from the other branch:; - Added docstring to AmbiguousBaseReadFilter argument; - Made filterOpticalOnly an argument; - Argument variables changed from uppercase to lowercase; - See above regarding the duplicates filters",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2665
https://github.com/broadinstitute/gatk/issues/2666:509,Availability,error,error,509,"Hi, ; I am experimenting with submitting a PrintReadsSpark job to a yarn spark cluster in AWS. I run the job with the following command. ```; spark-submit --deploy-mode cluster --class org.broadinstitute.hellbender.Main --deploy-mode cluster --master yarn gatk-package-4.alpha.2-248-gcd449bf-SNAPSHOT-spark.jar PrintReadsSpark -I hdfs://chr1.bam -O hdfs://output.bam; ```. I can see from the output files that the job finished successfully, however the cluster tells me that it failed. It shows the following error message:. ```; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); ```. I believe this may be due to the `System.exit(0)` statement at line 144 in hellbender.Main, though I am not sure. . Here is a more complete snippet from the stderr log. . ```; 17/05/05 06:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1; 17/05/05 06:03:52 WARN DFSClient: Caught exception ; java.lang.InterruptedException; 	at java.lang.Object.wait(Native Method); 	at java.lang.Thread.join(Thread.java:1249); 	at java.lang.Thread.join(Thread.java:1323); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546); 17/05/05 06:03:52 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000001_0' to hdfs://ip-172-30-0-86.ec2.internal:8020/output.bam.parts/_temporary/0/task_20170505060336_0011_r_000001; 17/05/05 06:03:52 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1921 bytes result sent to driver; 17/05/05 06:03:52 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 10260 ms on localhost (executor driver) (1/4); 17/05/05 06:03:53 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000000_0' to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666
https://github.com/broadinstitute/gatk/issues/2666:157,Deployability,deploy,deploy-mode,157,"Hi, ; I am experimenting with submitting a PrintReadsSpark job to a yarn spark cluster in AWS. I run the job with the following command. ```; spark-submit --deploy-mode cluster --class org.broadinstitute.hellbender.Main --deploy-mode cluster --master yarn gatk-package-4.alpha.2-248-gcd449bf-SNAPSHOT-spark.jar PrintReadsSpark -I hdfs://chr1.bam -O hdfs://output.bam; ```. I can see from the output files that the job finished successfully, however the cluster tells me that it failed. It shows the following error message:. ```; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); ```. I believe this may be due to the `System.exit(0)` statement at line 144 in hellbender.Main, though I am not sure. . Here is a more complete snippet from the stderr log. . ```; 17/05/05 06:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1; 17/05/05 06:03:52 WARN DFSClient: Caught exception ; java.lang.InterruptedException; 	at java.lang.Object.wait(Native Method); 	at java.lang.Thread.join(Thread.java:1249); 	at java.lang.Thread.join(Thread.java:1323); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546); 17/05/05 06:03:52 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000001_0' to hdfs://ip-172-30-0-86.ec2.internal:8020/output.bam.parts/_temporary/0/task_20170505060336_0011_r_000001; 17/05/05 06:03:52 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1921 bytes result sent to driver; 17/05/05 06:03:52 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 10260 ms on localhost (executor driver) (1/4); 17/05/05 06:03:53 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000000_0' to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666
https://github.com/broadinstitute/gatk/issues/2666:222,Deployability,deploy,deploy-mode,222,"Hi, ; I am experimenting with submitting a PrintReadsSpark job to a yarn spark cluster in AWS. I run the job with the following command. ```; spark-submit --deploy-mode cluster --class org.broadinstitute.hellbender.Main --deploy-mode cluster --master yarn gatk-package-4.alpha.2-248-gcd449bf-SNAPSHOT-spark.jar PrintReadsSpark -I hdfs://chr1.bam -O hdfs://output.bam; ```. I can see from the output files that the job finished successfully, however the cluster tells me that it failed. It shows the following error message:. ```; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); ```. I believe this may be due to the `System.exit(0)` statement at line 144 in hellbender.Main, though I am not sure. . Here is a more complete snippet from the stderr log. . ```; 17/05/05 06:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1; 17/05/05 06:03:52 WARN DFSClient: Caught exception ; java.lang.InterruptedException; 	at java.lang.Object.wait(Native Method); 	at java.lang.Thread.join(Thread.java:1249); 	at java.lang.Thread.join(Thread.java:1323); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546); 17/05/05 06:03:52 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000001_0' to hdfs://ip-172-30-0-86.ec2.internal:8020/output.bam.parts/_temporary/0/task_20170505060336_0011_r_000001; 17/05/05 06:03:52 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1921 bytes result sent to driver; 17/05/05 06:03:52 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 10260 ms on localhost (executor driver) (1/4); 17/05/05 06:03:53 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000000_0' to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666
https://github.com/broadinstitute/gatk/issues/2666:4216,Deployability,pipeline,pipelines,4216,"or driver) (4/4); 17/05/05 06:03:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 17/05/05 06:03:53 INFO DAGScheduler: ResultStage 2 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) finished in 10.796 s; 17/05/05 06:03:53 INFO DAGScheduler: Job 1 finished: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 17.010114 s; 17/05/05 06:03:53 INFO SparkUI: Stopped Spark web UI at http://172.30.0.122:35794; 17/05/05 06:03:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/05/05 06:03:53 INFO MemoryStore: MemoryStore cleared; 17/05/05 06:03:53 INFO BlockManager: BlockManager stopped; 17/05/05 06:03:53 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/05/05 06:03:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/05/05 06:03:53 INFO SparkContext: Successfully stopped SparkContext; [May 5, 2017 6:03:53 AM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=799080448; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); 17/05/05 06:03:53 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: Shutdown hook called before final status was reported.); 17/05/05 06:03:53 INFO ApplicationMaster: Deleting staging directory hdfs://ip-172-30-0-86.ec2.internal:8020/user/hadoop/.sparkStaging/application_1493961816416_0002; 17/05/05 06:03:53 INFO ShutdownHookManager: Shutdown hook called; 17/05/05 06:03:53 INFO ShutdownHookManager: Deleting directory /mnt1/yarn/usercache/hadoop/appcache/application_1493961816416_0002/spark-23bc1c26-8133-475d-8418-1def58e976fd; 17/05/05 06:03:53 INFO ShutdownHookManager: Deleting directory /mnt/yarn/usercache/hadoop/appcache/application_1493961816416_0002/spark-edb79d30-a7d3-4ed2-ad61-2dc8ea95c5b1; ```. Thanks in helping ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666
https://github.com/broadinstitute/gatk/issues/2666:515,Integrability,message,message,515,"Hi, ; I am experimenting with submitting a PrintReadsSpark job to a yarn spark cluster in AWS. I run the job with the following command. ```; spark-submit --deploy-mode cluster --class org.broadinstitute.hellbender.Main --deploy-mode cluster --master yarn gatk-package-4.alpha.2-248-gcd449bf-SNAPSHOT-spark.jar PrintReadsSpark -I hdfs://chr1.bam -O hdfs://output.bam; ```. I can see from the output files that the job finished successfully, however the cluster tells me that it failed. It shows the following error message:. ```; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); ```. I believe this may be due to the `System.exit(0)` statement at line 144 in hellbender.Main, though I am not sure. . Here is a more complete snippet from the stderr log. . ```; 17/05/05 06:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1; 17/05/05 06:03:52 WARN DFSClient: Caught exception ; java.lang.InterruptedException; 	at java.lang.Object.wait(Native Method); 	at java.lang.Thread.join(Thread.java:1249); 	at java.lang.Thread.join(Thread.java:1323); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546); 17/05/05 06:03:52 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000001_0' to hdfs://ip-172-30-0-86.ec2.internal:8020/output.bam.parts/_temporary/0/task_20170505060336_0011_r_000001; 17/05/05 06:03:52 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1921 bytes result sent to driver; 17/05/05 06:03:52 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 10260 ms on localhost (executor driver) (1/4); 17/05/05 06:03:53 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000000_0' to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666
https://github.com/broadinstitute/gatk/issues/2666:4550,Integrability,message,message,4550,"/05/05 06:03:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 17/05/05 06:03:53 INFO DAGScheduler: ResultStage 2 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) finished in 10.796 s; 17/05/05 06:03:53 INFO DAGScheduler: Job 1 finished: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 17.010114 s; 17/05/05 06:03:53 INFO SparkUI: Stopped Spark web UI at http://172.30.0.122:35794; 17/05/05 06:03:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/05/05 06:03:53 INFO MemoryStore: MemoryStore cleared; 17/05/05 06:03:53 INFO BlockManager: BlockManager stopped; 17/05/05 06:03:53 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/05/05 06:03:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/05/05 06:03:53 INFO SparkContext: Successfully stopped SparkContext; [May 5, 2017 6:03:53 AM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=799080448; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); 17/05/05 06:03:53 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: Shutdown hook called before final status was reported.); 17/05/05 06:03:53 INFO ApplicationMaster: Deleting staging directory hdfs://ip-172-30-0-86.ec2.internal:8020/user/hadoop/.sparkStaging/application_1493961816416_0002; 17/05/05 06:03:53 INFO ShutdownHookManager: Shutdown hook called; 17/05/05 06:03:53 INFO ShutdownHookManager: Deleting directory /mnt1/yarn/usercache/hadoop/appcache/application_1493961816416_0002/spark-23bc1c26-8133-475d-8418-1def58e976fd; 17/05/05 06:03:53 INFO ShutdownHookManager: Deleting directory /mnt/yarn/usercache/hadoop/appcache/application_1493961816416_0002/spark-edb79d30-a7d3-4ed2-ad61-2dc8ea95c5b1; ```. Thanks in helping me solve this issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666
https://github.com/broadinstitute/gatk/issues/2666:847,Testability,log,log,847,"Hi, ; I am experimenting with submitting a PrintReadsSpark job to a yarn spark cluster in AWS. I run the job with the following command. ```; spark-submit --deploy-mode cluster --class org.broadinstitute.hellbender.Main --deploy-mode cluster --master yarn gatk-package-4.alpha.2-248-gcd449bf-SNAPSHOT-spark.jar PrintReadsSpark -I hdfs://chr1.bam -O hdfs://output.bam; ```. I can see from the output files that the job finished successfully, however the cluster tells me that it failed. It shows the following error message:. ```; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); ```. I believe this may be due to the `System.exit(0)` statement at line 144 in hellbender.Main, though I am not sure. . Here is a more complete snippet from the stderr log. . ```; 17/05/05 06:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1; 17/05/05 06:03:52 WARN DFSClient: Caught exception ; java.lang.InterruptedException; 	at java.lang.Object.wait(Native Method); 	at java.lang.Thread.join(Thread.java:1249); 	at java.lang.Thread.join(Thread.java:1323); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546); 17/05/05 06:03:52 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000001_0' to hdfs://ip-172-30-0-86.ec2.internal:8020/output.bam.parts/_temporary/0/task_20170505060336_0011_r_000001; 17/05/05 06:03:52 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1921 bytes result sent to driver; 17/05/05 06:03:52 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 10260 ms on localhost (executor driver) (1/4); 17/05/05 06:03:53 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000000_0' to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666
https://github.com/broadinstitute/gatk/issues/2666:3820,Usability,clear,cleared,3820,"ec2.internal:8020/output.bam.parts/_temporary/0/task_20170505060336_0011_r_000002; 17/05/05 06:03:53 INFO Executor: Finished task 2.0 in stage 2.0 (TID 4). 1921 bytes result sent to driver; 17/05/05 06:03:53 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 10792 ms on localhost (executor driver) (4/4); 17/05/05 06:03:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 17/05/05 06:03:53 INFO DAGScheduler: ResultStage 2 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) finished in 10.796 s; 17/05/05 06:03:53 INFO DAGScheduler: Job 1 finished: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 17.010114 s; 17/05/05 06:03:53 INFO SparkUI: Stopped Spark web UI at http://172.30.0.122:35794; 17/05/05 06:03:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/05/05 06:03:53 INFO MemoryStore: MemoryStore cleared; 17/05/05 06:03:53 INFO BlockManager: BlockManager stopped; 17/05/05 06:03:53 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/05/05 06:03:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/05/05 06:03:53 INFO SparkContext: Successfully stopped SparkContext; [May 5, 2017 6:03:53 AM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=799080448; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); 17/05/05 06:03:53 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: Shutdown hook called before final status was reported.); 17/05/05 06:03:53 INFO ApplicationMaster: Deleting staging directory hdfs://ip-172-30-0-86.ec2.internal:8020/user/hadoop/.sparkStaging/application_1493961816416_0002; 17/05/05 06:03:53 INFO ShutdownHookManager: Shutdown hook called; 17/05/05 06:03:53 INFO ShutdownHookManager: Deleting directory /mnt1/yarn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666
https://github.com/broadinstitute/gatk/pull/2669:13,Availability,error,error,13,Removing the error check in RMSMappingQuality.getNumReads which was causing a crash when depth was being reported as 0 or less.; It will now return -1 which should match gatk3 behavior.; Fixes #2658,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2669
https://github.com/broadinstitute/gatk/issues/2674:2469,Testability,test,test,2469,"CTTTCTTTCTTTCTTTCTT,CTCTTTCTTTCTTTCTT,CTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTT,TTCTTTCTTTCTTTCTTTCTT,CTTCTTTCTTTCTTTCTTTCTTTCTT,CTTTCTTTCTCTTTCTTTCTTTCTTTCTTTCTT,CTTTCTTTCTTTCTTTCTTTCTTTCTTTCTT,CTTTCTCTTTCTTTCTTTCTTTCTTTCTTTCTT,*,CTTTCTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTT,CTCTT,CTTTCTTTCTCTTTCTTTCTTTCTTTCTTTCTTTCTT,CTTCTTTCTTTCTTTCTT,CTTCTTTCTTTCTTTCTTTCTT,CTTTCTTTCTTTCTTTCTCTTTCTTTCTTTCTTTCTTTCTT,CTTTCTTTCTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTT,CTTTCTTTCTTTCTCTTTCTTTCTTTCTTTCTTTCTT,CTCTTTCTTTCTTTCTTTCCTTCTTTCTTTCTTTCTTTCTT,CTTCTTTCTTTCTTTCTTTCTTTCTTTCTT,CTTTCTTCTTTCTTTCTTTCTTTCTT	174463.40	.	AC=187,86,161,58,27,33,4,150,57,56,18,99,1,14,2,20,87,7,2,8,1,1,1,3,9,2,1,1;AF=0.042,0.019,0.036,0.013,6.054e-03,7.399e-03,8.969e-04,0.034,0.013,0.013,4.036e-03,0.022,2.242e-04,3.139e-03,4.484e-04,4.484e-03,0.020,1.570e-03,4.484e-04,1.794e-03,2.242e-04,2.242e-04,2.242e-04,6.726e-04,2.018e-03,4.484e-04,2.242e-04,2.242e-04;AN=4460;BaseQRankSum=0.358;ClippingRankSum=0.198;DP=46213;ExcessHet=0.0000;FS=6.806;InbreedingCoeff=0.2453;MLEAC=60,92,186,69,34,35,4,159,68,72,16,119,1,16,3,25,119,8,3,10,1,1,1,4,9,3,1,1;MLEAF=0.013,0.021,0.042,0.015,7.623e-03,7.848e-03,8.969e-04,0.036,0.015,0.016,3.587e-03,0.027,2.242e-04,3.587e-03,6.726e-04,5.605e-03,0.027,1.794e-03,6.726e-04,2.242e-03,2.242e-04,2.242e-04,2.242e-04,8.969e-04,2.018e-03,6.726e-04,2.242e-04,2.242e-04;MQ=67.44;MQRankSum=-6.960e-01;QD=26.22;ReadPosRankSum=0.804;SOR=1.593; ```. GATK4 output:; ```; chr5	43198207	.	CTCTTTCTTTCTTTCTTTCTT	CTCTTTCTTTCTT,CTCTTTCTT,CTTTCTTTCTTTCTTTCTTTCTT,CTCTTTCTTTCTTTCTTTCTTTCTT,TTCTTTCTTTCTTTCTTTCTT,*	148098.81	.	AC=341,97,161,163,100,87;AF=0.077,0.022,0.036,0.037,0.022,0.020;AN=4450;BaseQRankSum=0.358;ClippingRankSum=0.198;DP=46213;ExcessHet=0.0000;FS=6.806;InbreedingCoeff=0.2868;MLEAC=86,125,234,200,160,174;MLEAF=0.019,0.028,0.053,0.045,0.036,0.039;MQ=68.92;MQRankSum=-6.960e-01;QD=31.22;ReadPosRankSum=0.804;SOR=1.602; ```. I'm happy to provide more test data if needed and I have the input that generated this site.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2674
https://github.com/broadinstitute/gatk/pull/2675:189,Performance,load,loading,189,Adding an advanced option --sampleNameMap to GenomicsDBImport which can be specified instead of -V; * This allows providing a file mapping samples to their vcf uri which is used instead of loading each header and checking the sample names.; * The first file in the mapping is chosen to act as the header template. If the headers are not consistent there may be data corruption. A mapping file is a tab seperated file with no header in the format. Sample1	file1; Sample2 file2; Sample3 file3; ...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2675
https://github.com/broadinstitute/gatk/issues/2676:0,Availability,Error,Error,0,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2676:13,Availability,error,error,13,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2676:1124,Availability,error,error,1124,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2676:51,Deployability,install,installation,51,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2676:465,Deployability,update,update,465,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2676:116,Security,Secur,SecurityException,116,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2676:203,Security,secur,security,203,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2676:292,Security,secur,security,292,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2676:814,Security,secur,security,814,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2676:823,Security,Access,AccessController,823,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676
https://github.com/broadinstitute/gatk/issues/2683:121,Energy Efficiency,meter,meter,121,"This tool always emits ""Processed 0 total records"" at traversal end. We just need to hook it up to the standard progress meter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2683
https://github.com/broadinstitute/gatk/pull/2684:294,Availability,heartbeat,heartbeats,294,"…bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/pull/2684:1892,Availability,down,down,1892," intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 48.71 minutes.; Runtime.totalMemory()=17815830528; ```. This branch with `minCoherentEvidence` set to 7 (and `minEvidenceCount` remaining at 15):. ```; 18:55:30.222 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 18:56:15.451 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 30678 intervals.; 18:56:15.547 INFO StructuralVariationDiscoveryPipelineSpark - Killed 387 intervals that were near reference gaps.; 18:56:45.252 INFO StructuralVariationDiscoveryPipelineSpark - Killed 174 intervals that had >1000x coverage.; 18:57:27.031 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 9424481 mapped template names.; 18:58:07.742 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 19:03:21.960 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 3930956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/pull/2684:3562,Availability,down,down,3562," intervals that had >1000x coverage.; 18:57:27.031 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 9424481 mapped template names.; 18:58:07.742 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 19:03:21.960 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 39309565 kmers.; 19:08:31.990 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34034345 unique template names for assembly.; 19:40:40.149 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 19:41:00.570 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 6239 variants.; 19:41:00.582 INFO StructuralVariationDiscoveryPipelineSpark - INV: 238; 19:41:00.582 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3639; 19:41:00.582 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1119; 19:41:00.582 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1243; 19:41:00.767 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 7:41:00 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 46.37 minutes.; ```. This branch with `minCoherentEvidenceCount` set to 4:. ```; 19:48:13.982 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 19:48:57.930 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 52918 intervals.; 19:48:58.053 INFO StructuralVariationDiscoveryPipelineSpark - Killed 417 intervals that were near reference gaps.; 19:49:27.873 INFO StructuralVariationDiscoveryPipelineSpark - Killed 177 intervals that had >1000x coverage.; 19:50:09.183 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 12444440 mapped template names.; 19:50:59.411 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 19:57:19.267 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 62077256 kmers.; 20:03:43.231 INFO StructuralVariationDiscoveryPipelineSpark ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/pull/2684:5162,Availability,down,down,5162,"intervals that had >1000x coverage.; 19:50:09.183 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 12444440 mapped template names.; 19:50:59.411 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 19:57:19.267 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 62077256 kmers.; 20:03:43.231 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 42811284 unique template names for assembly.; 21:11:03.427 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 21:11:24.841 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 6799 variants.; 21:11:24.854 INFO StructuralVariationDiscoveryPipelineSpark - INV: 253; 21:11:24.855 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 4002; 21:11:24.855 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1179; 21:11:24.855 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1365; 21:11:25.045 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 9:11:25 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 84.05 minutes.; Runtime.totalMemory()=15439757312; ```. Master with `minEvidenceCount` set to 7:. ```; 15:51:52.187 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 15:52:34.778 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 185595 intervals.; 15:52:34.923 INFO StructuralVariationDiscoveryPipelineSpark - Killed 434 intervals that were near reference gaps.; 15:53:05.883 INFO StructuralVariationDiscoveryPipelineSpark - Killed 188 intervals that had >1000x coverage.; 15:54:13.400 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 29691982 mapped template names.; 15:54:45.921 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 16:05:36.053 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 191872350 kmers.; ```; Which I killed after 108 minute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/pull/2684:256,Deployability,pipeline,pipeline,256,"…bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/pull/2684:105,Modifiability,refactor,refactoring,105,"…bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/pull/2684:402,Performance,perform,performance,402,"…bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/pull/2684:6701,Performance,tune,tuned,6701,"ralVariationDiscoveryPipelineSpark - INS: 1365; 21:11:25.045 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 9:11:25 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 84.05 minutes.; Runtime.totalMemory()=15439757312; ```. Master with `minEvidenceCount` set to 7:. ```; 15:51:52.187 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 15:52:34.778 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 185595 intervals.; 15:52:34.923 INFO StructuralVariationDiscoveryPipelineSpark - Killed 434 intervals that were near reference gaps.; 15:53:05.883 INFO StructuralVariationDiscoveryPipelineSpark - Killed 188 intervals that had >1000x coverage.; 15:54:13.400 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 29691982 mapped template names.; 15:54:45.921 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 16:05:36.053 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 191872350 kmers.; ```; Which I killed after 108 minutes since it seemed to be stalled. When examining deletion calls from the run with `minCoherentEvidenceCount` set to 7:. There are 294 new deletion calls that don't have 50% reciprocal overlap with a call made by the master branch. Of those, 143 have 50% reciprocal overlap with a call made in the haploid assembly of CHM1, and 121 overlap with a PacBio call for CHM13. Manual inspection also suggests that most of these calls match a call in the truth set, most of them heterozygous. The value to set for `minCoherentEvidence` should eventually be tuned, adjusted for coverage, etc, in our work to make a better evidence model, but for now this seems like a good way to add a number of good-quality het calls to our output without sacrificing too much performance (I got a faster runtime with this branch but that must be due to run-to-run-variance). @tedsharpe @SHuang-Broad please review.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/pull/2684:6905,Performance,perform,performance,6905,"ralVariationDiscoveryPipelineSpark - INS: 1365; 21:11:25.045 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 9:11:25 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 84.05 minutes.; Runtime.totalMemory()=15439757312; ```. Master with `minEvidenceCount` set to 7:. ```; 15:51:52.187 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 15:52:34.778 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 185595 intervals.; 15:52:34.923 INFO StructuralVariationDiscoveryPipelineSpark - Killed 434 intervals that were near reference gaps.; 15:53:05.883 INFO StructuralVariationDiscoveryPipelineSpark - Killed 188 intervals that had >1000x coverage.; 15:54:13.400 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 29691982 mapped template names.; 15:54:45.921 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 16:05:36.053 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 191872350 kmers.; ```; Which I killed after 108 minutes since it seemed to be stalled. When examining deletion calls from the run with `minCoherentEvidenceCount` set to 7:. There are 294 new deletion calls that don't have 50% reciprocal overlap with a call made by the master branch. Of those, 143 have 50% reciprocal overlap with a call made in the haploid assembly of CHM1, and 121 overlap with a PacBio call for CHM13. Manual inspection also suggests that most of these calls match a call in the truth set, most of them heterozygous. The value to set for `minCoherentEvidence` should eventually be tuned, adjusted for coverage, etc, in our work to make a better evidence model, but for now this seems like a good way to add a number of good-quality het calls to our output without sacrificing too much performance (I got a faster runtime with this branch but that must be due to run-to-run-variance). @tedsharpe @SHuang-Broad please review.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/pull/2684:230,Safety,timeout,timeout,230,"…bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684
https://github.com/broadinstitute/gatk/issues/2685:2700,Availability,avail,available,2700,ByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at htsjdk.samtools.seekablestream.SeekableBufferedStream.read(SeekableBufferedStream.java:100); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:539); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:493); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:451); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:441); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:322); 	at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:215); 	at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:269); 	at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:161); 	at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:125); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:84); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinst,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:7784,Availability,down,down,7784,cutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:491); 	... 12 more; Caused by: java.io.EOFException: SSL peer shut down incorrectly; 	at sun.security.ssl.InputRecord.read(InputRecord.java:505); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	... 25 more; ```; The error seems to appear after `org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 19.15 minutes.` is logged which is surprising.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:7957,Availability,error,error,7957,cutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:491); 	... 12 more; Caused by: java.io.EOFException: SSL peer shut down incorrectly; 	at sun.security.ssl.InputRecord.read(InputRecord.java:505); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	... 25 more; ```; The error seems to appear after `org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 19.15 minutes.` is logged which is surprising.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6626,Integrability,protocol,protocol,6626,el.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6705,Integrability,protocol,protocol,6705,ils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6825,Integrability,protocol,protocol,6825,bender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:491); 	... 12 more; Caused by: java.io.EOFException: SSL peer shut down incorrectly; 	at sun.security.ssl.InputRe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:1509,Performance,concurren,concurrent,1509,"al_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 2 --cloudIndexPrefetchBuffer 2 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [May 9, 2017 4:30:53 PM UTC] Executing as root@f045553cd221 on Linux 3.16.0-0.bpo.4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-8u121-b13-1~bpo8+1-b13; Version: Version:e1ffbff4-SNAPSHOT; Created workspace /cromwell_root/genomicsdb; [May 9, 2017 4:50:01 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 19.15 minutes.; Runtime.totalMemory()=9498001408; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Remote host closed connection during handshake; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at htsjdk.samtools.seekablestream.SeekableBufferedStream.read(SeekableBufferedStream.java:100); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:539); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:493); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(Block",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:4245,Performance,concurren,concurrent,4245,Reader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Remote host closed connection during handshake; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Remote host closed connection during handshake; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:4381,Performance,concurren,concurrent,4381,DBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Remote host closed connection during handshake; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Remote host closed connection during handshake; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:4446,Performance,concurren,concurrent,4446,icsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Remote host closed connection during handshake; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Remote host closed connection during handshake; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelpe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:5945,Performance,concurren,concurrent,5945,ed connection during handshake; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.htt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6007,Performance,concurren,concurrent,6007,pi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6092,Performance,concurren,concurrent,6092,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6408,Performance,perform,performInitialHandshake,6408,m.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6310,Security,secur,security,6310,haded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.Abstra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6381,Security,secur,security,6381,it(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6466,Security,secur,security,6466,etryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleCl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:6542,Security,secur,security,6542,eadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:7810,Security,secur,security,7810,cutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:491); 	... 12 more; Caused by: java.io.EOFException: SSL peer shut down incorrectly; 	at sun.security.ssl.InputRecord.read(InputRecord.java:505); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	... 25 more; ```; The error seems to appear after `org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 19.15 minutes.` is logged which is surprising.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:7871,Security,secur,security,7871,cutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:491); 	... 12 more; Caused by: java.io.EOFException: SSL peer shut down incorrectly; 	at sun.security.ssl.InputRecord.read(InputRecord.java:505); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	... 25 more; ```; The error seems to appear after `org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 19.15 minutes.` is logged which is surprising.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2685:8089,Testability,log,logged,8089,cutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:491); 	... 12 more; Caused by: java.io.EOFException: SSL peer shut down incorrectly; 	at sun.security.ssl.InputRecord.read(InputRecord.java:505); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	... 25 more; ```; The error seems to appear after `org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 19.15 minutes.` is logged which is surprising.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685
https://github.com/broadinstitute/gatk/issues/2686:44,Availability,error,error,44,"@kcibul encountered the following transient error when running GenomicsDBImport with vcfs in GCS. ```; [May 9, 2017 1:35:47 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 2.44 minutes.; Runtime.totalMemory()=2436366336; htsjdk.samtools.FileTruncatedException: Premature end of file: /temp/10C102545.5783b88e-6d16-4da7-be0e-590811cd6d2f.g.vcf.gz; at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:513); at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:451); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:441); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:234); at htsjdk.tribble.readers.TabixReader.readLine(TabixReader.java:199); at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:416); at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:45); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:178); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:216); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:156); at com.intel.genomicsdb.GenomicsDBImporterStreamWrapper.next(GenomicsDBImporterStreamWrapper.java:100); at com.intel.genomicsdb.GenomicsDBImporter.importBatch(GenomicsDBImporter.java:1313); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:361); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2686
https://github.com/broadinstitute/gatk/issues/2686:746,Availability,avail,available,746,"@kcibul encountered the following transient error when running GenomicsDBImport with vcfs in GCS. ```; [May 9, 2017 1:35:47 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 2.44 minutes.; Runtime.totalMemory()=2436366336; htsjdk.samtools.FileTruncatedException: Premature end of file: /temp/10C102545.5783b88e-6d16-4da7-be0e-590811cd6d2f.g.vcf.gz; at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:513); at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:451); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:441); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:234); at htsjdk.tribble.readers.TabixReader.readLine(TabixReader.java:199); at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:416); at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:45); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:178); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:216); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:156); at com.intel.genomicsdb.GenomicsDBImporterStreamWrapper.next(GenomicsDBImporterStreamWrapper.java:100); at com.intel.genomicsdb.GenomicsDBImporter.importBatch(GenomicsDBImporter.java:1313); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:361); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2686
https://github.com/broadinstitute/gatk/issues/2687:13,Integrability,message,messages,13,"We're seeing messages like the following when running `GenomicsDBImport`:. ```; Column 948660 has too many alleles in the combined VCF record : 61 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; Column 948710 has too many alleles in the combined VCF record : 83 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; ```. Is this limit of 50 configurable, if we wanted to raise it, and if not, could it be made configurable?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687
https://github.com/broadinstitute/gatk/issues/2687:489,Modifiability,config,configurable,489,"We're seeing messages like the following when running `GenomicsDBImport`:. ```; Column 948660 has too many alleles in the combined VCF record : 61 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; Column 948710 has too many alleles in the combined VCF record : 83 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; ```. Is this limit of 50 configurable, if we wanted to raise it, and if not, could it be made configurable?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687
https://github.com/broadinstitute/gatk/issues/2687:558,Modifiability,config,configurable,558,"We're seeing messages like the following when running `GenomicsDBImport`:. ```; Column 948660 has too many alleles in the combined VCF record : 61 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; Column 948710 has too many alleles in the combined VCF record : 83 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; ```. Is this limit of 50 configurable, if we wanted to raise it, and if not, could it be made configurable?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687
https://github.com/broadinstitute/gatk/issues/2688:25,Testability,test,test,25,"running GenotypeGVCFs on test data produced warnings such as. `Column 948660 has too many alleles in the combined VCF record : 61 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.`. It seems like the limit might need to be raised / raisable ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2688
https://github.com/broadinstitute/gatk/pull/2690:132,Deployability,Update,Updated,132,"-Tools can now customize the progress meter to use a different word than; ""records"" in its output (eg., ""reads"", ""regions"", etc.). -Updated standard walker classes to specify appropriate labels. -Hooked up GenomicsDBImport to the progress meter (it was always reporting; ""Processed 0 records"" at traversal end). Resolves #1943; Resolves #2683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2690
https://github.com/broadinstitute/gatk/pull/2690:38,Energy Efficiency,meter,meter,38,"-Tools can now customize the progress meter to use a different word than; ""records"" in its output (eg., ""reads"", ""regions"", etc.). -Updated standard walker classes to specify appropriate labels. -Hooked up GenomicsDBImport to the progress meter (it was always reporting; ""Processed 0 records"" at traversal end). Resolves #1943; Resolves #2683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2690
https://github.com/broadinstitute/gatk/pull/2690:239,Energy Efficiency,meter,meter,239,"-Tools can now customize the progress meter to use a different word than; ""records"" in its output (eg., ""reads"", ""regions"", etc.). -Updated standard walker classes to specify appropriate labels. -Hooked up GenomicsDBImport to the progress meter (it was always reporting; ""Processed 0 records"" at traversal end). Resolves #1943; Resolves #2683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2690
https://github.com/broadinstitute/gatk/issues/2691:1172,Availability,down,down,1172,"Currently we dump all htsjdk properties in `Defaults` to the logger, even though only a few are applicable to GATK:. ```; 16:30:25.790 INFO GenomicsDBImport - Defaults.BUFFER_SIZE : 131072; 16:30:25.790 INFO GenomicsDBImport - Defaults.COMPRESSION_LEVEL : 1; 16:30:25.790 INFO GenomicsDBImport - Defaults.CREATE_INDEX : false; 16:30:25.791 INFO GenomicsDBImport - Defaults.CREATE_MD5 : false; 16:30:25.791 INFO GenomicsDBImport - Defaults.CUSTOM_READER_FACTORY : ; 16:30:25.791 INFO GenomicsDBImport - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 16:30:25.791 INFO GenomicsDBImport - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:30:25.791 INFO GenomicsDBImport - Defaults.REFERENCE_FASTA : null; 16:30:25.791 INFO GenomicsDBImport - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:30:25.791 INFO GenomicsDBImport - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:30:25.792 INFO GenomicsDBImport - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:30:25.792 INFO GenomicsDBImport - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:30:25.792 INFO GenomicsDBImport - Defaults.USE_CRAM_REF_DOWNLOAD : false; ```. We should subset these down to only the relevant ones.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2691
https://github.com/broadinstitute/gatk/issues/2691:61,Testability,log,logger,61,"Currently we dump all htsjdk properties in `Defaults` to the logger, even though only a few are applicable to GATK:. ```; 16:30:25.790 INFO GenomicsDBImport - Defaults.BUFFER_SIZE : 131072; 16:30:25.790 INFO GenomicsDBImport - Defaults.COMPRESSION_LEVEL : 1; 16:30:25.790 INFO GenomicsDBImport - Defaults.CREATE_INDEX : false; 16:30:25.791 INFO GenomicsDBImport - Defaults.CREATE_MD5 : false; 16:30:25.791 INFO GenomicsDBImport - Defaults.CUSTOM_READER_FACTORY : ; 16:30:25.791 INFO GenomicsDBImport - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 16:30:25.791 INFO GenomicsDBImport - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:30:25.791 INFO GenomicsDBImport - Defaults.REFERENCE_FASTA : null; 16:30:25.791 INFO GenomicsDBImport - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:30:25.791 INFO GenomicsDBImport - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:30:25.792 INFO GenomicsDBImport - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:30:25.792 INFO GenomicsDBImport - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:30:25.792 INFO GenomicsDBImport - Defaults.USE_CRAM_REF_DOWNLOAD : false; ```. We should subset these down to only the relevant ones.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2691
https://github.com/broadinstitute/gatk/pull/2692:43,Testability,log,logger,43,"We were dumping all HTSJDK defaults to our logger, even though many settings are not; really relevant to GATK. Resolves #2691",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2692
https://github.com/broadinstitute/gatk/pull/2695:7,Testability,test,tests,7,Adding tests on openJDK since it seems like we're going to have to support it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2695
https://github.com/broadinstitute/gatk/issues/2697:215,Testability,test,test,215,"GATK4 should alert developers if a change in a pull request has broken the docker image. Functionality should be similar to TEST_DOCKER in the travis yml of gatk-protected. In other words, one of the entries in the test matrix should build the gatk4 docker image (does not have to rebuild the gatk4 base image) and then run the unit tests in the image. ``build_docker.sh`` will include the steps to build the GATK4 docker image (without rebuilding gatkbase docker image) and run unit tests inside it. This functionality is trivially copied from gatk-protected repo. This issue requires #2457 to be completed before it can be started. And this description has been written as if issue #2457 has already been completed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2697
https://github.com/broadinstitute/gatk/issues/2697:333,Testability,test,tests,333,"GATK4 should alert developers if a change in a pull request has broken the docker image. Functionality should be similar to TEST_DOCKER in the travis yml of gatk-protected. In other words, one of the entries in the test matrix should build the gatk4 docker image (does not have to rebuild the gatk4 base image) and then run the unit tests in the image. ``build_docker.sh`` will include the steps to build the GATK4 docker image (without rebuilding gatkbase docker image) and run unit tests inside it. This functionality is trivially copied from gatk-protected repo. This issue requires #2457 to be completed before it can be started. And this description has been written as if issue #2457 has already been completed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2697
https://github.com/broadinstitute/gatk/issues/2697:484,Testability,test,tests,484,"GATK4 should alert developers if a change in a pull request has broken the docker image. Functionality should be similar to TEST_DOCKER in the travis yml of gatk-protected. In other words, one of the entries in the test matrix should build the gatk4 docker image (does not have to rebuild the gatk4 base image) and then run the unit tests in the image. ``build_docker.sh`` will include the steps to build the GATK4 docker image (without rebuilding gatkbase docker image) and run unit tests inside it. This functionality is trivially copied from gatk-protected repo. This issue requires #2457 to be completed before it can be started. And this description has been written as if issue #2457 has already been completed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2697
https://github.com/broadinstitute/gatk/issues/2699:185,Deployability,install,installation,185,"Once #2457 is merged, the gatkbase docker image will be the basis for GATK4 docker images. However, gatkbase depends on no files in the GATK4 repo. However, we *might* want to move the installation of R libraries into gatkbase. Currently the custom R installation script takes a substantial portion of time in the GATK4 docker image build. The question is whether we expect the R libraries to change much. Once we answer this question, we can decide whether this issue should be addressed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2699
https://github.com/broadinstitute/gatk/issues/2699:251,Deployability,install,installation,251,"Once #2457 is merged, the gatkbase docker image will be the basis for GATK4 docker images. However, gatkbase depends on no files in the GATK4 repo. However, we *might* want to move the installation of R libraries into gatkbase. Currently the custom R installation script takes a substantial portion of time in the GATK4 docker image build. The question is whether we expect the R libraries to change much. Once we answer this question, we can decide whether this issue should be addressed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2699
https://github.com/broadinstitute/gatk/issues/2699:109,Integrability,depend,depends,109,"Once #2457 is merged, the gatkbase docker image will be the basis for GATK4 docker images. However, gatkbase depends on no files in the GATK4 repo. However, we *might* want to move the installation of R libraries into gatkbase. Currently the custom R installation script takes a substantial portion of time in the GATK4 docker image build. The question is whether we expect the R libraries to change much. Once we answer this question, we can decide whether this issue should be addressed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2699
https://github.com/broadinstitute/gatk/issues/2700:128,Availability,error,error,128,"(assumes that #2457 is already complete); Currently, the docker image creation will grab all the local files, which is prone to error. The github hash/tag parameters are simply to determine how to tag the image in docker hub. In gatk-protected, we used to actually grab the source code from github in the Dockerfile. I'm not suggesting we go back to this model. What we might want to consider:; The build_docker.sh script could download the github hash/tag to a temp dir and then build the docker image from the temp dir. This solution would be more robust and would decrease errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700
https://github.com/broadinstitute/gatk/issues/2700:428,Availability,down,download,428,"(assumes that #2457 is already complete); Currently, the docker image creation will grab all the local files, which is prone to error. The github hash/tag parameters are simply to determine how to tag the image in docker hub. In gatk-protected, we used to actually grab the source code from github in the Dockerfile. I'm not suggesting we go back to this model. What we might want to consider:; The build_docker.sh script could download the github hash/tag to a temp dir and then build the docker image from the temp dir. This solution would be more robust and would decrease errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700
https://github.com/broadinstitute/gatk/issues/2700:550,Availability,robust,robust,550,"(assumes that #2457 is already complete); Currently, the docker image creation will grab all the local files, which is prone to error. The github hash/tag parameters are simply to determine how to tag the image in docker hub. In gatk-protected, we used to actually grab the source code from github in the Dockerfile. I'm not suggesting we go back to this model. What we might want to consider:; The build_docker.sh script could download the github hash/tag to a temp dir and then build the docker image from the temp dir. This solution would be more robust and would decrease errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700
https://github.com/broadinstitute/gatk/issues/2700:576,Availability,error,errors,576,"(assumes that #2457 is already complete); Currently, the docker image creation will grab all the local files, which is prone to error. The github hash/tag parameters are simply to determine how to tag the image in docker hub. In gatk-protected, we used to actually grab the source code from github in the Dockerfile. I'm not suggesting we go back to this model. What we might want to consider:; The build_docker.sh script could download the github hash/tag to a temp dir and then build the docker image from the temp dir. This solution would be more robust and would decrease errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700
https://github.com/broadinstitute/gatk/issues/2700:146,Security,hash,hash,146,"(assumes that #2457 is already complete); Currently, the docker image creation will grab all the local files, which is prone to error. The github hash/tag parameters are simply to determine how to tag the image in docker hub. In gatk-protected, we used to actually grab the source code from github in the Dockerfile. I'm not suggesting we go back to this model. What we might want to consider:; The build_docker.sh script could download the github hash/tag to a temp dir and then build the docker image from the temp dir. This solution would be more robust and would decrease errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700
https://github.com/broadinstitute/gatk/issues/2700:448,Security,hash,hash,448,"(assumes that #2457 is already complete); Currently, the docker image creation will grab all the local files, which is prone to error. The github hash/tag parameters are simply to determine how to tag the image in docker hub. In gatk-protected, we used to actually grab the source code from github in the Dockerfile. I'm not suggesting we go back to this model. What we might want to consider:; The build_docker.sh script could download the github hash/tag to a temp dir and then build the docker image from the temp dir. This solution would be more robust and would decrease errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700
https://github.com/broadinstitute/gatk/issues/2700:170,Usability,simpl,simply,170,"(assumes that #2457 is already complete); Currently, the docker image creation will grab all the local files, which is prone to error. The github hash/tag parameters are simply to determine how to tag the image in docker hub. In gatk-protected, we used to actually grab the source code from github in the Dockerfile. I'm not suggesting we go back to this model. What we might want to consider:; The build_docker.sh script could download the github hash/tag to a temp dir and then build the docker image from the temp dir. This solution would be more robust and would decrease errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700
https://github.com/broadinstitute/gatk/pull/2701:66,Testability,test,test,66,"Solves #2698.; Three commits, dealing with different problems.; A test failed because of the repackaging but was solved by regenerating the expected file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2701
https://github.com/broadinstitute/gatk/issues/2702:107,Usability,simpl,simple,107,1st step in ; * connecting inversion breakpoints and ; * making discoveries of one type of complex event:; simple inversions with small-medium sized ins/del around breakpoints.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2702
https://github.com/broadinstitute/gatk/issues/2705:396,Performance,multi-thread,multi-thread,396,"I tried to ERR000589 process data with BwaSpark. The bam file size is 1.3G. The average time spent is about 25 min (5 nodes).; However it would only cost 5 min in processing same data if I tried to use original C bwa with 32 threads.; Base on this observation, I have several questions list as follow:; 1. If there is anything wrong with my params?; 2. For each Partition, is BwaSpark running in multi-thread mode?; 3. How to control the number of the bwa threads inside BwaSpark?. P.S. ; The running command is:; ./gatk-launch BwaSpark -I hdfs:///user/XX/ERR000589/ERR000589.bam -O hdfs:///user/XX/ERR000589/ERR000589_bwa.bam -R hdfs:///user/xx/refs/ucsc.hg19.fasta --bwamemIndexImage ~/data/ref/ucsc.hg19.img -disableSequenceDictionaryValidation true -- --sparkRunner SPARK --sparkMaster <master_url> --executor-cores 1 --total-executor-cores 16 --executor-memory 4G. I tried to further adjust the following parameters,; --executor-cores --total-executor-cores --executor-memory --driver-memory; but none of these took less time than 16 min. Besides, I alsow tried to run it in local mode, while it won't end successfully. It seems that CPU was in endless waiting. I guess it occupied so much memory that the swap space is in use? Pic 1 shows the memory consumed while running ; This time, the command is:; ./gatk-launch BwaSpark -I hdfs:///user/XX/ERR000589/ERR000589.bam -O hdfs:///user/xx/ERR000589/ERR000589.bwa.bam -R /software/home/xx/data/ref/ucsc.hg19.fasta \ --bwamemIndexImage ~/data/ref/ucsc.hg19.img -disableSequenceDictionaryValidation true -- --sparkRunner SPARK --sparkMaster local[*] --total-executor-cores 8 --executor-memory 20G --driver-memory 30G. BTW, the testing environment is:; CPU 2 X 8 physical core; node: 5 ; network: GBE; memory: 64G; ![memory](https://cloud.githubusercontent.com/assets/22440517/25929003/19a38210-3632-11e7-8dfc-c18f75604de6.png); ![cpu](https://cloud.githubusercontent.com/assets/22440517/25929008/205497ac-3632-11e7-8aa9-9c459d99371d.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2705
https://github.com/broadinstitute/gatk/issues/2705:1679,Testability,test,testing,1679,"I tried to ERR000589 process data with BwaSpark. The bam file size is 1.3G. The average time spent is about 25 min (5 nodes).; However it would only cost 5 min in processing same data if I tried to use original C bwa with 32 threads.; Base on this observation, I have several questions list as follow:; 1. If there is anything wrong with my params?; 2. For each Partition, is BwaSpark running in multi-thread mode?; 3. How to control the number of the bwa threads inside BwaSpark?. P.S. ; The running command is:; ./gatk-launch BwaSpark -I hdfs:///user/XX/ERR000589/ERR000589.bam -O hdfs:///user/XX/ERR000589/ERR000589_bwa.bam -R hdfs:///user/xx/refs/ucsc.hg19.fasta --bwamemIndexImage ~/data/ref/ucsc.hg19.img -disableSequenceDictionaryValidation true -- --sparkRunner SPARK --sparkMaster <master_url> --executor-cores 1 --total-executor-cores 16 --executor-memory 4G. I tried to further adjust the following parameters,; --executor-cores --total-executor-cores --executor-memory --driver-memory; but none of these took less time than 16 min. Besides, I alsow tried to run it in local mode, while it won't end successfully. It seems that CPU was in endless waiting. I guess it occupied so much memory that the swap space is in use? Pic 1 shows the memory consumed while running ; This time, the command is:; ./gatk-launch BwaSpark -I hdfs:///user/XX/ERR000589/ERR000589.bam -O hdfs:///user/xx/ERR000589/ERR000589.bwa.bam -R /software/home/xx/data/ref/ucsc.hg19.fasta \ --bwamemIndexImage ~/data/ref/ucsc.hg19.img -disableSequenceDictionaryValidation true -- --sparkRunner SPARK --sparkMaster local[*] --total-executor-cores 8 --executor-memory 20G --driver-memory 30G. BTW, the testing environment is:; CPU 2 X 8 physical core; node: 5 ; network: GBE; memory: 64G; ![memory](https://cloud.githubusercontent.com/assets/22440517/25929003/19a38210-3632-11e7-8dfc-c18f75604de6.png); ![cpu](https://cloud.githubusercontent.com/assets/22440517/25929008/205497ac-3632-11e7-8aa9-9c459d99371d.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2705
https://github.com/broadinstitute/gatk/issues/2706:324,Availability,error,error,324,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706
https://github.com/broadinstitute/gatk/issues/2706:425,Modifiability,config,configured,425,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706
https://github.com/broadinstitute/gatk/issues/2706:84,Security,authenticat,authentication,84,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706
https://github.com/broadinstitute/gatk/issues/2706:40,Testability,test,tests,40,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706
https://github.com/broadinstitute/gatk/issues/2706:211,Testability,log,log,211,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706
https://github.com/broadinstitute/gatk/issues/2706:284,Testability,test,tests,284,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706
https://github.com/broadinstitute/gatk/issues/2706:304,Testability,test,tests,304,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706
https://github.com/broadinstitute/gatk/issues/2706:446,Testability,log,logging,446,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706
https://github.com/broadinstitute/gatk/issues/2706:439,Usability,simpl,simply,439,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706
https://github.com/broadinstitute/gatk/issues/2707:2952,Modifiability,refactor,refactor,2952,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2914,Safety,safe,safe,2914,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:1250,Testability,test,testNoIllegalArgumentException,1250,the builder. 	at shaded.cloud_nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); 	at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:248); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:82); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:30); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:77); 	at com.google.cloud.storage.StorageOptions.getDefaultInstance(StorageOptions.java:121); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.initStorage(CloudStorageFileSystemProvider.java:153); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:1608,Testability,test,testng,1608,.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:77); 	at com.google.cloud.storage.StorageOptions.getDefaultInstance(StorageOptions.java:121); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.initStorage(CloudStorageFileSystemProvider.java:153); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:1702,Testability,test,testng,1702,ud.storage.StorageOptions.getDefaultInstance(StorageOptions.java:121); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.initStorage(CloudStorageFileSystemProvider.java:153); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:1766,Testability,test,testng,1766,:121); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.initStorage(CloudStorageFileSystemProvider.java:153); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:1834,Testability,test,testng,1834,emProvider.initStorage(CloudStorageFileSystemProvider.java:153); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.Re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:1904,Testability,test,testng,1904,om.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:1920,Testability,Test,TestMethodWorker,1920,ge.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:1955,Testability,Test,TestMethodWorker,1955,eFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either ref,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:1991,Testability,test,testng,1991,FileSystemProvider.java:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to no,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2007,Testability,Test,TestMethodWorker,2007,va:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2028,Testability,Test,TestMethodWorker,2028,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2064,Testability,test,testng,2064,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2071,Testability,Test,TestRunner,2071,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2093,Testability,Test,TestRunner,2093,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2123,Testability,test,testng,2123,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2130,Testability,Test,TestRunner,2130,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2145,Testability,Test,TestRunner,2145,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2175,Testability,test,testng,2175,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2233,Testability,test,testng,2233,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2299,Testability,test,testng,2299,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2360,Testability,test,testng,2360,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2414,Testability,test,testng,2414,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2484,Testability,test,testng,2484,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2549,Testability,test,testng,2549,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2556,Testability,Test,TestNG,2556,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2585,Testability,Test,TestNG,2585,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2612,Testability,test,testng,2612,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2619,Testability,Test,TestNG,2619,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2643,Testability,Test,TestNG,2643,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2670,Testability,test,testng,2670,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2677,Testability,Test,TestNG,2677,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2694,Testability,Test,TestNG,2694,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2721,Testability,test,testng,2721,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2728,Testability,Test,TestNG,2728,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2739,Testability,Test,TestNG,2739,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2766,Testability,test,testng,2766,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/issues/2707:2829,Testability,test,testng,2829,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707
https://github.com/broadinstitute/gatk/pull/2709:310,Testability,test,tests,310,- Create a GATK4 Dockerfile; - Added a script to create the docker image for the GATK4 Dockerfile. This will create a staging directory if desired.; - Created a GATK Base Dockerfile that other GATK4 Dockerfiles can use in ``FROM``; - Added a script to create the GATK Base image; - Documentation; - Fixed unit tests that fail in the docker image.; - docker images based on OpenJDK 8. Closes #2457 ; Closes #2708 ; Closes #2700,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2709
https://github.com/broadinstitute/gatk/issues/2711:408,Usability,Simpl,Simply,408,"@sooheelee Had the very reasonable question of ""what version of bwa is being used in gatk"". There currently doesn't seem to be a way to query this from the command line. Since you need the matching bwa to build the index files this is important information for the users. Ideally it could be handled by running --version on a tool that uses BWA, but that probably needs changes in the command line parser. . Simply printing the version when a tool that uses bwa starts up would probably be fine. . We added a hook in gatk-bwamem-jni to ask it what version it's using, so it should be simple just wire that up. We may want to do the same for fermilite or other packaged native code at the same time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2711
https://github.com/broadinstitute/gatk/issues/2711:584,Usability,simpl,simple,584,"@sooheelee Had the very reasonable question of ""what version of bwa is being used in gatk"". There currently doesn't seem to be a way to query this from the command line. Since you need the matching bwa to build the index files this is important information for the users. Ideally it could be handled by running --version on a tool that uses BWA, but that probably needs changes in the command line parser. . Simply printing the version when a tool that uses bwa starts up would probably be fine. . We added a hook in gatk-bwamem-jni to ask it what version it's using, so it should be simple just wire that up. We may want to do the same for fermilite or other packaged native code at the same time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2711
https://github.com/broadinstitute/gatk/issues/2712:197,Availability,avail,available,197,"We've identified a site that has a genotype with pls of 0,0,1 and reports GQ0. GVCF is `/humgen/gsa-hpprojects/dev/mshand/palantir/Analysis/535_GenotypeGVCFs/shard-314.vcf.gz` which cannot be made available publicly. site is 43291092. Gatk3 outputs a GQ0 hom-ref call while gatk4 outputs a no call. From discussion with @ldgauthier it seems like the no-call is a reasonable output, but it's strange that a) we have GQ0 when the pl difference is 1, and b) that there's a difference between gatk3 and 4. | | `GT:AD:DP:GQ:PGT:PID:PL` | ; | ----|------|; | gatk3 | `0/0:18,0:18:0:.:.:0,0,1 ` |; | gatk4 | `./.:18,0:18:.:.:.:0,0,1 ` |",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2712
https://github.com/broadinstitute/gatk/issues/2714:184,Availability,down,download,184,"I noticed, while looking at https://github.com/broadinstitute/gatk/issues/2713, that the main `GenomicsDBImporter` constructor calls `GenomicsDBImporter.generateSortedCallSetMap()` to download all VCF headers to get the sample names, but it already has the sample names passed in to it (as `variants.keySet()`). This sort of defeats the purpose of the `--sampleNameMap` argument in the `GenomicsDBImport` tool...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2714
https://github.com/broadinstitute/gatk/issues/2715:95,Availability,error,error,95,"It should instead notice that `sampleToVCMap.get()` has returned null, and throw a descriptive error message (including, crucially, the sample name in question) instead of allowing a `NullPointerException` to occur.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2715
https://github.com/broadinstitute/gatk/issues/2715:101,Integrability,message,message,101,"It should instead notice that `sampleToVCMap.get()` has returned null, and throw a descriptive error message (including, crucially, the sample name in question) instead of allowing a `NullPointerException` to occur.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2715
https://github.com/broadinstitute/gatk/issues/2717:157,Availability,avail,available,157,"Currently it defaults to 1 core. It seems like it scales well to 4 cores and correctly throttles back if there are fewer cores, so we should make it use the available resources if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2717
https://github.com/broadinstitute/gatk/issues/2717:87,Performance,throttle,throttles,87,"Currently it defaults to 1 core. It seems like it scales well to 4 cores and correctly throttles back if there are fewer cores, so we should make it use the available resources if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2717
https://github.com/broadinstitute/gatk/issues/2718:250,Deployability,deploy,deploy-mode,250,"While testing BwaSpark on a yarn cluster, I noticed that only a single executor (the driver) was being created and running only on the master node. The worker nodes were idle. This happened when running the following command . ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true; ```. I checked the spark environment settings in the spark web UI and found that `spark.master` was set to `local` despite the `--master yarn` cmd-line argument. Next I checked the gatk source and found a suspicious call to `SparkConf.setMaster()` at line 138 in ; `src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java`. I commented out this call, recompiled, and tried again, and the issue went away. I suspect that `setMaster()` was overriding the `--master yarn` cmd-line argument. I believe this is a bug and propose somehow avoiding `setMaster()` when the app is run with `spark-submit`. Thanks,; David",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718
https://github.com/broadinstitute/gatk/issues/2718:1155,Safety,avoid,avoiding,1155,"While testing BwaSpark on a yarn cluster, I noticed that only a single executor (the driver) was being created and running only on the master node. The worker nodes were idle. This happened when running the following command . ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true; ```. I checked the spark environment settings in the spark web UI and found that `spark.master` was set to `local` despite the `--master yarn` cmd-line argument. Next I checked the gatk source and found a suspicious call to `SparkConf.setMaster()` at line 138 in ; `src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java`. I commented out this call, recompiled, and tried again, and the issue went away. I suspect that `setMaster()` was overriding the `--master yarn` cmd-line argument. I believe this is a bug and propose somehow avoiding `setMaster()` when the app is run with `spark-submit`. Thanks,; David",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718
https://github.com/broadinstitute/gatk/issues/2718:6,Testability,test,testing,6,"While testing BwaSpark on a yarn cluster, I noticed that only a single executor (the driver) was being created and running only on the master node. The worker nodes were idle. This happened when running the following command . ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true; ```. I checked the spark environment settings in the spark web UI and found that `spark.master` was set to `local` despite the `--master yarn` cmd-line argument. Next I checked the gatk source and found a suspicious call to `SparkConf.setMaster()` at line 138 in ; `src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java`. I commented out this call, recompiled, and tried again, and the issue went away. I suspect that `setMaster()` was overriding the `--master yarn` cmd-line argument. I believe this is a bug and propose somehow avoiding `setMaster()` when the app is run with `spark-submit`. Thanks,; David",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718
https://github.com/broadinstitute/gatk/issues/2720:191,Integrability,depend,dependencies,191,- [x] Mount the large files rather than copying into the docker image when running the tests.; - [x] Cleanup the build directory before publishing. We only need the jar.; - [ ] Remove native dependencies that are not required for the docker image.; - [ ] Uninstall gcc and clean the ubuntu image a bit.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2720
https://github.com/broadinstitute/gatk/issues/2720:87,Testability,test,tests,87,- [x] Mount the large files rather than copying into the docker image when running the tests.; - [x] Cleanup the build directory before publishing. We only need the jar.; - [ ] Remove native dependencies that are not required for the docker image.; - [ ] Uninstall gcc and clean the ubuntu image a bit.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2720
https://github.com/broadinstitute/gatk/issues/2722:333,Availability,error,error,333,Bug report recieved via email: . I am testing GATK4 walker version. I tried the following command. ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list -jdk_inflater=true**. It will fail with the following error message `htsjdk.samtools.SAMFormatException: Invalid GZIP header`. If I run ; ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-jdk_inflater=true**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf. all three will run with no problem. However when combined **-L and -jdk_inflater=true** it will fail with error. This problem is on both Intel system and Power system. It feels like an easy fix. Would you please look into that? Thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2722
https://github.com/broadinstitute/gatk/issues/2722:1019,Availability,error,error,1019,Bug report recieved via email: . I am testing GATK4 walker version. I tried the following command. ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list -jdk_inflater=true**. It will fail with the following error message `htsjdk.samtools.SAMFormatException: Invalid GZIP header`. If I run ; ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-jdk_inflater=true**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf. all three will run with no problem. However when combined **-L and -jdk_inflater=true** it will fail with error. This problem is on both Intel system and Power system. It feels like an easy fix. Would you please look into that? Thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2722
https://github.com/broadinstitute/gatk/issues/2722:1067,Energy Efficiency,Power,Power,1067,Bug report recieved via email: . I am testing GATK4 walker version. I tried the following command. ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list -jdk_inflater=true**. It will fail with the following error message `htsjdk.samtools.SAMFormatException: Invalid GZIP header`. If I run ; ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-jdk_inflater=true**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf. all three will run with no problem. However when combined **-L and -jdk_inflater=true** it will fail with error. This problem is on both Intel system and Power system. It feels like an easy fix. Would you please look into that? Thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2722
https://github.com/broadinstitute/gatk/issues/2722:339,Integrability,message,message,339,Bug report recieved via email: . I am testing GATK4 walker version. I tried the following command. ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list -jdk_inflater=true**. It will fail with the following error message `htsjdk.samtools.SAMFormatException: Invalid GZIP header`. If I run ; ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-jdk_inflater=true**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf. all three will run with no problem. However when combined **-L and -jdk_inflater=true** it will fail with error. This problem is on both Intel system and Power system. It feels like an easy fix. Would you please look into that? Thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2722
https://github.com/broadinstitute/gatk/issues/2722:38,Testability,test,testing,38,Bug report recieved via email: . I am testing GATK4 walker version. I tried the following command. ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list -jdk_inflater=true**. It will fail with the following error message `htsjdk.samtools.SAMFormatException: Invalid GZIP header`. If I run ; ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-jdk_inflater=true**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf. all three will run with no problem. However when combined **-L and -jdk_inflater=true** it will fail with error. This problem is on both Intel system and Power system. It feels like an easy fix. Would you please look into that? Thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2722
https://github.com/broadinstitute/gatk/issues/2724:41,Integrability,depend,dependency,41,"From @vruano 's comment in #2709 ... The dependency of localJar on shadowJar seems semantically ""off"".",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2724
https://github.com/broadinstitute/gatk/pull/2725:156,Deployability,Update,Updated,156,"This also includes:; * Added FPGA to ```Implementation``` enum of ```VectorLoglessPairHMM```; * Added FPGA to ```Implementation``` enum of ```PairHMM```; * Updated ```FASTEST_AVAILABLE``` value to try FPGA first (then AVX+OMP, then AVX, then regular)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2725
https://github.com/broadinstitute/gatk/pull/2726:11,Testability,test,testing,11,- Attempts testing of docker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2726
https://github.com/broadinstitute/gatk/pull/2729:125,Energy Efficiency,adapt,adapted,125,"@cwhelan @tedsharpe please review. There are 4 new classes here:. 1. LongHopscotchSet - based on HopscotchCollection/Set but adapted to store primitive longs instead of objects. The most significant bit is used to tell if a bucket is null or not, so the longs being stored must be non-negative. This works for use with k-mers, which we are assume are odd-length up to 31 and thus consume up to 62 bits.; 2. LargeLongHopscotchSet - for sets of longs greater than ~2 billion (the max Java array size) using a List of LongHopscotchSets.; 3. LongBloomFilter - Bloom filter for long's; 4. LargeLongBloomFilter - Bloom filter when the filter index size exceeds 2GB using a List of LongBloomFilters. - LongIterator and QueryableLongSet interfaces for convenience.; - Minor change to HopscotchSet max legal size, which was higher than the actual allowed Java array size. PS I just had a thought that the Bloom filters could use long instead of byte buckets to expand the max index size 8-fold. Could maybe be done for the Hopscotch sets as well, but with considerably more difficulty. Thoughts? On the other hand, the performance is already adequate so perhaps I'll save this idea for later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2729
https://github.com/broadinstitute/gatk/pull/2729:729,Integrability,interface,interfaces,729,"@cwhelan @tedsharpe please review. There are 4 new classes here:. 1. LongHopscotchSet - based on HopscotchCollection/Set but adapted to store primitive longs instead of objects. The most significant bit is used to tell if a bucket is null or not, so the longs being stored must be non-negative. This works for use with k-mers, which we are assume are odd-length up to 31 and thus consume up to 62 bits.; 2. LargeLongHopscotchSet - for sets of longs greater than ~2 billion (the max Java array size) using a List of LongHopscotchSets.; 3. LongBloomFilter - Bloom filter for long's; 4. LargeLongBloomFilter - Bloom filter when the filter index size exceeds 2GB using a List of LongBloomFilters. - LongIterator and QueryableLongSet interfaces for convenience.; - Minor change to HopscotchSet max legal size, which was higher than the actual allowed Java array size. PS I just had a thought that the Bloom filters could use long instead of byte buckets to expand the max index size 8-fold. Could maybe be done for the Hopscotch sets as well, but with considerably more difficulty. Thoughts? On the other hand, the performance is already adequate so perhaps I'll save this idea for later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2729
https://github.com/broadinstitute/gatk/pull/2729:125,Modifiability,adapt,adapted,125,"@cwhelan @tedsharpe please review. There are 4 new classes here:. 1. LongHopscotchSet - based on HopscotchCollection/Set but adapted to store primitive longs instead of objects. The most significant bit is used to tell if a bucket is null or not, so the longs being stored must be non-negative. This works for use with k-mers, which we are assume are odd-length up to 31 and thus consume up to 62 bits.; 2. LargeLongHopscotchSet - for sets of longs greater than ~2 billion (the max Java array size) using a List of LongHopscotchSets.; 3. LongBloomFilter - Bloom filter for long's; 4. LargeLongBloomFilter - Bloom filter when the filter index size exceeds 2GB using a List of LongBloomFilters. - LongIterator and QueryableLongSet interfaces for convenience.; - Minor change to HopscotchSet max legal size, which was higher than the actual allowed Java array size. PS I just had a thought that the Bloom filters could use long instead of byte buckets to expand the max index size 8-fold. Could maybe be done for the Hopscotch sets as well, but with considerably more difficulty. Thoughts? On the other hand, the performance is already adequate so perhaps I'll save this idea for later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2729
https://github.com/broadinstitute/gatk/pull/2729:1110,Performance,perform,performance,1110,"@cwhelan @tedsharpe please review. There are 4 new classes here:. 1. LongHopscotchSet - based on HopscotchCollection/Set but adapted to store primitive longs instead of objects. The most significant bit is used to tell if a bucket is null or not, so the longs being stored must be non-negative. This works for use with k-mers, which we are assume are odd-length up to 31 and thus consume up to 62 bits.; 2. LargeLongHopscotchSet - for sets of longs greater than ~2 billion (the max Java array size) using a List of LongHopscotchSets.; 3. LongBloomFilter - Bloom filter for long's; 4. LargeLongBloomFilter - Bloom filter when the filter index size exceeds 2GB using a List of LongBloomFilters. - LongIterator and QueryableLongSet interfaces for convenience.; - Minor change to HopscotchSet max legal size, which was higher than the actual allowed Java array size. PS I just had a thought that the Bloom filters could use long instead of byte buckets to expand the max index size 8-fold. Could maybe be done for the Hopscotch sets as well, but with considerably more difficulty. Thoughts? On the other hand, the performance is already adequate so perhaps I'll save this idea for later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2729
https://github.com/broadinstitute/gatk/pull/2730:270,Availability,avail,available,270,"This tool creates a data file containing a map from reference accessions to NCBI taxonomic IDs, and the taxonomic tree, which includes parent/child relationships as well as other metadata like the reference length and scientific name of each node. . The input files are available from the NCBI FTP server. One is a ""catalog"" file that gives the mapping from reference contig accession to taxonomic ID. There are catalog files available for RefSeq and for Genbank - the tool can take in either. . There are two other files - a ""names"" and ""nodes"" file contained in a single tarball - that contain the scientific names of each node and parent/child relationships. For convenience, the tool takes in the path to the tarball and extracts the two files automatically. The resulting database size is minimized using the given reference. Once the full NCBI taxonomy tree is built, any organism node that is neither in the reference nor an ancestor of a reference organism is removed. The resulting datafile is read in by the ClassifyReads tool (coming in a future PR) to assign relative abundance scores to each taxonomic node. Also made some changed to the way the PSTree and PSTreeNode are serialized (using Kryo read/writeObject instead of read/writeClassAndObject) so that loading old files won't break if these classes change packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2730
https://github.com/broadinstitute/gatk/pull/2730:426,Availability,avail,available,426,"This tool creates a data file containing a map from reference accessions to NCBI taxonomic IDs, and the taxonomic tree, which includes parent/child relationships as well as other metadata like the reference length and scientific name of each node. . The input files are available from the NCBI FTP server. One is a ""catalog"" file that gives the mapping from reference contig accession to taxonomic ID. There are catalog files available for RefSeq and for Genbank - the tool can take in either. . There are two other files - a ""names"" and ""nodes"" file contained in a single tarball - that contain the scientific names of each node and parent/child relationships. For convenience, the tool takes in the path to the tarball and extracts the two files automatically. The resulting database size is minimized using the given reference. Once the full NCBI taxonomy tree is built, any organism node that is neither in the reference nor an ancestor of a reference organism is removed. The resulting datafile is read in by the ClassifyReads tool (coming in a future PR) to assign relative abundance scores to each taxonomic node. Also made some changed to the way the PSTree and PSTreeNode are serialized (using Kryo read/writeObject instead of read/writeClassAndObject) so that loading old files won't break if these classes change packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2730
https://github.com/broadinstitute/gatk/pull/2730:1270,Performance,load,loading,1270,"This tool creates a data file containing a map from reference accessions to NCBI taxonomic IDs, and the taxonomic tree, which includes parent/child relationships as well as other metadata like the reference length and scientific name of each node. . The input files are available from the NCBI FTP server. One is a ""catalog"" file that gives the mapping from reference contig accession to taxonomic ID. There are catalog files available for RefSeq and for Genbank - the tool can take in either. . There are two other files - a ""names"" and ""nodes"" file contained in a single tarball - that contain the scientific names of each node and parent/child relationships. For convenience, the tool takes in the path to the tarball and extracts the two files automatically. The resulting database size is minimized using the given reference. Once the full NCBI taxonomy tree is built, any organism node that is neither in the reference nor an ancestor of a reference organism is removed. The resulting datafile is read in by the ClassifyReads tool (coming in a future PR) to assign relative abundance scores to each taxonomic node. Also made some changed to the way the PSTree and PSTreeNode are serialized (using Kryo read/writeObject instead of read/writeClassAndObject) so that loading old files won't break if these classes change packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2730
https://github.com/broadinstitute/gatk/pull/2730:62,Security,access,accessions,62,"This tool creates a data file containing a map from reference accessions to NCBI taxonomic IDs, and the taxonomic tree, which includes parent/child relationships as well as other metadata like the reference length and scientific name of each node. . The input files are available from the NCBI FTP server. One is a ""catalog"" file that gives the mapping from reference contig accession to taxonomic ID. There are catalog files available for RefSeq and for Genbank - the tool can take in either. . There are two other files - a ""names"" and ""nodes"" file contained in a single tarball - that contain the scientific names of each node and parent/child relationships. For convenience, the tool takes in the path to the tarball and extracts the two files automatically. The resulting database size is minimized using the given reference. Once the full NCBI taxonomy tree is built, any organism node that is neither in the reference nor an ancestor of a reference organism is removed. The resulting datafile is read in by the ClassifyReads tool (coming in a future PR) to assign relative abundance scores to each taxonomic node. Also made some changed to the way the PSTree and PSTreeNode are serialized (using Kryo read/writeObject instead of read/writeClassAndObject) so that loading old files won't break if these classes change packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2730
https://github.com/broadinstitute/gatk/pull/2730:375,Security,access,accession,375,"This tool creates a data file containing a map from reference accessions to NCBI taxonomic IDs, and the taxonomic tree, which includes parent/child relationships as well as other metadata like the reference length and scientific name of each node. . The input files are available from the NCBI FTP server. One is a ""catalog"" file that gives the mapping from reference contig accession to taxonomic ID. There are catalog files available for RefSeq and for Genbank - the tool can take in either. . There are two other files - a ""names"" and ""nodes"" file contained in a single tarball - that contain the scientific names of each node and parent/child relationships. For convenience, the tool takes in the path to the tarball and extracts the two files automatically. The resulting database size is minimized using the given reference. Once the full NCBI taxonomy tree is built, any organism node that is neither in the reference nor an ancestor of a reference organism is removed. The resulting datafile is read in by the ClassifyReads tool (coming in a future PR) to assign relative abundance scores to each taxonomic node. Also made some changed to the way the PSTree and PSTreeNode are serialized (using Kryo read/writeObject instead of read/writeClassAndObject) so that loading old files won't break if these classes change packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2730
https://github.com/broadinstitute/gatk/pull/2731:790,Deployability,Rolling,Rolling,790,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:861,Integrability,Wrap,Wrapped,861,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:539,Security,validat,validation,539,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:198,Testability,test,tests,198,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:245,Testability,test,tests,245,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:259,Testability,test,tests,259,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:322,Testability,test,tests,322,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:386,Testability,test,tests,386,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:594,Testability,test,testing,594,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:610,Testability,test,test,610,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/pull/2731:603,Usability,Simpl,Simple,603,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731
https://github.com/broadinstitute/gatk/issues/2732:257,Availability,error,error,257,"When running the following command. gatk4 BaseRecalibratorSpark -I xx_markduplicatespark.bam -knownSites dbsnp_138.b37.vcf -knownSites Mills_and_1000G_gold_standard.indels.b37.vcf -O xx_baserecalibratespark.table -R humann_g1k_v37.2bit --TMP_DIR tmp. I got error message,. Using GATK wrapper script /curr/tianj/software/gatk/build/install/gatk/bin/gatk; Running:; /curr/tianj/software/gatk/build/install/gatk/bin/gatk BaseRecalibratorSpark -I A15_markduplicatespark.bam -knownSites ref/Mills_and_1000G_gold_standard.indels.b37.vcf -O A15_baserecalibratespark.table -R /curr/tianj/data/humann_g1k_v37.2bit; 17:19:00.338 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/software/gatk/build/instabgkl_compression.so; [May 17, 2017 5:19:00 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark --knownSites /genome/ref/db_1000G_gold_standard.indels.b37.vcf --output A15_baserecalibratespark.table --reference /curr/tianj/data/humann_g1k_v37.2bp --joinStrategy BROADCAST --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_defdeletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_lles false --useOriginalQualities false --defaultBaseQualities -1 --readShardSize 10000 --readShardPadding 1000 --readValid-interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --sharl[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inf; [May 17, 2017 5:19:00 PM UTC] Executing as tianj@ip-172-31-78-66 on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM:4.alpha.2-261-gb8d32ee-SNAPSHOT; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.COMPRESSION_LEVEL : 1; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.CREATE_INDEX : false; 17:19:00.371 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/issues/2732:3280,Availability,down,down,3280,"park - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.REFERENCE_FASTA : null; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:19:00.371 INFO BaseRecalibratorSpark - Deflater IntelDeflater; 17:19:00.372 INFO BaseRecalibratorSpark - Inflater IntelInflater; 17:19:00.372 INFO BaseRecalibratorSpark - Initializing engine; 17:19:00.372 INFO BaseRecalibratorSpark - Done initializing engine; 17:19:00.872 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java clas; 17:22:09.153 INFO BaseRecalibratorSpark - Shutting down engine; [May 17, 2017 5:22:09 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 3.15 min; Runtime.totalMemory()=15504244736; java.lang.ArrayIndexOutOfBoundsException: 1073741865; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:195); at org.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)atorg.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)atorg.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236); at org.apache.spark.util.Utils$.tryWithSafeFinally(Uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/issues/2732:6110,Availability,error,error,6110,"a:236)atorg.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:237); at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:107); at org.apache.spark.broadcast.TorrentBroadcast.(TorrentBroadcast.scala:86); at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:56); at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1387); at org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:646); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.join(BroadcastJoinReadsWithVariants.j; at org.broadinstitute.hellbender.engine.spark.AddContextDataToReadSpark.add(AddContextDataToReadSpark.java:67); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:93); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220). However, if I use only Mills_and_1000G_gold_standard.indels.b37.vcf instead of both of them. It ran through. Use only dbsnp_138.b37.vcf causes the same error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/issues/2732:331,Deployability,install,install,331,"When running the following command. gatk4 BaseRecalibratorSpark -I xx_markduplicatespark.bam -knownSites dbsnp_138.b37.vcf -knownSites Mills_and_1000G_gold_standard.indels.b37.vcf -O xx_baserecalibratespark.table -R humann_g1k_v37.2bit --TMP_DIR tmp. I got error message,. Using GATK wrapper script /curr/tianj/software/gatk/build/install/gatk/bin/gatk; Running:; /curr/tianj/software/gatk/build/install/gatk/bin/gatk BaseRecalibratorSpark -I A15_markduplicatespark.bam -knownSites ref/Mills_and_1000G_gold_standard.indels.b37.vcf -O A15_baserecalibratespark.table -R /curr/tianj/data/humann_g1k_v37.2bit; 17:19:00.338 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/software/gatk/build/instabgkl_compression.so; [May 17, 2017 5:19:00 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark --knownSites /genome/ref/db_1000G_gold_standard.indels.b37.vcf --output A15_baserecalibratespark.table --reference /curr/tianj/data/humann_g1k_v37.2bp --joinStrategy BROADCAST --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_defdeletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_lles false --useOriginalQualities false --defaultBaseQualities -1 --readShardSize 10000 --readShardPadding 1000 --readValid-interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --sharl[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inf; [May 17, 2017 5:19:00 PM UTC] Executing as tianj@ip-172-31-78-66 on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM:4.alpha.2-261-gb8d32ee-SNAPSHOT; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.COMPRESSION_LEVEL : 1; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.CREATE_INDEX : false; 17:19:00.371 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/issues/2732:396,Deployability,install,install,396,"When running the following command. gatk4 BaseRecalibratorSpark -I xx_markduplicatespark.bam -knownSites dbsnp_138.b37.vcf -knownSites Mills_and_1000G_gold_standard.indels.b37.vcf -O xx_baserecalibratespark.table -R humann_g1k_v37.2bit --TMP_DIR tmp. I got error message,. Using GATK wrapper script /curr/tianj/software/gatk/build/install/gatk/bin/gatk; Running:; /curr/tianj/software/gatk/build/install/gatk/bin/gatk BaseRecalibratorSpark -I A15_markduplicatespark.bam -knownSites ref/Mills_and_1000G_gold_standard.indels.b37.vcf -O A15_baserecalibratespark.table -R /curr/tianj/data/humann_g1k_v37.2bit; 17:19:00.338 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/software/gatk/build/instabgkl_compression.so; [May 17, 2017 5:19:00 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark --knownSites /genome/ref/db_1000G_gold_standard.indels.b37.vcf --output A15_baserecalibratespark.table --reference /curr/tianj/data/humann_g1k_v37.2bp --joinStrategy BROADCAST --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_defdeletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_lles false --useOriginalQualities false --defaultBaseQualities -1 --readShardSize 10000 --readShardPadding 1000 --readValid-interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --sharl[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inf; [May 17, 2017 5:19:00 PM UTC] Executing as tianj@ip-172-31-78-66 on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM:4.alpha.2-261-gb8d32ee-SNAPSHOT; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.COMPRESSION_LEVEL : 1; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.CREATE_INDEX : false; 17:19:00.371 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/issues/2732:263,Integrability,message,message,263,"When running the following command. gatk4 BaseRecalibratorSpark -I xx_markduplicatespark.bam -knownSites dbsnp_138.b37.vcf -knownSites Mills_and_1000G_gold_standard.indels.b37.vcf -O xx_baserecalibratespark.table -R humann_g1k_v37.2bit --TMP_DIR tmp. I got error message,. Using GATK wrapper script /curr/tianj/software/gatk/build/install/gatk/bin/gatk; Running:; /curr/tianj/software/gatk/build/install/gatk/bin/gatk BaseRecalibratorSpark -I A15_markduplicatespark.bam -knownSites ref/Mills_and_1000G_gold_standard.indels.b37.vcf -O A15_baserecalibratespark.table -R /curr/tianj/data/humann_g1k_v37.2bit; 17:19:00.338 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/software/gatk/build/instabgkl_compression.so; [May 17, 2017 5:19:00 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark --knownSites /genome/ref/db_1000G_gold_standard.indels.b37.vcf --output A15_baserecalibratespark.table --reference /curr/tianj/data/humann_g1k_v37.2bp --joinStrategy BROADCAST --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_defdeletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_lles false --useOriginalQualities false --defaultBaseQualities -1 --readShardSize 10000 --readShardPadding 1000 --readValid-interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --sharl[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inf; [May 17, 2017 5:19:00 PM UTC] Executing as tianj@ip-172-31-78-66 on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM:4.alpha.2-261-gb8d32ee-SNAPSHOT; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.COMPRESSION_LEVEL : 1; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.CREATE_INDEX : false; 17:19:00.371 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/issues/2732:284,Integrability,wrap,wrapper,284,"When running the following command. gatk4 BaseRecalibratorSpark -I xx_markduplicatespark.bam -knownSites dbsnp_138.b37.vcf -knownSites Mills_and_1000G_gold_standard.indels.b37.vcf -O xx_baserecalibratespark.table -R humann_g1k_v37.2bit --TMP_DIR tmp. I got error message,. Using GATK wrapper script /curr/tianj/software/gatk/build/install/gatk/bin/gatk; Running:; /curr/tianj/software/gatk/build/install/gatk/bin/gatk BaseRecalibratorSpark -I A15_markduplicatespark.bam -knownSites ref/Mills_and_1000G_gold_standard.indels.b37.vcf -O A15_baserecalibratespark.table -R /curr/tianj/data/humann_g1k_v37.2bit; 17:19:00.338 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/software/gatk/build/instabgkl_compression.so; [May 17, 2017 5:19:00 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark --knownSites /genome/ref/db_1000G_gold_standard.indels.b37.vcf --output A15_baserecalibratespark.table --reference /curr/tianj/data/humann_g1k_v37.2bp --joinStrategy BROADCAST --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_defdeletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_lles false --useOriginalQualities false --defaultBaseQualities -1 --readShardSize 10000 --readShardPadding 1000 --readValid-interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --sharl[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inf; [May 17, 2017 5:19:00 PM UTC] Executing as tianj@ip-172-31-78-66 on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM:4.alpha.2-261-gb8d32ee-SNAPSHOT; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.COMPRESSION_LEVEL : 1; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.CREATE_INDEX : false; 17:19:00.371 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/issues/2732:646,Performance,Load,Loading,646,"When running the following command. gatk4 BaseRecalibratorSpark -I xx_markduplicatespark.bam -knownSites dbsnp_138.b37.vcf -knownSites Mills_and_1000G_gold_standard.indels.b37.vcf -O xx_baserecalibratespark.table -R humann_g1k_v37.2bit --TMP_DIR tmp. I got error message,. Using GATK wrapper script /curr/tianj/software/gatk/build/install/gatk/bin/gatk; Running:; /curr/tianj/software/gatk/build/install/gatk/bin/gatk BaseRecalibratorSpark -I A15_markduplicatespark.bam -knownSites ref/Mills_and_1000G_gold_standard.indels.b37.vcf -O A15_baserecalibratespark.table -R /curr/tianj/data/humann_g1k_v37.2bit; 17:19:00.338 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/software/gatk/build/instabgkl_compression.so; [May 17, 2017 5:19:00 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark --knownSites /genome/ref/db_1000G_gold_standard.indels.b37.vcf --output A15_baserecalibratespark.table --reference /curr/tianj/data/humann_g1k_v37.2bp --joinStrategy BROADCAST --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_defdeletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_lles false --useOriginalQualities false --defaultBaseQualities -1 --readShardSize 10000 --readShardPadding 1000 --readValid-interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --sharl[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inf; [May 17, 2017 5:19:00 PM UTC] Executing as tianj@ip-172-31-78-66 on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM:4.alpha.2-261-gb8d32ee-SNAPSHOT; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.COMPRESSION_LEVEL : 1; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.CREATE_INDEX : false; 17:19:00.371 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/issues/2732:3156,Performance,load,load,3156,"rSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.REFERENCE_FASTA : null; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:19:00.371 INFO BaseRecalibratorSpark - Deflater IntelDeflater; 17:19:00.372 INFO BaseRecalibratorSpark - Inflater IntelInflater; 17:19:00.372 INFO BaseRecalibratorSpark - Initializing engine; 17:19:00.372 INFO BaseRecalibratorSpark - Done initializing engine; 17:19:00.872 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java clas; 17:22:09.153 INFO BaseRecalibratorSpark - Shutting down engine; [May 17, 2017 5:22:09 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 3.15 min; Runtime.totalMemory()=15504244736; java.lang.ArrayIndexOutOfBoundsException: 1073741865; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:195); at org.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)atorg.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)atorg.apache.spark.broadcast.Torrent",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/issues/2732:3561,Usability,clear,clear,3561,"lse; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:19:00.371 INFO BaseRecalibratorSpark - Deflater IntelDeflater; 17:19:00.372 INFO BaseRecalibratorSpark - Inflater IntelInflater; 17:19:00.372 INFO BaseRecalibratorSpark - Initializing engine; 17:19:00.372 INFO BaseRecalibratorSpark - Done initializing engine; 17:19:00.872 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java clas; 17:22:09.153 INFO BaseRecalibratorSpark - Shutting down engine; [May 17, 2017 5:22:09 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 3.15 min; Runtime.totalMemory()=15504244736; java.lang.ArrayIndexOutOfBoundsException: 1073741865; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:195); at org.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)atorg.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)atorg.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:237); at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:107); at org.apache.spark.broadcast.TorrentBroadcast.(TorrentBroadcast.scala:86); at org.apache.spark.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732
https://github.com/broadinstitute/gatk/pull/2733:71,Deployability,Update,Updated,71,Added validateSampleNameMap command line parameter; Added a unit test; Updated genomicsdb version to 0.6.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733
https://github.com/broadinstitute/gatk/pull/2733:6,Security,validat,validateSampleNameMap,6,Added validateSampleNameMap command line parameter; Added a unit test; Updated genomicsdb version to 0.6.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733
https://github.com/broadinstitute/gatk/pull/2733:65,Testability,test,test,65,Added validateSampleNameMap command line parameter; Added a unit test; Updated genomicsdb version to 0.6.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733
https://github.com/broadinstitute/gatk/issues/2734:11,Availability,error,error,11,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:135,Availability,ERROR,ERROR,135,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:253,Availability,ERROR,ERROR,253,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:319,Availability,ERROR,ERROR,319,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:400,Availability,ERROR,ERROR,400,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:521,Availability,ERROR,ERROR,521,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:582,Availability,ERROR,ERROR,582,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:697,Availability,ERROR,ERROR,697,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:763,Availability,ERROR,ERROR,763,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:844,Availability,ERROR,ERROR,844,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:962,Availability,ERROR,ERROR,962,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1020,Availability,ERROR,ERROR,1020,"ges popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a63",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1138,Availability,ERROR,ERROR,1138,"org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1204,Availability,ERROR,ERROR,1204,"ssignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1285,Availability,ERROR,ERROR,1285," The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1406,Availability,ERROR,ERROR,1406,"oleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:2854,Availability,ERROR,ERROR,2854,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:3202,Energy Efficiency,schedul,scheduler,3202,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:17,Integrability,message,messages,17,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:236,Modifiability,variab,variable,236,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:680,Modifiability,variab,variable,680,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1121,Modifiability,variab,variable,1121,"ly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set ever",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:301,Performance,load,loaded,301,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:445,Performance,load,loaded,445,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:745,Performance,load,loaded,745,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:886,Performance,load,loaded,886,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1186,Performance,load,loaded,1186,"ssignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1330,Performance,load,loaded,1330,"un.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1585,Testability,log,logger,1585," ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:1705,Testability,log,logging,1705,"ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:2134,Testability,log,logged,2134,"ariable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:2518,Testability,log,logs,2518,"operties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetch",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:2726,Testability,log,logs,2726,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:2759,Testability,log,logger,2759,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:2796,Testability,log,logger,2796,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:2867,Testability,log,logger,2867,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:2929,Testability,log,logger,2929,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:3004,Testability,log,logger,3004,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:3067,Testability,log,logger,3067,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:3178,Testability,log,logger,3178,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:3240,Testability,log,logger,3240,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:3318,Testability,log,logger,3318,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2734:3473,Testability,log,logger,3473,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734
https://github.com/broadinstitute/gatk/issues/2735:387,Deployability,toggle,toggle,387,"When `GenotypeGVCFs` is sharded by `-L` interval, as it typically is in production, it needs a way to avoid emitting the same call across shards in the case of indels that span interval boundaries. The simplest way to do this is to only emit records that start in the current interval. It might make sense to do this at the VCF writer level, and hook it up to an engine-wide argument to toggle this behavior. `HaplotypeCaller` and `Mutect` currently do something similar internally, and could possibly leverage this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2735
https://github.com/broadinstitute/gatk/issues/2735:102,Safety,avoid,avoid,102,"When `GenotypeGVCFs` is sharded by `-L` interval, as it typically is in production, it needs a way to avoid emitting the same call across shards in the case of indels that span interval boundaries. The simplest way to do this is to only emit records that start in the current interval. It might make sense to do this at the VCF writer level, and hook it up to an engine-wide argument to toggle this behavior. `HaplotypeCaller` and `Mutect` currently do something similar internally, and could possibly leverage this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2735
https://github.com/broadinstitute/gatk/issues/2735:202,Usability,simpl,simplest,202,"When `GenotypeGVCFs` is sharded by `-L` interval, as it typically is in production, it needs a way to avoid emitting the same call across shards in the case of indels that span interval boundaries. The simplest way to do this is to only emit records that start in the current interval. It might make sense to do this at the VCF writer level, and hook it up to an engine-wide argument to toggle this behavior. `HaplotypeCaller` and `Mutect` currently do something similar internally, and could possibly leverage this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2735
https://github.com/broadinstitute/gatk/pull/2736:159,Security,validat,validating,159,This only works for the conventional gather. adding useConventionalGather argument to force using conventional gather. adding --ignoreSafetyChecks to skip pre-validating the headers. continuing the tradition of having no tests for this tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2736
https://github.com/broadinstitute/gatk/pull/2736:221,Testability,test,tests,221,This only works for the conventional gather. adding useConventionalGather argument to force using conventional gather. adding --ignoreSafetyChecks to skip pre-validating the headers. continuing the tradition of having no tests for this tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2736
https://github.com/broadinstitute/gatk/pull/2737:96,Availability,redundant,redundant,96,"VariantWalker/MultiVariantWalker create two FeatureDataSources for each input, which results in redundant dynamic discovery of which codec to use. For a MultiVariantWalker with a lot of inputs, like VariantRecalibrator, this can be a lot of path/stream opening/closing. Propose this small change to cache the codec in the FeatureInput. Ideally FeatureManager would keep track of this, but thats bigger refactor as not all of the FeatureDataSources are created by FeatureManager.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2737
https://github.com/broadinstitute/gatk/pull/2737:402,Modifiability,refactor,refactor,402,"VariantWalker/MultiVariantWalker create two FeatureDataSources for each input, which results in redundant dynamic discovery of which codec to use. For a MultiVariantWalker with a lot of inputs, like VariantRecalibrator, this can be a lot of path/stream opening/closing. Propose this small change to cache the codec in the FeatureInput. Ideally FeatureManager would keep track of this, but thats bigger refactor as not all of the FeatureDataSources are created by FeatureManager.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2737
https://github.com/broadinstitute/gatk/pull/2737:299,Performance,cache,cache,299,"VariantWalker/MultiVariantWalker create two FeatureDataSources for each input, which results in redundant dynamic discovery of which codec to use. For a MultiVariantWalker with a lot of inputs, like VariantRecalibrator, this can be a lot of path/stream opening/closing. Propose this small change to cache the codec in the FeatureInput. Ideally FeatureManager would keep track of this, but thats bigger refactor as not all of the FeatureDataSources are created by FeatureManager.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2737
https://github.com/broadinstitute/gatk/pull/2737:96,Safety,redund,redundant,96,"VariantWalker/MultiVariantWalker create two FeatureDataSources for each input, which results in redundant dynamic discovery of which codec to use. For a MultiVariantWalker with a lot of inputs, like VariantRecalibrator, this can be a lot of path/stream opening/closing. Propose this small change to cache the codec in the FeatureInput. Ideally FeatureManager would keep track of this, but thats bigger refactor as not all of the FeatureDataSources are created by FeatureManager.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2737
https://github.com/broadinstitute/gatk/issues/2739:192,Availability,error,error,192,"Trying to generate the documentation with gatk-protected to explore how a dependent-project could include ReadFilters and other utilities, running in my local computer it throws the following error:. ```; javadoc: error - In doclet class org.broadinstitute.hellbender.utils.help.GATKHelpDoclet, method start has thrown an exception java.lang.reflect.InvocationTargetException; java.lang.RuntimeException: Could not find the field corresponding to java.lang.Throwable.backtrace, presumably because the field is inaccessible; at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:604); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:591); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDocForCommandLineArgument(DefaultDocWorkUnitHandler.java:403); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.processNamedArgument(DefaultDocWorkUnitHandler.java:357); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.lambda$addCommandLineArgumentBindings$5(DefaultDocWorkUnitHandler.java:287); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.addCommandLineArgumentBindings(DefaultDocWorkUnitHandler.java:287); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.processWorkUnit(DefaultDocWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:214,Availability,error,error,214,"Trying to generate the documentation with gatk-protected to explore how a dependent-project could include ReadFilters and other utilities, running in my local computer it throws the following error:. ```; javadoc: error - In doclet class org.broadinstitute.hellbender.utils.help.GATKHelpDoclet, method start has thrown an exception java.lang.reflect.InvocationTargetException; java.lang.RuntimeException: Could not find the field corresponding to java.lang.Throwable.backtrace, presumably because the field is inaccessible; at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:604); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:591); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDocForCommandLineArgument(DefaultDocWorkUnitHandler.java:403); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.processNamedArgument(DefaultDocWorkUnitHandler.java:357); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.lambda$addCommandLineArgumentBindings$5(DefaultDocWorkUnitHandler.java:287); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.addCommandLineArgumentBindings(DefaultDocWorkUnitHandler.java:287); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.processWorkUnit(DefaultDocWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:3154,Availability,error,error,3154,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:3204,Availability,error,error,3204,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:74,Integrability,depend,dependent-project,74,"Trying to generate the documentation with gatk-protected to explore how a dependent-project could include ReadFilters and other utilities, running in my local computer it throws the following error:. ```; javadoc: error - In doclet class org.broadinstitute.hellbender.utils.help.GATKHelpDoclet, method start has thrown an exception java.lang.reflect.InvocationTargetException; java.lang.RuntimeException: Could not find the field corresponding to java.lang.Throwable.backtrace, presumably because the field is inaccessible; at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:604); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:591); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDocForCommandLineArgument(DefaultDocWorkUnitHandler.java:403); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.processNamedArgument(DefaultDocWorkUnitHandler.java:357); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.lambda$addCommandLineArgumentBindings$5(DefaultDocWorkUnitHandler.java:287); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.addCommandLineArgumentBindings(DefaultDocWorkUnitHandler.java:287); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.processWorkUnit(DefaultDocWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:3419,Modifiability,plugin,plugin,3419,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:3448,Modifiability,plugin,plugin,3448,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:3564,Modifiability,extend,extends,3564,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:3619,Modifiability,extend,extends,3619,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:3414,Testability,Test,Test,3414,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/issues/2739:3443,Testability,Test,Test,3443,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739
https://github.com/broadinstitute/gatk/pull/2740:18,Availability,redundant,redundantly,18,"Propose to reduce redundantly cracking open a path/stream to discover the correct feature codec. We do this twice for each feature input, which for multi-variant walkers with large # of inputs can be a lot. This caches the codec class in a FeatureInout the first time we find it. Ideally FeatureManager would remember it, but not all of the FeatureDataSources are created by Feature Manager (and fixing that is a bigger refactoring).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740
https://github.com/broadinstitute/gatk/pull/2740:11,Energy Efficiency,reduce,reduce,11,"Propose to reduce redundantly cracking open a path/stream to discover the correct feature codec. We do this twice for each feature input, which for multi-variant walkers with large # of inputs can be a lot. This caches the codec class in a FeatureInout the first time we find it. Ideally FeatureManager would remember it, but not all of the FeatureDataSources are created by Feature Manager (and fixing that is a bigger refactoring).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740
https://github.com/broadinstitute/gatk/pull/2740:420,Modifiability,refactor,refactoring,420,"Propose to reduce redundantly cracking open a path/stream to discover the correct feature codec. We do this twice for each feature input, which for multi-variant walkers with large # of inputs can be a lot. This caches the codec class in a FeatureInout the first time we find it. Ideally FeatureManager would remember it, but not all of the FeatureDataSources are created by Feature Manager (and fixing that is a bigger refactoring).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740
https://github.com/broadinstitute/gatk/pull/2740:212,Performance,cache,caches,212,"Propose to reduce redundantly cracking open a path/stream to discover the correct feature codec. We do this twice for each feature input, which for multi-variant walkers with large # of inputs can be a lot. This caches the codec class in a FeatureInout the first time we find it. Ideally FeatureManager would remember it, but not all of the FeatureDataSources are created by Feature Manager (and fixing that is a bigger refactoring).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740
https://github.com/broadinstitute/gatk/pull/2740:18,Safety,redund,redundantly,18,"Propose to reduce redundantly cracking open a path/stream to discover the correct feature codec. We do this twice for each feature input, which for multi-variant walkers with large # of inputs can be a lot. This caches the codec class in a FeatureInout the first time we find it. Ideally FeatureManager would remember it, but not all of the FeatureDataSources are created by Feature Manager (and fixing that is a bigger refactoring).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740
https://github.com/broadinstitute/gatk/issues/2742:70,Integrability,depend,depends,70,We should make sure for beta that we use a version of Hadoop-BAM that depends on the same version of htsjdk as GATK uses.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2742
https://github.com/broadinstitute/gatk/issues/2743:10,Availability,error,error,10,Confusing error from GenomicsDBImport:. ```; $ ./gatk-launch GenomicsDBImport --genomicsDBWorkspace /tmp/gendb --intervals chr20:1-64444167 --batchSize 300 --genomicsDBSegmentSize 1048576 --overwriteExistingGenomicsDBWorkspace true --cloudPrefetchBuffer 2 --cloudIndexPrefetchBuffer 2 --variant gs://hellbender/test/resources/large/gvcfs/NA19625.g.vcf.gz; (...); [TileDB::Fragment] Error: Cannot rename fragment directory; Directory not empty.; ```. /tmp/gendb is empty before my call to GenomicsDBImport.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2743
https://github.com/broadinstitute/gatk/issues/2743:382,Availability,Error,Error,382,Confusing error from GenomicsDBImport:. ```; $ ./gatk-launch GenomicsDBImport --genomicsDBWorkspace /tmp/gendb --intervals chr20:1-64444167 --batchSize 300 --genomicsDBSegmentSize 1048576 --overwriteExistingGenomicsDBWorkspace true --cloudPrefetchBuffer 2 --cloudIndexPrefetchBuffer 2 --variant gs://hellbender/test/resources/large/gvcfs/NA19625.g.vcf.gz; (...); [TileDB::Fragment] Error: Cannot rename fragment directory; Directory not empty.; ```. /tmp/gendb is empty before my call to GenomicsDBImport.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2743
https://github.com/broadinstitute/gatk/issues/2743:311,Testability,test,test,311,Confusing error from GenomicsDBImport:. ```; $ ./gatk-launch GenomicsDBImport --genomicsDBWorkspace /tmp/gendb --intervals chr20:1-64444167 --batchSize 300 --genomicsDBSegmentSize 1048576 --overwriteExistingGenomicsDBWorkspace true --cloudPrefetchBuffer 2 --cloudIndexPrefetchBuffer 2 --variant gs://hellbender/test/resources/large/gvcfs/NA19625.g.vcf.gz; (...); [TileDB::Fragment] Error: Cannot rename fragment directory; Directory not empty.; ```. /tmp/gendb is empty before my call to GenomicsDBImport.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2743
https://github.com/broadinstitute/gatk/issues/2744:237,Deployability,update,update,237,"Barclay defines the ExperimentalFeature annotation; displays it in command line program usage output, and also propagates an ""experimental"" property to the freemarker template used for doc. To fully manifest these in GATK we need to:. - update the custom doc templates used by GATK to recognize the ""experimental"" property; - update the command line output from --list/--help, which is generated directly by GATK, to look for the ExperimentalFeature annotation and display accordingly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2744
https://github.com/broadinstitute/gatk/issues/2744:326,Deployability,update,update,326,"Barclay defines the ExperimentalFeature annotation; displays it in command line program usage output, and also propagates an ""experimental"" property to the freemarker template used for doc. To fully manifest these in GATK we need to:. - update the custom doc templates used by GATK to recognize the ""experimental"" property; - update the command line output from --list/--help, which is generated directly by GATK, to look for the ExperimentalFeature annotation and display accordingly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2744
https://github.com/broadinstitute/gatk/issues/2745:6,Deployability,patch,patched,6,"We've patched GatherVCFs to allow it to read from NIO, but since it's a picard tool it's going to be reverted when we switch to depending on the real picard. We need to either port the changes to picard or make a gatk-tool version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2745
https://github.com/broadinstitute/gatk/issues/2745:128,Integrability,depend,depending,128,"We've patched GatherVCFs to allow it to read from NIO, but since it's a picard tool it's going to be reverted when we switch to depending on the real picard. We need to either port the changes to picard or make a gatk-tool version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2745
https://github.com/broadinstitute/gatk/issues/2748:63,Modifiability,config,configured,63,"The docker image only uses OpenJDK. However, the way travis is configured, the docker tests will be run once for OpenJDK and once for OracleJDK, but that distinction has no meaning, since the native JVM on the travis VM is irrelevant to what is happening in the docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2748
https://github.com/broadinstitute/gatk/issues/2748:86,Testability,test,tests,86,"The docker image only uses OpenJDK. However, the way travis is configured, the docker tests will be run once for OpenJDK and once for OracleJDK, but that distinction has no meaning, since the native JVM on the travis VM is irrelevant to what is happening in the docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2748
https://github.com/broadinstitute/gatk/issues/2749:26,Availability,error,errors,26,"503 (Service Unavailable) errors are transient and should be retried. Saw this today running GenomicsDBImport. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: 503 Service Unavailable; Service Unavailable, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/d0aac891-4d32-43b9-8adb-edb6b5204af9/call-GatherVCFs/S76-1-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:443); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:425); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:349); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:747); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:174); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:193); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; Service Unavailable; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:332); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); 	at com.google.cloud.storage.Storage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749
https://github.com/broadinstitute/gatk/issues/2749:194,Availability,error,error,194,"503 (Service Unavailable) errors are transient and should be retried. Saw this today running GenomicsDBImport. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: 503 Service Unavailable; Service Unavailable, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/d0aac891-4d32-43b9-8adb-edb6b5204af9/call-GatherVCFs/S76-1-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:443); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:425); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:349); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:747); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:174); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:193); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; Service Unavailable; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:332); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); 	at com.google.cloud.storage.Storage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749
https://github.com/broadinstitute/gatk/pull/2750:7,Usability,simpl,simple,7,"A very simple and easy to review PR that backs off on retries a bit more, and retries for a bit longer. This may help when we're opening many cloud files in parallel.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2750
https://github.com/broadinstitute/gatk/issues/2753:647,Deployability,patch,patch,647,"Broad production is finding that after running HG38 GVCFs through `GenomicsDBImport`, `GenomicsDB`, and then `GenotypeGVCFs`, the sort order of the contigs in the header is scrambled. Specifically, the contig names appear to be sorted by ASCII value, so, eg., ""chr10"" comes before ""chr9"". . This is a problem, since there are tools that use the contig ordering in the header to generate contig indices used for comparison of relative contig ordering. `GatherVcfs` in Picard is one example. We need to identify which of the three possible suspects above (`GenomicsDBImport`, `GenomicsDB`, or `GenotypeGVCFs`) is clobbering the contig ordering, and patch it quickly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2753
https://github.com/broadinstitute/gatk/issues/2754:271,Usability,Intuit,Intuitively,271,"For a read with the following id line in the fastq:; `@HJYFJCCXX160204:4:1124:22424:1133/1 mapping=20:61724009;151M`; This method would return pretty much the whole descriptor except for the starting '@':; ` HJYFJCCXX160204:4:1124:22424:1133/1 mapping=20:61724009;151M`; Intuitively it should be only the first part, what an equivalent SAMRecord object getName() would return:; `HJYFJCCXX160204:4:1124:22424:1133`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2754
https://github.com/broadinstitute/gatk/issues/2756:263,Testability,test,tests,263,"There are some insignificant differences between the QUAL score in GATK3 and GATK4 that we haven't been able to explain. This also means that QD is also different at those sites. Here are some examples (note that this data is private and cannot be used in public tests):. Input Combined gvcf: `/humgen/gsa-hpprojects/dev/mshand/palantir/Analysis/535_GenotypeGVCFs/inputVCF/shard-314.vcf.gz`. Chr | Site | QUAL in GATK3 | QUAL in GATK4 | QD in GATK3 | QD in GATK4; ---|---|---|---|---|---; chr5 | 43191532 | 3233.06 | 3234.67 | 11.93 | 11.94; chr5 | 43298363 | 3233.06 | 3232.12 | 14.9 | 14.89. While these sites are clearly not significantly different, it would still be nice to know why these differences are occurring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2756
https://github.com/broadinstitute/gatk/issues/2756:616,Usability,clear,clearly,616,"There are some insignificant differences between the QUAL score in GATK3 and GATK4 that we haven't been able to explain. This also means that QD is also different at those sites. Here are some examples (note that this data is private and cannot be used in public tests):. Input Combined gvcf: `/humgen/gsa-hpprojects/dev/mshand/palantir/Analysis/535_GenotypeGVCFs/inputVCF/shard-314.vcf.gz`. Chr | Site | QUAL in GATK3 | QUAL in GATK4 | QD in GATK3 | QD in GATK4; ---|---|---|---|---|---; chr5 | 43191532 | 3233.06 | 3234.67 | 11.93 | 11.94; chr5 | 43298363 | 3233.06 | 3232.12 | 14.9 | 14.89. While these sites are clearly not significantly different, it would still be nice to know why these differences are occurring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2756
https://github.com/broadinstitute/gatk/issues/2757:549,Testability,test,tests,549,"There are insignificant differences between InbreedingCoeff when calculated in GATK3 and GATK4. I've tried to recreate these differences and haven't been able to find them when I run with -L over a single site. There is nothing random in the Inbreeding Coefficient calculation as far as I can tell, so I'm not sure why I haven't been able to recreate the difference. When I run on a single site I get the same value that is output by GATK4 in both GATK3 and GATK4. Here are some examples (note that this data is private and cannot be used in public tests):. Input Combined gvcf: `/humgen/gsa-hpprojects/dev/mshand/palantir/Analysis/535_GenotypeGVCFs/inputVCF/shard-314.vcf.gz`. Chr | Site | InbreedingCoeff in GATK3 | InbreedingCoeff in GATK4; ---|---|---|---; chr5 | 43645663 | 0.2240 | 0.2253 ; chr5 | 43645685 | 0.1697 | 0.1684",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2757
https://github.com/broadinstitute/gatk/pull/2759:6,Security,Validat,ValidateVariants,6,GATK4 ValidateVariants Doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2759
https://github.com/broadinstitute/gatk/pull/2767:141,Testability,test,test,141,"This is fixed by switching to the latest HTSJDK snapshot, which contains a; fix for VCFUtils.smartMergeHeaders() to preserve ordering. Add a test that fails without the new HTSJDK snapshot and passes with it. Resolves #2753",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2767
https://github.com/broadinstitute/gatk/issues/2768:349,Testability,test,test,349,"Merge the two repos into a single code base. There are a number of goals, some somewhat contradictory:; * maintain history of both repos; * minimize repo size by removing large files that were accidentally committed to gatk-protected; * merge git-lfs files; * migrate issues from gatk-protected to gatk; * merge travis builds; * maintain reasonable test runtimes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2768
https://github.com/broadinstitute/gatk/issues/2769:109,Availability,avail,available,109,Currently the html doc we build looks pretty ugly in the browser. We don't have the appropriate style sheets available locally. We should check those into the repo so that it can build a nice looking copy of the documentation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2769
https://github.com/broadinstitute/gatk/pull/2770:9,Availability,redundant,redundant,9,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:73,Deployability,integrat,integration,73,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:120,Deployability,integrat,integration,120,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:171,Deployability,integrat,integration,171,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:73,Integrability,integrat,integration,73,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:120,Integrability,integrat,integration,120,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:171,Integrability,integrat,integration,171,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:9,Safety,redund,redundant,9,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:95,Testability,test,tests,95,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:141,Testability,test,tests,141,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/pull/2770:183,Testability,test,tests,183,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770
https://github.com/broadinstitute/gatk/issues/2773:55,Testability,test,test,55,combine the two existing travis.ymls so we run all the test from both protected and public while producing the appropriate docker files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2773
https://github.com/broadinstitute/gatk/issues/2775:0,Deployability,update,update,0,update the readme of gatk so that it contains all necessary information from protected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2775
https://github.com/broadinstitute/gatk/issues/2777:22,Modifiability,rewrite,rewrite,22,this is our chance to rewrite gatk-protected history and un-commit the large files that were added by mistake,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2777
https://github.com/broadinstitute/gatk/pull/2778:56,Integrability,wrap,wrapper,56,Thank you for all the work on GATK4 and for including a wrapper script to help in setting up Java options. I've included GATK4 in bioconda (https://anaconda.org/bioconda/gatk4) with the `gatk-launch` wrapper and wanted a way to be able to pass java options to the local run. This PR uses the `GATK_JVM_OPTS` environmental variable to pass Java options like memory specification to the gatk-launch script. Thanks for considering this change,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2778
https://github.com/broadinstitute/gatk/pull/2778:200,Integrability,wrap,wrapper,200,Thank you for all the work on GATK4 and for including a wrapper script to help in setting up Java options. I've included GATK4 in bioconda (https://anaconda.org/bioconda/gatk4) with the `gatk-launch` wrapper and wanted a way to be able to pass java options to the local run. This PR uses the `GATK_JVM_OPTS` environmental variable to pass Java options like memory specification to the gatk-launch script. Thanks for considering this change,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2778
https://github.com/broadinstitute/gatk/pull/2778:322,Modifiability,variab,variable,322,Thank you for all the work on GATK4 and for including a wrapper script to help in setting up Java options. I've included GATK4 in bioconda (https://anaconda.org/bioconda/gatk4) with the `gatk-launch` wrapper and wanted a way to be able to pass java options to the local run. This PR uses the `GATK_JVM_OPTS` environmental variable to pass Java options like memory specification to the gatk-launch script. Thanks for considering this change,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2778
https://github.com/broadinstitute/gatk/pull/2781:53,Availability,avail,available,53,For GATK4 docs. Should add in ApplyVQSR webpage when available.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2781
https://github.com/broadinstitute/gatk/pull/2783:180,Modifiability,variab,variable,180,Options only used when running locally. Internally options are passed either by appending to java invocation directly with; command-line options or using the JAVA_OPTS environment variable. Also added environment variable printout with --dryRun; Other minor formatting / whitespace changes. resolves #2694,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2783
https://github.com/broadinstitute/gatk/pull/2783:213,Modifiability,variab,variable,213,Options only used when running locally. Internally options are passed either by appending to java invocation directly with; command-line options or using the JAVA_OPTS environment variable. Also added environment variable printout with --dryRun; Other minor formatting / whitespace changes. resolves #2694,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2783
https://github.com/broadinstitute/gatk/pull/2785:24,Deployability,update,update,24,Also remove imports and update comments. Metrics are saved using NIO so we don't need authHolder. This work is part of #2402.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2785
https://github.com/broadinstitute/gatk/pull/2791:77,Deployability,upgrade,upgrade,77,Fixes https://github.com/broadinstitute/gatk/issues/2744. Depends on Barclay upgrade.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2791
https://github.com/broadinstitute/gatk/pull/2791:58,Integrability,Depend,Depends,58,Fixes https://github.com/broadinstitute/gatk/issues/2744. Depends on Barclay upgrade.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2791
https://github.com/broadinstitute/gatk/issues/2793:87,Availability,failure,failure,87,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:208,Availability,error,error,208,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:938,Deployability,pipeline,pipelines,938,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:2044,Deployability,deploy,deploy,2044,TKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:2081,Deployability,deploy,deploy,2081,3); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.get,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:2154,Deployability,deploy,deploy,2154,neProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.getFilesMatching(NIOFileUtil.java:91); 	at org.seqdoop.hadoop_bam.ut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:2231,Deployability,deploy,deploy,2231,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.getFilesMatching(NIOFileUtil.java:91); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:61); 	at org.broadinstitute.he,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:2303,Deployability,deploy,deploy,2303, 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.getFilesMatching(NIOFileUtil.java:91); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:61); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:2373,Deployability,deploy,deploy,2373,MainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.getFilesMatching(NIOFileUtil.java:91); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:61); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.dat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:334,Testability,test,test,334,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:408,Testability,test,test-bucket,408,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:486,Testability,test,test-cluster,486,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:705,Testability,test,test-bucket,705,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:768,Testability,test,test-bucket,768,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2793:2468,Testability,test,test-bucket,2468,ne.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.getFilesMatching(NIOFileUtil.java:91); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:61); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.he,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793
https://github.com/broadinstitute/gatk/issues/2794:56,Deployability,update,updated,56,Our wiki has a bunch of outdated things which should be updated: ex: https://github.com/broadinstitute/gatk/wiki/Why-are-there-multiple-GATK-repositories%3F,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2794
https://github.com/broadinstitute/gatk/issues/2797:4069,Integrability,Message,MessageHub,4069,esting.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:4092,Integrability,Message,MessageHub,4092,esting.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:4138,Performance,concurren,concurrent,4138,esting.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:4246,Performance,concurren,concurrent,4246,esting.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:4331,Performance,concurren,concurrent,4331,esting.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:4416,Performance,concurren,concurrent,4416,esting.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:262,Testability,test,testAllAnnotations,262,"The annotation `StrandArtifact` explodes with an NPE if tumor lod isn't found. It should probably just not annotate if it's prerequisites aren't found. Alternatively, it could throw a helpful UserException. This was discovered by `VariantAnnotatorEngineUnitTest.testAllAnnotations`; ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.MathUtils.maxElementIndex(MathUtils.java:723); 	at org.broadinstitute.hellbender.tools.walkers.annotator.StrandArtifact.annotate(StrandArtifact.java:68); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateGenotypes(VariantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:867,Testability,test,testAllAnnotations,867,"The annotation `StrandArtifact` explodes with an NPE if tumor lod isn't found. It should probably just not annotate if it's prerequisites aren't found. Alternatively, it could throw a helpful UserException. This was discovered by `VariantAnnotatorEngineUnitTest.testAllAnnotations`; ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.MathUtils.maxElementIndex(MathUtils.java:723); 	at org.broadinstitute.hellbender.tools.walkers.annotator.StrandArtifact.annotate(StrandArtifact.java:68); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateGenotypes(VariantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1229,Testability,test,testng,1229,ariantAnnotatorEngineUnitTest.testAllAnnotations`; ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.MathUtils.maxElementIndex(MathUtils.java:723); 	at org.broadinstitute.hellbender.tools.walkers.annotator.StrandArtifact.annotate(StrandArtifact.java:68); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateGenotypes(VariantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1323,Testability,test,testng,1323,g.broadinstitute.hellbender.utils.MathUtils.maxElementIndex(MathUtils.java:723); 	at org.broadinstitute.hellbender.tools.walkers.annotator.StrandArtifact.annotate(StrandArtifact.java:68); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateGenotypes(VariantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1387,Testability,test,testng,1387,Utils.java:723); 	at org.broadinstitute.hellbender.tools.walkers.annotator.StrandArtifact.annotate(StrandArtifact.java:68); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateGenotypes(VariantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1455,Testability,test,testng,1455,otator.StrandArtifact.annotate(StrandArtifact.java:68); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateGenotypes(VariantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1525,Testability,test,testng,1525,nstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateGenotypes(VariantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1541,Testability,Test,TestMethodWorker,1541,ools.walkers.annotator.VariantAnnotatorEngine.annotateGenotypes(VariantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.Tes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1576,Testability,Test,TestMethodWorker,1576,iantAnnotatorEngine.annotateGenotypes(VariantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1612,Testability,test,testng,1612,riantAnnotatorEngine.java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1628,Testability,Test,TestMethodWorker,1628,java:188); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.intern,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1649,Testability,Test,TestMethodWorker,1649,broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.Su,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1685,Testability,test,testng,1685,alkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1692,Testability,Test,TestRunner,1692,notator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1714,Testability,Test,TestRunner,1714,notatorEngine.annotateContext(VariantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1744,Testability,test,testng,1744,iantAnnotatorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAcc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1751,Testability,Test,TestRunner,1751,atorEngine.java:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1766,Testability,Test,TestRunner,1766,va:155); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Nati,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1796,Testability,test,testng,1796,e.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1854,Testability,test,testng,1854,eUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:224); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1920,Testability,test,testng,1920,24); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delegat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:1981,Testability,test,testng,1981,Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2035,Testability,test,testng,2035,ke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2105,Testability,test,testng,2105,AccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.g,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2170,Testability,test,testng,2170,va.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2177,Testability,Test,TestNG,2177,.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2206,Testability,Test,TestNG,2206,voke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.g,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2233,Testability,test,testng,2233,internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2240,Testability,Test,TestNG,2240,l.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2264,Testability,Test,TestNG,2264,onHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2291,Testability,test,testng,2291,ionHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2298,Testability,Test,TestNG,2298,er.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2315,Testability,Test,TestNG,2315,; 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
