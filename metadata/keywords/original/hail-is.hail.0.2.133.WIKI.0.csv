id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://hail.is/gethelp.html:159,Energy Efficiency,efficient,efficient,159,"﻿. Hail | Get Help . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. ",MatchSource.WIKI,gethelp.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/gethelp.html
https://hail.is/gethelp.html:230,Performance,load,loaded,230,"﻿. Hail | Get Help . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. ",MatchSource.WIKI,gethelp.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/gethelp.html
https://hail.is/gethelp.html:658,Performance,scalab,scalable,658,"﻿. Hail | Get Help . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. ",MatchSource.WIKI,gethelp.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/gethelp.html
https://hail.is/index-wcopy.html:17786,Deployability,install,install,17786,ariant_indices_EAS.rst.txt; panukb_ld_variant_indices_EUR.rst.txt; panukb_ld_variant_indices_MID.rst.txt; panukb_meta_analysis_all_ancestries.rst.txt; panukb_meta_analysis_high_quality.rst.txt; panukb_summary_stats.rst.txt; UK_Biobank_Rapid_GWAS_both_sexes.rst.txt; UK_Biobank_Rapid_GWAS_female.rst.txt; UK_Biobank_Rapid_GWAS_male.rst.txt. schemas.rst.txt. /experimental; ; hail.experimental.DB.rst.txt; index.rst.txt; ldscsim.rst.txt. /functions; ; collections.rst.txt; constructors.rst.txt; core.rst.txt; genetics.rst.txt; hail.expr.builders.CaseBuilder.rst.txt; hail.expr.builders.SwitchBuilder.rst.txt; index.rst.txt; numeric.rst.txt; random.rst.txt; stats.rst.txt; string.rst.txt. /genetics; ; hail.genetics.AlleleType.rst.txt; hail.genetics.Call.rst.txt; hail.genetics.Locus.rst.txt; hail.genetics.Pedigree.rst.txt; hail.genetics.ReferenceGenome.rst.txt; hail.genetics.Trio.rst.txt; index.rst.txt. /ggplot; ; index.rst.txt. /guides; ; agg.rst.txt; annotation.rst.txt; genetics.rst.txt. /install; ; azure.rst.txt; dataproc.rst.txt; linux.rst.txt; macosx.rst.txt; other-cluster.rst.txt; try.rst.txt. /linalg; . /utils; ; index.rst.txt. hail.linalg.BlockMatrix.rst.txt; index.rst.txt. /methods; ; genetics.rst.txt; impex.rst.txt; index.rst.txt; misc.rst.txt; relatedness.rst.txt; stats.rst.txt. /nd; ; index.rst.txt. /overview; ; expressions.rst.txt; index.rst.txt; matrix_table.rst.txt; table.rst.txt. /stats; ; hail.stats.LinearMixedModel.rst.txt; index.rst.txt. /tutorials; ; 01-genome-wide-association-study.ipynb.txt; 03-tables.ipynb.txt; 04-aggregation.ipynb.txt; 05-filter-annotate.ipynb.txt; 06-joins.ipynb.txt; 07-matrixtable.ipynb.txt; 08-plotting.ipynb.txt; 09-ggplot.ipynb.txt. /utils; ; index.rst.txt. /vds; ; hail.vds.combiner.load_combiner.rst.txt; hail.vds.combiner.new_combiner.rst.txt; hail.vds.combiner.VariantDatasetCombiner.rst.txt; hail.vds.combiner.VDSMetadata.rst.txt; hail.vds.filter_chromosomes.rst.txt; hail.vds.filter_intervals.rst.txt; hail.vds.filter_samples.rst.txt; ,MatchSource.WIKI,index-wcopy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index-wcopy.html
https://hail.is/index-wcopy.html:21628,Deployability,toggle,toggle,21628,.rst.txt; scans.rst.txt; tutorials-landing.rst.txt; types.rst.txt. /_static; . /annotationdb; ; annotationdb.css; annotationdb.js. /cheatsheets; ; hail_matrix_tables_cheat_sheet.pdf; hail_tables_cheat_sheet.pdf. /css; . /fonts; ; fontawesome-webfont.eot; fontawesome-webfont-1.eot; fontawesome-webfont.svg; fontawesome-webfont.ttf; fontawesome-webfont.woff; fontawesome-webfont.woff2; lato-bold.woff; lato-bold.woff2; lato-bold-italic.woff; lato-bold-italic.woff2; lato-normal.woff; lato-normal.woff2; lato-normal-italic.woff; lato-normal-italic.woff2; Roboto-Slab-Bold.woff; Roboto-Slab-Bold.woff2; Roboto-Slab-Regular.woff; Roboto-Slab-Regular.woff2. theme.css. /datasets; ; datasets.js. /js; ; theme.js. _sphinx_javascript_frameworks_compat.js; auto-render.min.js; doctools.js; documentation_options.js; goto.js; hail_version.js; jquery.js; katex.min.js; katex_autorenderer.js; katex-math.css; LeveneHaldane.pdf; nbsphinx-code-cells.css; pygments.css; rtd_modifications.css; sphinx_highlight.js; toggle.js. Hail | Aggregators; Hail | Annotation Database; Hail | Hail Query Python API; Hail | hailtop.batch Python API; Hail | Change Log And Version Policy; Hail | Cheat Sheets. /cloud; ; Hail | Amazon Web Services; Hail | Microsoft Azure; Hail | Databricks; Hail | General Advice; Hail | Google Cloud Platform; Hail | Hail Query-on-Batch. Hail | Configuration Reference. /datasets; . /schemas; ; Hail | 1000_Genomes_autosomes; Hail | 1000_Genomes_chrMT; Hail | 1000_Genomes_chrX; Hail | 1000_Genomes_chrY; Hail | 1000_Genomes_HighCov_autosomes; Hail | 1000_Genomes_HighCov_chrX; Hail | 1000_Genomes_HighCov_chrY; Hail | 1000_Genomes_Retracted_autosomes; Hail | 1000_Genomes_Retracted_chrX; Hail | 1000_Genomes_Retracted_chrY; Hail | CADD; Hail | clinvar_gene_summary; Hail | clinvar_variant_summary; Hail | DANN; Hail | dbNSFP_genes; Hail | dbNSFP_variants; Hail | dbSNP; Hail | dbSNP_rsid; Hail | Ensembl_homo_sapiens_low_complexity_regions; Hail | Ensembl_homo_sapiens_reference_genome; Hail | ge,MatchSource.WIKI,index-wcopy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index-wcopy.html
https://hail.is/index-wcopy.html:33391,Deployability,install,install,33391,Installing Hail; Hail | For Software Developers. /ggplot; ; Hail | Plotting With hail.ggplot Overview. /guides; ; Hail | Aggregation; Hail | Annotation; Hail | Genetics. Hail | How-To Guides; Hail | Hadoop Glob Patterns; Hail | ArrayExpression; Hail | ArrayNumericExpression; Hail | BooleanExpression; Hail | CallExpression; Hail | CollectionExpression; Hail | DictExpression. Hail | Expression. Hail | Expression; Hail | Expression. Hail | Float32Expression; Hail | Float64Expression; Hail | Int32Expression; Hail | Int64Expression; Hail | IntervalExpression; Hail | LocusExpression; Hail | NDArrayExpression; Hail | NDArrayNumericExpression; Hail | NumericExpression; Hail | SetExpression; Hail | StringExpression; Hail | StructExpression; Hail | TupleExpression; Hail | GroupedMatrixTable; Hail | GroupedTable; Hail | MatrixTable; Hail | Table; Hail | Hail on the Cloud; Hail | Hail 0.2. /install; ; Hail | Use Hail on Azure HDInsight; Hail | Use Hail on Google Dataproc; Hail | Install Hail on GNU/Linux; Hail | Install Hail on Mac OS X; Hail | Install Hail on a Spark Cluster; Hail | Your First Hail Query. Hail | Libraries. /linalg; . /utils; ; Hail | linalg/utils. Hail | BlockMatrix; Hail | linalg. /methods; ; Hail | Genetics; Hail | Import / Export; Hail | Methods; Hail | Miscellaneous; Hail | Relatedness; Hail | Statistics. /nd; ; Hail | nd. Hail | Other Resources. /overview; ; Hail | Expressions Overview; Hail | Hail Overview. Hail | MatrixTable Overview. Hail | MatrixTable Overview; Hail | MatrixTable Overview. Hail | MatrixTable Overview; Hail | Table Overview. Hail | Plot; Hail | Python API; Hail | Scans; Hail | Search. /stats; ; Hail | LinearMixedModel; Hail | stats. /tutorials; . /iframe_figures; ; figure_11.html; figure_12.html; figure_13.html; figure_15.html; figure_16.html; figure_17.html; figure_18.html; figure_3.html; figure_4.html; figure_5.html; figure_6.html; figure_7.html; figure_8.html. Hail | GWAS Tutorial; Hail | Table Tutorial; Hail | Aggregation Tutorial; ,MatchSource.WIKI,index-wcopy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index-wcopy.html
https://hail.is/index-wcopy.html:17724,Usability,guid,guides,17724,AMR.rst.txt; panukb_ld_variant_indices_CSA.rst.txt; panukb_ld_variant_indices_EAS.rst.txt; panukb_ld_variant_indices_EUR.rst.txt; panukb_ld_variant_indices_MID.rst.txt; panukb_meta_analysis_all_ancestries.rst.txt; panukb_meta_analysis_high_quality.rst.txt; panukb_summary_stats.rst.txt; UK_Biobank_Rapid_GWAS_both_sexes.rst.txt; UK_Biobank_Rapid_GWAS_female.rst.txt; UK_Biobank_Rapid_GWAS_male.rst.txt. schemas.rst.txt. /experimental; ; hail.experimental.DB.rst.txt; index.rst.txt; ldscsim.rst.txt. /functions; ; collections.rst.txt; constructors.rst.txt; core.rst.txt; genetics.rst.txt; hail.expr.builders.CaseBuilder.rst.txt; hail.expr.builders.SwitchBuilder.rst.txt; index.rst.txt; numeric.rst.txt; random.rst.txt; stats.rst.txt; string.rst.txt. /genetics; ; hail.genetics.AlleleType.rst.txt; hail.genetics.Call.rst.txt; hail.genetics.Locus.rst.txt; hail.genetics.Pedigree.rst.txt; hail.genetics.ReferenceGenome.rst.txt; hail.genetics.Trio.rst.txt; index.rst.txt. /ggplot; ; index.rst.txt. /guides; ; agg.rst.txt; annotation.rst.txt; genetics.rst.txt. /install; ; azure.rst.txt; dataproc.rst.txt; linux.rst.txt; macosx.rst.txt; other-cluster.rst.txt; try.rst.txt. /linalg; . /utils; ; index.rst.txt. hail.linalg.BlockMatrix.rst.txt; index.rst.txt. /methods; ; genetics.rst.txt; impex.rst.txt; index.rst.txt; misc.rst.txt; relatedness.rst.txt; stats.rst.txt. /nd; ; index.rst.txt. /overview; ; expressions.rst.txt; index.rst.txt; matrix_table.rst.txt; table.rst.txt. /stats; ; hail.stats.LinearMixedModel.rst.txt; index.rst.txt. /tutorials; ; 01-genome-wide-association-study.ipynb.txt; 03-tables.ipynb.txt; 04-aggregation.ipynb.txt; 05-filter-annotate.ipynb.txt; 06-joins.ipynb.txt; 07-matrixtable.ipynb.txt; 08-plotting.ipynb.txt; 09-ggplot.ipynb.txt. /utils; ; index.rst.txt. /vds; ; hail.vds.combiner.load_combiner.rst.txt; hail.vds.combiner.new_combiner.rst.txt; hail.vds.combiner.VariantDatasetCombiner.rst.txt; hail.vds.combiner.VDSMetadata.rst.txt; hail.vds.filter_chromosomes.rst.txt; hail,MatchSource.WIKI,index-wcopy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index-wcopy.html
https://hail.is/index-wcopy.html:19642,Usability,guid,guides,19642,t.txt; hail.vds.combiner.VDSMetadata.rst.txt; hail.vds.filter_chromosomes.rst.txt; hail.vds.filter_intervals.rst.txt; hail.vds.filter_samples.rst.txt; hail.vds.filter_variants.rst.txt; hail.vds.impute_sex_chr_ploidy_from_interval_coverage.rst.txt; hail.vds.impute_sex_chromosome_ploidy.rst.txt; hail.vds.interval_coverage.rst.txt; hail.vds.lgt_to_gt.rst.txt; hail.vds.local_to_global.rst.txt; hail.vds.merge_reference_blocks.rst.txt; hail.vds.read_vds.rst.txt; hail.vds.sample_qc.rst.txt; hail.vds.split_multi.rst.txt; hail.vds.store_ref_block_max_length.rst.txt; hail.vds.to_dense_mt.rst.txt; hail.vds.to_merged_sparse_mt.rst.txt; hail.vds.truncate_reference_blocks.rst.txt; hail.vds.VariantDataset.rst.txt; index.rst.txt. aggregators.rst.txt; annotation_database_ui.rst.txt; api.rst.txt; batch_api.rst.txt; change_log.rst.txt; cheatsheets.rst.txt; configuration_reference.rst.txt; datasets.rst.txt; expressions.rst.txt; fs_api.rst.txt; getting_started.rst.txt; getting_started_developing.rst.txt; guides.rst.txt; hadoop_glob_patterns.rst.txt; hail.expr.ArrayExpression.rst.txt; hail.expr.ArrayNumericExpression.rst.txt; hail.expr.BooleanExpression.rst.txt; hail.expr.CallExpression.rst.txt; hail.expr.CollectionExpression.rst.txt; hail.expr.DictExpression.rst.txt; hail.expr.Expression.rst.txt; hail.expr.Float32Expression.rst.txt; hail.expr.Float64Expression.rst.txt; hail.expr.Int32Expression.rst.txt; hail.expr.Int64Expression.rst.txt; hail.expr.IntervalExpression.rst.txt; hail.expr.LocusExpression.rst.txt; hail.expr.NDArrayExpression.rst.txt; hail.expr.NDArrayNumericExpression.rst.txt; hail.expr.NumericExpression.rst.txt; hail.expr.SetExpression.rst.txt; hail.expr.StringExpression.rst.txt; hail.expr.StructExpression.rst.txt; hail.expr.TupleExpression.rst.txt; hail.GroupedMatrixTable.rst.txt; hail.GroupedTable.rst.txt; hail.MatrixTable.rst.txt; hail.Table.rst.txt; hail_on_the_cloud.rst.txt; index.rst.txt; libraries.rst.txt; other_resources.rst.txt; plot.rst.txt; root_api.rst.txt; scan,MatchSource.WIKI,index-wcopy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index-wcopy.html
https://hail.is/index-wcopy.html:32603,Usability,guid,guides,32603,; Hail | panukb_ld_variant_indices_MID; Hail | panukb_meta_analysis_all_ancestries; Hail | panukb_meta_analysis_high_quality; Hail | panukb_summary_stats; Hail | UK_Biobank_Rapid_GWAS_both_sexes; Hail | UK_Biobank_Rapid_GWAS_female; Hail | UK_Biobank_Rapid_GWAS_male. Hail | Schemas. Hail | Datasets. /experimental; ; Hail | DB; Hail | Experimental; Hail | ldscsim. Hail | Expressions; Hail | hailtop.fs Python API. /functions; ; Hail | Collection functions; Hail | Constructor functions; Hail | Core language functions; Hail | Genetics functions; Hail | CaseBuilder; Hail | SwitchBuilder; Hail | Functions; Hail | Numeric functions; Hail | Random functions; Hail | Statistical functions; Hail | String functions. /genetics; ; Hail | AlleleType; Hail | Call; Hail | Locus; Hail | Pedigree; Hail | ReferenceGenome; Hail | Trio; Hail | genetics. Hail | Index; Hail | Installing Hail; Hail | For Software Developers. /ggplot; ; Hail | Plotting With hail.ggplot Overview. /guides; ; Hail | Aggregation; Hail | Annotation; Hail | Genetics. Hail | How-To Guides; Hail | Hadoop Glob Patterns; Hail | ArrayExpression; Hail | ArrayNumericExpression; Hail | BooleanExpression; Hail | CallExpression; Hail | CollectionExpression; Hail | DictExpression. Hail | Expression. Hail | Expression; Hail | Expression. Hail | Float32Expression; Hail | Float64Expression; Hail | Int32Expression; Hail | Int64Expression; Hail | IntervalExpression; Hail | LocusExpression; Hail | NDArrayExpression; Hail | NDArrayNumericExpression; Hail | NumericExpression; Hail | SetExpression; Hail | StringExpression; Hail | StructExpression; Hail | TupleExpression; Hail | GroupedMatrixTable; Hail | GroupedTable; Hail | MatrixTable; Hail | Table; Hail | Hail on the Cloud; Hail | Hail 0.2. /install; ; Hail | Use Hail on Azure HDInsight; Hail | Use Hail on Google Dataproc; Hail | Install Hail on GNU/Linux; Hail | Install Hail on Mac OS X; Hail | Install Hail on a Spark Cluster; Hail | Your First Hail Query. Hail | Libraries. /linal,MatchSource.WIKI,index-wcopy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index-wcopy.html
https://hail.is/index.html:787,Deployability,install,install,787,"﻿. Hail | Index . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scala",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:922,Deployability,install,installed,922,"﻿. Hail | Index . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scala",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:1981,Deployability,integrat,integrated,1981,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:1009,Energy Efficiency,power,powerful,1009,"Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis plat",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:1638,Energy Efficiency,power,powerful,1638,"mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources o",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:2299,Energy Efficiency,efficient,efficient,2299," Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose str",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:1981,Integrability,integrat,integrated,1981,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:2498,Integrability,depend,dependencies,2498,"a combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose strategic advice and generous; philanthropy have been essential for growing the impact of Hail.; . We are grateful for generous ",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:1888,Performance,scalab,scalable,1888,"++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowled",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:1996,Performance,scalab,scalable,1996,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:2636,Performance,queue,queueing,2636,"es of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose strategic advice and generous; philanthropy have been essential for growing the impact of Hail.; . We are grateful for generous support from:. The National Institute of Diabetes and Digestive and Kidney; Diseases; ; The National Institute of Mental Health; The National Human Genome Research Institute. We are grateful for generous past support from:. The ",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/index.html:2708,Performance,queue,queue,2708,"es of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose strategic advice and generous; philanthropy have been essential for growing the impact of Hail.; . We are grateful for generous support from:. The National Institute of Diabetes and Digestive and Kidney; Diseases; ; The National Institute of Mental Health; The National Human Genome Research Institute. We are grateful for generous past support from:. The ",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
https://hail.is/references.html:308,Deployability,install,installed,308,"﻿. Hail | References . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226–237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336–347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333–342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leońska-Duniec, A., Borczyk, M., Korostyński, M. et al. Genetic variants in myostatin; 	 and its receptors promote elite athlete status. BMC Genomics 2",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:713,Deployability,update,updated,713,"﻿. Hail | References . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226–237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336–347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333–342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leońska-Duniec, A., Borczyk, M., Korostyński, M. et al. Genetic variants in myostatin; 	 and its receptors promote elite athlete status. BMC Genomics 2",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:9778,Deployability,integrat,integrate,9778,"pulation; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al. “Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services”, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/ar",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:9398,Energy Efficiency,power,power,9398,"6-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis converge to implicate negative selection and neuronal processes in the; 	 aetiology of schizophrenia. Mol Psychiatry 27, 3699–3707; 	 (2022). https://doi.org/10.1038/s41380-022-01621-8 https://www.nature.com/articles/s41380-022-01621-8. 	 Mitja, K.I., et al. FinnGen: Unique genetic insights from combining isolated population; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders wit",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:9778,Integrability,integrat,integrate,9778,"pulation; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al. “Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services”, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/ar",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:12210,Integrability,depend,dependence,12210,"ctive protein with PTSD, traumatic events, and social support. Neuropsychopharmacol. (2020). https://doi.org/10.1038/s41386-020-0655-6 https://www.nature.com/articles/s41386-020-0655-6#citeas. 2019. Farhan, Sali MK, et al. “Exome sequencing in amyotrophic lateral sclerosis implicates a novel gene, DNAJC7, encoding a heat-shock protein” Nature Neuroscience (2019): 307835. https://www.nature.com/articles/s41593-019-0530-0; Gay, Nicole R. et al. “Impact of admixture and ancestry on eQTL analysis and GWAS colocalization in GTEx” bioRxiv (2019) 836825; https://www.biorxiv.org/content/10.1101/836825v1; Sakaue, Saori et al. “Trans-biobank analysis with 676,000 individuals elucidates the association of polygenic risk scores of complex traits with human lifespan” bioRxiv (2019): 856351 https://www.biorxiv.org/content/10.1101/856351v1; Polimanti, Renato et al. “Leveraging genome-wide data to investigate differences between opioid use vs. opioid dependence in 41,176 individuals from the Psychiatric Genomics Consortium” bioRxiv (2019): 765065 https://www.biorxiv.org/content/10.1101/765065v1; Lescai, Francesco et al. “Meta-analysis of Scandinavian Schizophrenia Exomes” bioRxiv (2019): 836957; https://www.biorxiv.org/content/10.1101/836957v2; Bolze, Alexandre, et al. “Selective constraints and pathogenicity of mitochondrial DNA variants inferred from a novel database of 196,554 unrelated individuals” bioRxiv (2019): 798264;https://www.biorxiv.org/content/10.1101/798264v1; De Lillo, A., De Angelis, F., Di Girolamo, M. et al. “Phenome-wide association study of TTR and RBP4 genes in 361,194 individuals reveals novel insights in the genetics of hereditary and wildtype transthyretin amyloidoses.” Hum Genet 138, 1331–1340 (2019). https://www.ncbi.nlm.nih.gov/pubmed/31659433; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies.” The Lancet Respiratory Medicine 7.6 (2019): 509-522. https",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:18158,Integrability,mediat,mediatum,18158,"ibutions to variation in human stature in prehistoric Europe.” bioRxiv (2019): 690545. https://www.biorxiv.org/content/10.1101/690545v1.abstract; Abrar, Faheem. A Modular Parallel Pipeline Architecture for GWAS Applications in a Cluster Environment. Diss. University of Saskatchewan, 2019. https://harvest.usask.ca/handle/10388/12087; Khera, Amit V., et al. “Whole-genome sequencing to characterize monogenic and polygenic contributions in patients hospitalized with early-onset myocardial infarction.” Circulation 139.13 (2019): 1593-1602. https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.035658. 2018. An, Joon-Yong, et al. “Genome-wide de novo risk score implicates promoter variation in autism spectrum disorder.” Science (2018): 1. https://science.sciencemag.org/content/362/6420/eaat6576.full; Molnos, Sophie Claudia. Metabolites: implications in type 2 diabetes and the effect of epigenome-wide interaction with genetic variation. Diss. Technische Universität München, 2018. https://mediatum.ub.tum.de/1372795f; Bis, Joshua C., et al. “Whole exome sequencing study identifies novel rare and common Alzheimer’s-associated variants involved in immune response and transcriptional regulation.” Molecular Psychiatry (2018): 1. https://www.nature.com/articles/s41380-018-0112-7; Gormley, Padhraig, et al. “Common variant burden contributes to the familial aggregation of migraine in 1,589 families.” Neuron 98.4 (2018): 743-753. https://www.ncbi.nlm.nih.gov/pubmed/30189203; Rivas, Manuel A., et al. “Insights into the genetic epidemiology of Crohn’s and rare diseases in the Ashkenazi Jewish population.” PLoS Genetics 14.5 (2018): e1007329. https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007329; Satterstrom, F. Kyle, et al. “ASD and ADHD have a similar burden of rare protein-truncating variants.” bioRxiv (2018): 277707. https://www.biorxiv.org/content/10.1101/277707v1; Zekavat, Seyedeh M., et al. “Deep coverage whole genome sequences and plasma lipoprotein",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:1132,Safety,risk,risk,1132," Science. Blog. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226–237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336–347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333–342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leońska-Duniec, A., Borczyk, M., Korostyński, M. et al. Genetic variants in myostatin; 	 and its receptors promote elite athlete status. BMC Genomics 24, 761; 	 (2023). https://doi.org/10.1186/s12864-023-09869-2 https://link.springer.com/article/1",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:2989,Safety,risk,risk,2989," 24, 761; 	 (2023). https://doi.org/10.1186/s12864-023-09869-2 https://link.springer.com/article/10.1186/s12864-023-09869-2. 	 Chen, S., Francioli, L.C., Goodrich, J.K. et al. A genomic mutational constraint map; 	 using variation in 76,156 human genomes. Nature 625, 92–100; 	 (2024). https://doi.org/10.1038/s41586-023-06045-0 https://www.nature.com/articles/s41586-023-06045-0. 	 Mosca, M.J., Cho, H. Reconstruction of private genomes through reference-based genotype; 	 imputation. Genome Biol 24, 271; 	 (2023). https://doi.org/10.1186/s13059-023-03105-6 https://link.springer.com/article/10.1186/s13059-023-03105-6. 	 Stöberl, N., Donaldson, J., Binda, C.S. et al. Mutant huntingtin confers cell-autonomous; 	 phenotypes on Huntington’s disease iPSC-derived microglia. Sci Rep 13, 20477; 	 (2023). https://doi.org/10.1038/s41598-023-46852-z https://www.nature.com/articles/s41598-023-46852-z. 	 Tamman, A.J.F., Koller, D., Nagamatsu, S. et al. Psychosocial moderators of polygenic; 	 risk scores of inflammatory biomarkers in relation to GrimAge. Neuropsychopharmacol. 49,; 	 699–708; 	 (2024). https://doi.org/10.1038/s41386-023-01747-5 https://www.nature.com/articles/s41386-023-01747-5. 	 Mignogna, G., Carey, C.E., Wedow, R. et al. Patterns of item nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 1371–1387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Al-Jumaan, M., Chu, H., Alsulaiman, A. et al. Interplay of Mendelian and polygenic risk; 	 factors in Arab breast cancer patients. Genome Med 15, 65; 	 (2023). https://doi.org/10.1186/s13073-023-01220-4 https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01220-4. 	 Ilves N, Pajusalu S, Kahre T, et al. High Prevalence of Collagenopathies in Preterm- and; 	 Term-Born Children With Periventricular Venous Hemorrhagic Infarction. Journal of Child; 	 Neurology. 2023;38(6-7):373-388. doi:10",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:3578,Safety,risk,risk,3578,"om/article/10.1186/s13059-023-03105-6. 	 Stöberl, N., Donaldson, J., Binda, C.S. et al. Mutant huntingtin confers cell-autonomous; 	 phenotypes on Huntington’s disease iPSC-derived microglia. Sci Rep 13, 20477; 	 (2023). https://doi.org/10.1038/s41598-023-46852-z https://www.nature.com/articles/s41598-023-46852-z. 	 Tamman, A.J.F., Koller, D., Nagamatsu, S. et al. Psychosocial moderators of polygenic; 	 risk scores of inflammatory biomarkers in relation to GrimAge. Neuropsychopharmacol. 49,; 	 699–708; 	 (2024). https://doi.org/10.1038/s41386-023-01747-5 https://www.nature.com/articles/s41386-023-01747-5. 	 Mignogna, G., Carey, C.E., Wedow, R. et al. Patterns of item nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 1371–1387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Al-Jumaan, M., Chu, H., Alsulaiman, A. et al. Interplay of Mendelian and polygenic risk; 	 factors in Arab breast cancer patients. Genome Med 15, 65; 	 (2023). https://doi.org/10.1186/s13073-023-01220-4 https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01220-4. 	 Ilves N, Pajusalu S, Kahre T, et al. High Prevalence of Collagenopathies in Preterm- and; 	 Term-Born Children With Periventricular Venous Hemorrhagic Infarction. Journal of Child; 	 Neurology. 2023;38(6-7):373-388. doi:10.1177/08830738231186233. https://journals.sagepub.com/doi/full/10.1177/08830738231186233. 	 Mignogna, G., Carey, C.E., Wedow, R. et al. Patterns of item nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 1371–1387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Josefine U Melchiorsen, Kimmie V Sørensen, Jette Bork-Jensen, Hüsün S Kizilkaya, Lærke S; 	 Gasbjerg, Alexander S Hauser, Jørgen Rungby, Henrik T Sørensen, Allan Vaag, Jens S; 	 Nielsen, Oluf P",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:5415,Safety,risk,risk,5415,"üsün S Kizilkaya, Lærke S; 	 Gasbjerg, Alexander S Hauser, Jørgen Rungby, Henrik T Sørensen, Allan Vaag, Jens S; 	 Nielsen, Oluf Pedersen, Allan Linneberg, Bolette Hartmann, Anette P Gjesing, Jens J; 	 Holst, Torben Hansen, Mette M Rosenkilde, Niels Grarup, Rare Heterozygous; 	 Loss-of-Function Variants in the Human GLP-1 Receptor Are Not Associated With; 	 Cardiometabolic Phenotypes, The Journal of Clinical Endocrinology & Metabolism, Volume; 	 108, Issue 11, November 2023, Pages; 	 2821–2833, https://doi.org/10.1210/clinem/dgad290. https://academic.oup.com/jcem/article/108/11/2821/7180819. 	 Vukadinovic, Milos et al. Deep learning-enabled analysis of medical images identifies; 	 cardiac sphericity as an early marker of cardiomyopathy and related outcomes. Med,; 	 Volume 4, Issue 4, 252 - 262.e3. https://www.cell.com/med/fulltext/S2666-6340(23)00069-7. 	 Epi25 Collaborative; Chen S, Neale BM, Berkovic SF. Shared and distinct ultra-rare; 	 genetic risk for diverse epilepsies: A whole-exome sequencing study of 54,423; 	 individuals across multiple genetic ancestries. medRxiv [Preprint]. 2023 Feb; 	 24:2023.02.22.23286310. doi: 10.1101/2023.02.22.23286310. PMID: 36865150; PMCID:; 	 PMC9980234. https://pubmed.ncbi.nlm.nih.gov/36865150/. 	 Kurki, M.I., Karjalainen, J., Palta, P. et al. FinnGen provides genetic insights from a; 	 well-phenotyped isolated population. Nature 613, 508–518; 	 (2023). https://doi.org/10.1038/s41586-022-05473-8 https://www.nature.com/articles/s41586-022-05473-8. 	 Mortensen, Ó., Thomsen, E., Lydersen, L.N. et al. FarGen: Elucidating the distribution; 	 of coding variants in the isolated population of the Faroe Islands. Eur J Hum Genet 31,; 	 329–337; 	 (2023). https://doi.org/10.1038/s41431-022-01227-2 https://www.nature.com/articles/s41431-022-01227-2. 	 Steiner, H.E., Carrion, K.C., Giles, J.B., Lima, A.R., Yee, K., Sun, X., Cavallari,; 	 L.H., Perera, M.A., Duconge, J. and Karnes, J.H. (2023), Local Ancestry-Informed; 	 Candidate Pathway Ana",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:7525,Safety,predict,prediction,7525,"ino Populations. Clin Pharmacol; 	 Ther, 113:; 	 680-691. https://doi.org/10.1002/cpt.2787 https://ascpt.onlinelibrary.wiley.com/doi/full/10.1002/cpt.2787. 2022. 	 Huang, J., Tao, Q., Ang, T.F.A. et al. The impact of increasing levels of blood; 	 C-reactive protein on the inflammatory loci SPI1 and CD33 in Alzheimer’s disease. Transl; 	 Psychiatry 12, 523; 	 (2022). https://doi.org/10.1038/s41398-022-02281-6 https://www.nature.com/articles/s41398-022-02281-6. Wadon, M.E., Fenner, E., Kendall, K.M. et al. Clinical and genotypic analysis in; 	 determining dystonia non-motor phenotypic heterogeneity: a UK Biobank study. J Neurol; 	 269, 6436–6451 (2022). https://doi.org/10.1007/s00415-022-11307-4 https://link.springer.com/article/10.1007/s00415-022-11307-4. 	 Andi Madihah Manggabarani, Takuyu Hashiguchi, Masatsugu Hashiguchi, Atsushi Hayashi,; 	 Masataka Kikuchi, Yusdar Mustamin, Masaru Bamba, Kunihiro Kodama, Takanari Tanabata,; 	 Sachiko Isobe, Hidenori Tanaka, Ryo Akashi, Akihiro Nakaya, Shusei Sato, Construction of; 	 prediction models for growth traits of soybean cultivars based on phenotyping in diverse; 	 genotype and environment combinations, DNA Research, Volume 29, Issue 4, August 2022,; 	 dsac024, https://doi.org/10.1093/dnares/dsac024 https://academic.oup.com/dnaresearch/article/29/4/dsac024/6653298?login=false. 	 Chaffin, M., Papangeli, I., Simonson, B. et al. Single-nucleus profiling of human; 	 dilated and hypertrophic cardiomyopathy. Nature 608, 174–180; 	 (2022). https://doi.org/10.1038/s41586-022-04817-8 https://www.nature.com/articles/s41586-022-04817-8. 	 Lee, J., Lee, J., Jeon, S. et al. A database of 5305 healthy Korean individuals reveals; 	 genetic and clinical implications for an East Asian population. Exp Mol Med 54,; 	 1862–1871; 	 (2022). https://doi.org/10.1038/s12276-022-00871-4 https://www.nature.com/articles/s12276-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis conv",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:10104,Safety,risk,risk,10104,"Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al. “Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services”, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/article/doi/10.1093/jamia/ocaa068/5876972; Karaca M, Atceken N, Karaca Ş, Civelek E, Şekerel BE, Polimanti R. “Phenotypic and Molecular Characterization of Risk Loci Associated With Asthma and Lung Function” Allergy Asthma Immunol Res. (2020) 12(5):806-820. https://doi.org/10.4168/aair.2020.12.5.806 https://e-aair.o",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:11975,Safety,risk,risk,11975,"ciated With Asthma and Lung Function” Allergy Asthma Immunol Res. (2020) 12(5):806-820. https://doi.org/10.4168/aair.2020.12.5.806 https://e-aair.org/DOIx.php?id=10.4168/aair.2020.12.5.806; Muniz Carvalho, C., Wendt, F.R., Maihofer, A.X. et al. Dissecting the genetic association of C-reactive protein with PTSD, traumatic events, and social support. Neuropsychopharmacol. (2020). https://doi.org/10.1038/s41386-020-0655-6 https://www.nature.com/articles/s41386-020-0655-6#citeas. 2019. Farhan, Sali MK, et al. “Exome sequencing in amyotrophic lateral sclerosis implicates a novel gene, DNAJC7, encoding a heat-shock protein” Nature Neuroscience (2019): 307835. https://www.nature.com/articles/s41593-019-0530-0; Gay, Nicole R. et al. “Impact of admixture and ancestry on eQTL analysis and GWAS colocalization in GTEx” bioRxiv (2019) 836825; https://www.biorxiv.org/content/10.1101/836825v1; Sakaue, Saori et al. “Trans-biobank analysis with 676,000 individuals elucidates the association of polygenic risk scores of complex traits with human lifespan” bioRxiv (2019): 856351 https://www.biorxiv.org/content/10.1101/856351v1; Polimanti, Renato et al. “Leveraging genome-wide data to investigate differences between opioid use vs. opioid dependence in 41,176 individuals from the Psychiatric Genomics Consortium” bioRxiv (2019): 765065 https://www.biorxiv.org/content/10.1101/765065v1; Lescai, Francesco et al. “Meta-analysis of Scandinavian Schizophrenia Exomes” bioRxiv (2019): 836957; https://www.biorxiv.org/content/10.1101/836957v2; Bolze, Alexandre, et al. “Selective constraints and pathogenicity of mitochondrial DNA variants inferred from a novel database of 196,554 unrelated individuals” bioRxiv (2019): 798264;https://www.biorxiv.org/content/10.1101/798264v1; De Lillo, A., De Angelis, F., Di Girolamo, M. et al. “Phenome-wide association study of TTR and RBP4 genes in 361,194 individuals reveals novel insights in the genetics of hereditary and wildtype transthyretin amyloidoses.” Hum G",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:13101,Safety,risk,risk,13101,"genome-wide data to investigate differences between opioid use vs. opioid dependence in 41,176 individuals from the Psychiatric Genomics Consortium” bioRxiv (2019): 765065 https://www.biorxiv.org/content/10.1101/765065v1; Lescai, Francesco et al. “Meta-analysis of Scandinavian Schizophrenia Exomes” bioRxiv (2019): 836957; https://www.biorxiv.org/content/10.1101/836957v2; Bolze, Alexandre, et al. “Selective constraints and pathogenicity of mitochondrial DNA variants inferred from a novel database of 196,554 unrelated individuals” bioRxiv (2019): 798264;https://www.biorxiv.org/content/10.1101/798264v1; De Lillo, A., De Angelis, F., Di Girolamo, M. et al. “Phenome-wide association study of TTR and RBP4 genes in 361,194 individuals reveals novel insights in the genetics of hereditary and wildtype transthyretin amyloidoses.” Hum Genet 138, 1331–1340 (2019). https://www.ncbi.nlm.nih.gov/pubmed/31659433; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies.” The Lancet Respiratory Medicine 7.6 (2019): 509-522. https://www.biorxiv.org/content/10.1101/427427v2; Werling, Donna, et al. “Whole-genome and RNA sequencing reveal variation and transcriptomic coordination in the developing human prefrontal cortex.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/585430v1; Satterstrom, Kyle F., et al. “Large-scale exome sequencing study implicates both developmental and functional changes in the neurobiology of autism.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/484113v3; Huang, Qin, et al. “Delivering genes across the blood-brain barrier: LY6A, a novel cellular receptor for AAV-PHP. B capsids.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/538421v1; Kurki, Mitja I., et al. “Contribution of rare and common variants to intellectual disability in a sub-isolate of Northern Finland.” Nature Communications 10.1 (2019): 410. https://www",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:14256,Safety,risk,risk,14256,"-522. https://www.biorxiv.org/content/10.1101/427427v2; Werling, Donna, et al. “Whole-genome and RNA sequencing reveal variation and transcriptomic coordination in the developing human prefrontal cortex.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/585430v1; Satterstrom, Kyle F., et al. “Large-scale exome sequencing study implicates both developmental and functional changes in the neurobiology of autism.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/484113v3; Huang, Qin, et al. “Delivering genes across the blood-brain barrier: LY6A, a novel cellular receptor for AAV-PHP. B capsids.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/538421v1; Kurki, Mitja I., et al. “Contribution of rare and common variants to intellectual disability in a sub-isolate of Northern Finland.” Nature Communications 10.1 (2019): 410. https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/30679432/; Martin, Alicia R., et al. “Current clinical use of polygenic scores will risk exacerbating health disparities.” bioRxiv (2019): 441261. https://www.biorxiv.org/content/10.1101/441261v3; Collaborative, Epi25, et al. “Ultra-rare genetic variation in the epilepsies: a whole-exome sequencing study of 17,606 individuals.” American Journal of Human Genetics (2019): https://www.cell.com/ajhg/fulltext/S0002-9297(19)30207-1; Karczewski, Konrad J., et al. “The mutational constraint spectrum quantified from variation in 141,456 humans.” bioRxiv (2019): 531210. https://www.biorxiv.org/content/10.1101/531210v4; Whiffin, Nicola, et al. “Human loss-of-function variants suggest that partial LRRK2 inhibition is a safe therapeutic strategy for Parkinsons disease.” bioRxiv() (2019): 561472. https://www.biorxiv.org/content/10.1101/561472v1; Cummings, Beryl B., et al. “Transcript expression-aware annotation improves rare variant discovery and interpretation.” bioRxiv (2019): 554444. https://www.biorxiv.org/content/10.1101/554444v1; Wang, Qingbo, et al. “Landscape of multi-",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:14889,Safety,safe,safe,14889,"s.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/538421v1; Kurki, Mitja I., et al. “Contribution of rare and common variants to intellectual disability in a sub-isolate of Northern Finland.” Nature Communications 10.1 (2019): 410. https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/30679432/; Martin, Alicia R., et al. “Current clinical use of polygenic scores will risk exacerbating health disparities.” bioRxiv (2019): 441261. https://www.biorxiv.org/content/10.1101/441261v3; Collaborative, Epi25, et al. “Ultra-rare genetic variation in the epilepsies: a whole-exome sequencing study of 17,606 individuals.” American Journal of Human Genetics (2019): https://www.cell.com/ajhg/fulltext/S0002-9297(19)30207-1; Karczewski, Konrad J., et al. “The mutational constraint spectrum quantified from variation in 141,456 humans.” bioRxiv (2019): 531210. https://www.biorxiv.org/content/10.1101/531210v4; Whiffin, Nicola, et al. “Human loss-of-function variants suggest that partial LRRK2 inhibition is a safe therapeutic strategy for Parkinsons disease.” bioRxiv() (2019): 561472. https://www.biorxiv.org/content/10.1101/561472v1; Cummings, Beryl B., et al. “Transcript expression-aware annotation improves rare variant discovery and interpretation.” bioRxiv (2019): 554444. https://www.biorxiv.org/content/10.1101/554444v1; Wang, Qingbo, et al. “Landscape of multi-nucleotide variants in 125,748 human exomes and 15,708 genomes.” bioRxiv (2019): 573378. https://www.biorxiv.org/content/10.1101/573378v2; Minikel, Eric Vallabh, et al. “Evaluating potential drug targets through human loss-of-function genetic variation.” bioRxiv (2019): 530881. https://www.biorxiv.org/content/10.1101/530881v2; Collins, Ryan L., et al. “An open resource of structural variation for medical and population genetics.” bioRxiv (2019): 578674. https://www.biorxiv.org/content/10.1101/578674v1; Whiffin, Nicola, et al. “Characterising the loss-of-function impact of 5’untranslated region variants in whole genom",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:16918,Safety,risk,risk,16918,"biorxiv.org/content/10.1101/543504v1; Lacaze, Paul, et al. “The Medical Genome Reference Bank: a whole-genome data resource of 4000 healthy elderly individuals. Rationale and cohort design.” European Journal of Human Genetics 27.2 (2019): 308. https://www.nature.com/articles/s41431-018-0279-z; Cirulli, Elizabeth T., et al. “Genome-wide rare variant analysis for thousands of phenotypes in 54,000 exomes.” bioRxiv (2019): 692368. https://www.biorxiv.org/content/10.1101/692368v1.abstract; Kerminen, Sini, et al. “Geographic Variation and Bias in the Polygenic Scores of Complex Diseases and Traits in Finland.” American Journal of Human Genetics (2019). https://www.biorxiv.org/content/10.1101/485441v1.abstract; Jiang, Fan, Kyle Ferriter, and Claris Castillo. “PIVOT: Cost-Aware Scheduling of Data-Intensive Applications in a Cloud-Agnostic System.” https://renci.org/wp-content/uploads/2019/02/Cloud_19.pdf; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies.” Lancet Respiratory Medicine 7.6 (2019): 509-522. https://www.biorxiv.org/content/10.1101/427427v2; Cox, Samantha L., et al. “Genetic contributions to variation in human stature in prehistoric Europe.” bioRxiv (2019): 690545. https://www.biorxiv.org/content/10.1101/690545v1.abstract; Abrar, Faheem. A Modular Parallel Pipeline Architecture for GWAS Applications in a Cluster Environment. Diss. University of Saskatchewan, 2019. https://harvest.usask.ca/handle/10388/12087; Khera, Amit V., et al. “Whole-genome sequencing to characterize monogenic and polygenic contributions in patients hospitalized with early-onset myocardial infarction.” Circulation 139.13 (2019): 1593-1602. https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.035658. 2018. An, Joon-Yong, et al. “Genome-wide de novo risk score implicates promoter variation in autism spectrum disorder.” Science (2018): 1. https://science.sciencemag.org/content/362/6420/",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:17815,Safety,risk,risk,17815,"ads/2019/02/Cloud_19.pdf; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies.” Lancet Respiratory Medicine 7.6 (2019): 509-522. https://www.biorxiv.org/content/10.1101/427427v2; Cox, Samantha L., et al. “Genetic contributions to variation in human stature in prehistoric Europe.” bioRxiv (2019): 690545. https://www.biorxiv.org/content/10.1101/690545v1.abstract; Abrar, Faheem. A Modular Parallel Pipeline Architecture for GWAS Applications in a Cluster Environment. Diss. University of Saskatchewan, 2019. https://harvest.usask.ca/handle/10388/12087; Khera, Amit V., et al. “Whole-genome sequencing to characterize monogenic and polygenic contributions in patients hospitalized with early-onset myocardial infarction.” Circulation 139.13 (2019): 1593-1602. https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.035658. 2018. An, Joon-Yong, et al. “Genome-wide de novo risk score implicates promoter variation in autism spectrum disorder.” Science (2018): 1. https://science.sciencemag.org/content/362/6420/eaat6576.full; Molnos, Sophie Claudia. Metabolites: implications in type 2 diabetes and the effect of epigenome-wide interaction with genetic variation. Diss. Technische Universität München, 2018. https://mediatum.ub.tum.de/1372795f; Bis, Joshua C., et al. “Whole exome sequencing study identifies novel rare and common Alzheimer’s-associated variants involved in immune response and transcriptional regulation.” Molecular Psychiatry (2018): 1. https://www.nature.com/articles/s41380-018-0112-7; Gormley, Padhraig, et al. “Common variant burden contributes to the familial aggregation of migraine in 1,589 families.” Neuron 98.4 (2018): 743-753. https://www.ncbi.nlm.nih.gov/pubmed/30189203; Rivas, Manuel A., et al. “Insights into the genetic epidemiology of Crohn’s and rare diseases in the Ashkenazi Jewish population.” PLoS Genetics 14.5 (2018): e1007329. https://journals.plos.org/",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:19841,Safety,risk,risk,19841,"ournals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007329; Satterstrom, F. Kyle, et al. “ASD and ADHD have a similar burden of rare protein-truncating variants.” bioRxiv (2018): 277707. https://www.biorxiv.org/content/10.1101/277707v1; Zekavat, Seyedeh M., et al. “Deep coverage whole genome sequences and plasma lipoprotein (a) in individuals of European and African ancestries.” Nature Communications 9.1 (2018): 2606. https://www.nature.com/articles/s41467-018-04668-w; Natarajan, Pradeep, et al. “Deep-coverage whole genome sequences and blood lipids among 16,324 individuals.” Nature Communications 9.1 (2018): 3391. https://www.nature.com/articles/s41467-018-04668-w; Ganna, Andrea, et al. “Quantifying the impact of rare and ultra-rare coding variation across the phenotypic spectrum.” American Journal of Human Genetics 102.6 (2018): 1204-1211. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5992130/; Khera, Amit V., et al. “Genome-wide polygenic scores for common diseases identify individuals with risk equivalent to monogenic mutations.” Nature Genetics 50.9 (2018): 1219. https://www.nature.com/articles/s41588-018-0183-z?_ga=2.263293700.980063710.1543017600-1151073636.1543017600; Roselli, Carolina, et al. “Multi-ethnic genome-wide association study for atrial fibrillation.” Nature Genetics 50.9 (2018): 1225. https://www.nature.com/articles/s41588-018-0133-9; Arachchi, Harindra, et al. “matchbox: An open‐source tool for patient matching via the Matchmaker Exchange.” Human Mutation 39.12 (2018): 1827-1834. https://onlinelibrary.wiley.com/doi/abs/10.1002/humu.23655; Laisk, Triin, et al. “GWAS meta-analysis highlights the hypothalamic-pituitary-gonadal axis (HPG axis) in the genetic regulation of menstrual cycle length.” bioRxiv (2018): 333708. https://www.biorxiv.org/content/10.1101/333708v1.abstract; Rees, Elliott, et al. “Association between schizophrenia and both loss of function and missense mutations in paralog conserved sites of voltage-gated sodium channel",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:21571,Safety,risk,risk,21571,".” bioRxiv (2018): 333708. https://www.biorxiv.org/content/10.1101/333708v1.abstract; Rees, Elliott, et al. “Association between schizophrenia and both loss of function and missense mutations in paralog conserved sites of voltage-gated sodium channels.” bioRxiv (2018): 246850. https://www.biorxiv.org/content/10.1101/246850v1.abstract; Haas, Mary E., et al. “Genetic association of albuminuria with cardiometabolic disease and blood pressure.” American Journal of Human Genetics 103.4 (2018): 461-473. https://www.cell.com/ajhg/pdf/S0002-9297(18)30270-2.pdf; Abel, Haley J., et al. “Mapping and characterization of structural variation in 17,795 deeply sequenced human genomes.” bioRxiv (2018): 508515. https://www.biorxiv.org/content/10.1101/508515v1.abstract; Lane, Jacqueline M., et al. “Biological and clinical insights from genetics of insomnia symptoms.” bioRxiv (2018): 257956. https://www.biorxiv.org/content/10.1101/257956v1.abstract; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood onset and adult onset asthma.” bioRxiv (2018): 427427. https://www.biorxiv.org/content/10.1101/427427v1.abstract. 2017. Lessard, Samuel, et al. “Human genetic variation alters CRISPR-Cas9 on-and off-targeting specificity at therapeutically implicated loci.” Proceedings of the National Academy of Sciences 114.52 (2017): E11257-E11266. https://www.pnas.org/content/114/52/E11257.long. 2016. Ganna, Andrea, et al. “Ultra-rare disruptive and damaging mutations influence educational attainment in the general population.” Nature Neuroscience 19.12 (2016): 1563. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5127781/. Footnote In addition to software development, the Hail team engages in theoretical, algorithmic, and empirical research inspired by scientific collaboration. Examples include Loss landscapes of regularized linear autoencoders, Secure multi-party linear regression at plaintext speed, and A synthetic-diploid benchmark for accurate variant-calling evaluation. ",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:7820,Testability,log,login,7820,"imer’s disease. Transl; 	 Psychiatry 12, 523; 	 (2022). https://doi.org/10.1038/s41398-022-02281-6 https://www.nature.com/articles/s41398-022-02281-6. Wadon, M.E., Fenner, E., Kendall, K.M. et al. Clinical and genotypic analysis in; 	 determining dystonia non-motor phenotypic heterogeneity: a UK Biobank study. J Neurol; 	 269, 6436–6451 (2022). https://doi.org/10.1007/s00415-022-11307-4 https://link.springer.com/article/10.1007/s00415-022-11307-4. 	 Andi Madihah Manggabarani, Takuyu Hashiguchi, Masatsugu Hashiguchi, Atsushi Hayashi,; 	 Masataka Kikuchi, Yusdar Mustamin, Masaru Bamba, Kunihiro Kodama, Takanari Tanabata,; 	 Sachiko Isobe, Hidenori Tanaka, Ryo Akashi, Akihiro Nakaya, Shusei Sato, Construction of; 	 prediction models for growth traits of soybean cultivars based on phenotyping in diverse; 	 genotype and environment combinations, DNA Research, Volume 29, Issue 4, August 2022,; 	 dsac024, https://doi.org/10.1093/dnares/dsac024 https://academic.oup.com/dnaresearch/article/29/4/dsac024/6653298?login=false. 	 Chaffin, M., Papangeli, I., Simonson, B. et al. Single-nucleus profiling of human; 	 dilated and hypertrophic cardiomyopathy. Nature 608, 174–180; 	 (2022). https://doi.org/10.1038/s41586-022-04817-8 https://www.nature.com/articles/s41586-022-04817-8. 	 Lee, J., Lee, J., Jeon, S. et al. A database of 5305 healthy Korean individuals reveals; 	 genetic and clinical implications for an East Asian population. Exp Mol Med 54,; 	 1862–1871; 	 (2022). https://doi.org/10.1038/s12276-022-00871-4 https://www.nature.com/articles/s12276-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis converge to implicate negative selection and neuronal processes in the; 	 aetiology of schizophrenia. Mol Psychiatry 27, 3699–3707; 	 (2022). https://doi.org/10.1038/s41380-022-01621-8 https://www.nature.com/articles/s41380-022-01621-8. 	 Mitja, K.I., et al. FinnGen: Unique genetic insights from combining isolated p",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:22521,Testability,benchmark,benchmark,22521,".” bioRxiv (2018): 333708. https://www.biorxiv.org/content/10.1101/333708v1.abstract; Rees, Elliott, et al. “Association between schizophrenia and both loss of function and missense mutations in paralog conserved sites of voltage-gated sodium channels.” bioRxiv (2018): 246850. https://www.biorxiv.org/content/10.1101/246850v1.abstract; Haas, Mary E., et al. “Genetic association of albuminuria with cardiometabolic disease and blood pressure.” American Journal of Human Genetics 103.4 (2018): 461-473. https://www.cell.com/ajhg/pdf/S0002-9297(18)30270-2.pdf; Abel, Haley J., et al. “Mapping and characterization of structural variation in 17,795 deeply sequenced human genomes.” bioRxiv (2018): 508515. https://www.biorxiv.org/content/10.1101/508515v1.abstract; Lane, Jacqueline M., et al. “Biological and clinical insights from genetics of insomnia symptoms.” bioRxiv (2018): 257956. https://www.biorxiv.org/content/10.1101/257956v1.abstract; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood onset and adult onset asthma.” bioRxiv (2018): 427427. https://www.biorxiv.org/content/10.1101/427427v1.abstract. 2017. Lessard, Samuel, et al. “Human genetic variation alters CRISPR-Cas9 on-and off-targeting specificity at therapeutically implicated loci.” Proceedings of the National Academy of Sciences 114.52 (2017): E11257-E11266. https://www.pnas.org/content/114/52/E11257.long. 2016. Ganna, Andrea, et al. “Ultra-rare disruptive and damaging mutations influence educational attainment in the general population.” Nature Neuroscience 19.12 (2016): 1563. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5127781/. Footnote In addition to software development, the Hail team engages in theoretical, algorithmic, and empirical research inspired by scientific collaboration. Examples include Loss landscapes of regularized linear autoencoders, Secure multi-party linear regression at plaintext speed, and A synthetic-diploid benchmark for accurate variant-calling evaluation. ",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/references.html:5085,Usability,learn,learning-enabled,5085,"tem nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 1371–1387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Josefine U Melchiorsen, Kimmie V Sørensen, Jette Bork-Jensen, Hüsün S Kizilkaya, Lærke S; 	 Gasbjerg, Alexander S Hauser, Jørgen Rungby, Henrik T Sørensen, Allan Vaag, Jens S; 	 Nielsen, Oluf Pedersen, Allan Linneberg, Bolette Hartmann, Anette P Gjesing, Jens J; 	 Holst, Torben Hansen, Mette M Rosenkilde, Niels Grarup, Rare Heterozygous; 	 Loss-of-Function Variants in the Human GLP-1 Receptor Are Not Associated With; 	 Cardiometabolic Phenotypes, The Journal of Clinical Endocrinology & Metabolism, Volume; 	 108, Issue 11, November 2023, Pages; 	 2821–2833, https://doi.org/10.1210/clinem/dgad290. https://academic.oup.com/jcem/article/108/11/2821/7180819. 	 Vukadinovic, Milos et al. Deep learning-enabled analysis of medical images identifies; 	 cardiac sphericity as an early marker of cardiomyopathy and related outcomes. Med,; 	 Volume 4, Issue 4, 252 - 262.e3. https://www.cell.com/med/fulltext/S2666-6340(23)00069-7. 	 Epi25 Collaborative; Chen S, Neale BM, Berkovic SF. Shared and distinct ultra-rare; 	 genetic risk for diverse epilepsies: A whole-exome sequencing study of 54,423; 	 individuals across multiple genetic ancestries. medRxiv [Preprint]. 2023 Feb; 	 24:2023.02.22.23286310. doi: 10.1101/2023.02.22.23286310. PMID: 36865150; PMCID:; 	 PMC9980234. https://pubmed.ncbi.nlm.nih.gov/36865150/. 	 Kurki, M.I., Karjalainen, J., Palta, P. et al. FinnGen provides genetic insights from a; 	 well-phenotyped isolated population. Nature 613, 508–518; 	 (2023). https://doi.org/10.1038/s41586-022-05473-8 https://www.nature.com/articles/s41586-022-05473-8. 	 Mortensen, Ó., Thomsen, E., Lydersen, L.N. et al. FarGen: Elucidating the distribution; 	 of coding variants in the isolated population of the Faroe Islands. Eur J Hum Genet 31,; 	 329–",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
https://hail.is/tutorial.html:715,Integrability,interface,interface,715,"﻿. Hail | Tutorial . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Import, prototype, scale; ; Perform analyses with distributed; dataframe-like; collections. import hail as hl; mt = hl.import_vcf('gs://bucket/path/myVCF.vcf.bgz'); mt.write('gs://bucket/path/dataset.mt', overwrite=True); # read matrix into env; mt = hl.read_matrix_table('gs://bucket/path/dataset.mt'); mt1 = hl.import_vcf('/path/to/my.vcf.bgz'); mt2 = hl.import_bgen('/path/to/my.bgen'); mt3 = hl.import_plink(bed='/path/to/my.bed',; bim='/path/to/my.bim',; fam='/path/to/my.fam'). Input Unification; ; Import formats such as bed, bgen, plink, or vcf, and manipulate them using a common dataframe-like interface. Genomic Dataframes; For large and dense structured matrices, like sequencing data, coordinate representations are; both; hard to work with and computationally inefficient. A core piece of Hail functionality is the; MatrixTable, a 2-dimensional generalization of Table. The MatrixTable makes it possible to; filter,; annotate, and aggregate symmetrically over rows and columns. # What is a MatrixTable?; mt.describe(widget=True). # filter to rare, loss-of-function variants; mt = mt.filter_rows(mt.variant_qc.AF[1] < 0.005); mt = mt.filter_rows(mt.csq == 'LOF'); . # run sample QC and save into matrix table; mt = hl.sample_qc(mt). # filter for samples that are > 95% call rate; mt = mt.filter_cols(mt.sample_qc.call_rate >= 0.95) . # run variant QC and save into matrix table; mt = hl.variant_qc(mt). # filter for variants that are >95% call rate and >1% frequency; mt = mt.filter_rows(mt.variant_qc.call_rate > 0.95); mt = mt.filter_rows(mt.variant_qc_.AF[1] > 0.01). Simplified Analysis; Hail makes it easy to analyze your data. Let's start by filtering a dataset by variant and sample; quality metrics, like call rate and allele frequency. Quality Control Procedures; Quality control procedures, like sex check, are made easy using Hail's declarative syntax. imputed_sex =",MatchSource.WIKI,tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/tutorial.html
https://hail.is/tutorial.html:2272,Safety,predict,predictor,2272,"= mt.filter_rows(mt.csq == 'LOF'); . # run sample QC and save into matrix table; mt = hl.sample_qc(mt). # filter for samples that are > 95% call rate; mt = mt.filter_cols(mt.sample_qc.call_rate >= 0.95) . # run variant QC and save into matrix table; mt = hl.variant_qc(mt). # filter for variants that are >95% call rate and >1% frequency; mt = mt.filter_rows(mt.variant_qc.call_rate > 0.95); mt = mt.filter_rows(mt.variant_qc_.AF[1] > 0.01). Simplified Analysis; Hail makes it easy to analyze your data. Let's start by filtering a dataset by variant and sample; quality metrics, like call rate and allele frequency. Quality Control Procedures; Quality control procedures, like sex check, are made easy using Hail's declarative syntax. imputed_sex = hl.impute_sex(mt.GT); mt = mt.annotate_cols(; sex_check = imputed_sex[mt.s].is_female == mt.reported_female; ). # must use Google cloud platform for this to work ; # annotation with vep; mt = hl.vep(mt). Variant Effect Predictor; Annotating variants with Variant effect predictor has never been easier. Rare-Variant Association Testing; Perform Gene Burden Tests on sequencing data with just a few lines of Python. gene_intervals = hl.read_table(""gs://my_bucket/gene_intervals.t""); mt = mt.annotate_rows(; gene = gene_intervals.index(mt.locus, all_matches=True).gene_name; ). mt = mt.explode_rows(mt.gene); mt = (mt.group_rows_by(mt.gene); .aggregate(burden = hl.agg.count_where(mt.GT.is_non_ref()))). result = hl.linear_regression_rows(y=mt.phenotype, x=mt.burden). # generate and save PC scores; eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(mt.GT, k=4). # run linear regression for the first 4 PCs; mt = mt.annotate_cols(scores = pca_scores[mt.sample_id].scores); results = hl.linear_regression_rows(; y=mt.phenotype,; x=mt.GT.n_alt_alleles(),; covariates=[; 1, mt.scores[0], mt.scores[1], mt.scores[2], mt.scores[3]]; ). Principal Component Analysis (PCA); Adjusting GWAS models with principal components as covariates has never been easier. ",MatchSource.WIKI,tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/tutorial.html
https://hail.is/docs/0.1/annotationdb.html:2135,Availability,down,downloads,2135," something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text files). Links to; the relevant publications and raw data downloads are included where applicable. Important Notes¶. Multiallelic variants¶; Annotations in the database are keyed by biallelic variants. For some annotations, this means Hail’s split_multi() method; has been used to split multiallelic variants into biallelics. Warning; It is recommended to run split_multi() on your VDS before using annotate_variants_db(). You can use; annotate_variants_db() without first splitting multiallelic variants, but any multiallelics in your VDS will not be annotated.; If you first split these variants, the resulting biallelic variants may then be annotated by the database. VEP annotations¶; VEP annotations are included in this database under the root va.vep. To add VEP annotations, the annotate_variants_db(); method runs Hail’s vep() method on your VDS. This means that your cluster must be properly initialized as described in the; Running VEP section in this discussion post. Warning; If you want to add VEP annotations to your VDS, make sure to add ",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
https://hail.is/docs/0.1/annotationdb.html:581,Deployability,pipeline,pipelines,581,"﻿. . Annotation Database — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected ",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
https://hail.is/docs/0.1/annotationdb.html:798,Deployability,pipeline,pipeline,798,"﻿. . Annotation Database — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected ",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
https://hail.is/docs/0.1/annotationdb.html:1004,Performance,load,load,1004,"0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text f",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
https://hail.is/docs/0.1/annotationdb.html:1262,Testability,test,test,1262,"entation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text files). Links to; the relevant publications and raw data downloads are included where applicable. Important Notes¶. Multiallelic variants¶; Annotations in the database are key",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
https://hail.is/docs/0.1/annotationdb.html:3715,Testability,log,logic,3715,"atabase. VEP annotations¶; VEP annotations are included in this database under the root va.vep. To add VEP annotations, the annotate_variants_db(); method runs Hail’s vep() method on your VDS. This means that your cluster must be properly initialized as described in the; Running VEP section in this discussion post. Warning; If you want to add VEP annotations to your VDS, make sure to add the initialization action; gs://hail-common/vep/vep/vep85-init.sh when starting your cluster. Gene-level annotations¶; Annotations beginning with va.gene. are gene-level annotations that can be used to annotate variants in your VDS. These; gene-level annotations are stored in the database as keytables keyed by HGNC gene symbols.; By default, if an annotation beginning with va.gene. is given to annotate_variants_db() and no gene_key; parameter is specified, the function will run VEP and parse the VEP output to define one gene symbol per variant in the VDS.; For each variant, the logic used to extract one gene symbol from the VEP output is as follows:. Collect all consequences found in canonical transcripts. Designate the most severe consequence in the collection, as defined by this hierarchy (from most severe to least severe):. Transcript ablation; Splice acceptor variant; Splice donor variant; Stop gained; Frameshift variant; Stop lost; Start lost; Transcript amplification; Inframe insertion; Missense variant; Protein altering variant; Incomplete terminal codon variant; Stop retained variant; Synonymous variant; Splice region variant; Coding sequence variant; Mature miRNA variant; 5’ UTR variant; 3’ UTR variant; Non-coding transcript exon variant; Intron variant; NMD transcript variant; Non-coding transcript variant; Upstream gene variant; Downstream gene variant; TFBS ablation; TFBS amplification; TF binding site variant; Regulatory region ablation; Regulatory region amplification; Feature elongation; Regulatory region variant; Feature truncation; Intergenic variant. If a canonical ",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
https://hail.is/docs/0.1/annotationdb.html:4928,Testability,log,logic,4928,"t severe to least severe):. Transcript ablation; Splice acceptor variant; Splice donor variant; Stop gained; Frameshift variant; Stop lost; Start lost; Transcript amplification; Inframe insertion; Missense variant; Protein altering variant; Incomplete terminal codon variant; Stop retained variant; Synonymous variant; Splice region variant; Coding sequence variant; Mature miRNA variant; 5’ UTR variant; 3’ UTR variant; Non-coding transcript exon variant; Intron variant; NMD transcript variant; Non-coding transcript variant; Upstream gene variant; Downstream gene variant; TFBS ablation; TFBS amplification; TF binding site variant; Regulatory region ablation; Regulatory region amplification; Feature elongation; Regulatory region variant; Feature truncation; Intergenic variant. If a canonical transcript with the most severe consequence exists, take that gene and transcript. Otherwise, take a non-canonical; transcript with the most severe consequence. Though this is the default logic, you may wish to define gene symbols differently. One way to do so while still using the VEP output; would be to add VEP annotations to your VDS, create a gene symbol variant annotation by parsing through the VEP output however you; wish, and then pass that annotation to annotate_variants_db() using the gene_key parameter.; Here’s an example that uses the gene symbol from the first VEP transcript:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db('va.vep'); .annotate_variants_expr('va.my_gene = va.vep.transcript_consequences[0].gene_symbol'); .annotate_variants_db('va.gene.constraint.pli', gene_key='va.my_gene'); ). pprint(vds.variant_schema). This code would return:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; vep: Struct{; ...; },; my_gene: String,; gene: Struct{; constraint: Struct{; pli: Double; }; }; }. Suggest additions or edits¶; ",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
https://hail.is/docs/0.1/annotationdb.html:5445,Testability,test,test,5445,"g variant; Incomplete terminal codon variant; Stop retained variant; Synonymous variant; Splice region variant; Coding sequence variant; Mature miRNA variant; 5’ UTR variant; 3’ UTR variant; Non-coding transcript exon variant; Intron variant; NMD transcript variant; Non-coding transcript variant; Upstream gene variant; Downstream gene variant; TFBS ablation; TFBS amplification; TF binding site variant; Regulatory region ablation; Regulatory region amplification; Feature elongation; Regulatory region variant; Feature truncation; Intergenic variant. If a canonical transcript with the most severe consequence exists, take that gene and transcript. Otherwise, take a non-canonical; transcript with the most severe consequence. Though this is the default logic, you may wish to define gene symbols differently. One way to do so while still using the VEP output; would be to add VEP annotations to your VDS, create a gene symbol variant annotation by parsing through the VEP output however you; wish, and then pass that annotation to annotate_variants_db() using the gene_key parameter.; Here’s an example that uses the gene symbol from the first VEP transcript:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db('va.vep'); .annotate_variants_expr('va.my_gene = va.vep.transcript_consequences[0].gene_symbol'); .annotate_variants_db('va.gene.constraint.pli', gene_key='va.my_gene'); ). pprint(vds.variant_schema). This code would return:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; vep: Struct{; ...; },; my_gene: String,; gene: Struct{; constraint: Struct{; pli: Double; }; }; }. Suggest additions or edits¶; Please contact Andrea Ganna (aganna@broadinstitute.org) or Liam Abbott (labbott@broadinstitute.org) with any questions. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
https://hail.is/docs/0.1/annotationdb.html:982,Usability,simpl,simple,982,"0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text f",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
https://hail.is/docs/0.1/api.html:491,Integrability,interface,interface,491,"﻿. . Python API — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API. View page source. Python API¶; This is the API documentation for Hail, and provides detailed information; on the Python programming interface.; Classes. hail.HailContext; The main entry point for Hail functionality. hail.VariantDataset; Hail’s primary representation of genomic data, a matrix keyed by sample and variant. hail.KeyTable; Hail’s version of a SQL table where columns can be designated as keys. hail.KinshipMatrix; Represents a symmetric matrix encoding the relatedness of each pair of samples in the accompanying sample list. hail.LDMatrix; Represents a symmetric matrix encoding the Pearson correlation between each pair of variants in the accompanying variant list. Modules. representation; expr; utils. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/api.html
https://hail.is/docs/0.1/functions.html:4451,Energy Efficiency,power,power,4451,"ean): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; logP (Boolean) – If true, probabilities are returned as log(p). dpois(x: Double, lambda: Double): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative. drop(s: Struct, identifiers: String*): Struct. Return a new Struct with the a subset of fields not matching identifiers.; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in drop(s, gene, function); result: {nHet: 12}. Arguments. s (Struct) – Struct to drop fields from.; identifiers (String*) – Field names to drop from s. Multiple arguments allowed. exp(x: Double): Double. Returns Euler’s number e raised to the power of the given value x.; Arguments. x (Double) – the exponent to raise e to. fet(a: Int, b: Int, c: Int, d: Int): Struct{pValue:Double,oddsRatio:Double,ci95Lower:Double,ci95Upper:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio; ci95Lower (Double) – lower bound for 95% confidence interval; ci95Upper (Double) – upper bound for 95% confidence interval. Calculates the p-value, odds ratio, and 95% confidence interval with Fisher’s exact test (FET) for 2x2 tables.; Examples; Annotate each variant with Fisher’s exact test association results (assumes minor/major allele count variant annotations have been computed):; >>> (vds.annotate_variants_expr(; ... 'va.fet = let macCase = gs.filter(g => sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'macControl = gs.filter(g => !sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'majCase = gs.filter(g => sa.pheno.isCase).map(g => 2 - g.nNonRefAlleles()).sum() and '; ... 'majControl = gs.filter(g =",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:13796,Energy Efficiency,power,power,13796,"les(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns r",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:13195,Modifiability,variab,variable,13195," (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at ",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:13658,Modifiability,variab,variable,13658,"rElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – No",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:14033,Modifiability,variab,variable,14033," true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:14549,Modifiability,variab,variable,14549,".0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, return",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:14871,Modifiability,variab,variable,14871,"uble) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; ",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:15160,Modifiability,variab,variable,15160,").; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Argumen",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:15459,Modifiability,variab,variable,15459,"are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative. range(start: Int, stop: Int, step: Int): Array[Int]. Generate an Array with values in the interval [start, stop) in increments of step.; let r = range(0, ",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:16059,Modifiability,variab,variable,16059," Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative. range(start: Int, stop: Int, step: Int): Array[Int]. Generate an Array with values in the interval [start, stop) in increments of step.; let r = range(0, 5, 2) in r; result: [0, 2, 4]. Arguments. start (Int) – Starting number of the sequence.; stop (Int) – Generate numbers up to, but not including this number.; step (Int) – Difference between each number in the sequence. range(start: Int, stop: Int): Array[Int]. Generate an Array with values in the interval [start, stop).; let r = range(5, 8) in r; result: [5, 6, 7]. Arguments. start (Int) – Starting number of the sequence.; stop (Int) – Generate numbers up to, but not including this number. range(stop: Int): Array[Int]. Generate an Array with values in the interval [0, stop).; let r = range(3) in r; result",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:478,Testability,test,test,478,"﻿. . Functions — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Functions. View page source. Functions¶. binomTest(x: Int, n: Int, p: Double, alternative: String): Double. Returns the p-value from the exact binomial test of the null hypothesis that success has probability p, given x successes in n trials.; Examples; Test each variant for allele balance across all heterozygous genotypes, under the null hypothesis that the two alleles are sampled with equal probability.; >>> (vds.split_multi(); ... .annotate_variants_expr(; ... 'va.ab_binom_test = let all_samples_ad = gs.filter(g => g.isHet).map(g => g.ad).sum() in '; ... 'binomTest(all_samples_ad[1], all_samples_ad.sum(), 0.5, ""two.sided"")')). Arguments. x (Int) – Number of successes; n (Int) – Number of trials; p (Double) – Probability of success under the null hypothesis; alternative (String) – Alternative hypothesis, must be “two.sided”, “greater” or “less”. chisq(c1: Int, c2: Int, c3: Int, c4: Int): Struct{pValue:Double,oddsRatio:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio. Calculates p-value (Chi-square approximation) and odds ratio for 2x2 table; Arguments. c1 (Int) – value for cell 1; c2 (Int) – value for cell 2; c3 (Int) – value for cell 3; c4 (Int) – value for cell 4. combineVariants(left: Variant, right: Variant): Struct{variant:Variant,laIndices:Dict[Int,Int],raIndices:Dict[Int,Int]}. variant (Variant) – Resulting combined variant.; laIndices (Dict[Int, Int]) – Mapping from new to old allele index for the left variant.; raIndices (Dict[Int, Int]) – Mapping from new to old allele index for the right variant. Combines the alleles of two variants at the same locus, making sure that ref and alt alleles are represented uniformely.;",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:2934,Testability,test,test,2934,"ing sure that ref and alt alleles are represented uniformely.; In addition to the resulting variant containing all alleles, this function also returns the mapping from the old to the new allele indices.; Note that this mapping counts the reference allele always contains the reference allele mapping 0 -> 0. let left = Variant(""1:1000:AT:A,CT"") and right = Variant(""1:1000:A:C,AGG"") in combineVariants(left,right); result: Struct{'variant': Variant(contig=1, start=1000, ref=AT, alts=[AltAllele(ref=AT, alt=A), AltAllele(ref=AT, alt=CT), AltAllele(ref=AT, alt=AGGT)]), 'laIndices': {0: 0, 1: 1, 2: 2}, 'raIndices': {0:0, 2: 1, 3: 2}}. Arguments. left (Variant) – Left variant to combine.; right (Variant) – Right variant to combine. ctt(c1: Int, c2: Int, c3: Int, c4: Int, minCellCount: Int): Struct{pValue:Double,oddsRatio:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio. Calculates p-value and odds ratio for 2x2 table. If any cell is lower than minCellCount Fishers exact test is used, otherwise faster chi-squared approximation is used.; Arguments. c1 (Int) – value for cell 1; c2 (Int) – value for cell 2; c3 (Int) – value for cell 3; c4 (Int) – value for cell 4; minCellCount (Int) – Minimum cell count for using chi-squared approximation. Dict(keys: Array[T], values: Array[U]): Dict[T, U]. Construct a Dict from an array of keys and an array of values.; Arguments. keys (Array[T]) – Keys of Dict.; values (Array[U]) – Values of Dict. dpois(x: Double, lambda: Double, logP: Boolean): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; logP (Boolean) – If true, probabilities are returned as log(p). dpois(x: Double, lambda: Double): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to com",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:3434,Testability,log,logP,3434,"llele(ref=AT, alt=A), AltAllele(ref=AT, alt=CT), AltAllele(ref=AT, alt=AGGT)]), 'laIndices': {0: 0, 1: 1, 2: 2}, 'raIndices': {0:0, 2: 1, 3: 2}}. Arguments. left (Variant) – Left variant to combine.; right (Variant) – Right variant to combine. ctt(c1: Int, c2: Int, c3: Int, c4: Int, minCellCount: Int): Struct{pValue:Double,oddsRatio:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio. Calculates p-value and odds ratio for 2x2 table. If any cell is lower than minCellCount Fishers exact test is used, otherwise faster chi-squared approximation is used.; Arguments. c1 (Int) – value for cell 1; c2 (Int) – value for cell 2; c3 (Int) – value for cell 3; c4 (Int) – value for cell 4; minCellCount (Int) – Minimum cell count for using chi-squared approximation. Dict(keys: Array[T], values: Array[U]): Dict[T, U]. Construct a Dict from an array of keys and an array of values.; Arguments. keys (Array[T]) – Keys of Dict.; values (Array[U]) – Values of Dict. dpois(x: Double, lambda: Double, logP: Boolean): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; logP (Boolean) – If true, probabilities are returned as log(p). dpois(x: Double, lambda: Double): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative. drop(s: Struct, identifiers: String*): Struct. Return a new Struct with the a subset of fields not matching identifiers.; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in drop(s, gene, function); result: {nHet: 12}. Arguments. s (Struct) – Struct to drop fields from.; identifiers (String*) – Field names to drop from s. Multiple arguments allowed. exp(x: Double): Double. Returns Euler’s n",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:3694,Testability,log,logP,3694,": Int): Struct{pValue:Double,oddsRatio:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio. Calculates p-value and odds ratio for 2x2 table. If any cell is lower than minCellCount Fishers exact test is used, otherwise faster chi-squared approximation is used.; Arguments. c1 (Int) – value for cell 1; c2 (Int) – value for cell 2; c3 (Int) – value for cell 3; c4 (Int) – value for cell 4; minCellCount (Int) – Minimum cell count for using chi-squared approximation. Dict(keys: Array[T], values: Array[U]): Dict[T, U]. Construct a Dict from an array of keys and an array of values.; Arguments. keys (Array[T]) – Keys of Dict.; values (Array[U]) – Values of Dict. dpois(x: Double, lambda: Double, logP: Boolean): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; logP (Boolean) – If true, probabilities are returned as log(p). dpois(x: Double, lambda: Double): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative. drop(s: Struct, identifiers: String*): Struct. Return a new Struct with the a subset of fields not matching identifiers.; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in drop(s, gene, function); result: {nHet: 12}. Arguments. s (Struct) – Struct to drop fields from.; identifiers (String*) – Field names to drop from s. Multiple arguments allowed. exp(x: Double): Double. Returns Euler’s number e raised to the power of the given value x.; Arguments. x (Double) – the exponent to raise e to. fet(a: Int, b: Int, c: Int, d: Int): Struct{pValue:Double,oddsRatio:Double,ci95Lower:Double,ci95Upper:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio; ci95Lower (Double) – l",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:3750,Testability,log,log,3750,": Int): Struct{pValue:Double,oddsRatio:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio. Calculates p-value and odds ratio for 2x2 table. If any cell is lower than minCellCount Fishers exact test is used, otherwise faster chi-squared approximation is used.; Arguments. c1 (Int) – value for cell 1; c2 (Int) – value for cell 2; c3 (Int) – value for cell 3; c4 (Int) – value for cell 4; minCellCount (Int) – Minimum cell count for using chi-squared approximation. Dict(keys: Array[T], values: Array[U]): Dict[T, U]. Construct a Dict from an array of keys and an array of values.; Arguments. keys (Array[T]) – Keys of Dict.; values (Array[U]) – Values of Dict. dpois(x: Double, lambda: Double, logP: Boolean): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; logP (Boolean) – If true, probabilities are returned as log(p). dpois(x: Double, lambda: Double): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative. drop(s: Struct, identifiers: String*): Struct. Return a new Struct with the a subset of fields not matching identifiers.; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in drop(s, gene, function); result: {nHet: 12}. Arguments. s (Struct) – Struct to drop fields from.; identifiers (String*) – Field names to drop from s. Multiple arguments allowed. exp(x: Double): Double. Returns Euler’s number e raised to the power of the given value x.; Arguments. x (Double) – the exponent to raise e to. fet(a: Int, b: Int, c: Int, d: Int): Struct{pValue:Double,oddsRatio:Double,ci95Lower:Double,ci95Upper:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio; ci95Lower (Double) – l",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:4911,Testability,test,test,4911,"Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative. drop(s: Struct, identifiers: String*): Struct. Return a new Struct with the a subset of fields not matching identifiers.; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in drop(s, gene, function); result: {nHet: 12}. Arguments. s (Struct) – Struct to drop fields from.; identifiers (String*) – Field names to drop from s. Multiple arguments allowed. exp(x: Double): Double. Returns Euler’s number e raised to the power of the given value x.; Arguments. x (Double) – the exponent to raise e to. fet(a: Int, b: Int, c: Int, d: Int): Struct{pValue:Double,oddsRatio:Double,ci95Lower:Double,ci95Upper:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio; ci95Lower (Double) – lower bound for 95% confidence interval; ci95Upper (Double) – upper bound for 95% confidence interval. Calculates the p-value, odds ratio, and 95% confidence interval with Fisher’s exact test (FET) for 2x2 tables.; Examples; Annotate each variant with Fisher’s exact test association results (assumes minor/major allele count variant annotations have been computed):; >>> (vds.annotate_variants_expr(; ... 'va.fet = let macCase = gs.filter(g => sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'macControl = gs.filter(g => !sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'majCase = gs.filter(g => sa.pheno.isCase).map(g => 2 - g.nNonRefAlleles()).sum() and '; ... 'majControl = gs.filter(g => !sa.pheno.isCase).map(g => 2 - g.nNonRefAlleles()).sum() in '; ... 'fet(macCase, macControl, majCase, majControl)')). Notes; fet is identical to the version implemented in R with default parameters (two-sided, alpha = 0.05, null hypothesis that the odds ratio equals 1).; Arguments. a (Int) – value for cell 1; b (Int) – value for cell 2; c (Int) – value for cell 3; d (Int) – value for cell 4. filtering_allele_frequency(ac: Int, an: ",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:4991,Testability,test,test,4991,"e non-negative. drop(s: Struct, identifiers: String*): Struct. Return a new Struct with the a subset of fields not matching identifiers.; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in drop(s, gene, function); result: {nHet: 12}. Arguments. s (Struct) – Struct to drop fields from.; identifiers (String*) – Field names to drop from s. Multiple arguments allowed. exp(x: Double): Double. Returns Euler’s number e raised to the power of the given value x.; Arguments. x (Double) – the exponent to raise e to. fet(a: Int, b: Int, c: Int, d: Int): Struct{pValue:Double,oddsRatio:Double,ci95Lower:Double,ci95Upper:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio; ci95Lower (Double) – lower bound for 95% confidence interval; ci95Upper (Double) – upper bound for 95% confidence interval. Calculates the p-value, odds ratio, and 95% confidence interval with Fisher’s exact test (FET) for 2x2 tables.; Examples; Annotate each variant with Fisher’s exact test association results (assumes minor/major allele count variant annotations have been computed):; >>> (vds.annotate_variants_expr(; ... 'va.fet = let macCase = gs.filter(g => sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'macControl = gs.filter(g => !sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'majCase = gs.filter(g => sa.pheno.isCase).map(g => 2 - g.nNonRefAlleles()).sum() and '; ... 'majControl = gs.filter(g => !sa.pheno.isCase).map(g => 2 - g.nNonRefAlleles()).sum() in '; ... 'fet(macCase, macControl, majCase, majControl)')). Notes; fet is identical to the version implemented in R with default parameters (two-sided, alpha = 0.05, null hypothesis that the odds ratio equals 1).; Arguments. a (Int) – value for cell 1; b (Int) – value for cell 2; c (Int) – value for cell 3; d (Int) – value for cell 4. filtering_allele_frequency(ac: Int, an: Int, ci: Double): Double. Computes a filtering allele frequency (described below); for ac and an with confidence ci.; The filtering allel",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:11901,Testability,log,log,11901,"ht exclusive. This means that [chr1:1, chr1:3) contains chr1:1 and chr1:2.; Arguments. startLocus (Locus) – Start position of interval; endLocus (Locus) – End position of interval. isDefined(a: T): Boolean – Returns true if item is non-missing. Otherwise, false. isMissing(a: T): Boolean – Returns true if item is missing. Otherwise, false. isnan(a: Double): Boolean – Returns true if the argument is NaN (not a number), false if the argument is defined but not NaN. Returns missing if the argument is missing. json(x: T): String – Returns the JSON representation of a data type. Locus(contig: String, pos: Int): Locus. Construct a Locus object.; let l = Locus(""1"", 10040532) in l.position; result: 10040532. Arguments. contig (String) – String representation of contig.; pos (Int) – SNP position or start of an indel. Locus(s: String): Locus. Construct a Locus object.; let l = Locus(""1:10040532"") in l.position; result: 10040532. Arguments. s (String) – String of the form CHR:POS. log(x: Double, b: Double): Double. Returns the base b logarithm of the given value x.; Arguments. x (Double) – the number to take the base b logarithm of.; b (Double) – the base. log(x: Double): Double. Returns the natural logarithm of the given value x.; Arguments. x (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.phe",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:11955,Testability,log,logarithm,11955,":3) contains chr1:1 and chr1:2.; Arguments. startLocus (Locus) – Start position of interval; endLocus (Locus) – End position of interval. isDefined(a: T): Boolean – Returns true if item is non-missing. Otherwise, false. isMissing(a: T): Boolean – Returns true if item is missing. Otherwise, false. isnan(a: Double): Boolean – Returns true if the argument is NaN (not a number), false if the argument is defined but not NaN. Returns missing if the argument is missing. json(x: T): String – Returns the JSON representation of a data type. Locus(contig: String, pos: Int): Locus. Construct a Locus object.; let l = Locus(""1"", 10040532) in l.position; result: 10040532. Arguments. contig (String) – String representation of contig.; pos (Int) – SNP position or start of an indel. Locus(s: String): Locus. Construct a Locus object.; let l = Locus(""1:10040532"") in l.position; result: 10040532. Arguments. s (String) – String of the form CHR:POS. log(x: Double, b: Double): Double. Returns the base b logarithm of the given value x.; Arguments. x (Double) – the number to take the base b logarithm of.; b (Double) – the base. log(x: Double): Double. Returns the natural logarithm of the given value x.; Arguments. x (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:12042,Testability,log,logarithm,12042,"tart position of interval; endLocus (Locus) – End position of interval. isDefined(a: T): Boolean – Returns true if item is non-missing. Otherwise, false. isMissing(a: T): Boolean – Returns true if item is missing. Otherwise, false. isnan(a: Double): Boolean – Returns true if the argument is NaN (not a number), false if the argument is defined but not NaN. Returns missing if the argument is missing. json(x: T): String – Returns the JSON representation of a data type. Locus(contig: String, pos: Int): Locus. Construct a Locus object.; let l = Locus(""1"", 10040532) in l.position; result: 10040532. Arguments. contig (String) – String representation of contig.; pos (Int) – SNP position or start of an indel. Locus(s: String): Locus. Construct a Locus object.; let l = Locus(""1:10040532"") in l.position; result: 10040532. Arguments. s (String) – String of the form CHR:POS. log(x: Double, b: Double): Double. Returns the base b logarithm of the given value x.; Arguments. x (Double) – the number to take the base b logarithm of.; b (Double) – the base. log(x: Double): Double. Returns the natural logarithm of the given value x.; Arguments. x (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns valu",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:12080,Testability,log,log,12080,"erval. isDefined(a: T): Boolean – Returns true if item is non-missing. Otherwise, false. isMissing(a: T): Boolean – Returns true if item is missing. Otherwise, false. isnan(a: Double): Boolean – Returns true if the argument is NaN (not a number), false if the argument is defined but not NaN. Returns missing if the argument is missing. json(x: T): String – Returns the JSON representation of a data type. Locus(contig: String, pos: Int): Locus. Construct a Locus object.; let l = Locus(""1"", 10040532) in l.position; result: 10040532. Arguments. contig (String) – String representation of contig.; pos (Int) – SNP position or start of an indel. Locus(s: String): Locus. Construct a Locus object.; let l = Locus(""1:10040532"") in l.position; result: 10040532. Arguments. s (String) – String of the form CHR:POS. log(x: Double, b: Double): Double. Returns the base b logarithm of the given value x.; Arguments. x (Double) – the number to take the base b logarithm of.; b (Double) – the base. log(x: Double): Double. Returns the natural logarithm of the given value x.; Arguments. x (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Dou",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:12124,Testability,log,logarithm,12124,"rns true if item is non-missing. Otherwise, false. isMissing(a: T): Boolean – Returns true if item is missing. Otherwise, false. isnan(a: Double): Boolean – Returns true if the argument is NaN (not a number), false if the argument is defined but not NaN. Returns missing if the argument is missing. json(x: T): String – Returns the JSON representation of a data type. Locus(contig: String, pos: Int): Locus. Construct a Locus object.; let l = Locus(""1"", 10040532) in l.position; result: 10040532. Arguments. contig (String) – String representation of contig.; pos (Int) – SNP position or start of an indel. Locus(s: String): Locus. Construct a Locus object.; let l = Locus(""1:10040532"") in l.position; result: 10040532. Arguments. s (String) – String of the form CHR:POS. log(x: Double, b: Double): Double. Returns the base b logarithm of the given value x.; Arguments. x (Double) – the number to take the base b logarithm of.; b (Double) – the base. log(x: Double): Double. Returns the natural logarithm of the given value x.; Arguments. x (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p ",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:12212,Testability,log,logarithm,12212," Boolean – Returns true if item is missing. Otherwise, false. isnan(a: Double): Boolean – Returns true if the argument is NaN (not a number), false if the argument is defined but not NaN. Returns missing if the argument is missing. json(x: T): String – Returns the JSON representation of a data type. Locus(contig: String, pos: Int): Locus. Construct a Locus object.; let l = Locus(""1"", 10040532) in l.position; result: 10040532. Arguments. contig (String) – String representation of contig.; pos (Int) – SNP position or start of an indel. Locus(s: String): Locus. Construct a Locus object.; let l = Locus(""1:10040532"") in l.position; result: 10040532. Arguments. s (String) – String of the form CHR:POS. log(x: Double, b: Double): Double. Returns the base b logarithm of the given value x.; Arguments. x (Double) – the number to take the base b logarithm of.; b (Double) – the base. log(x: Double): Double. Returns the natural logarithm of the given value x.; Arguments. x (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random v",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:12272,Testability,log,logarithm,12272,"oolean – Returns true if the argument is NaN (not a number), false if the argument is defined but not NaN. Returns missing if the argument is missing. json(x: T): String – Returns the JSON representation of a data type. Locus(contig: String, pos: Int): Locus. Construct a Locus object.; let l = Locus(""1"", 10040532) in l.position; result: 10040532. Arguments. contig (String) – String representation of contig.; pos (Int) – SNP position or start of an indel. Locus(s: String): Locus. Construct a Locus object.; let l = Locus(""1:10040532"") in l.position; result: 10040532. Arguments. s (String) – String of the form CHR:POS. log(x: Double, b: Double): Double. Returns the base b logarithm of the given value x.; Arguments. x (Double) – the number to take the base b logarithm of.; b (Double) – the base. log(x: Double): Double. Returns the natural logarithm of the given value x.; Arguments. x (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. ",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:12360,Testability,log,logarithm,12360,"if the argument is defined but not NaN. Returns missing if the argument is missing. json(x: T): String – Returns the JSON representation of a data type. Locus(contig: String, pos: Int): Locus. Construct a Locus object.; let l = Locus(""1"", 10040532) in l.position; result: 10040532. Arguments. contig (String) – String representation of contig.; pos (Int) – SNP position or start of an indel. Locus(s: String): Locus. Construct a Locus object.; let l = Locus(""1:10040532"") in l.position; result: 10040532. Arguments. s (String) – String of the form CHR:POS. log(x: Double, b: Double): Double. Returns the base b logarithm of the given value x.; Arguments. x (Double) – the number to take the base b logarithm of.; b (Double) – the base. log(x: Double): Double. Returns the natural logarithm of the given value x.; Arguments. x (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Doubl",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:13924,Testability,log,logP,13924,"lse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom speci",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:14370,Testability,log,logP,14370,"rns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:14426,Testability,log,log,14426,"rns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:15305,Testability,log,logP,15305,"(Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative. range(start: Int, stop: Int, ste",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:15850,Testability,log,logP,15850,"with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative. range(start: Int, stop: Int, step: Int): Array[Int]. Generate an Array with values in the interval [start, stop) in increments of step.; let r = range(0, 5, 2) in r; result: [0, 2, 4]. Arguments. start (Int) – Starting number of the sequence.; stop (Int) – Generate numbers up to, but not including this number.; step (Int) – Difference between each number in the sequence. range(start: Int, stop: Int): Array[Int]. Generate an Array with values in the interval [start, stop).; let r = range(5, 8) in r; result: [5, 6, 7]. Arguments. start (Int) – Starting number of the sequence.; stop (Int) – Generate numbers up to, but n",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/functions.html:15905,Testability,log,log,15905,"with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative. range(start: Int, stop: Int, step: Int): Array[Int]. Generate an Array with values in the interval [start, stop) in increments of step.; let r = range(0, 5, 2) in r; result: [0, 2, 4]. Arguments. start (Int) – Starting number of the sequence.; stop (Int) – Generate numbers up to, but not including this number.; step (Int) – Difference between each number in the sequence. range(start: Int, stop: Int): Array[Int]. Generate an Array with values in the interval [start, stop).; let r = range(5, 8) in r; result: [5, 6, 7]. Arguments. start (Int) – Starting number of the sequence.; stop (Int) – Generate numbers up to, but n",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
https://hail.is/docs/0.1/genindex.html:1411,Performance,cache,cache,1411,bute). aggregate_by_key() (hail.KeyTable method). (hail.VariantDataset method). allele() (hail.representation.Variant method). alt (hail.representation.AltAllele attribute). alt() (hail.representation.Variant method). alt_allele() (hail.representation.Variant method). alt_alleles (hail.representation.Variant attribute). AltAllele (class in hail.representation). annotate() (hail.KeyTable method). annotate_alleles_expr() (hail.VariantDataset method). annotate_genotypes_expr() (hail.VariantDataset method). annotate_global() (hail.VariantDataset method). annotate_global_expr() (hail.VariantDataset method). annotate_samples_expr() (hail.VariantDataset method). annotate_samples_table() (hail.VariantDataset method). annotate_variants_db() (hail.VariantDataset method). annotate_variants_expr() (hail.VariantDataset method). annotate_variants_table() (hail.VariantDataset method). annotate_variants_vds() (hail.VariantDataset method). B. balding_nichols_model() (hail.HailContext method). C. cache() (hail.KeyTable method). (hail.VariantDataset method). Call (class in hail.representation). category() (hail.representation.AltAllele method). colkey_schema (hail.VariantDataset attribute). collect() (hail.KeyTable method). columns (hail.KeyTable attribute). complete_trios() (hail.representation.Pedigree method). concordance() (hail.VariantDataset method). contains() (hail.representation.Interval method). contig (hail.representation.Locus attribute). (hail.representation.Variant attribute). count() (hail.KeyTable method). (hail.VariantDataset method). count_variants() (hail.VariantDataset method). D. deduplicate() (hail.VariantDataset method). delete_va_attribute() (hail.VariantDataset method). dosage() (hail.representation.Genotype method). dp (hail.representation.Genotype attribute). drop() (hail.KeyTable method). drop_samples() (hail.VariantDataset method). drop_variants() (hail.VariantDataset method). E. end (hail.representation.Interval attribute). eval_expr() (hail.HailContext me,MatchSource.WIKI,docs/0.1/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/genindex.html
https://hail.is/docs/0.1/genindex.html:8062,Testability,log,logreg,8062,hondrial() (hail.representation.Variant method). is_MNP() (hail.representation.AltAllele method). is_not_called() (hail.representation.Call method). (hail.representation.Genotype method). is_SNP() (hail.representation.AltAllele method). is_transition() (hail.representation.AltAllele method). is_transversion() (hail.representation.AltAllele method). J. join() (hail.KeyTable method). (hail.VariantDataset method). K. key (hail.KeyTable attribute). key_by() (hail.KeyTable method). key_schema (hail.KinshipMatrix attribute). KeyTable (class in hail). KinshipMatrix (class in hail). L. ld_matrix() (hail.VariantDataset method). ld_prune() (hail.VariantDataset method). LDMatrix (class in hail). linreg() (hail.VariantDataset method). linreg3() (hail.VariantDataset method). linreg_burden() (hail.VariantDataset method). linreg_multi_pheno() (hail.VariantDataset method). lmmreg() (hail.VariantDataset method). Locus (class in hail.representation). locus() (hail.representation.Variant method). logreg() (hail.VariantDataset method). logreg_burden() (hail.VariantDataset method). M. make_table() (hail.VariantDataset method). matrix() (hail.KinshipMatrix method). (hail.LDMatrix method). maximal_independent_set() (hail.KeyTable method). mendel_errors() (hail.VariantDataset method). min_rep() (hail.VariantDataset method). mother (hail.representation.Trio attribute). N. naive_coalesce() (hail.VariantDataset method). num_alleles() (hail.representation.Variant method). num_alt_alleles() (hail.representation.Call method). (hail.representation.Genotype method). (hail.representation.Variant method). num_columns (hail.KeyTable attribute). num_genotypes() (hail.representation.Variant method). num_mismatch() (hail.representation.AltAllele method). num_partitions() (hail.KeyTable method). (hail.VariantDataset method). num_samples (hail.VariantDataset attribute). O. od() (hail.representation.Genotype method). one_hot_alleles() (hail.representation.Call method). (hail.representation.Genotype method),MatchSource.WIKI,docs/0.1/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/genindex.html
https://hail.is/docs/0.1/getting_started.html:812,Availability,down,download,812,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:882,Availability,down,download,882,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:1051,Availability,down,download,1051,"gation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:1995,Availability,avail,available,1995,"ark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2063,Availability,down,downloaded,2063," the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2331,Availability,down,download,2331,"package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluste",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:3180,Availability,echo,echo,3180,"stall with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/distributions/hail-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by runnin",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:773,Deployability,continuous,continuous,773,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:784,Deployability,integrat,integration,784,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:1948,Deployability,install,install,1948,"Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:1980,Deployability,install,install,1980,"ark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2121,Deployability,install,install,2121,"iables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$H",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2156,Deployability,install,install,2156,"iables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$H",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2400,Deployability,install,install,2400,"L_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Clouder",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2482,Deployability,install,install,2482,"L_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Clouder",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2505,Deployability,install,install,2505,"L_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Clouder",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2834,Deployability,install,installs,2834,"ew” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipyt",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:3366,Deployability,install,installed,3366,"wnload the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/distributions/hail-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by running the following commands. Note that you have to pass in; the existing SparkContext instance sc to the HailContext; constructor. >>> from hail import *; >>> hc = HailContext(). Files can be accessed from both Ha",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:5383,Deployability,install,install,5383," running on Google’s Dataproc, you’ll want to store your files in Google Storage. In most on premises clusters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributi",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:5511,Deployability,install,installed,5511,"ters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:5544,Deployability,install,installed,5544,"ds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spar",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:6775,Deployability,install,installing,6775,"a version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:7458,Deployability,install,installation,7458,"PartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:7864,Deployability,install,installed,7864," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:7907,Deployability,install,install,7907," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:1630,Energy Efficiency,power,powerful,1630,"patible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in t",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:784,Integrability,integrat,integration,784,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2852,Integrability,depend,dependencies,2852,"ew” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipyt",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:8249,Integrability,depend,dependencies,8249," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:8337,Integrability,depend,depends,8337," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:1140,Modifiability,variab,variables,1140,"ODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:2899,Modifiability,variab,variables,2899," install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/dis",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:4986,Modifiability,variab,variables,4986,"l-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by running the following commands. Note that you have to pass in; the existing SparkContext instance sc to the HailContext; constructor. >>> from hail import *; >>> hc = HailContext(). Files can be accessed from both Hadoop and Google Storage. If you’re running on Google’s Dataproc, you’ll want to store your files in Google Storage. In most on premises clusters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloude",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:6666,Performance,optimiz,optimized,6666,"a version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:6682,Performance,perform,performance,6682,"a version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:6711,Performance,scalab,scalability,6711,"a version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:7709,Performance,optimiz,optimized,7709," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:7758,Performance,load,load,7758," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:8006,Performance,load,load,8006," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:8087,Performance,load,load,8087," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:6927,Safety,avoid,avoid,6927,"ilarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log wi",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:4313,Security,access,accessed,4313,"n run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/distributions/hail-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by running the following commands. Note that you have to pass in; the existing SparkContext instance sc to the HailContext; constructor. >>> from hail import *; >>> hc = HailContext(). Files can be accessed from both Hadoop and Google Storage. If you’re running on Google’s Dataproc, you’ll want to store your files in Google Storage. In most on premises clusters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:353,Testability,test,tests,353,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:3569,Testability,log,log,3569,"ollowing commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/distributions/hail-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by running the following commands. Note that you have to pass in; the existing SparkContext instance sc to the HailContext; constructor. >>> from hail import *; >>> hc = HailContext(). Files can be accessed from both Hadoop and Google Storage. If you’re running on Google’s Dataproc, you’ll want to store your files in Google Storage. In most on premises clusters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vc",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:7965,Testability,log,log,7965," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:8206,Testability,test,tests,8206," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:8227,Testability,test,tests,8227," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:8322,Testability,log,logistf,8322," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:8462,Testability,test,tests,8462," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:8511,Testability,test,test,8511," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:1614,Usability,learn,learn,1614,"patible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in t",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:6960,Usability,learn,learn,6960,"ilarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log wi",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/getting_started.html:7066,Usability,simpl,simplify,7066,"ra1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
https://hail.is/docs/0.1/hail.HailContext.html:1290,Availability,error,error,1290,"rix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_running; Return the running Hail context in this Python session. grep; Grep big files, like, really fast. import_bgen; Import .bgen file(s) as variant dataset. import_gen; Import .gen file(s) as variant dataset. import_plink; Import PLINK binary file (BED, BIM, FAM",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:2610,Availability,down,down,2610,"Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_running; Return the running Hail context in this Python session. grep; Grep big files, like, really fast. import_bgen; Import .bgen file(s) as variant dataset. import_gen; Import .gen file(s) as variant dataset. import_plink; Import PLINK binary file (BED, BIM, FAM) as variant dataset. import_table; Import delimited text file (text table) as key table. import_vcf; Import VCF file(s) as variant dataset. index_bgen; Index .bgen files. read; Read .vds files as variant dataset. read_table; Read a KT file as key table. report; Print information and warnings about VCF + GEN import and deduplication. stop; Shut down the Hail context. write_partitioning; Write partitioning.json.gz file for legacy VDS file. balding_nichols_model(populations, samples, variants, num_partitions=None, pop_dist=None, fst=None, af_dist=<hail.stats.UniformDist instance>, seed=0)[source]¶; Simulate a variant dataset using the Balding-Nichols model.; Examples; To generate a VDS with 3 populations, 100 samples in total, and 1000 variants:; >>> vds = hc.balding_nichols_model(3, 100, 1000). To generate a VDS with 4 populations, 2000 samples, 5000 variants, 10 partitions, population distribution [0.1, 0.2, 0.3, 0.4], \(F_{ST}\) values [.02, .06, .04, .12], ancestral allele frequencies drawn from a truncated beta distribution with a = .01 and b = .05 over the interval [0.05, 1], and random seed 1:; >>> from hail.stats import TruncatedBetaDist; >>> vds = hc.balding_nichols_model(4, 40, 150, 10,; ... pop_dist=[0.1, 0.2, 0.3, 0.4],; ... fst=[.02, .06, .04, .12],; ... af_dist=TruncatedBetaDist(a=0.01, b=2.0, minVal=0.05, maxVal=1.0),",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:7535,Availability,recover,recovery,7535,"Parameters:; populations (int) – Number of populations.; samples (int) – Number of samples.; variants (int) – Number of variants.; num_partitions (int) – Number of partitions.; pop_dist (array of float or None) – Unnormalized population distribution; fst (array of float or None) – \(F_{ST}\) values; af_dist (UniformDist or BetaDist or TruncatedBetaDist) – Ancestral allele frequency distribution; seed (int) – Random seed. Returns:Variant dataset simulated using the Balding-Nichols model. Return type:VariantDataset. eval_expr(expr)[source]¶; Evaluate an expression. Parameters:expr (str) – Expression to evaluate. Return type:annotation. eval_expr_typed(expr)[source]¶; Evaluate an expression and return the result as well as its type. Parameters:expr (str) – Expression to evaluate. Return type:(annotation, Type). static get_running()[source]¶; Return the running Hail context in this Python session.; Example; >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen f",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:7555,Availability,recover,recover,7555,"ations.; samples (int) – Number of samples.; variants (int) – Number of variants.; num_partitions (int) – Number of partitions.; pop_dist (array of float or None) – Unnormalized population distribution; fst (array of float or None) – \(F_{ST}\) values; af_dist (UniformDist or BetaDist or TruncatedBetaDist) – Ancestral allele frequency distribution; seed (int) – Random seed. Returns:Variant dataset simulated using the Balding-Nichols model. Return type:VariantDataset. eval_expr(expr)[source]¶; Evaluate an expression. Parameters:expr (str) – Expression to evaluate. Return type:annotation. eval_expr_typed(expr)[source]¶; Evaluate an expression and return the result as well as its type. Parameters:expr (str) – Expression to evaluate. Return type:(annotation, Type). static get_running()[source]¶; Return the running Hail context in this Python session.; Example; >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing ",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:8454,Availability,toler,tolerance,8454,"> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing a BGEN file as a VDS (assuming it has already been indexed).; >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities ar",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:9632,Availability,toler,tolerance,9632,"as variant dataset.; Examples; Importing a BGEN file as a VDS (assuming it has already been indexed).; >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. ",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:9674,Availability,toler,tolerance,9674,"ample3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. Returns:Variant dataset imported from .bgen file. Return type:VariantDataset. import_gen(path, sample_file=None, tolerance=0.2, min_part",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:10257,Availability,toler,tolerance,10257," probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. Returns:Variant dataset imported from .bgen file. Return type:VariantDataset. import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None)[source]¶; Import .gen file(s) as variant dataset.; Examples; Read a .gen file and a .sample file and write to a .vds file:; >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with Hadoop glob patterns:; >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). Notes; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:10358,Availability,toler,tolerance,10358,"made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. Returns:Variant dataset imported from .bgen file. Return type:VariantDataset. import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None)[source]¶; Import .gen file(s) as variant dataset.; Examples; Read a .gen file and a .sample file and write to a .vds file:; >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with Hadoop glob patterns:; >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). Notes; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5 columns before the start of the genotype probability data (chromosome f",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:10652,Availability,toler,tolerance,10652,"e parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. Returns:Variant dataset imported from .bgen file. Return type:VariantDataset. import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None)[source]¶; Import .gen file(s) as variant dataset.; Examples; Read a .gen file and a .sample file and write to a .vds file:; >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with Hadoop glob patterns:; >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). Notes; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the chromosome parameter; No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID s.; Also, see section in import_bgen() linked here for information about Hail’s genotype p",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:12015,Availability,toler,tolerance,12015,"_file='data/example.sample'); ... .write('output/gen_example2.vds')). Notes; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the chromosome parameter; No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID s.; Also, see section in import_bgen() linked here for information about Hail’s genotype probability representation.; Annotations; import_gen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .gen files to import.; sample_file (str) – The sample file.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing.; min_partitions (int or None) – Number of partitions.; chromosome (str or None) – Chromosome if not listed in the .gen file. Returns:Variant dataset imported from .gen and .sample files. Return type:VariantDataset. import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False)[source]¶; Import PLINK binary file (BED, BIM, FAM) as variant dataset.; Examples; Import data from a PLINK binary file:; >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). Notes; Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the --make-bed option.; The centiMorgan position is not currently used in Hail (Column 3 in BIM file).; The ID (s) used by Hail is the individual ID (column 2 in FAM file). Warning; No d",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:12124,Availability,toler,tolerance,12124,"; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the chromosome parameter; No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID s.; Also, see section in import_bgen() linked here for information about Hail’s genotype probability representation.; Annotations; import_gen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .gen files to import.; sample_file (str) – The sample file.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing.; min_partitions (int or None) – Number of partitions.; chromosome (str or None) – Chromosome if not listed in the .gen file. Returns:Variant dataset imported from .gen and .sample files. Return type:VariantDataset. import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False)[source]¶; Import PLINK binary file (BED, BIM, FAM) as variant dataset.; Examples; Import data from a PLINK binary file:; >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). Notes; Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the --make-bed option.; The centiMorgan position is not currently used in Hail (Column 3 in BIM file).; The ID (s) used by Hail is the individual ID (column 2 in FAM file). Warning; No duplicate individual IDs are allowed. Chromosome names (Column 1) are automa",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:23100,Availability,down,downstream,23100,"be queried for filter membership with expressions ; like va.filters.contains(""VQSRTranche99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Ret",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:25555,Availability,down,down,25555,"t as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a waste of resources, so indexing should generally be done; as a one-time step separately from large analyses. Parameters:path (str or list of str) – .bgen files to index. read(path, drop_samples=False, drop_variants=False)[source]¶; Read .vds files as variant dataset.; When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. Parameters:; path (str or list of str) – VDS files to read.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations; or gneotypes.; drop_variants (bool) – If True, create samples-only variant; dataset (no variants or genotypes). Returns:Variant dataset read from disk. Return type:VariantDataset. read_table(path)[source]¶; Read a KT file as key table. Parameters:path (str) – KT file to read. Returns:Key table read from disk. Return type:KeyTable. report()[source]¶; Print information and warnings about VCF + GEN import and deduplication. stop()[source]¶; Shut down the Hail context.; It is not possible to have multiple Hail contexts running in a; single Python session, so use this if you need to reconfigure the Hail; context. Note that this also stops a running Spark context. version¶; Return the version of Hail associated with this HailContext. Return type:str. write_partitioning(path)[source]¶; Write partitioning.json.gz file for legacy VDS file. Parameters:path (str) – path to VDS file. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:900,Deployability,configurat,configuration,900,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:5122,Deployability,continuous,continuous,5122,"s.; The population distribution \(\pi\) defaults to uniform; The \(F_{ST}\) values default to 0.1; The number of partitions defaults to one partition per million genotypes (i.e., samples * variants / 10^6) or 8, whichever is larger. The Balding-Nichols model models genotypes of individuals from a structured population comprising \(K\) homogeneous subpopulations; that have each diverged from a single ancestral population (a star phylogeny). We take \(N\) samples and \(M\) bi-allelic variants in perfect; linkage equilibrium. The relative sizes of the subpopulations are given by a probability vector \(\pi\); the ancestral allele frequencies are; drawn independently from a frequency spectrum \(P_0\); the subpopulations have diverged with possibly different \(F_{ST}\) parameters \(F_k\); (here and below, lowercase indices run over a range bounded by the corresponding uppercase parameter, e.g. \(k = 1, \ldots, K\)).; For each variant, the subpopulation allele frequencies are drawn a beta distribution, a useful continuous approximation of; the effect of genetic drift. We denote the individual subpopulation memberships by \(k_n\), the ancestral allele frequences by \(p_{0, m}\),; the subpopulation allele frequencies by \(p_{k, m}\), and the genotypes by \(g_{n, m}\). The generative model in then given by:. \[ \begin{align}\begin{aligned}k_n \,&\sim\, \pi\\p_{0,m}\,&\sim\, P_0\\p_{k,m}\mid p_{0,m}\,&\sim\, \mathrm{Beta}(\mu = p_{0,m},\, \sigma^2 = F_k p_{0,m}(1 - p_{0,m}))\\g_{n,m}\mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m})\end{aligned}\end{align} \]; We have parametrized the beta distribution by its mean and variance; the usual parameters are \(a = (1 - p)(1 - F)/F,\; b = p(1-F)/F\) with \(F = F_k,\; p = p_{0,m}\).; Annotations; balding_nichols_model() adds the following global, sample, and variant annotations:. global.nPops (Int) – Number of populations; global.nSamples (Int) – Number of samples; global.nVariants (Int) – Number of variants; global.popDist",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:18146,Integrability,depend,depends,18146,"es the representation of missing data in the table. Note; The comment and missing parameters are NOT regexes. The no_header option indicates that the file has no header line. If this option is passed, ; then the column names will be f0, f1, … fN (0-indexed).; The types option allows the user to pass the types of columns in the table. This is a ; dict keyed by str, with Type values. See the examples above for; a standard usage. Additionally, this option can be used to override type imputation. For example,; if a column in a file refers to chromosome and does not contain any sex chromosomes, it will be; imputed as an integer, while most Hail methods expect chromosome to be passed as a string. Using; the impute=True mode and passing types={'Chromosome': TString()} will solve this problem.; The min_partitions option can be used to increase the number of partitions (level of sharding); of an imported table. The default partition size depends on file system and a number of other ; factors (including the min_block_size of the hail context), but usually is between 32M and 128M. Parameters:; paths (str or list of str) – Files to import.; key (str or list of str) – Key column(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – File has no header and the N columns are named f0, f1, … fN (0-indexed); impute (bool) – Impute column types from the file; comment (str or None) – Skip lines beginning with the given pattern; delimiter (str) – Field delimiter regex; missing (str) – Specify identifier to be treated as missing; types (dict with str keys and Type values) – Define types of fields in annotations files; quote (str or None) – Quote character. Returns:Key table constructed from text table. Return type:KeyTable. import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, store_gq=False, pp_as_pl=False, skip_bad_ad=False, generic=False, call_fields=[])[source]¶; Import VCF file(s) as variant dataset.;",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:900,Modifiability,config,configuration,900,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:9224,Performance,load,load,9224,"s at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing a BGEN file as a VDS (assuming it has already been indexed).; >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:19392,Performance,load,load,19392,"r) – Key column(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – File has no header and the N columns are named f0, f1, … fN (0-indexed); impute (bool) – Impute column types from the file; comment (str or None) – Skip lines beginning with the given pattern; delimiter (str) – Field delimiter regex; missing (str) – Specify identifier to be treated as missing; types (dict with str keys and Type values) – Define types of fields in annotations files; quote (str or None) – Quote character. Returns:Key table constructed from text table. Return type:KeyTable. import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, store_gq=False, pp_as_pl=False, skip_bad_ad=False, generic=False, call_fields=[])[source]¶; Import VCF file(s) as variant dataset.; Examples; >>> vds = hc.import_vcf('data/example2.vcf.bgz'). Notes; Hail is designed to be maximally compatible with files in the VCF v4.2 spec.; import_vcf() takes a list of VCF files to load. All files must have the same header and the same set of samples in the same order; (e.g., a variant dataset split by chromosome). Files can be specified as Hadoop glob patterns.; Ensure that the VCF file is correctly prepared for import: VCFs should either be uncompressed (.vcf) or block compressed; (.vcf.bgz). If you have a large compressed VCF that ends in .vcf.gz, it is likely that the file is actually block-compressed,; and you should rename the file to “.vcf.bgz” accordingly. If you actually have a standard gzipped file, it is possible to import; it to Hail using the force optional parameter. However, this is not recommended – all parsing will have to take place on one node because; gzip decompression is not parallelizable. In this case, import could take significantly longer.; If generic equals False (default), Hail makes certain assumptions about the genotype fields, see Representation. On import, Hail filters; (sets to no-call) any genotype that violate",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:21062,Performance,perform,performance,21062,"mmended – all parsing will have to take place on one node because; gzip decompression is not parallelizable. In this case, import could take significantly longer.; If generic equals False (default), Hail makes certain assumptions about the genotype fields, see Representation. On import, Hail filters; (sets to no-call) any genotype that violates these assumptions. Hail interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are; silently dropped.; If generic equals True, the genotype schema is a TStruct with field names equal to the IDs of the FORMAT fields.; The GT field is automatically read in as a TCall type. To specify additional fields to import as a; TCall type, use the call_fields parameter. All other fields are imported as the type specified in the FORMAT header field.; An example genotype schema after importing a VCF with generic=True is; Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. Warning. The variant dataset generated with generic=True will have significantly slower performance.; Not all VariantDataset methods will work with a generic genotype schema.; The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. import_vcf() does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant.; Since Hail’s genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If generic=False, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS “genotype schema.; Below are two example haploid genotypes and diploid equivalents that Hail sees.; Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:0,0,9:9:24:24,1000,40,1000:1000:0. Note; Using the FILTER field:; Th",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:21314,Performance,perform,perform,21314," interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are; silently dropped.; If generic equals True, the genotype schema is a TStruct with field names equal to the IDs of the FORMAT fields.; The GT field is automatically read in as a TCall type. To specify additional fields to import as a; TCall type, use the call_fields parameter. All other fields are imported as the type specified in the FORMAT header field.; An example genotype schema after importing a VCF with generic=True is; Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. Warning. The variant dataset generated with generic=True will have significantly slower performance.; Not all VariantDataset methods will work with a generic genotype schema.; The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. import_vcf() does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant.; Since Hail’s genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If generic=False, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS “genotype schema.; Below are two example haploid genotypes and diploid equivalents that Hail sees.; Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:0,0,9:9:24:24,1000,40,1000:1000:0. Note; Using the FILTER field:; The information in the FILTER field of a VCF is contained in the va.filters annotation.; This annotation is a Set and can be queried for filter membership with expressions ; like va.filters.contains(""VQSRTranche99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variant",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:23056,Performance,load,load,23056,"he FILTER field of a VCF is contained in the va.filters annotation.; This annotation is a Set and can be queried for filter membership with expressions ; like va.filters.contains(""VQSRTranche99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fi",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:23247,Performance,load,load,23247,"he99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:23328,Performance,load,load,23328,"lters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cann",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:23528,Performance,load,load,23528,"otations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a wast",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:24820,Performance,load,loading,24820,"to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a waste of resources, so indexing should generally be done; as a one-time step separately from large analyses. Parameters:path (str or list of str) – .bgen files to index. read(path, drop_samples=False, drop_variants=False)[source]¶; Read .vds files as variant dataset.; When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. Parameters:; path (str or list of str) – VDS files to read.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations; or gneotypes.; drop_variants (bool) – If True, create samples-only variant; dataset (no variants or genotypes). Returns:Variant dataset read from disk. Return type:VariantDataset. read_table(path)[source]¶; Read a KT file as key table. Parameters:path (str) – KT file to read. Returns:Key table read from disk. Return type:KeyTable. report()[source]¶; Print information and warnings about VCF + GEN import and deduplication. stop()[source]¶; Shut down the Hail context.; It is not possible to have multiple Hail contexts running in a; single Python session, so use this if you need to reconfigure the Hail; context. Note that this also stops a running Spark context. version¶; Return the version of Hail associated with this HailContext. Return type:str. write_partit",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:25071,Performance,load,load,25071,"CF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a waste of resources, so indexing should generally be done; as a one-time step separately from large analyses. Parameters:path (str or list of str) – .bgen files to index. read(path, drop_samples=False, drop_variants=False)[source]¶; Read .vds files as variant dataset.; When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. Parameters:; path (str or list of str) – VDS files to read.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations; or gneotypes.; drop_variants (bool) – If True, create samples-only variant; dataset (no variants or genotypes). Returns:Variant dataset read from disk. Return type:VariantDataset. read_table(path)[source]¶; Read a KT file as key table. Parameters:path (str) – KT file to read. Returns:Key table read from disk. Return type:KeyTable. report()[source]¶; Print information and warnings about VCF + GEN import and deduplication. stop()[source]¶; Shut down the Hail context.; It is not possible to have multiple Hail contexts running in a; single Python session, so use this if you need to reconfigure the Hail; context. Note that this also stops a running Spark context. version¶; Return the version of Hail associated with this HailContext. Return type:str. write_partitioning(path)[source]¶; Write partitioning.json.gz file for legacy VDS file. Parameters:path (str) – path to VDS file. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:7535,Safety,recover,recovery,7535,"Parameters:; populations (int) – Number of populations.; samples (int) – Number of samples.; variants (int) – Number of variants.; num_partitions (int) – Number of partitions.; pop_dist (array of float or None) – Unnormalized population distribution; fst (array of float or None) – \(F_{ST}\) values; af_dist (UniformDist or BetaDist or TruncatedBetaDist) – Ancestral allele frequency distribution; seed (int) – Random seed. Returns:Variant dataset simulated using the Balding-Nichols model. Return type:VariantDataset. eval_expr(expr)[source]¶; Evaluate an expression. Parameters:expr (str) – Expression to evaluate. Return type:annotation. eval_expr_typed(expr)[source]¶; Evaluate an expression and return the result as well as its type. Parameters:expr (str) – Expression to evaluate. Return type:(annotation, Type). static get_running()[source]¶; Return the running Hail context in this Python session.; Example; >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen f",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:7555,Safety,recover,recover,7555,"ations.; samples (int) – Number of samples.; variants (int) – Number of variants.; num_partitions (int) – Number of partitions.; pop_dist (array of float or None) – Unnormalized population distribution; fst (array of float or None) – \(F_{ST}\) values; af_dist (UniformDist or BetaDist or TruncatedBetaDist) – Ancestral allele frequency distribution; seed (int) – Random seed. Returns:Variant dataset simulated using the Balding-Nichols model. Return type:VariantDataset. eval_expr(expr)[source]¶; Evaluate an expression. Parameters:expr (str) – Expression to evaluate. Return type:annotation. eval_expr_typed(expr)[source]¶; Evaluate an expression and return the result as well as its type. Parameters:expr (str) – Expression to evaluate. Return type:(annotation, Type). static get_running()[source]¶; Return the running Hail context in this Python session.; Example; >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing ",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:487,Testability,log,log,487,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:497,Testability,log,log,497,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:1214,Testability,log,log,1214,"antDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_running; Return the running Hail context in this Python session. grep; Grep big files, like, really fast. import_bgen; Import .bgen file(s) as variant dataset. import_gen; Import .gen file(s) as variant dataset. import_plin",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:1258,Testability,log,logging,1258,"rix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_running; Return the running Hail context in this Python session. grep; Grep big files, like, really fast. import_bgen; Import .bgen file(s) as variant dataset. import_gen; Import .gen file(s) as variant dataset. import_plink; Import PLINK binary file (BED, BIM, FAM",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:1323,Testability,log,log,1323,"sources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_running; Return the running Hail context in this Python session. grep; Grep big files, like, really fast. import_bgen; Import .bgen file(s) as variant dataset. import_gen; Import .gen file(s) as variant dataset. import_plink; Import PLINK binary file (BED, BIM, FAM) as variant dataset. import_table; Import delimited text file ",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:12637,Testability,test,test,12637,"s genotype probability representation.; Annotations; import_gen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .gen files to import.; sample_file (str) – The sample file.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing.; min_partitions (int or None) – Number of partitions.; chromosome (str or None) – Chromosome if not listed in the .gen file. Returns:Variant dataset imported from .gen and .sample files. Return type:VariantDataset. import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False)[source]¶; Import PLINK binary file (BED, BIM, FAM) as variant dataset.; Examples; Import data from a PLINK binary file:; >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). Notes; Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the --make-bed option.; The centiMorgan position is not currently used in Hail (Column 3 in BIM file).; The ID (s) used by Hail is the individual ID (column 2 in FAM file). Warning; No duplicate individual IDs are allowed. Chromosome names (Column 1) are automatically converted in the following cases:. 23 => “X”; 24 => “Y”; 25 => “X”; 26 => “MT”. Annotations; import_plink() adds the following annotations:. va.rsid (String) – Column 2 in the BIM file.; sa.famID (String) – Column 1 in the FAM file. Set to missing if ID equals “0”.; sa.patID (String) – Column 3 in the FAM file. Set to missing if ID equals “0”.; sa.matID (String) – Column 4 in the FAM file. Set to missing if ID equals “0”.; sa.isFemale (String) – Column 5 in the FAM file. Set to mis",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:12663,Testability,test,test,12663,"ation.; Annotations; import_gen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .gen files to import.; sample_file (str) – The sample file.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing.; min_partitions (int or None) – Number of partitions.; chromosome (str or None) – Chromosome if not listed in the .gen file. Returns:Variant dataset imported from .gen and .sample files. Return type:VariantDataset. import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False)[source]¶; Import PLINK binary file (BED, BIM, FAM) as variant dataset.; Examples; Import data from a PLINK binary file:; >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). Notes; Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the --make-bed option.; The centiMorgan position is not currently used in Hail (Column 3 in BIM file).; The ID (s) used by Hail is the individual ID (column 2 in FAM file). Warning; No duplicate individual IDs are allowed. Chromosome names (Column 1) are automatically converted in the following cases:. 23 => “X”; 24 => “Y”; 25 => “X”; 26 => “MT”. Annotations; import_plink() adds the following annotations:. va.rsid (String) – Column 2 in the BIM file.; sa.famID (String) – Column 1 in the FAM file. Set to missing if ID equals “0”.; sa.patID (String) – Column 3 in the FAM file. Set to missing if ID equals “0”.; sa.matID (String) – Column 4 in the FAM file. Set to missing if ID equals “0”.; sa.isFemale (String) – Column 5 in the FAM file. Set to missing if value equals “-9”, “0”, ",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.HailContext.html:12689,Testability,test,test,12689,"t_gen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .gen files to import.; sample_file (str) – The sample file.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing.; min_partitions (int or None) – Number of partitions.; chromosome (str or None) – Chromosome if not listed in the .gen file. Returns:Variant dataset imported from .gen and .sample files. Return type:VariantDataset. import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False)[source]¶; Import PLINK binary file (BED, BIM, FAM) as variant dataset.; Examples; Import data from a PLINK binary file:; >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). Notes; Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the --make-bed option.; The centiMorgan position is not currently used in Hail (Column 3 in BIM file).; The ID (s) used by Hail is the individual ID (column 2 in FAM file). Warning; No duplicate individual IDs are allowed. Chromosome names (Column 1) are automatically converted in the following cases:. 23 => “X”; 24 => “Y”; 25 => “X”; 26 => “MT”. Annotations; import_plink() adds the following annotations:. va.rsid (String) – Column 2 in the BIM file.; sa.famID (String) – Column 1 in the FAM file. Set to missing if ID equals “0”.; sa.patID (String) – Column 3 in the FAM file. Set to missing if ID equals “0”.; sa.matID (String) – Column 4 in the FAM file. Set to missing if ID equals “0”.; sa.isFemale (String) – Column 5 in the FAM file. Set to missing if value equals “-9”, “0”, or “N/A”.; Set to true if ",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
https://hail.is/docs/0.1/hail.KeyTable.html:13548,Availability,error,error,13548,"val. If the .bed file has four or more columns, then Hail will store the fourth column in the table:. interval (Interval) - Genomic interval.; target (String) - Fourth column of .bed file. UCSC bed files can have up to 12 fields, ; but Hail will only ever look at the first four. Hail ignores header lines in BED files. Caution; UCSC BED files are 0-indexed and end-exclusive. The line “5 100 105” will contain; locus 5:105 but not 5:100. Details here. Parameters:path (str) – Path to .bed file. Return type:KeyTable. static import_fam(path, quantitative=False, delimiter='\\\\s+', missing='NA')[source]¶; Import PLINK .fam file into a key table.; Examples; Import case-control phenotype data from a tab-separated PLINK .fam file into sample; annotations:; >>> fam_kt = KeyTable.import_fam('data/myStudy.fam'). In Hail, unlike PLINK, the user must explicitly distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype without quantitative=True will return an error; (unless all values happen to be 0, 1, 2, and -9):; >>> fam_kt = KeyTable.import_fam('data/myStudy.fam', quantitative=True). Columns; The column, types, and missing values are shown below. ID (String) – Sample ID (key column); famID (String) – Family ID (missing = “0”); patID (String) – Paternal ID (missing = “0”); matID (String) – Maternal ID (missing = “0”); isFemale (Boolean) – Sex (missing = “NA”, “-9”, “0”). One of:. isCase (Boolean) – Case-control phenotype (missing = “0”, “-9”, non-numeric or the missing argument, if given.; qPheno (Double) – Quantitative phenotype (missing = “NA” or the missing argument, if given. Parameters:; path (str) – Path to .fam file.; quantitative (bool) – If True, .fam phenotype is interpreted as quantitative.; delimiter (str) – .fam file field delimiter regex.; missing (str) – The string used to denote missing values.; For case-control, 0, -9, and non-numeric are also treated; as missing. Returns:Key table with information from .fam file. Return ",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:22271,Availability,redundant,redundant,22271,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:22331,Deployability,pipeline,pipelines,22331,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:8659,Modifiability,config,config,8659,"; [[3,4], []]. a; 2; [[3,4], []]. Explode c2 once and c3 twice:; >>> kt3.explode(['c2', 'c3', 'c3']). c1; c2; c3. a; 1; 3. a; 2; 3. a; 1; 4. a; 2; 4. Parameters:column_names (str or list of str) – Column name(s) to be exploded. Returns:Key table with columns exploded. Return type:KeyTable. export(output, types_file=None, header=True, parallel=False)[source]¶; Export to a TSV file.; Examples; Rename column names of key table and export to file:; >>> (kt1.rename({'HT' : 'Height'}); ... .export(""output/kt1_renamed.tsv"")). Parameters:; output (str) – Output file path.; types_file (str) – Output path of types file.; header (bool) – Write a header using the column names.; parallel (bool) – If true, writes a set of files (one per partition) rather than serially concatenating these files. export_cassandra(address, keyspace, table, block_size=100, rate=1000)[source]¶; Export to Cassandra. Warning; export_cassandra() is EXPERIMENTAL. export_elasticsearch(host, port, index, index_type, block_size, config=None, verbose=True)[source]¶; Export to Elasticsearch. Warning; export_elasticsearch() is EXPERIMENTAL. export_mongodb(mode='append')[source]¶; Export to MongoDB. Warning; export_mongodb() is EXPERIMENTAL. export_solr(zk_host, collection, block_size=100)[source]¶; Export to Solr. Warning; export_solr() is EXPERIMENTAL. filter(expr, keep=True)[source]¶; Filter rows.; Examples; Keep rows where C1 equals 5:; >>> kt_result = kt1.filter(""C1 == 5""). Remove rows where C1 equals 10:; >>> kt_result = kt1.filter(""C1 == 10"", keep=False). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Caution; When expr evaluates to missing, the row will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep rows where expr is true. Returns:Filtered key table. Return type:KeyTable. flatten()[source]¶; ",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:1504,Performance,cache,cache,1504,"mported from a text file or Spark DataFrame with import_table(); or from_dataframe(), generated from a variant dataset; with aggregate_by_key(), make_table(),; samples_table(), or variants_table().; In the examples below, we have imported two key tables from text files (kt1 and kt2).; >>> kt1 = hc.import_table('data/kt_example1.tsv', impute=True). ID; HT; SEX; X; Z; C1; C2; C3. 1; 65; M; 5; 4; 2; 50; 5. 2; 72; M; 6; 3; 2; 61; 1. 3; 70; F; 7; 3; 10; 81; -5. 4; 60; F; 8; 2; 11; 90; -10. >>> kt2 = hc.import_table('data/kt_example2.tsv', impute=True). ID; A; B. 1; 65; cat. 2; 72; dog. 3; 70; mouse. 4; 60; rabbit. Variables:hc (HailContext) – Hail Context. Attributes. columns; Names of all columns. key; List of key columns. num_columns; Number of columns. schema; Table schema. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate columns programmatically. annotate; Add new columns computed from existing columns. cache; Mark this key table to be cached in memory. collect; Collect table to a local list. count; Count the number of rows. drop; Drop columns. exists; Evaluate whether a boolean expression is true for at least one row. expand_types; Expand types Locus, Interval, AltAllele, Variant, Genotype, Char, Set and Dict. explode; Explode columns of this key table. export; Export to a TSV file. export_cassandra; Export to Cassandra. export_elasticsearch; Export to Elasticsearch. export_mongodb; Export to MongoDB. export_solr; Export to Solr. filter; Filter rows. flatten; Flatten nested Structs. forall; Evaluate whether a boolean expression is true for all rows. from_dataframe; Convert Spark SQL DataFrame to key table. from_pandas; Convert Pandas DataFrame to key table. from_py. import_bed; Import a UCSC .bed file as a key table. import_fam; Import PLINK .fam file into a key table. import_interval_list; Import an interval list file in the GATK standard format. indexed; Add the numerical index of each row as a new column.",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:1537,Performance,cache,cached,1537,"mported from a text file or Spark DataFrame with import_table(); or from_dataframe(), generated from a variant dataset; with aggregate_by_key(), make_table(),; samples_table(), or variants_table().; In the examples below, we have imported two key tables from text files (kt1 and kt2).; >>> kt1 = hc.import_table('data/kt_example1.tsv', impute=True). ID; HT; SEX; X; Z; C1; C2; C3. 1; 65; M; 5; 4; 2; 50; 5. 2; 72; M; 6; 3; 2; 61; 1. 3; 70; F; 7; 3; 10; 81; -5. 4; 60; F; 8; 2; 11; 90; -10. >>> kt2 = hc.import_table('data/kt_example2.tsv', impute=True). ID; A; B. 1; 65; cat. 2; 72; dog. 3; 70; mouse. 4; 60; rabbit. Variables:hc (HailContext) – Hail Context. Attributes. columns; Names of all columns. key; List of key columns. num_columns; Number of columns. schema; Table schema. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate columns programmatically. annotate; Add new columns computed from existing columns. cache; Mark this key table to be cached in memory. collect; Collect table to a local list. count; Count the number of rows. drop; Drop columns. exists; Evaluate whether a boolean expression is true for at least one row. expand_types; Expand types Locus, Interval, AltAllele, Variant, Genotype, Char, Set and Dict. explode; Explode columns of this key table. export; Export to a TSV file. export_cassandra; Export to Cassandra. export_elasticsearch; Export to Elasticsearch. export_mongodb; Export to MongoDB. export_solr; Export to Solr. filter; Filter rows. flatten; Flatten nested Structs. forall; Evaluate whether a boolean expression is true for all rows. from_dataframe; Convert Spark SQL DataFrame to key table. from_pandas; Convert Pandas DataFrame to key table. from_py. import_bed; Import a UCSC .bed file as a key table. import_fam; Import PLINK .fam file into a key table. import_interval_list; Import an interval list file in the GATK standard format. indexed; Add the numerical index of each row as a new column.",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:5075,Performance,cache,cache,5075,"put KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:; key_expr (str or list of str) – Named expression(s) for how to compute the keys of the new key table.; agg_expr (str or list of str) – Named aggregation expression(s). Returns:A new key table with the keys computed from the key_expr and the remaining columns computed from the agg_expr. Return type:KeyTable. annotate(expr)[source]¶; Add new columns computed from existing columns.; Examples; Add new column Y which is equal to 5 times X:; >>> kt_result = kt1.annotate(""Y = 5 * X""). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:expr (str or list of str) – Annotation expression or multiple annotation expressions. Returns:Key table with new columns specified by expr. Return type:KeyTable. cache()[source]¶; Mark this key table to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:KeyTable. collect()[source]¶; Collect table to a local list.; Examples; >>> id_to_sex = {row.ID : row.SEX for row in kt1.collect()}. Notes; This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. Return type:list of hail.representation.Struct. columns¶; Names of all columns.; >>> kt1.columns; [u'ID', u'HT', u'SEX', u'X', u'Z', u'C1', u'C2', u'C3']. Return type:list of str. count()[source]¶; Count the number of rows.; Examples; >>> kt1.count(). Return type:int. drop(column_names)[source]¶; Drop columns.; Examples; Assume kt1 is a KeyTable with three columns: C1, C2 and; C3.; Drop columns:; >>> kt_result = kt1.drop('C1'). >>> kt_result = kt1.drop(['C1', 'C2']). Parameters:column_names – List of columns to be dropped. Type:str",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:5119,Performance,cache,cached,5119,"put KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:; key_expr (str or list of str) – Named expression(s) for how to compute the keys of the new key table.; agg_expr (str or list of str) – Named aggregation expression(s). Returns:A new key table with the keys computed from the key_expr and the remaining columns computed from the agg_expr. Return type:KeyTable. annotate(expr)[source]¶; Add new columns computed from existing columns.; Examples; Add new column Y which is equal to 5 times X:; >>> kt_result = kt1.annotate(""Y = 5 * X""). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:expr (str or list of str) – Annotation expression or multiple annotation expressions. Returns:Key table with new columns specified by expr. Return type:KeyTable. cache()[source]¶; Mark this key table to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:KeyTable. collect()[source]¶; Collect table to a local list.; Examples; >>> id_to_sex = {row.ID : row.SEX for row in kt1.collect()}. Notes; This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. Return type:list of hail.representation.Struct. columns¶; Names of all columns.; >>> kt1.columns; [u'ID', u'HT', u'SEX', u'X', u'Z', u'C1', u'C2', u'C3']. Return type:list of str. count()[source]¶; Count the number of rows.; Examples; >>> kt1.count(). Return type:int. drop(column_names)[source]¶; Drop columns.; Examples; Assume kt1 is a KeyTable with three columns: C1, C2 and; C3.; Drop columns:; >>> kt_result = kt1.drop('C1'). >>> kt_result = kt1.drop(['C1', 'C2']). Parameters:column_names – List of columns to be dropped. Type:str",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:5138,Performance,cache,cache,5138,"ion on writing expressions; and using the Hail Expression Language. Parameters:; key_expr (str or list of str) – Named expression(s) for how to compute the keys of the new key table.; agg_expr (str or list of str) – Named aggregation expression(s). Returns:A new key table with the keys computed from the key_expr and the remaining columns computed from the agg_expr. Return type:KeyTable. annotate(expr)[source]¶; Add new columns computed from existing columns.; Examples; Add new column Y which is equal to 5 times X:; >>> kt_result = kt1.annotate(""Y = 5 * X""). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:expr (str or list of str) – Annotation expression or multiple annotation expressions. Returns:Key table with new columns specified by expr. Return type:KeyTable. cache()[source]¶; Mark this key table to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:KeyTable. collect()[source]¶; Collect table to a local list.; Examples; >>> id_to_sex = {row.ID : row.SEX for row in kt1.collect()}. Notes; This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. Return type:list of hail.representation.Struct. columns¶; Names of all columns.; >>> kt1.columns; [u'ID', u'HT', u'SEX', u'X', u'Z', u'C1', u'C2', u'C3']. Return type:list of str. count()[source]¶; Count the number of rows.; Examples; >>> kt1.count(). Return type:int. drop(column_names)[source]¶; Drop columns.; Examples; Assume kt1 is a KeyTable with three columns: C1, C2 and; C3.; Drop columns:; >>> kt_result = kt1.drop('C1'). >>> kt_result = kt1.drop(['C1', 'C2']). Parameters:column_names – List of columns to be dropped. Type:str or list of str. Returns:Key table with dropped columns",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:22186,Performance,cache,cache,22186,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:22311,Performance,perform,performance,22311,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:22343,Performance,cache,cache,22343,"on used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is passed, a list is returned.; The namespace of the expressions includes one aggregable for each column; of the key table",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:29871,Performance,perform,performed,29871,"before converting to; DataFrame.; flatten (bool) – If true, flatten before converting to; DataFrame. If both are true, flatten is run after expand so; that expanded types are flattened. Return type:pyspark.sql.DataFrame. to_pandas(expand=True, flatten=True)[source]¶; Converts this key table into a Pandas DataFrame. Parameters:; expand (bool) – If true, expand_types before converting to; Pandas DataFrame.; flatten (bool) – If true, flatten before converting to Pandas; DataFrame. If both are true, flatten is run after expand so; that expanded types are flattened. Returns:Pandas DataFrame constructed from the key table. Return type:pandas.DataFrame. union(*kts)[source]¶; Union the rows of multiple tables.; Examples; Take the union of rows from two tables:; >>> other = hc.import_table('data/kt_example1.tsv', impute=True); >>> union_kt = kt1.union(other). Notes; If a row appears in both tables identically, it is duplicated in; the result. The left and right tables must have the same schema; and key. Parameters:kts (args of type KeyTable) – Tables to merge. Returns:A table with all rows from the left and right tables. Return type:KeyTable. unpersist()[source]¶; Unpersists this table from memory/disk.; Notes; This function will have no effect on a table that was not previously persisted.; There’s nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist; a table when you are done with it. write(output, overwrite=False)[source]¶; Write as KT file.; *Examples*; >>> kt1.write('output/kt1.kt'). Note; The write path must end in “.kt”. Parameters:; output (str) – Path of KT file to write.; overwrite (bool) – If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:22265,Safety,avoid,avoid,22265,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:22271,Safety,redund,redundant,22271,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:16382,Usability,simpl,simple,16382,"ive of the end position. Note; Hail uses the following ordering for contigs: 1-22 sorted numerically, then X, Y, MT,; then alphabetically for any contig not matching the standard human chromosomes. Caution; The interval parser for these files does not support the full range of formats supported; by the python parser parse(). ‘k’, ‘m’, ‘start’, and ‘end’ are all; invalid motifs in the contig:start-end format here. Parameters:filename (str) – Path to file. Returns:Interval-keyed table. Return type:KeyTable. indexed(name='index')[source]¶; Add the numerical index of each row as a new column.; Examples; >>> ind_kt = kt1.indexed(). Notes; This method returns a table with a new column whose name is; given by the name parameter, with type Long. The value; of this column is the numerical index of each row, starting; from 0. Methods that respect ordering (like KeyTable.take(); or KeyTable.export() will return rows in order.; This method is helpful for creating a unique integer index for rows; of a table, so that more complex types can be encoded as a simple; number. Parameters:name (str) – Name of index column. Returns:Table with a new index column. Return type:KeyTable. join(right, how='inner')[source]¶; Join two key tables together.; Examples; Join kt1 to kt2 to produce kt_joined:; >>> kt_result = kt1.key_by('ID').join(kt2.key_by('ID')). Notes:; Hail supports four types of joins specified by how:. inner – Key must be present in both kt1 and kt2.; outer – Key present in kt1 or kt2. For keys only in kt1, the value of non-key columns from kt2 is set to missing.; Likewise, for keys only in kt2, the value of non-key columns from kt1 is set to missing.; left – Key present in kt1. For keys only in kt1, the value of non-key columns from kt2 is set to missing.; right – Key present in kt2. For keys only in kt2, the value of non-key columns from kt1 is set to missing. The non-key fields in kt2 must have non-overlapping column names with kt1.; Both key tables must have the same number ",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KeyTable.html:25906,Usability,simpl,simply,25906,"llion rows and twenty partitions:; >>> range_kt = KeyTable.range(1000000, num_partitions=20). Notes; The resulting table has one column:. index (Int) – Unique row index from 0 until n. Parameters:; n (int) – Number of rows.; num_partitions (int or None) – Number of partitions. Return type:KeyTable. rename(column_names)[source]¶; Rename columns of key table.; column_names can be either a list of new names or a dict; mapping old names to new names. If column_names is a list,; its length must be the number of columns in this KeyTable.; Examples; Rename using a list:; >>> kt2.rename(['newColumn1', 'newColumn2', 'newColumn3']). Rename using a dict:; >>> kt2.rename({'A' : 'C1'}). Parameters:column_names – list of new column names or a dict mapping old names to new names. Returns:Key table with renamed columns. Return type:KeyTable. repartition(n, shuffle=True)[source]¶; Change the number of distributed partitions. Warning; When shuffle is False, repartition can only decrease the number of partitions and simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance and so can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions. Parameters:; n (int) – Desired number of partitions.; shuffle (bool) – Whether to shuffle or naively coalesce. Return type:KeyTable. same(other)[source]¶; Test whether two key tables are identical.; Examples; >>> if kt1.same(kt2):; ... print(""KeyTables are the same!""). Parameters:other (KeyTable) – key table to compare against. Return type:bool. schema¶; Table schema.; Examples; >>> print(kt1.schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(kt1.schema). Return type:TStruct. select(column_names)[source]¶; Select a subset of columns.; Examples; Assume kt1 is a KeyTable with three columns: C1, C2 and; C3.; Select/drop columns:; >>",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
https://hail.is/docs/0.1/hail.KinshipMatrix.html:921,Integrability,depend,depending,921,"﻿. . KinshipMatrix — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; KinshipMatrix. View page source. KinshipMatrix¶. class hail.KinshipMatrix(jkm)[source]¶; Represents a symmetric matrix encoding the relatedness of each pair of samples in the accompanying sample list.; The output formats are consistent with PLINK formats as created by the make-rel and make-grm commands and used by GCTA.; Attributes. key_schema; Returns the signature of the key indexing this matrix. Methods. __init__. export_gcta_grm; Export kinship matrix as .grm file. export_gcta_grm_bin; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. export_id_file; Export samples as .id file. export_rel; Export kinship matrix as .rel file. export_tsv; Export kinship matrix to tab-delimited text file with sample list as header. matrix; Gets the matrix backing this kinship matrix. sample_list; Gets the list of samples. export_gcta_grm(output)[source]¶; Export kinship matrix as .grm file. See PLINK formats. Parameters:output (str) – File path for output. export_gcta_grm_bin(output, opt_n_file=None)[source]¶; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. See PLINK formats. Parameters:; output (str) – File path for output.; opt_n_file (str or None) – The file path to the N file. export_id_file(output)[source]¶; Export samples as .id file. See PLINK formats. Parameters:output (str) – File path for output. export_rel(output)[source]¶; Export kinship matrix as .rel file. See PLINK formats. Parameters:output (str) – File path for output. export_tsv(output)[source]¶; Export kinship matrix to tab-delimited text file",MatchSource.WIKI,docs/0.1/hail.KinshipMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KinshipMatrix.html
https://hail.is/docs/0.1/hail.KinshipMatrix.html:1493,Integrability,depend,depending,1493," each pair of samples in the accompanying sample list.; The output formats are consistent with PLINK formats as created by the make-rel and make-grm commands and used by GCTA.; Attributes. key_schema; Returns the signature of the key indexing this matrix. Methods. __init__. export_gcta_grm; Export kinship matrix as .grm file. export_gcta_grm_bin; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. export_id_file; Export samples as .id file. export_rel; Export kinship matrix as .rel file. export_tsv; Export kinship matrix to tab-delimited text file with sample list as header. matrix; Gets the matrix backing this kinship matrix. sample_list; Gets the list of samples. export_gcta_grm(output)[source]¶; Export kinship matrix as .grm file. See PLINK formats. Parameters:output (str) – File path for output. export_gcta_grm_bin(output, opt_n_file=None)[source]¶; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. See PLINK formats. Parameters:; output (str) – File path for output.; opt_n_file (str or None) – The file path to the N file. export_id_file(output)[source]¶; Export samples as .id file. See PLINK formats. Parameters:output (str) – File path for output. export_rel(output)[source]¶; Export kinship matrix as .rel file. See PLINK formats. Parameters:output (str) – File path for output. export_tsv(output)[source]¶; Export kinship matrix to tab-delimited text file with sample list as header. Parameters:output (str) – File path for output. key_schema¶; Returns the signature of the key indexing this matrix. Return type:Type. matrix()[source]¶; Gets the matrix backing this kinship matrix. Returns:Matrix of kinship values. Return type:IndexedRowMatrix. sample_list()[source]¶; Gets the list of samples. The (i, j) entry of the matrix encodes the relatedness of the ith and jth samples. Returns:List of samples. Return type:list of str. Next ; Previous. © Copyright 2016, ",MatchSource.WIKI,docs/0.1/hail.KinshipMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KinshipMatrix.html
https://hail.is/docs/0.1/hail.VariantDataset.html:5657,Availability,error,errors,5657,"is VDS. ld_prune; Prune variants in linkage disequilibrium (LD). linreg; Test each variant for association using linear regression. linreg3; Test each variant for association with multiple phenotypes using linear regression. linreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the linear regression model. linreg_multi_pheno; Test each variant for association with multiple phenotypes using linear regression. lmmreg; Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. logreg; Test each variant for association using logistic regression. logreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the logistic regression model. make_table; Produce a key with one row per variant and one or more columns per sample. mendel_errors; Find Mendel errors; count per variant, individual and nuclear family. min_rep; Gives minimal, left-aligned representation of alleles. naive_coalesce; Naively descrease the number of partitions. num_partitions; Number of partitions. pc_relate; Compute relatedness estimates between individuals using a variant of the PC-Relate method. pca; Run Principal Component Analysis (PCA) on the matrix of genotypes. persist; Persist this variant dataset to memory and/or disk. query_genotypes; Performs aggregation queries over genotypes, and returns Python object(s). query_genotypes_typed; Performs aggregation queries over genotypes, and returns Python object(s) and type(s). query_samples; Performs aggregation queries over samples and sample annotations, and returns Python object(s). query_samples_typed; Performs aggregation queries over samples and sample annotations, and returns Python object(s) and type(s). query_variants; Performs aggregation queries over variants and variant annotations, and returns Python object(s). quer",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:9470,Availability,down,downcoded,9470,"e, Gene, nHet). Parameters:; key_exprs (str or list of str) – Named expression(s) for which fields are keys.; agg_exprs (str or list of str) – Named aggregation expression(s). Return type:KeyTable. annotate_alleles_expr(expr, propagate_gq=False)[source]¶; Annotate alleles with expression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To create a variant annotation va.nNonRefSamples: Array[Int] where the ith entry of; the array is the number of samples carrying the ith alternate allele:; >>> vds_result = vds.annotate_alleles_expr('va.nNonRefSamples = gs.filter(g => g.isCalledNonRef()).count()'). Notes; This method is similar to annotate_variants_expr(). annotate_alleles_expr() dynamically splits multi-allelic sites,; evaluates each expression on each split allele separately, and for each expression annotates with an array with one element per alternate allele. In the splitting, genotypes are downcoded and each alternate allele is represented; using its minimal representation (see split_multi() for more details). Parameters:; expr (str or list of str) – Annotation expression.; propagate_gq (bool) – Propagate GQ instead of computing from (split) PL. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_genotypes_expr(expr)[source]¶; Annotate genotypes with expression.; Examples; Convert the genotype schema to a TStruct with two fields GT and CASE_HET:; >>> vds_result = vds.annotate_genotypes_expr('g = {GT: g.gt, CASE_HET: sa.pheno.isCase && g.isHet()}'). Assume a VCF is imported with generic=True and the resulting genotype schema; is a Struct and the field GTA is a Call type. Use the .toGenotype() method in the; expression language to convert a Call to a Genotype. vds_gta will have a genotype schema equal to; TGenotype; >>> vds_gta = (hc.import_vcf('data/example3.vcf.bgz', generic=True, call_fields=['GTA']); ... .annotate_genotypes_expr('g = g.GTA.toGenotype()')). Notes; annotate_genotypes_expr() eval",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:12159,Availability,down,downstream,12159,"e resulting genotype schema is not TGenotype,; subsequent function calls on the annotated variant dataset may not work such as; pca() and linreg().; Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to TGenotype.; Genotypes are immutable. For example, if g is initially of type Genotype, the expression; g.gt = g.gt + 1 will return a Struct with one field gt of type Int and NOT a Genotype; with the gt incremented by 1. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global(path, annotation, annotation_type)[source]¶; Add global annotations from Python objects.; Examples; Add populations as a global annotation:; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotation to add to global; annotation_type (Type) – Hail type of annotation. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global_expr(expr)[source]¶; Annotate global with expression.; Example; Annotate global with an array of populations:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. global: global annotations. Parameters:expr (str or list of str) – Annotation expression. Returns:Annota",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:46425,Availability,down,downstream,46425,"ernal tools (such as bcftools and GATK) unless they are explicitly inserted using the append_to_header option.; Hail only exports the contents of va.info to the INFO field. No other annotations besides va.info are exported.; The genotype schema must have the type TGenotype or TStruct. If the type is; TGenotype, then the FORMAT fields will be GT, AD, DP, GQ, and PL (or PP if export_pp is True).; If the type is TStruct, then the exported FORMAT fields will be the names of each field of the Struct.; Each field must have a type of String, Char, Int, Double, or Call. Arrays and Sets are also allowed as long as they are not nested.; For example, a field with type Array[Int] can be exported but not a field with type Array[Array[Int]].; Nested Structs are also not allowed. Caution; If samples or genotypes are filtered after import, the value stored in va.info.AC value may no longer reflect the number of called alternate alleles in the filtered VDS. If the filtered VDS is then exported to VCF, downstream tools may produce erroneous results. The solution is to create new annotations in va.info or overwrite existing annotations. For example, in order to produce an accurate AC field, one can run variant_qc() and copy the va.qc.AC field to va.info.AC:; >>> (vds.filter_genotypes('g.gq >= 20'); ... .variant_qc(); ... .annotate_variants_expr('va.info.AC = va.qc.AC'); ... .export_vcf('output/example.vcf.bgz')). Parameters:; output (str) – Path of .vcf file to write.; append_to_header (str or None) – Path of file to append to VCF header.; export_pp (bool) – If true, export linear-scaled probabilities (Hail’s pp field on genotype) as the VCF PP FORMAT field.; parallel (bool) – If true, return a set of VCF files (one per partition) rather than serially concatenating these files. file_version()[source]¶; File version of variant dataset. Return type:int. filter_alleles(expr, annotation='va = va', subset=True, keep=True, filter_altered_genotypes=False, max_shift=100, keep_star=False)[sourc",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:48426,Availability,down,downcode,48426,"x_shift=100, keep_star=False)[source]¶; Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. aIndex will never be zero). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:; >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of aIndices because; we are mapping between the old and new allele indices, not; the alternate allele indices.; Notes; If filter_altered_genotypes is true, genotypes that contain filtered-out alleles are set to missing.; filter_alleles() implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. Subset algorithm; The subset algorithm (the default, subset=True) subsets the; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets GT to the genotype with the minimum PL. Note; that if the genotype changes (as in the example), the PLs; are re-normalized (shifted) so that the most likely genotype has a PL of; 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on t",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:49741,Availability,down,downcode,49741,"e; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets GT to the genotype with the minimum PL. Note; that if the genotype changes (as in the example), the PLs; are re-normalized (shifted) so that the most likely genotype has a PL of; 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 25,20.; DP: No change.; PL: The filtered alleles’ columns are eliminated and the remaining columns shifted so the minimum value is 0.; GQ: The second-lowest PL (after shifting). Downcode algorithm; The downcode algorithm (subset=False) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to split_multi().; The downcoding algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:49988,Availability,down,downcodeing,49988,"litatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 25,20.; DP: No change.; PL: The filtered alleles’ columns are eliminated and the remaining columns shifted so the minimum value is 0.; GQ: The second-lowest PL (after shifting). Downcode algorithm; The downcode algorithm (subset=False) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to split_multi().; The downcoding algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Expression Variables; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; aIndex (Int): the index of the allele being tested. The following s",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:50349,Availability,down,downcoding,50349,"d on the PLs ignoring the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 25,20.; DP: No change.; PL: The filtered alleles’ columns are eliminated and the remaining columns shifted so the minimum value is 0.; GQ: The second-lowest PL (after shifting). Downcode algorithm; The downcode algorithm (subset=False) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to split_multi().; The downcoding algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Expression Variables; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; aIndex (Int): the index of the allele being tested. The following symbols are in scope for annotation:. v (Variant): Variant; va: variant annotations; aIndices (Array[Int]): the array of old indices (such that aIndices[newIndex] = oldIndex and aIndices[0] = 0). Parameters:; expr (str) – Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele inde",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:51597,Availability,down,downcodes,51597," 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Expression Variables; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; aIndex (Int): the index of the allele being tested. The following symbols are in scope for annotation:. v (Variant): Variant; va: variant annotations; aIndices (Array[Int]): the array of old indices (such that aIndices[newIndex] = oldIndex and aIndices[0] = 0). Parameters:; expr (str) – Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index); annotation (str) – Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices); subset (bool) – If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs.; keep (bool) – If true, keep variants matching expr; filter_altered_genotypes (bool) – If true, genotypes that contain filtered-out alleles are set to missing.; max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered.; keepStar (bool) – If true, keep variants where the only allele left is a * allele. Returns:Filtered variant dataset. Return type:VariantDataset. filter_genotypes(expr, keep=True)[source]¶; Filter genotypes based on expression.; Examples; Filter genotypes by allele balance dependent on genotype call:; >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:51974,Availability,error,error,51974,"a: variant annotations; aIndex (Int): the index of the allele being tested. The following symbols are in scope for annotation:. v (Variant): Variant; va: variant annotations; aIndices (Array[Int]): the array of old indices (such that aIndices[newIndex] = oldIndex and aIndices[0] = 0). Parameters:; expr (str) – Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index); annotation (str) – Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices); subset (bool) – If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs.; keep (bool) – If true, keep variants matching expr; filter_altered_genotypes (bool) – If true, genotypes that contain filtered-out alleles are set to missing.; max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered.; keepStar (bool) – If true, keep variants where the only allele left is a * allele. Returns:Filtered variant dataset. Return type:VariantDataset. filter_genotypes(expr, keep=True)[source]¶; Filter genotypes based on expression.; Examples; Filter genotypes by allele balance dependent on genotype call:; >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.1) || ' +; ... '(g.isHet() && ab >= 0.25 && ab <= 0.75) || ' +; ... '(g.isHomVar() && ab >= 0.9))'). Notes; expr is in genotype context so the following symbols are in scope:. s (Sample): sample; v (Variant): Variant; sa: sample annotations; va: variant annotations; global: global annotations. For more information, see the documentation on data representation, annotations, and; the expression language. Caution; When expr evaluates to missing, the genotype will be removed regardless of whether keep=True or keep=False. Parameters",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:59509,Availability,down,down,59509,"gene CHD8 (assumes the variant annotation va.gene exists):; >>> vds_result = vds.filter_variants_expr('va.gene == ""CHD8""'). Remove all variants on chromosome 1:; >>> vds_result = vds.filter_variants_expr('v.contig == ""1""', keep=False). Caution; The double quotes on ""1"" are necessary because v.contig is of type String. Notes; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. For more information, see the Overview and the Expression Language. Caution; When expr evaluates to missing, the variant will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep variants where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_list(variants, keep=True)[source]¶; Filter variants with a list of variants.; Examples; Filter VDS down to a list of variants:; >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True). Notes; This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap with any supplied variant will not be loaded at all. This property; enables filter_variants_list to be used for reasonably low-latency queries of one; or more variants, even on large datasets. Parameters:; variants (list of Variant) – List of variants to keep or remove.; keep (bool) – If true, keep variants in variants, otherwise remove them. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_table(table, keep=True)[source]¶; Filter variants with a Variant keyed key table.; Example; Filter variants of a VDS to those appearing in a text file:; >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Keep all variants whose ch",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:66085,Availability,down,downstream,66085," in Hardy-Weinberg equilibrium and is further motivated in the paper Patterson, Price and Reich, 2006. (The resulting amplification of signal from the low end of the allele frequency spectrum will also introduce noise for rare variants; common practice is to filter out variants with minor allele frequency below some cutoff.) The factor \(1/m\) gives each sample row approximately unit total variance (assuming linkage equilibrium) so that the diagonal entries of the GRM are approximately 1. Equivalently,. \[G_{ik} = \frac{1}{m} \sum_{j=1}^m \frac{(C_{ij}-2p_j)(C_{kj}-2p_j)}{2 p_j (1-p_j)} \]. Returns:Genetic Relatedness Matrix for all samples. Return type:KinshipMatrix. hardcalls()[source]¶; Drop all genotype fields except the GT field. Important; The genotype_schema() must be of type TGenotype in order to use this method. A hard-called variant dataset is about two orders of magnitude; smaller than a standard sequencing dataset. Use this; method to create a smaller, faster; representation for downstream processing that only; requires the GT field. Returns:Variant dataset with no genotype metadata. Return type:VariantDataset. ibd(maf=None, bounded=True, min=None, max=None)[source]¶; Compute matrix of identity-by-descent estimations. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:; >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; PI_HAT in [0.2, 0.9], using minor allele frequencies stored in; va.panel_maf:; >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). Notes; The implementation is based on the IBD algorithm described in the PLINK; paper.; ibd() requires the dataset to be; bi-allelic (otherwise run split_multi() or otherwise run filter_multi()); and does not perform LD pruning. Linkage disequilibrium may bias the; result so consider filtering variants first.; The resulting KeyTable ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:69472,Availability,avail,available,69472,"les so that no two have a PI_HAT value greater than or equal to 0.6.; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:; >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). Notes; The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted.; The tiebreaking_expr namespace has the following variables available:. s1: The first sample id.; sa1: The annotations associated with s1.; s2: The second sample id.; sa2: The annotations associated with s2. The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping s1. Any positive integer expresses a preference for keeping s2. A zero expresses no preference. This function must induce a preorder on the samples, in particular:. tiebreaking_expr(sample1, sample2) must equal -1 * tie breaking_expr(sample2, sample1), which evokes the common sense understanding that if x < y then y > x`.; tiebreaking_expr(sample1, sample1) must equal 0, i.e. x = x; if sample1 is preferred to sample2 and sample2 is preferred to sample3, then sample1 must also be preferred to sample3. The last requirement is only important if you have three related samples with the same number of relatives and all three are related to one another. In cases like this ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:77755,Availability,avail,available,77755," 1000kb 1 0.2.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK.; Be sure to provide enough disk space per worker because ld_prune() persists up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters r2 and window. The number of bytes stored in memory per variant is about nSamples / 4 + 50. Warning; The variants in the pruned set are not guaranteed to be identical each time ld_prune() is run. We recommend running ld_prune() once and exporting the list of LD pruned variants using; export_variants() for future use. Parameters:; r2 (float) – Maximum \(R^2\) threshold between two variants in the pruned set within a given window.; window (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The number of cores available. Equivalent to the total number of workers times the number of cores per worker. Returns:Variant dataset filtered to those variants which remain after LD pruning. Return type:VariantDataset. linreg(y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association using linear regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run linear regression per variant using a phenotype and two covariates stored in sample annotations:; >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The linreg() method computes, for each variant, statistics of; the \(t\)-test for the genotype coefficient of the linear function; of best fit from sample genotype and covariates to quantitative; phenotype or case-control status",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:77859,Availability,avail,available,77859,"n-imputes missing values and tests pairs of variants in a different order than PLINK.; Be sure to provide enough disk space per worker because ld_prune() persists up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters r2 and window. The number of bytes stored in memory per variant is about nSamples / 4 + 50. Warning; The variants in the pruned set are not guaranteed to be identical each time ld_prune() is run. We recommend running ld_prune() once and exporting the list of LD pruned variants using; export_variants() for future use. Parameters:; r2 (float) – Maximum \(R^2\) threshold between two variants in the pruned set within a given window.; window (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The number of cores available. Equivalent to the total number of workers times the number of cores per worker. Returns:Variant dataset filtered to those variants which remain after LD pruning. Return type:VariantDataset. linreg(y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association using linear regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run linear regression per variant using a phenotype and two covariates stored in sample annotations:; >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The linreg() method computes, for each variant, statistics of; the \(t\)-test for the genotype coefficient of the linear function; of best fit from sample genotype and covariates to quantitative; phenotype or case-control status. Hail only includes samples for which; phenotype and all covariates are defined. For each variant",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:81658,Availability,error,error,81658,".; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; >>> vds_result = vds.linreg('if (sa.pheno.isFemale) sa.pheno.age else (2 * sa.pheno.age + 10)'). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition. See; equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 2\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; genotype and intercept.; Annotations; With the default root, the following four variant annotations are added. va.linreg.beta (Double) – fit genotype coefficient, \(\hat\beta_1\); va.linreg.se (Double) – estimated standard error, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Double) – \(p\)-value. Parameters:; y (str) – Response expression; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosages genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg3(ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:83365,Availability,error,errors,83365,"lock_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter on allele count or allele; frequency. Warning; linreg3() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; variant_block_size (int) – Number of variant regressions to perform simultaneously. Larger block size requires more memmory. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg_burden(key_name, variant_keys, single_key, agg_expr, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. Important; The genotype_schema() must be of type TGenotype in order to u",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:87658,Availability,error,error,87658,"or each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the linear regression model using the supplied phenotype and covariates.; The model is that of linreg() with sample genotype gt replaced by the score in the sample; key table. For each key, missing scores are mean-imputed across all samples.; The resulting linear regression key table has the following columns:. value of key_name (String) – descriptor of variant group key (key column); beta (Double) – fit coefficient, \(\hat\beta_1\); se (Double) – estimated standard error, \(\widehat{\mathrm{se}}\); tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); pval (Double) – \(p\)-value. linreg_burden() returns both the linear regression key table and the sample key table.; Extended example; Let’s walk through these steps in the max() toy example above.; There are six samples with the following annotations:. Sample; pheno; cov1; cov2. A; 0; 0; -1. B; 0; 2; 3. C; 1; 1; 5. D; 1; -2; 0. E; 1; -2; -4. F; 1; 4; 3. There are three variants with the following gt values:. Variant; A; B; C; D; E; F. 1:1:A:C; 0; 1; 0; 0; 0; 1. 1:2:C:T; .; 2; .; 2; 0; 0. 1:3:G:C; 0; .; 1; 1; 1; . The va.genes annotation of type Set[String] on example_burden.vds was created; using annotate_variants_table() with product=True on the interval list:; 1	1	2	+	geneA; 1	2	2	+	geneB; 1	1	3	+	geneC. So there are three overlapping genes: gene A contains two variants,; gene B contains one variant, and gene C contains all three variants. gene; 1:1:A:C; 1:2:C:",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:90673,Availability,error,errors,90673,"expr (str) – Sample aggregation expression (per key).; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of linear regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). linreg_multi_pheno(ys, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes more efficiently; than looping over linreg(). Warning; linreg_multi_pheno() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of these annotations corresponds to that of y. va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. lmmreg(kinshipMatrix, y, covariates=[], global_root='global.lmmreg', va_root='va.lmmreg', run_assoc=True, use_ml=False, delta=None, sparsity_threshold=1.0, use_dosages=False, n_eigs=None, dropped_variance_fraction=None)[source]¶; Use a kinship-based linear mixed model to estimate the genetic component of phenotypi",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:94259,Availability,error,error,94259," annotations in Step 4, depending on whether \(\delta\) is set or fit. Annotation; Type; Value. global.lmmreg.useML; Boolean; true if fit by ML, false if fit by REML. global.lmmreg.beta; Dict[String, Double]; map from intercept and the given covariates expressions to the corresponding fit \(\beta\) coefficients. global.lmmreg.sigmaG2; Double; fit coefficient of genetic variance, \(\hat{\sigma}_g^2\). global.lmmreg.sigmaE2; Double; fit coefficient of environmental variance \(\hat{\sigma}_e^2\). global.lmmreg.delta; Double; fit ratio of variance component coefficients, \(\hat{\delta}\). global.lmmreg.h2; Double; fit narrow-sense heritability, \(\hat{h}^2\). global.lmmreg.nEigs; Int; number of eigenvectors of kinship matrix used to fit model. global.lmmreg.dropped_variance_fraction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigma",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:96908,Availability,avail,available,96908,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:96991,Availability,avail,available,96991,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:102401,Availability,toler,tolerance,102401,"delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:103643,Availability,error,error,103643,") is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. \[\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\frac{1}{2 \sigma^2}\]; The standard error \(\hat{\sigma}\) is then estimated by solving for \(\sigma\).; Note that the mean and standard deviation of the (discretized or continuous) distribution held in global.lmmreg.fit.normLkhdH2 will not coincide with \(\hat{h}^2\) and \(\hat{\sigma}\), since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distributio",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:104260,Availability,error,error,104260,"lihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. \[\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\frac{1}{2 \sigma^2}\]; The standard error \(\hat{\sigma}\) is then estimated by solving for \(\sigma\).; Note that the mean and standard deviation of the (discretized or continuous) distribution held in global.lmmreg.fit.normLkhdH2 will not coincide with \(\hat{h}^2\) and \(\hat{\sigma}\), since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on \(h^2\).; Testing each variant for association; Fixing a single variant, we define:. \(v = n \times 1\) vector of genotypes, with missing genotypes imputed as the mean of called genotypes; \(X_v = \left[v | X \right] = n \times (1 + c)\) matrix concatenating \(v\) and \(X\); \(\beta_v = (\beta^0_v, \beta^1_v, \ldots, \beta^c_v) = (1 + c) \times 1\) vector of covariate coefficients. Fixing \(\delta\) at the global R",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111144,Availability,error,error,111144,"rm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:113518,Availability,error,errors,113518,"by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; th",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:123160,Availability,error,errors,123160,"99 ./. 0/0:99; 1:2:G:C GT:GQ 0/1:89 0/1:99 1/1:93. Then; >>> kt = vds.make_table('v = v', ['gt = g.gt', 'gq = g.gq']). returns a KeyTable with schema; v: Variant; A.gt: Int; A.gq: Int; B.gt: Int; B.gq: Int; C.gt: Int; C.gq: Int. and values; v A.gt A.gq B.gt B.gq C.gt C.gq; 1:1:A:T 1 99 NA NA 0 99; 1:2:G:C 1 89 1 99 2 93. The above table can be generated and exported as a TSV using KeyTable export().; Notes; Per sample field names in the result are formed by; concatenating the sample ID with the genotype_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:123560,Availability,error,errors,123560,".; Notes; Per sample field names in the result are formed by; concatenating the sample ID with the genotype_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:123675,Availability,error,errors,123675,"type_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:123806,Availability,error,errors,123806,"iant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclea",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:124268,Availability,error,errors,124268,"genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors pe",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:124315,Availability,error,error,124315,"delian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table clo",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:124601,Availability,error,error,124601,"l.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:124639,Availability,error,error,124639,"e samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:124663,Availability,error,error,124663,"notated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:124714,Availability,error,error,124714,"notated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:124735,Availability,error,errors,124735,"e, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125112,Availability,error,errors,125112," grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lme",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125174,Availability,error,errors,125174,"K mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FI",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125226,Availability,error,errors,125226,"ing columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125530,Availability,error,errors,125530,"ly ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.;",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125601,Availability,error,errors,125601,"as found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125650,Availability,error,error,125650," – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125701,Availability,error,error,125701," – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125722,Availability,error,errors,125722,"ond table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125879,Availability,error,errors,125879,"e “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, Ho",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125916,Availability,error,error,125916,"Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement i",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:125954,Availability,error,errors,125954,"ring) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:126002,Availability,error,error,126002,"en (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3;",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:126066,Availability,error,error,126066,"rrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomV",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:126117,Availability,error,error,126117,"clear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; Ho",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:126362,Availability,error,error,126362,"tructure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; HomVar; ! HomVar; HomRef; Auto. 7; ! HomVar; HomVar; HomRef; Auto. 8; HomVar; HomVar; HomRef; Auto. 9; Any; HomVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This met",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:126454,Availability,error,error,126454,"ng) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; HomVar; ! HomVar; HomRef; Auto. 7; ! HomVar; HomVar; HomRef; Auto. 8; HomVar; HomVar; HomRef; Auto. 9; Any; HomVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This method only considers children with two parents and a defined sex.; PAR is currently defined with respect to reference; ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:127655,Availability,error,error,127655,"d as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; HomVar; ! HomVar; HomRef; Auto. 7; ! HomVar; HomVar; HomRef; Auto. 8; HomVar; HomVar; HomRef; Auto. 9; Any; HomVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This method only considers children with two parents and a defined sex.; PAR is currently defined with respect to reference; GRCh37:. X: 60001 - 2699520, 154931044 - 155260560; Y: 10001 - 2649520, 59034050 - 59363566. Parameters:pedigree (Pedigree) – Sample pedigree. Returns:Four tables with Mendel error statistics. Return type:(KeyTable, KeyTable, KeyTable, KeyTable). min_rep(max_shift=100)[source]¶; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; Examples; 1. Simple trimming of a multi-allelic site, no change in variant position; 1:10000:TAA:TAA,AA => 1:10000:TA:T,A; 2. Trimming of a bi-allelic site leading to a change in position; 1:10000:AATAA,AAGAA => 1:10002:T:G. Parameters:max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. Return type:VariantDataset. naive_coalesce(max_partitions)[source]¶; Naively descrease the number of partitions. Warning; naive_coalesce() simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike repartition(), so it can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:128238,Availability,error,error,128238,"omVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This method only considers children with two parents and a defined sex.; PAR is currently defined with respect to reference; GRCh37:. X: 60001 - 2699520, 154931044 - 155260560; Y: 10001 - 2649520, 59034050 - 59363566. Parameters:pedigree (Pedigree) – Sample pedigree. Returns:Four tables with Mendel error statistics. Return type:(KeyTable, KeyTable, KeyTable, KeyTable). min_rep(max_shift=100)[source]¶; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; Examples; 1. Simple trimming of a multi-allelic site, no change in variant position; 1:10000:TAA:TAA,AA => 1:10000:TA:T,A; 2. Trimming of a bi-allelic site leading to a change in position; 1:10000:AATAA,AAGAA => 1:10002:T:G. Parameters:max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. Return type:VariantDataset. naive_coalesce(max_partitions)[source]¶; Naively descrease the number of partitions. Warning; naive_coalesce() simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike repartition(), so it can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions. Parameters:max_partitions (int) – Desired number of partitions. If the current number of partitions is less than max_partitions, do nothing. Returns:Variant dataset with the number of partitions equal to at most max_partitions. Return type:VariantDataset. num_partitions()[source]¶; Number of partitions.; Notes; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. P",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:129219,Availability,avail,available,129219,"ber of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. Return type:VariantDataset. naive_coalesce(max_partitions)[source]¶; Naively descrease the number of partitions. Warning; naive_coalesce() simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike repartition(), so it can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions. Parameters:max_partitions (int) – Desired number of partitions. If the current number of partitions is less than max_partitions, do nothing. Returns:Variant dataset with the number of partitions equal to at most max_partitions. Return type:VariantDataset. num_partitions()[source]¶; Number of partitions.; Notes; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:131097,Availability,down,down,131097,"iplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of their first k principal; component coordinates. As such, the efficacy of this method rests on two; assumptions:. an individual’s first k principal component coordinates fully; describe their allele-frequency-relevant ancestry, and; the relationship between ancestry (as described by",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:134659,Availability,avail,available,134659,"idehat{\sigma_{js}}}\]; The estimator for identity-by-descent two is given by:. \[\widehat{k^{(2)}_{ij}} := \frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}\widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}\]; The estimator for identity-by-descent zero is given by:. \[\begin{split}\widehat{k^{(0)}_{ij}} :=; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2 + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}\end{split}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} := 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Details; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference; implementation in a couple key ways:. the principal components analysis does not use an unrelated set of; individuals; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Notes; The block_size controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation’s time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; block_size larger than 512 tends to cause memory exhaustion errors.; The minimum allele frequency filter is applied p",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:135609,Availability,error,errors,135609," TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference; implementation in a couple key ways:. the principal components analysis does not use an unrelated set of; individuals; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Notes; The block_size controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation’s time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; block_size larger than 512 tends to cause memory exhaustion errors.; The minimum allele frequency filter is applied per-pair: if either of; the two individual’s individual-specific minor allele frequency is below; the threshold, then the variant’s contribution to relatedness estimates; is zero.; Under the PC-Relate model, kinship, [ phi_{ij} ], ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, [ k^{(2)}_{ij} ],; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; [ 2^{-3} = 12.5 %",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:136648,Availability,reliab,reliably,136648,"er is applied per-pair: if either of; the two individual’s individual-specific minor allele frequency is below; the threshold, then the variant’s contribution to relatedness estimates; is zero.; Under the PC-Relate model, kinship, [ phi_{ij} ], ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, [ k^{(2)}_{ij} ],; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; [ 2^{-3} = 12.5 % ] of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. The resulting KeyTable entries have the type: { i: String,; j: String, kin: Double, k2: Double, k1: Double, k0: Double }. The key; list is: i: String, j: String. Parameters:; k (int) – The number of principal components to use to distinguish; ancestries.; maf (float) – The minimum individual-specific allele frequency for; an allele used to measure relatedness.; block_size (int) – the side length of the blocks of the block-; distributed matrices; this should be set such; that at least three of these matrices fit in; memory (in addition to all other objects; necessary for Spark and Hail).; min_kinship (float) – Pairs of samples with kinship lower than; min_kinship are excluded from the results.; statistics (str) – the set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:144367,Availability,redundant,redundant,144367,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:151939,Availability,avail,available,151939,"expressions. Return type:(annotation or list of annotation, Type or list of Type). rename_samples(mapping)[source]¶; Rename samples.; Examples; >>> vds_result = vds.rename_samples({'ID1': 'id1', 'ID2': 'id2'}). Use a file with an “old_id” and “new_id” column to rename samples:; >>> mapping_table = hc.import_table('data/sample_mapping.txt'); >>> mapping_dict = {row.old_id: row.new_id for row in mapping_table.collect()}; >>> vds_result = vds.rename_samples(mapping_dict). Parameters:mapping (dict) – Mapping from old to new sample IDs. Returns:Dataset with remapped sample IDs. Return type:VariantDataset. repartition(num_partitions, shuffle=True)[source]¶; Increase or decrease the number of variant dataset partitions.; Examples; Repartition the variant dataset to have 500 partitions:; >>> vds_result = vds.repartition(500). Notes; Check the current number of partitions with num_partitions().; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with \(M\) variants is first imported, each of the \(k\) partition will contain about \(M/k\) of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it’s recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores.; Partitions are a core concept of distributed computation in Spark, see here for details. With shuffle=True, Hail does a full shuffle of the data and creates equal sized partitions. With shuffle=False, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the repartition and coalesce commands in Spark, respectively. In particular, when shuffle=False, num_partitions cannot exceed current number of partitions. Parameters:; num_parti",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:155121,Availability,toler,tolerance,155121,"ij}-2p_j)^2}},\]; with \(M_{ij} = 0\) for \(C_{ij}\) missing (i.e. mean genotype imputation). This scaling normalizes each variant column to have empirical variance \(1/m\), which gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the \(n \times n\) sample correlation or realized relationship matrix (RRM) \(K\) as simply. \[K = MM^T\]; Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the r",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:155326,Availability,toler,tolerance,155326,"gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the \(n \times n\) sample correlation or realized relationship matrix (RRM) \(K\) as simply. \[K = MM^T\]; Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fra",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:155355,Availability,toler,tolerance,155355,"gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the \(n \times n\) sample correlation or realized relationship matrix (RRM) \(K\) as simply. \[K = MM^T\]; Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fra",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:155480,Availability,toler,tolerance,155480,"correlation or realized relationship matrix (RRM) \(K\) as simply. \[K = MM^T\]; Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of genotypes called. nHomRef; Int; Number of homozygous reference genotypes. nHet; Int; Number of heterozygous genoty",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:155590,Availability,toler,tolerance,155590,"e only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of genotypes called. nHomRef; Int; Number of homozygous reference genotypes. nHet; Int; Number of heterozygous genotypes. nHomVar; Int; Number of homozygous alternate genotypes. nCalled; Int; Sum of nHomRef + nH",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:155625,Availability,toler,tolerance,155625,"e only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of genotypes called. nHomRef; Int; Number of homozygous reference genotypes. nHet; Int; Number of heterozygous genotypes. nHomVar; Int; Number of homozygous alternate genotypes. nCalled; Int; Sum of nHomRef + nH",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:162659,Availability,down,downcoded,162659,"er=A,Type=String,Description=""Allele count for high quality genotypes (DP >= 10, GQ >= 20)""; ##FILTER=<ID=HardFilter,Description=""This site fails GATK suggested hard filters."">. Parameters:; ann_path (str) – Path to variant annotation beginning with va.; attributes (dict) – A str-str dict containing the attributes to set. Returns:Annotated dataset with the attribute added to the variant annotation. Return type:VariantDataset. split_multi(propagate_gq=False, keep_star_alleles=False, max_shift=100)[source]¶; Split multiallelic variants. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds.split_multi().write('output/split.vds'). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:163239,Availability,down,downcode,163239,"lic variants. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds.split_multi().write('output/split.vds'). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype.; By default, GQ is recomputed from PL. If propagate_gq=True; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged.; Here is a second example for a het non-ref; A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as; A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. VCF Info Fields; Hail does not split annotations in the info field. This means; that if a multiallelic site with inf",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:163411,Availability,down,downcoding,163411,"riant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype.; By default, GQ is recomputed from PL. If propagate_gq=True; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged.; Here is a second example for a het non-ref; A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as; A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. VCF Info Fields; Hail does not split annotations in the info field. This means; that if a multiallelic site with info.AC value [10, 2] is; split, each split site will contain the same array [10,; 2]. The provided allele index annotation va.aIndex can be used; to select the value corresponding to the split allele’s; position:; >>> vds_result = (vd",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:166167,Availability,error,error,166167,"; split_multi() adds the; following annotations:. va.wasSplit (Boolean) – true if this variant was; originally multiallelic, otherwise false.; va.aIndex (Int) – The original index of this; alternate allele in the multiallelic representation (NB: 1; is the first alternate allele or the only alternate allele; in a biallelic variant). For example, 1:100:A:T,C splits; into two variants: 1:100:A:T with aIndex = 1 and; 1:100:A:C with aIndex = 2. Parameters:; propagate_gq (bool) – Set the GQ of output (split); genotypes to be the GQ of the input (multi-allelic) variants; instead of recompute GQ as the difference between the two; smallest PL values. Intended to be used in conjunction with; import_vcf(store_gq=True). This option will be obviated; in the future by generic genotype schemas. Experimental.; keep_star_alleles (bool) – Do not filter out * alleles.; max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. Returns:A biallelic variant dataset. Return type:VariantDataset. storage_level()[source]¶; Returns the storage (persistence) level of the variant dataset.; Notes; See the Spark documentation for details on persistence levels. Return type:str. summarize()[source]¶; Returns a summary of useful information about the dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> s = vds.summarize(); >>> print(s.contigs); >>> print('call rate is %.2f' % s.call_rate); >>> s.report(). The following information is contained in the summary:. samples (int) - Number of samples.; variants (int) - Number of variants.; call_rate (float) - Fraction of all genotypes called.; contigs (list of str) - List of all unique contigs found in the dataset.; multiallelics (int) - Number of multiallelic variants.; snps (int) - Number of SNP alternate alleles.; mnps (int) - Number of MNP alternate alleles.; inse",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:33950,Deployability,pipeline,pipeline,33950,"ul concordance statistics. This value is the number of genotypes ; which were called (homozygous reference, heterozygous, or homozygous variant) in both datasets, ; but where the call did not match between the two.; The column concordance matches the structure of the global summmary, which is detailed above. Once again,; the first index into this array is the state on the left, and the second index is the state on the right.; For example, concordance[1][4] is the number of “no call” genotypes on the left that were called ; homozygous variant on the right. Parameters:right (VariantDataset) – right hand variant dataset for concordance. Returns:The global concordance statistics, a key table with sample concordance; statistics, and a key table with variant concordance statistics. Return type:(list of list of int, KeyTable, KeyTable). count()[source]¶; Returns number of samples and variants in the dataset.; Examples; >>> samples, variants = vds.count(). Notes; This is also the fastest way to force evaluation of a Hail pipeline. Returns:The sample and variant counts. Return type:(int, int). count_variants()[source]¶; Count number of variants in variant dataset. Return type:long. deduplicate()[source]¶; Remove duplicate variants. Returns:Deduplicated variant dataset. Return type:VariantDataset. delete_va_attribute(ann_path, attribute)[source]¶; Removes an attribute from a variant annotation field.; Attributes are key/value pairs that can be attached to a variant annotation field.; The following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. INFO fields attributes (attached to (va.info.*)):; ‘Number’: The arity of the field. Can take values; 0 (Boolean flag),; 1 (single value),; R (one value per allele, including the reference),; A (one value per non-reference allele),; G (one value per genotype), and; . (any number of values); When importing: The value in read from the VCF INFO field definition; When expor",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:35416,Deployability,update,updated,35416,"e following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. INFO fields attributes (attached to (va.info.*)):; ‘Number’: The arity of the field. Can take values; 0 (Boolean flag),; 1 (single value),; R (one value per allele, including the reference),; A (one value per non-reference allele),; G (one value per genotype), and; . (any number of values); When importing: The value in read from the VCF INFO field definition; When exporting: The default value is 0 for Boolean, . for Arrays and 1 for all other types. ‘Description’ (default is ‘’). FILTER entries in the VCF header are generated based on the attributes; of va.filters. Each key/value pair in the attributes will generate a; FILTER entry in the VCF with ID = key and Description = value. Parameters:; ann_path (str) – Variant annotation path starting with ‘va’, period-delimited.; attribute (str) – The attribute to remove (key). Returns:Annotated dataset with the updated variant annotation without the attribute. Return type:VariantDataset. drop_samples()[source]¶; Removes all samples from variant dataset.; The variants, variant annotations, and global annnotations will remain,; producing a sites-only variant dataset. Returns:Sites-only variant dataset. Return type:VariantDataset. drop_variants()[source]¶; Discard all variants, variant annotations and genotypes.; Samples, sample annotations and global annotations are retained. This; is the same as filter_variants_expr('false'), but much faster.; Examples; >>> vds_result = vds.drop_variants(). Returns:Samples-only variant dataset. Return type:VariantDataset. export_gen(output, precision=4)[source]¶; Export variant dataset as GEN and SAMPLE file. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Import genotype probability data, filter variants based on INFO score, and export data to a GEN and SAMPLE file:; >>> vds3 = hc.import_bgen(""data/example3.bgen"", sa",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:47865,Deployability,update,update,47865,"of .vcf file to write.; append_to_header (str or None) – Path of file to append to VCF header.; export_pp (bool) – If true, export linear-scaled probabilities (Hail’s pp field on genotype) as the VCF PP FORMAT field.; parallel (bool) – If true, return a set of VCF files (one per partition) rather than serially concatenating these files. file_version()[source]¶; File version of variant dataset. Return type:int. filter_alleles(expr, annotation='va = va', subset=True, keep=True, filter_altered_genotypes=False, max_shift=100, keep_star=False)[source]¶; Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. aIndex will never be zero). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:; >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of aIndices because; we are mapping between the old and new allele indices, not; the alternate allele indices.; Notes; If filter_altered_genotypes is true, genotypes that contain filtered-out alleles are set to missing.; filter_alleles() implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. Subset algorithm; The subset algorithm (the default, subset=True) subsets the; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:97971,Deployability,configurat,configuration,97971,"bic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X = n \times c\) matrix of sample covariates and intercept column of ones; \(K = n \times n\) kinship matrix; \(I = n \times n\) identity matrix; \(\beta = c \times 1\) vector of covariate coefficients; \(\sigma_g^2 =\) coefficient of genetic variance component \(K\); \(\sigma_e^2 =\) coefficient of environmental variance component \(I\); \(\delta = \frac{\sigma_e^2}{\sigma_g^2} =\) ratio of environmental and genetic variance component coeffici",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:103042,Deployability,integrat,integrate,103042,"imum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the le",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:104394,Deployability,continuous,continuous,104394,"hood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. \[\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\frac{1}{2 \sigma^2}\]; The standard error \(\hat{\sigma}\) is then estimated by solving for \(\sigma\).; Note that the mean and standard deviation of the (discretized or continuous) distribution held in global.lmmreg.fit.normLkhdH2 will not coincide with \(\hat{h}^2\) and \(\hat{\sigma}\), since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on \(h^2\).; Testing each variant for association; Fixing a single variant, we define:. \(v = n \times 1\) vector of genotypes, with missing genotypes imputed as the mean of called genotypes; \(X_v = \left[v | X \right] = n \times (1 + c)\) matrix concatenating \(v\) and \(X\); \(\beta_v = (\beta^0_v, \beta^1_v, \ldots, \beta^c_v) = (1 + c) \times 1\) vector of covariate coefficients. Fixing \(\delta\) at the global REML estimate \(\hat{\delta}\), we find the REML estimate \((\hat{\beta}_v, \hat{\sigma}_{g,v}^2)\) via rotation of the model. \[y \sim \mathrm{N}\left(X_v\b",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:144427,Deployability,pipeline,pipelines,144427,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:174466,Deployability,configurat,configuration,174466,"ataset with new variant QC annotations. Return type:VariantDataset. variant_schema¶; Returns the signature of the variant annotations contained in this VDS.; Examples; >>> print(vds.variant_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175825,Deployability,configurat,configuration,175825,"lt PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: Stri",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175985,Deployability,release,release-,175985,"f the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_a",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:179572,Deployability,configurat,configuration,179572,"on_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. Parameters:; config (str) – Path to VEP configuration file.; block_size (int) – Number of variants to annotate per VEP invocation.; root (str) – Variant annotation path to store VEP output.; csq (bool) – If True, annotates VCF CSQ field as a String.; If False, annotates with the full nested struct schema. Returns:An annotated with variant annotations from VEP. Return type:VariantDataset. was_split()[source]¶; True if multiallelic variants have been split into multiple biallelic variants.; Result is True if split_multi() or filter_multi() has been called on this variant dataset,; or if the variant dataset was imported with import_plink(), import_gen(),; or import_bgen(), or if the variant dataset was simulated with balding_nichols_model(). Return type:bool. write(output, overwrite=False, parquet_genotypes=False)[source]¶; Write variant dataset as VDS file.; Examples; Import data from a VCF file and then write the data to a VDS file:; >>> vds.write(""output/sample.vds""). Parameters:; output (str) – Path of VDS file t",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:54956,Energy Efficiency,efficient,efficiently,54956,"iants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove variants overlapping; an interval if False. Returns:Filtered variant dataset. Return type:VariantDataset. filter_multi()[source]¶; Filter out multi-allelic sites. Important; The genotype_schema() must be of type TGenotype in order to use this method. This method is much less computationally expensive than; split_multi(), and can also be used to produce; a variant dataset that can be used with methods that do not; support multiallelic variants. Returns:Dataset with no multiallelic sites, which can; be used for biallelic-only methods. Return type:VariantDataset. filter_samples_expr(expr, keep=True)[source]¶; Filter samples with the expression language.; Examples; ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:68980,Energy Efficiency,efficient,efficient,68980,"AT above this value will; not be included in the output. Must be in [0,1]. Returns:A KeyTable mapping pairs of samples to their IBD; statistics. Return type:KeyTable. ibd_prune(threshold, tiebreaking_expr=None, maf=None, bounded=True)[source]¶; Prune samples from the VariantDataset based on ibd() PI_HAT measures of relatedness. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Prune samples so that no two have a PI_HAT value greater than or equal to 0.6.; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:; >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). Notes; The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted.; The tiebreaking_expr namespace has the following variables available:. s1: The first sample id.; sa1: The annotations associated with s1.; s2: The second sample id.; sa2: The annotations associated with s2. The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping s1. Any positive integer expresses a preference for keeping s2. A zero expresses no preference. This function must induce a preorder on the samples, in particular:. tiebreaking_expr(sample1, sample2) must equal -1 * tie breaking_expr(sa",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:74848,Energy Efficiency,reduce,reduce,74848,"atus (both split or both multi-allelic). Parameters:right (VariantDataset) – right-hand variant dataset. Returns:Joined variant dataset. Return type:VariantDataset. ld_matrix(force_local=False)[source]¶; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS.; Examples; >>> ld_mat = vds.ld_matrix(). Notes; Each entry (i, j) in the LD matrix gives the \(r\) value between variants i and j, defined as; Pearson’s correlation coefficient; \(\rho_{x_i,x_j}\) between the two genotype vectors \(x_i\) and \(x_j\). \[\rho_{x_i,x_j} = \frac{\mathrm{Cov}(X_i,X_j)}{\sigma_{X_i} \sigma_{X_j}}\]; Also note that variants with zero variance (\(\sigma = 0\)) will be dropped from the matrix. Caution; The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you’ll want to reduce the number of variants with methods like; sample_variants(), filter_variants_expr(), or ld_prune() before; calling this unless your dataset is very small. Parameters:force_local (bool) – If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds \(5000^2\) and locally otherwise. Returns:Matrix of r values between pairs of variants. Return type:LDMatrix. ld_prune(r2=0.2, window=1000000, memory_per_core=256, num_cores=1)[source]¶; Prune variants in linkage disequilibrium (LD). Important; The genotype_schema() must be of type TGenotype in order to use this method. Requires was_split equals True.; Examples; Export the set of common LD pruned variants to a file:; >>> vds_result = (vds.variant_qc(); ... .filter_variants_expr(""va.qc.AF >= 0.05 && va.qc.AF <= 0.95""); ... .ld_prune(); ... .export_variants(""output/",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:82533,Energy Efficiency,efficient,efficiently,82533,"ant annotations are added. va.linreg.beta (Double) – fit genotype coefficient, \(\hat\beta_1\); va.linreg.se (Double) – estimated standard error, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Double) – \(p\)-value. Parameters:; y (str) – Response expression; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosages genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg3(ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter on allele count or allele; frequency. Warning; linreg3() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linre",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:82594,Energy Efficiency,efficient,efficient,82594,") – estimated standard error, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Double) – \(p\)-value. Parameters:; y (str) – Response expression; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosages genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg3(ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter on allele count or allele; frequency. Warning; linreg3() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covaria",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:90181,Energy Efficiency,efficient,efficiently,90181,"ear regression key table is:. gene; beta; se; tstat; pval. geneA; -0.084; 0.368; -0.227; 0.841. geneB; -0.542; 0.335; -1.617; 0.247. geneC; 0.075; 0.515; 0.145; 0.898. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of linear regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). linreg_multi_pheno(ys, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes more efficiently; than looping over linreg(). Warning; linreg_multi_pheno() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of these annotations corresponds to that of y. va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; min_ac (int)",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:101256,Energy Efficiency,efficient,efficient,101256,"n \times n\) diagonal matrix of eigenvalues of \(K\) in descending order. \(S_{ii}\) is the eigenvalue of eigenvector \(U_{:,i}\); \(U^T = n \times n\) orthonormal matrix, the transpose (and inverse) of \(U\). A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. \[U^Ty \sim \mathrm{N}\left(U^TX\beta, \sigma_g^2 (S + \delta I)\right)\]; for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (\(y\)) and covariate vectors (columns of \(X\)) in \(\mathbb{R}^n\) by \(U^T\) transforms the model to one with independent residuals. For any particular value of \(\delta\), the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in \(n\). In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over \(\delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:106290,Energy Efficiency,reduce,reduces,106290,"t{\delta}\), we find the REML estimate \((\hat{\beta}_v, \hat{\sigma}_{g,v}^2)\) via rotation of the model. \[y \sim \mathrm{N}\left(X_v\beta_v, \sigma_{g,v}^2 (K + \hat{\delta} I)\right)\]; Note that the only new rotation to compute here is \(U^T v\).; To test the null hypothesis that the genotype coefficient \(\beta^0_v\) is zero, we consider the restricted model with parameters \(((0, \beta^1_v, \ldots, \beta^c_v), \sigma_{g,v}^2)\) within the full model with parameters \((\beta^0_v, \beta^1_v, \ldots, \beta^c_v), \sigma_{g_v}^2)\), with \(\delta\) fixed at \(\hat\delta\) in both. The latter fit is simply that of the global model, \(((0, \hat{\beta}^1, \ldots, \hat{\beta}^c), \hat{\sigma}_g^2)\). The likelihood ratio test statistic is given by. \[\chi^2 = n \, \mathrm{ln}\left(\frac{\hat{\sigma}^2_g}{\hat{\sigma}_{g,v}^2}\right)\]; and follows a chi-squared distribution with one degree of freedom. Here the ratio \(\hat{\sigma}^2_g / \hat{\sigma}_{g,v}^2\) captures the degree to which adding the variant \(v\) to the global model reduces the residual phenotypic variance.; Kinship Matrix; FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with rrm(). However, any instance of KinshipMatrix may be used, so long as sample_list contains the complete samples of the caller variant dataset in the same order.; Low-rank approximation of kinship for improved performance; lmmreg() can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter n_eigs to use only the top n_eigs eigenvectors. Alternatively, specify dropped_variance_fraction to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also known as the trace, or the sum of the eigenvalues). For example, dropped_varian",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:114640,Energy Efficiency,reduce,reduces,114640,"f quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper fu",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:130248,Energy Efficiency,efficient,efficient,130248,"ed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-R",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:17903,Integrability,depend,depends,17903,"pulation or cohort, then the vds_key argument must be passed to describe the key in the dataset ; to use for the join. This argument expects a list of Hail expressions whose types match, in order, the ; table’s key types.; Each expression in the list vds_key has the following symbols in; scope:. s (String): sample ID; sa: sample annotations. The root and expr arguments. Note; One of root or expr is required, but not both. The expr parameter expects an annotation expression involving sa (the existing ; sample annotations in the dataset) and table (a struct containing the columns in ; the table), like sa.col1 = table.col1, sa.col2 = table.col2 or sa = merge(sa, table).; The root parameter expects an annotation path beginning in sa, like sa.annotations.; Passing root='sa.annotations' is exactly the same as passing expr='sa.annotations = table'.; expr has the following symbols in scope:. sa: sample annotations; table: See note. Note; The value of table inside root/expr depends on the number of values in the key table, ; as well as the product argument. There are three behaviors based on the number of values; and one branch for product being true and false, for a total of six modes:. Number of value columns; product; Type of table; Value of table. More than 2; False; Struct; Struct with an element for each column. 1; False; T; The value column. 0; False; Boolean; Existence of any matching key. More than 2; True; Array[Struct]; An array with a struct for each matching key. 1; True; Array[T]; An array with a value for each matching key. 0; True; Int; The number of matching keys. Common uses for the expr argument; Put annotations on the top level under sa; expr='sa = merge(sa, table)'. Annotate only specific annotations from the table; expr='sa.annotations = select(table, toKeep1, toKeep2, toKeep3)'. The above is equivalent to; expr='''sa.annotations.toKeep1 = table.toKeep1,; sa.annotations.toKeep2 = table.toKeep2,; sa.annotations.toKeep3 = table.toKeep3'''. Finally, for mor",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:25935,Integrability,depend,depends,25935,"stance) for these types, then the vds_key argument ; should be passed. This argument expects a list of expressions whose types match, in order, ; the table’s key types. Note that using vds_key is slower than annotation with a standard ; key type.; Each expression in the list vds_key has the following symbols in; scope:. v (Variant): Variant; va: variant annotations. The root and expr arguments. Note; One of root or expr is required, but not both. The expr parameter expects an annotation assignment involving va (the existing ; variant annotations in the dataset) and table (the values(s) in the table),; like va.col1 = table.col1, va.col2 = table.col2 or va = merge(va, table).; The root parameter expects an annotation path beginning in va, like va.annotations.; Passing root='va.annotations' is the same as passing expr='va.annotations = table'.; expr has the following symbols in scope:. va: variant annotations; table: See note. Note; The value of table inside root/expr depends on the number of values in the key table, ; as well as the product argument. There are three behaviors based on the number of values; and one branch for product being true and false, for a total of six modes:. Number of value columns; product; Type of table; Value of table. More than 2; False; Struct; Struct with an element for each column. 1; False; T; The value column. 0; False; Boolean; Existence of any matching key. More than 2; True; Array[Struct]; An array with a struct for each matching key. 1; True; Array[T]; An array with a value for each matching key. 0; True; Int; The number of matching keys. Common uses for the expr argument; Put annotations on the top level under va:; expr='va = merge(va, table)'. Annotate only specific annotations from the table:; expr='va.annotations = select(table, toKeep1, toKeep2, toKeep3)'. The above is roughly equivalent to:; expr='''va.annotations.toKeep1 = table.toKeep1,; va.annotations.toKeep2 = table.toKeep2,; va.annotations.toKeep3 = table.toKeep3'''. Final",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:52303,Integrability,depend,dependent,52303,"ession involving v (variant), va (variant annotations), ; and aIndex (allele index); annotation (str) – Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices); subset (bool) – If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs.; keep (bool) – If true, keep variants matching expr; filter_altered_genotypes (bool) – If true, genotypes that contain filtered-out alleles are set to missing.; max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered.; keepStar (bool) – If true, keep variants where the only allele left is a * allele. Returns:Filtered variant dataset. Return type:VariantDataset. filter_genotypes(expr, keep=True)[source]¶; Filter genotypes based on expression.; Examples; Filter genotypes by allele balance dependent on genotype call:; >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.1) || ' +; ... '(g.isHet() && ab >= 0.25 && ab <= 0.75) || ' +; ... '(g.isHomVar() && ab >= 0.9))'). Notes; expr is in genotype context so the following symbols are in scope:. s (Sample): sample; v (Variant): Variant; sa: sample annotations; va: variant annotations; global: global annotations. For more information, see the documentation on data representation, annotations, and; the expression language. Caution; When expr evaluates to missing, the genotype will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep genotypes where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_intervals(intervals, keep=True)[source]¶; Filter variants with an interval or list of intervals.; Examples; Filter to one interval:; >>> vds_result = vds.filter",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:77099,Integrability,depend,depend,77099,"_set:; if ((v1.position - v2.position) <= window and correlation(v1, v2) >= r2):; keep = False; if keep:; pruned_set.append(v1). The parameter window defines the maximum distance in base pairs between two variants to check whether; the variants are independent (\(R^2\) < r2) where r2 is the maximum \(R^2\) allowed.; \(R^2\) is defined as the square of Pearson’s correlation coefficient; \({\rho}_{x,y}\) between the two genotype vectors \({\mathbf{x}}\) and \({\mathbf{y}}\). \[{\rho}_{x,y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}\]; ld_prune() with default arguments is equivalent to plink --indep-pairwise 1000kb 1 0.2.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK.; Be sure to provide enough disk space per worker because ld_prune() persists up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters r2 and window. The number of bytes stored in memory per variant is about nSamples / 4 + 50. Warning; The variants in the pruned set are not guaranteed to be identical each time ld_prune() is run. We recommend running ld_prune() once and exporting the list of LD pruned variants using; export_variants() for future use. Parameters:; r2 (float) – Maximum \(R^2\) threshold between two variants in the pruned set within a given window.; window (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The number of cores available. Equivalent to the total number of workers times the number of cores per worker. Returns:Variant dataset filtered to those variants which remain after LD pruning. Return type:VariantDataset. linreg(y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:93302,Integrability,depend,depending,93302,"ollowing four steps in order:. filter to samples in given kinship matrix to those for which sa.pheno, sa.cov, and sa.cov2 are all defined; compute the eigendecomposition \(K = USU^T\) of the kinship matrix; fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likelihood (REML), storing results in global annotations under global.lmmreg; test each variant for association, storing results under va.lmmreg in variant annotations. This plan can be modified as follows:. Set run_assoc=False to not test any variants for association, i.e. skip Step 5.; Set use_ml=True to use maximum likelihood instead of REML in Steps 4 and 5.; Set the delta argument to manually set the value of \(\delta\) rather that fitting \(\delta\) in Step 4.; Set the global_root argument to change the global annotation root in Step 4.; Set the va_root argument to change the variant annotation root in Step 5. lmmreg() adds 9 or 13 global annotations in Step 4, depending on whether \(\delta\) is set or fit. Annotation; Type; Value. global.lmmreg.useML; Boolean; true if fit by ML, false if fit by REML. global.lmmreg.beta; Dict[String, Double]; map from intercept and the given covariates expressions to the corresponding fit \(\beta\) coefficients. global.lmmreg.sigmaG2; Double; fit coefficient of genetic variance, \(\hat{\sigma}_g^2\). global.lmmreg.sigmaE2; Double; fit coefficient of environmental variance \(\hat{\sigma}_e^2\). global.lmmreg.delta; Double; fit ratio of variance component coefficients, \(\hat{\delta}\). global.lmmreg.h2; Double; fit narrow-sense heritability, \(\hat{h}^2\). global.lmmreg.nEigs; Int; number of eigenvectors of kinship matrix used to fit model. global.lmmreg.dropped_variance_fraction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:96828,Integrability,rout,routine,96828,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:96918,Integrability,rout,routines,96918,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:99950,Integrability,depend,depends,99950,"etic variance component coefficients; \(h^2 = \frac{\sigma_g^2}{\sigma_g^2 + \sigma_e^2} = \frac{1}{1 + \delta} =\) genetic proportion of residual phenotypic variance. Under a linear mixed model, \(y\) is sampled from the \(n\)-dimensional multivariate normal distribution with mean \(X \beta\) and variance components that are scalar multiples of \(K\) and \(I\):. \[y \sim \mathrm{N}\left(X\beta, \sigma_g^2 K + \sigma_e^2 I\right)\]; Thus the model posits that the residuals \(y_i - X_{i,:}\beta\) and \(y_j - X_{j,:}\beta\) have covariance \(\sigma_g^2 K_{ij}\) and approximate correlation \(h^2 K_{ij}\). Informally: phenotype residuals are correlated as the product of overall heritability and pairwise kinship. By contrast, standard (unmixed) linear regression is equivalent to fixing \(\sigma_2\) (equivalently, \(h^2\)) at 0 above, so that all phenotype residuals are independent.; Caution: while it is tempting to interpret \(h^2\) as the narrow-sense heritability of the phenotype alone, note that its value depends not only the phenotype and genetic data, but also on the choice of sample covariates.; Fitting the global model; The core algorithm is essentially a distributed implementation of the spectral approach taken in FastLMM. Let \(K = USU^T\) be the eigendecomposition of the real symmetric matrix \(K\). That is:. \(U = n \times n\) orthonormal matrix whose columns are the eigenvectors of \(K\); \(S = n \times n\) diagonal matrix of eigenvalues of \(K\) in descending order. \(S_{ii}\) is the eigenvalue of eigenvector \(U_{:,i}\); \(U^T = n \times n\) orthonormal matrix, the transpose (and inverse) of \(U\). A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. \[U^Ty \sim \mathrm{N}\left(U^TX\beta, \sigma_g^2 (S + \delta I)\right)\]; for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (\(y\)) and covariate vectors (columns of \(X\)) in \(\m",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:103042,Integrability,integrat,integrate,103042,"imum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the le",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:110933,Integrability,depend,depend,110933,"called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton i",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:112844,Integrability,depend,dependent,112844,"mum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:129622,Integrability,interface,interface,129622,"erate on because the work is not evenly distributed across partitions. Parameters:max_partitions (int) – Desired number of partitions. If the current number of partitions is less than max_partitions, do nothing. Returns:Variant dataset with the number of partitions equal to at most max_partitions. Return type:VariantDataset. num_partitions()[source]¶; Number of partitions.; Notes; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:13023,Modifiability,variab,variable,13023,"],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotation to add to global; annotation_type (Type) – Hail type of annotation. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global_expr(expr)[source]¶; Annotate global with expression.; Example; Annotate global with an array of populations:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. global: global annotations. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_samples_expr(expr)[source]¶; Annotate samples with expression.; Examples; Compute per-sample GQ statistics for hets:; >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:; >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).co",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:56021,Modifiability,variab,variable,56021,"ble() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove variants overlapping; an interval if False. Returns:Filtered variant dataset. Return type:VariantDataset. filter_multi()[source]¶; Filter out multi-allelic sites. Important; The genotype_schema() must be of type TGenotype in order to use this method. This method is much less computationally expensive than; split_multi(), and can also be used to produce; a variant dataset that can be used with methods that do not; support multiallelic variants. Returns:Dataset with no multiallelic sites, which can; be used for biallelic-only methods. Return type:VariantDataset. filter_samples_expr(expr, keep=True)[source]¶; Filter samples with the expression language.; Examples; Filter samples by phenotype (assumes sample annotation sa.isCase exists and is a Boolean variable):; >>> vds_result = vds.filter_samples_expr(""sa.isCase""). Remove samples with an ID that matches a regular expression:; >>> vds_result = vds.filter_samples_expr('""^NA"" ~ s' , keep=False). Filter samples from sample QC metrics and write output to a new variant dataset:; >>> (vds.sample_qc(); ... .filter_samples_expr('sa.qc.callRate >= 0.99 && sa.qc.dpMean >= 10'); ... .write(""output/filter_samples.vds"")). Notes; expr is in sample context so the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. For more information, see the documentation on data representation, annotations, and; the expression language. Caution; When expr evaluates to missing, the sample will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep samples where expr evaluates to true. Returns:Filtered variant dataset. Retu",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:69462,Modifiability,variab,variables,69462,"les so that no two have a PI_HAT value greater than or equal to 0.6.; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:; >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). Notes; The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted.; The tiebreaking_expr namespace has the following variables available:. s1: The first sample id.; sa1: The annotations associated with s1.; s2: The second sample id.; sa2: The annotations associated with s2. The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping s1. Any positive integer expresses a preference for keeping s2. A zero expresses no preference. This function must induce a preorder on the samples, in particular:. tiebreaking_expr(sample1, sample2) must equal -1 * tie breaking_expr(sample2, sample1), which evokes the common sense understanding that if x < y then y > x`.; tiebreaking_expr(sample1, sample1) must equal 0, i.e. x = x; if sample1 is preferred to sample2 and sample2 is preferred to sample3, then sample1 must also be preferred to sample3. The last requirement is only important if you have three related samples with the same number of relatives and all three are related to one another. In cases like this ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:97971,Modifiability,config,configuration,97971,"bic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X = n \times c\) matrix of sample covariates and intercept column of ones; \(K = n \times n\) kinship matrix; \(I = n \times n\) identity matrix; \(\beta = c \times 1\) vector of covariate coefficients; \(\sigma_g^2 =\) coefficient of genetic variance component \(K\); \(\sigma_e^2 =\) coefficient of environmental variance component \(I\); \(\delta = \frac{\sigma_e^2}{\sigma_g^2} =\) ratio of environmental and genetic variance component coeffici",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:102853,Modifiability,variab,variables,102853,"imum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the le",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:103108,Modifiability,variab,variables,103108,"imum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the le",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:123350,Modifiability,inherit,inheritance,123350,"Int; C.gq: Int. and values; v A.gt A.gq B.gt B.gq C.gt C.gq; 1:1:A:T 1 99 NA NA 0 99; 1:2:G:C 1 89 1 99 2 93. The above table can be generated and exported as a TSV using KeyTable export().; Notes; Per sample field names in the result are formed by; concatenating the sample ID with the genotype_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:126495,Modifiability,extend,extending,126495,"ng) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; HomVar; ! HomVar; HomRef; Auto. 7; ! HomVar; HomVar; HomRef; Auto. 8; HomVar; HomVar; HomRef; Auto. 9; Any; HomVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This method only considers children with two parents and a defined sex.; PAR is currently defined with respect to reference; ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:131001,Modifiability,inherit,inherited,131001,"or every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of their first k principal; co",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:174112,Modifiability,config,config,174112,"eviation across all samples. Missing values NA may result (for example, due to division by zero) and are handled properly ; in filtering and written as “NA” in export modules. The empirical standard deviation is computed; with zero degrees of freedom. Parameters:root (str) – Variant annotation root for computed struct. Returns:Annotated variant dataset with new variant QC annotations. Return type:VariantDataset. variant_schema¶; Returns the signature of the variant annotations contained in this VDS.; Examples; >>> print(vds.variant_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Requi",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:174255,Modifiability,plugin,plugin,174255,"itten as “NA” in export modules. The empirical standard deviation is computed; with zero degrees of freedom. Parameters:root (str) – Variant annotation root for computed struct. Returns:Annotated variant dataset with new variant QC annotations. Return type:VariantDataset. variant_schema¶; Returns the signature of the variant annotations contained in this VDS.; Examples; >>> print(vds.variant_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; h",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:174466,Modifiability,config,configuration,174466,"ataset with new variant QC annotations. Return type:VariantDataset. variant_schema¶; Returns the signature of the variant annotations contained in this VDS.; Examples; >>> print(vds.variant_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:174786,Modifiability,variab,variable,174786,"ort pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Requir",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:174903,Modifiability,variab,variable,174903,"tations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/l",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175356,Modifiability,plugin,plugin,175356,"; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175369,Modifiability,plugin,plugin,175369,"; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175401,Modifiability,plugin,plugin,175401,"; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175582,Modifiability,plugin,plugin,175582,"ghly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175610,Modifiability,plugin,plugin,175610," key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175728,Modifiability,plugin,plugin,175728,"f Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175756,Modifiability,plugin,plugin,175756,"e PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset sch",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175825,Modifiability,config,configuration,175825,"lt PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: Stri",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:176472,Modifiability,plugin,plugin,176472,"nservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_allele: String,; exac_afr_maf: Double,; exac_amr_allele: String,; exac_amr_maf: Double,; exac_eas_allele: String,; exac_eas_maf: Double,; exac_fin_allele: String,; exac_fin_maf: Double,; exac_maf: Double,; exac_nfe_a",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:179545,Modifiability,config,config,179545,"on_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. Parameters:; config (str) – Path to VEP configuration file.; block_size (int) – Number of variants to annotate per VEP invocation.; root (str) – Variant annotation path to store VEP output.; csq (bool) – If True, annotates VCF CSQ field as a String.; If False, annotates with the full nested struct schema. Returns:An annotated with variant annotations from VEP. Return type:VariantDataset. was_split()[source]¶; True if multiallelic variants have been split into multiple biallelic variants.; Result is True if split_multi() or filter_multi() has been called on this variant dataset,; or if the variant dataset was imported with import_plink(), import_gen(),; or import_bgen(), or if the variant dataset was simulated with balding_nichols_model(). Return type:bool. write(output, overwrite=False, parquet_genotypes=False)[source]¶; Write variant dataset as VDS file.; Examples; Import data from a VCF file and then write the data to a VDS file:; >>> vds.write(""output/sample.vds""). Parameters:; output (str) – Path of VDS file t",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:179572,Modifiability,config,configuration,179572,"on_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. Parameters:; config (str) – Path to VEP configuration file.; block_size (int) – Number of variants to annotate per VEP invocation.; root (str) – Variant annotation path to store VEP output.; csq (bool) – If True, annotates VCF CSQ field as a String.; If False, annotates with the full nested struct schema. Returns:An annotated with variant annotations from VEP. Return type:VariantDataset. was_split()[source]¶; True if multiallelic variants have been split into multiple biallelic variants.; Result is True if split_multi() or filter_multi() has been called on this variant dataset,; or if the variant dataset was imported with import_plink(), import_gen(),; or import_bgen(), or if the variant dataset was simulated with balding_nichols_model(). Return type:bool. write(output, overwrite=False, parquet_genotypes=False)[source]¶; Write variant dataset as VDS file.; Examples; Import data from a VCF file and then write the data to a VDS file:; >>> vds.write(""output/sample.vds""). Parameters:; output (str) – Path of VDS file t",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:841,Performance,load,load,841,"﻿. . VariantDataset — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; VariantDataset. View page source. VariantDataset¶. class hail.VariantDataset(hc, jvds)[source]¶; Hail’s primary representation of genomic data, a matrix keyed by sample and variant.; Variant datasets may be generated from other formats using the HailContext import methods,; constructed from a variant-keyed KeyTable using VariantDataset.from_table(),; and simulated using balding_nichols_model().; Once a variant dataset has been written to disk with write(),; use read() to load the variant dataset into the environment.; >>> vds = hc.read(""data/example.vds""). Variables:hc (HailContext) – Hail Context. Attributes. colkey_schema; Returns the signature of the column key (sample) contained in this VDS. genotype_schema; Returns the signature of the genotypes contained in this VDS. global_schema; Returns the signature of the global annotations contained in this VDS. globals; Return global annotations as a Python object. num_samples; Number of samples. rowkey_schema; Returns the signature of the row key (variant) contained in this VDS. sample_annotations; Return a dict of sample annotations. sample_ids; Return sampleIDs. sample_schema; Returns the signature of the sample annotations contained in this VDS. variant_schema; Returns the signature of the variant annotations contained in this VDS. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate by user-defined key and aggregation expressions to produce a KeyTable. annotate_alleles_expr; Annotate alleles with expression. annotate_genotypes_expr; Annotate genotypes with expression. annotate_global; Add global annotat",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:2476,Performance,cache,cache,2476,"hema; Returns the signature of the sample annotations contained in this VDS. variant_schema; Returns the signature of the variant annotations contained in this VDS. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate by user-defined key and aggregation expressions to produce a KeyTable. annotate_alleles_expr; Annotate alleles with expression. annotate_genotypes_expr; Annotate genotypes with expression. annotate_global; Add global annotations from Python objects. annotate_global_expr; Annotate global with expression. annotate_samples_expr; Annotate samples with expression. annotate_samples_table; Annotate samples with a key table. annotate_variants_db; Annotate variants using the Hail annotation database. annotate_variants_expr; Annotate variants with expression. annotate_variants_table; Annotate variants with a key table. annotate_variants_vds; Annotate variants with variant annotations from .vds file. cache; Mark this variant dataset to be cached in memory. concordance; Calculate call concordance with another variant dataset. count; Returns number of samples and variants in the dataset. count_variants; Count number of variants in variant dataset. deduplicate; Remove duplicate variants. delete_va_attribute; Removes an attribute from a variant annotation field. drop_samples; Removes all samples from variant dataset. drop_variants; Discard all variants, variant annotations and genotypes. export_gen; Export variant dataset as GEN and SAMPLE file. export_genotypes; Export genotype-level information to delimited text file. export_plink; Export variant dataset as PLINK2 BED, BIM and FAM. export_samples; Export sample information to delimited text file. export_variants; Export variant information to delimited text file. export_vcf; Export variant dataset as a .vcf or .vcf.bgz file. file_version; File version of variant dataset. filter_alleles; Filter a user-defined set of alternate alleles for each variant. filter_gen",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:2515,Performance,cache,cached,2515,"hema; Returns the signature of the sample annotations contained in this VDS. variant_schema; Returns the signature of the variant annotations contained in this VDS. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate by user-defined key and aggregation expressions to produce a KeyTable. annotate_alleles_expr; Annotate alleles with expression. annotate_genotypes_expr; Annotate genotypes with expression. annotate_global; Add global annotations from Python objects. annotate_global_expr; Annotate global with expression. annotate_samples_expr; Annotate samples with expression. annotate_samples_table; Annotate samples with a key table. annotate_variants_db; Annotate variants using the Hail annotation database. annotate_variants_expr; Annotate variants with expression. annotate_variants_table; Annotate variants with a key table. annotate_variants_vds; Annotate variants with variant annotations from .vds file. cache; Mark this variant dataset to be cached in memory. concordance; Calculate call concordance with another variant dataset. count; Returns number of samples and variants in the dataset. count_variants; Count number of variants in variant dataset. deduplicate; Remove duplicate variants. delete_va_attribute; Removes an attribute from a variant annotation field. drop_samples; Removes all samples from variant dataset. drop_variants; Discard all variants, variant annotations and genotypes. export_gen; Export variant dataset as GEN and SAMPLE file. export_genotypes; Export genotype-level information to delimited text file. export_plink; Export variant dataset as PLINK2 BED, BIM and FAM. export_samples; Export sample information to delimited text file. export_variants; Export variant information to delimited text file. export_vcf; Export variant dataset as a .vcf or .vcf.bgz file. file_version; File version of variant dataset. filter_alleles; Filter a user-defined set of alternate alleles for each variant. filter_gen",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:11292,Performance,perform,performance,11292,"port_vcf('data/example3.vcf.bgz', generic=True, call_fields=['GTA']); ... .annotate_genotypes_expr('g = g.GTA.toGenotype()')). Notes; annotate_genotypes_expr() evaluates the expression given by expr and assigns; the result of the right hand side to the annotation path specified by the left-hand side (must; begin with g). This is analogous to annotate_variants_expr() and; annotate_samples_expr() where the annotation paths are va and sa respectively.; expr is in genotype context so the following symbols are in scope:. g: genotype annotation; v (Variant): Variant; va: variant annotations; s (Sample): sample; sa: sample annotations; global: global annotations. For more information, see the documentation on writing expressions; and using the Hail Expression Language. Warning. If the resulting genotype schema is not TGenotype,; subsequent function calls on the annotated variant dataset may not work such as; pca() and linreg().; Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to TGenotype.; Genotypes are immutable. For example, if g is initially of type Genotype, the expression; g.gt = g.gt + 1 will return a Struct with one field gt of type Int and NOT a Genotype; with the gt incremented by 1. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global(path, annotation, annotation_type)[source]¶; Add global annotations from Python objects.; Examples; Add populations as a global annotation:; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotat",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:14261,Performance,perform,performance,14261,"cs for hets:; >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:; >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).collect()')). To create an annotation for only a subset of samples based on an existing annotation:; >>> vds_result = vds.annotate_samples_expr('sa.newpheno = if (sa.pheno.cohortName == ""cohort1"") sa.pheno.bloodPressure else NA: Double'). Note; For optimal performance, be sure to explicitly give the alternative (NA) the same type as the consequent (sa.pheno.bloodPressure). Notes; expr is in sample context so the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_samples_table(table, root=None, expr=None, vds_key=None, product=False)[source]¶; Annotate samples with a key table.; Examples; To annotates samples using samples1.tsv with type imputation:; >>> table = hc.import_table('data/samples1.tsv', impute=True).key_by('Sample'); >>> vds_result = vds.annotate_samples_table(table, root='sa.pheno'). Given this file; $ cat data/samples1.tsv; Sample Height Status Age; PT-1234 154.1 ADHD 24; PT-1236 160.9 Control 19; PT-1238 NA ADHD 89; PT-1239 170.3 Control 55. the three new sample annotations are sa.pheno.Height: Double, sa.pheno.Sta",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:29787,Performance,cache,cache,29787,"r; to the specified annotation path.; The expr argument expects an annotation assignment whose scope; includes, va, the variant annotations in the current VDS, and vds,; the variant annotations in other.; VDSes with multi-allelic variants may produce surprising results because; all alternate alleles are considered part of the variant key. For; example:. The variant 22:140012:A:T,TTT will not be annotated by; 22:140012:A:T or 22:140012:A:TTT; The variant 22:140012:A:T will not be annotated by; 22:140012:A:T,TTT. It is possible that an unsplit variant dataset contains no multiallelic; variants, so ignore any warnings Hail prints if you know that to be the; case. Otherwise, run split_multi() before annotate_variants_vds(). Parameters:; other (VariantDataset) – Variant dataset to annotate with.; root (str) – Sample annotation path to add variant annotations.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no d",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:29837,Performance,cache,cached,29837,"r; to the specified annotation path.; The expr argument expects an annotation assignment whose scope; includes, va, the variant annotations in the current VDS, and vds,; the variant annotations in other.; VDSes with multi-allelic variants may produce surprising results because; all alternate alleles are considered part of the variant key. For; example:. The variant 22:140012:A:T,TTT will not be annotated by; 22:140012:A:T or 22:140012:A:TTT; The variant 22:140012:A:T will not be annotated by; 22:140012:A:T,TTT. It is possible that an unsplit variant dataset contains no multiallelic; variants, so ignore any warnings Hail prints if you know that to be the; case. Otherwise, run split_multi() before annotate_variants_vds(). Parameters:; other (VariantDataset) – Variant dataset to annotate with.; root (str) – Sample annotation path to add variant annotations.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no d",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:29856,Performance,cache,cache,29856,"pects an annotation assignment whose scope; includes, va, the variant annotations in the current VDS, and vds,; the variant annotations in other.; VDSes with multi-allelic variants may produce surprising results because; all alternate alleles are considered part of the variant key. For; example:. The variant 22:140012:A:T,TTT will not be annotated by; 22:140012:A:T or 22:140012:A:TTT; The variant 22:140012:A:T will not be annotated by; 22:140012:A:T,TTT. It is possible that an unsplit variant dataset contains no multiallelic; variants, so ignore any warnings Hail prints if you know that to be the; case. Otherwise, run split_multi() before annotate_variants_vds(). Parameters:; other (VariantDataset) – Variant dataset to annotate with.; root (str) – Sample annotation path to add variant annotations.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no data” in the other.; This method returns a tuple of three o",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:30627,Performance,perform,performs,30627,"ons.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no data” in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant concordance ; statistics.; Using the global summary result; The global summary is a list of list of int (conceptually a 5 by 5 matrix), ; where the indices have special meaning:. No Data (missing variant); No Call (missing genotype call); Hom Ref; Heterozygous; Hom Var. The first index is the state in the left dataset (the one on which concordance was called), and the second; index is the state in the right dataset (the argument to the concordance method call). Typical uses of ; the summary list are shown below.; >>> summary, samples, variants = vds.concordance(hc.read('data/example2.vds')); >>> left_homref_right_homvar = summary[2][4]; >",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:54206,Performance,perform,performs,54206,"nterval:; >>> vds_result = vds.filter_intervals(Interval.parse('17:38449840-38530994')). Another way of writing this same query:; >>> vds_result = vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))). Two identical ways of parsing a list of intervals:; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]. Use this interval list to filter:; >>> vds_result = vds.filter_intervals(intervals). Notes; This method takes an argument of Interval or list of Interval.; Based on the keep argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove varian",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:54329,Performance,load,loaded,54329,"nterval:; >>> vds_result = vds.filter_intervals(Interval.parse('17:38449840-38530994')). Another way of writing this same query:; >>> vds_result = vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))). Two identical ways of parsing a list of intervals:; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]. Use this interval list to filter:; >>> vds_result = vds.filter_intervals(intervals). Notes; This method takes an argument of Interval or list of Interval.; Based on the keep argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove varian",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:54414,Performance,latency,latency,54414," vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))). Two identical ways of parsing a list of intervals:; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]. Use this interval list to filter:; >>> vds_result = vds.filter_intervals(intervals). Notes; This method takes an argument of Interval or list of Interval.; Based on the keep argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove variants overlapping; an interval if False. Returns:Filtered variant dataset. Return type:VariantDataset. filter_multi()[source]¶; Filter out multi-alle",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:59690,Performance,perform,performs,59690,"=False). Caution; The double quotes on ""1"" are necessary because v.contig is of type String. Notes; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. For more information, see the Overview and the Expression Language. Caution; When expr evaluates to missing, the variant will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep variants where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_list(variants, keep=True)[source]¶; Filter variants with a list of variants.; Examples; Filter VDS down to a list of variants:; >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True). Notes; This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap with any supplied variant will not be loaded at all. This property; enables filter_variants_list to be used for reasonably low-latency queries of one; or more variants, even on large datasets. Parameters:; variants (list of Variant) – List of variants to keep or remove.; keep (bool) – If true, keep variants in variants, otherwise remove them. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_table(table, keep=True)[source]¶; Filter variants with a Variant keyed key table.; Example; Filter variants of a VDS to those appearing in a text file:; >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; >>> kt = hc.import_table('data/locus-table.tsv', impute=True).key_by('Locus'); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Re",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:59817,Performance,load,loaded,59817,"=False). Caution; The double quotes on ""1"" are necessary because v.contig is of type String. Notes; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. For more information, see the Overview and the Expression Language. Caution; When expr evaluates to missing, the variant will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep variants where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_list(variants, keep=True)[source]¶; Filter variants with a list of variants.; Examples; Filter VDS down to a list of variants:; >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True). Notes; This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap with any supplied variant will not be loaded at all. This property; enables filter_variants_list to be used for reasonably low-latency queries of one; or more variants, even on large datasets. Parameters:; variants (list of Variant) – List of variants to keep or remove.; keep (bool) – If true, keep variants in variants, otherwise remove them. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_table(table, keep=True)[source]¶; Filter variants with a Variant keyed key table.; Example; Filter variants of a VDS to those appearing in a text file:; >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; >>> kt = hc.import_table('data/locus-table.tsv', impute=True).key_by('Locus'); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Re",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:59906,Performance,latency,latency,59906,"riant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. For more information, see the Overview and the Expression Language. Caution; When expr evaluates to missing, the variant will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep variants where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_list(variants, keep=True)[source]¶; Filter variants with a list of variants.; Examples; Filter VDS down to a list of variants:; >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True). Notes; This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap with any supplied variant will not be loaded at all. This property; enables filter_variants_list to be used for reasonably low-latency queries of one; or more variants, even on large datasets. Parameters:; variants (list of Variant) – List of variants to keep or remove.; keep (bool) – If true, keep variants in variants, otherwise remove them. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_table(table, keep=True)[source]¶; Filter variants with a Variant keyed key table.; Example; Filter variants of a VDS to those appearing in a text file:; >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; >>> kt = hc.import_table('data/locus-table.tsv', impute=True).key_by('Locus'); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Remove all variants which overlap an interval in a UCSC BED file:; >>> kt = KeyTable.import_bed('data/file2.bed'); >>> filtered_vds = vds.filter_variant",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:66954,Performance,perform,perform,66954,"s method. A hard-called variant dataset is about two orders of magnitude; smaller than a standard sequencing dataset. Use this; method to create a smaller, faster; representation for downstream processing that only; requires the GT field. Returns:Variant dataset with no genotype metadata. Return type:VariantDataset. ibd(maf=None, bounded=True, min=None, max=None)[source]¶; Compute matrix of identity-by-descent estimations. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:; >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; PI_HAT in [0.2, 0.9], using minor allele frequencies stored in; va.panel_maf:; >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). Notes; The implementation is based on the IBD algorithm described in the PLINK; paper.; ibd() requires the dataset to be; bi-allelic (otherwise run split_multi() or otherwise run filter_multi()); and does not perform LD pruning. Linkage disequilibrium may bias the; result so consider filtering variants first.; The resulting KeyTable entries have the type: { i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }. The key list is: *i: String, j:; String*.; Conceptually, the output is a symmetric, sample-by-sample matrix. The; output key table has the following form; i j ibd.Z0 ibd.Z1 ibd.Z2 ibd.PI_HAT ibs0 ibs1 ibs2; sample1 sample2 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample3 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample4 0.6807 0.0000 0.3193 0.3193 ...; sample1 sample5 0.1966 0.0000 0.8034 0.8034 ... Parameters:; maf (str or None) – Expression for the minor allele frequency.; bounded (bool) – Forces the estimations for Z0, Z1, Z2,; and PI_HAT to take on biologically meaningful values; (in the range [0,1]).; min (float or None) – Sample pairs with a PI_HAT below this v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:73696,Performance,perform,performs,73696,"ange this behavior. Annotations; The below annotations can be accessed with sa.imputesex. isFemale (Boolean) – True if the imputed sex is female, false if male, missing if undetermined; Fstat (Double) – Inbreeding coefficient; nTotal (Long) – Total number of variants considered; nCalled (Long) – Number of variants with a genotype call; expectedHoms (Double) – Expected number of homozygotes; observedHoms (Long) – Observed number of homozygotes. Parameters:; maf_threshold (float) – Minimum minor allele frequency threshold.; include_par (bool) – Include pseudoautosomal regions.; female_threshold (float) – Samples are called females if F < femaleThreshold; male_threshold (float) – Samples are called males if F > maleThreshold; pop_freq (str) – Variant annotation for estimate of MAF.; If None, MAF will be computed. Returns:Annotated dataset. Return type:VariantDataset. join(right)[source]¶; Join two variant datasets.; Notes; This method performs an inner join on variants,; concatenates samples, and takes variant and; global annotations from the left dataset (self).; The datasets must have distinct samples, the same sample schema, and the same split status (both split or both multi-allelic). Parameters:right (VariantDataset) – right-hand variant dataset. Returns:Joined variant dataset. Return type:VariantDataset. ld_matrix(force_local=False)[source]¶; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS.; Examples; >>> ld_mat = vds.ld_matrix(). Notes; Each entry (i, j) in the LD matrix gives the \(r\) value between variants i and j, defined as; Pearson’s correlation coefficient; \(\rho_{x_i,x_j}\) between the two genotype vectors \(x_i\) and \(x_j\). \[\rho_{x_i,x_j} = \frac{\mathrm{Cov}(X_i,X_j)}{\sigma_{X_i} \sigma_{X_j}}\]; Also note that variants with zero variance (\(\sigma = 0\)) will be dropped from the matrix. Caution; The matrix returned by this function can easily be very large with most entries near zero; (for example, entries be",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:75149,Performance,perform,performance,75149,"e variants in this VDS.; Examples; >>> ld_mat = vds.ld_matrix(). Notes; Each entry (i, j) in the LD matrix gives the \(r\) value between variants i and j, defined as; Pearson’s correlation coefficient; \(\rho_{x_i,x_j}\) between the two genotype vectors \(x_i\) and \(x_j\). \[\rho_{x_i,x_j} = \frac{\mathrm{Cov}(X_i,X_j)}{\sigma_{X_i} \sigma_{X_j}}\]; Also note that variants with zero variance (\(\sigma = 0\)) will be dropped from the matrix. Caution; The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you’ll want to reduce the number of variants with methods like; sample_variants(), filter_variants_expr(), or ld_prune() before; calling this unless your dataset is very small. Parameters:force_local (bool) – If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds \(5000^2\) and locally otherwise. Returns:Matrix of r values between pairs of variants. Return type:LDMatrix. ld_prune(r2=0.2, window=1000000, memory_per_core=256, num_cores=1)[source]¶; Prune variants in linkage disequilibrium (LD). Important; The genotype_schema() must be of type TGenotype in order to use this method. Requires was_split equals True.; Examples; Export the set of common LD pruned variants to a file:; >>> vds_result = (vds.variant_qc(); ... .filter_variants_expr(""va.qc.AF >= 0.05 && va.qc.AF <= 0.95""); ... .ld_prune(); ... .export_variants(""output/ldpruned.variants"", ""v"")). Notes; Variants are pruned in each contig from smallest to largest start position. The LD pruning algorithm is as follows:; pruned_set = []; for v1 in contig:; keep = True; for v2 in pruned_set:; if ((v1.position - v2.position) <= window and",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:83908,Performance,perform,perform,83908,"ollowing four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; variant_block_size (int) – Number of variant regressions to perform simultaneously. Larger block size requires more memmory. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg_burden(key_name, variant_keys, single_key, agg_expr, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using linear regression on the maximum genotype per gene. Here va.genes is a variant; annotation of type Set[String] giving the set of genes containing the variant (see Extended example below; for a deep dive):; >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.co",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:95097,Performance,perform,performed,95097,"bal.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:96885,Performance,perform,performant,96885,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:96935,Performance,perform,performance,96935,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:97129,Performance,perform,performance,97129,"om the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:97187,Performance,load,loaded,97187,"om the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:101280,Performance,optimiz,optimization,101280,"n \times n\) diagonal matrix of eigenvalues of \(K\) in descending order. \(S_{ii}\) is the eigenvalue of eigenvector \(U_{:,i}\); \(U^T = n \times n\) orthonormal matrix, the transpose (and inverse) of \(U\). A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. \[U^Ty \sim \mathrm{N}\left(U^TX\beta, \sigma_g^2 (S + \delta I)\right)\]; for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (\(y\)) and covariate vectors (columns of \(X\)) in \(\mathbb{R}^n\) by \(U^T\) transforms the model to one with independent residuals. For any particular value of \(\delta\), the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in \(n\). In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over \(\delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:106649,Performance,perform,performance,106649,"model with parameters \((\beta^0_v, \beta^1_v, \ldots, \beta^c_v), \sigma_{g_v}^2)\), with \(\delta\) fixed at \(\hat\delta\) in both. The latter fit is simply that of the global model, \(((0, \hat{\beta}^1, \ldots, \hat{\beta}^c), \hat{\sigma}_g^2)\). The likelihood ratio test statistic is given by. \[\chi^2 = n \, \mathrm{ln}\left(\frac{\hat{\sigma}^2_g}{\hat{\sigma}_{g,v}^2}\right)\]; and follows a chi-squared distribution with one degree of freedom. Here the ratio \(\hat{\sigma}^2_g / \hat{\sigma}_{g,v}^2\) captures the degree to which adding the variant \(v\) to the global model reduces the residual phenotypic variance.; Kinship Matrix; FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with rrm(). However, any instance of KinshipMatrix may be used, so long as sample_list contains the complete samples of the caller variant dataset in the same order.; Low-rank approximation of kinship for improved performance; lmmreg() can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter n_eigs to use only the top n_eigs eigenvectors. Alternatively, specify dropped_variance_fraction to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also known as the trace, or the sum of the eigenvalues). For example, dropped_variance_fraction=0.01 will use the minimal number of eigenvectors to account for 99% of the sample variance. Specifying both parameters will apply the more stringent (fewest eigenvectors) of the two.; Further background; For the history and mathematics of linear mixed models in genetics, including FastLMM, see Christoph Lippert’s PhD thesis. For an investigation of various approaches to defining kinship, see Comparison of Methods to Account for Relatedness ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109365,Performance,perform,performs,109365," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:130027,Performance,perform,performing,130027,"nt dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry grou",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:134889,Performance,perform,perform,134889,"}\widehat{k^{(0)}_{ij}} :=; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2 + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}\end{split}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} := 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Details; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference; implementation in a couple key ways:. the principal components analysis does not use an unrelated set of; individuals; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Notes; The block_size controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation’s time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; block_size larger than 512 tends to cause memory exhaustion errors.; The minimum allele frequency filter is applied per-pair: if either of; the two individual’s individual-specific minor allele frequency is below; the threshold, then the variant’s contribution to relatedness estimates; is zero.; Under the PC-Relate model, kinship, [ phi_{ij} ], ranges from 0 to; 0.5, and is precisely half of the; fracti",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:137945,Performance,load,loadings,137945,"– The minimum individual-specific allele frequency for; an allele used to measure relatedness.; block_size (int) – the side length of the blocks of the block-; distributed matrices; this should be set such; that at least three of these matrices fit in; memory (in addition to all other objects; necessary for Spark and Hail).; min_kinship (float) – Pairs of samples with kinship lower than; min_kinship are excluded from the results.; statistics (str) – the set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:138389,Performance,load,loadings,138389,"an; min_kinship are excluded from the results.; statistics (str) – the set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For eac",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:138452,Performance,load,loadings,138452,"set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is compu",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:138545,Performance,load,loadings,138545,"compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is computed as half the mean of the non-missing entries of column \(j\). Entries of \(M\) are then m",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:138885,Performance,load,loadings,138885," one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is computed as half the mean of the non-missing entries of column \(j\). Entries of \(M\) are then mean-centered and variance-normalized as. \[M_{ij} = \frac{C_{ij}-2p_j}{\sqrt{2p_j(1-p_j)m}},\]; with \(M_{ij} = 0\) for \(C_{ij}\) missing (i.e. mean genotype imputation). This scaling normalizes genotype variances to a common value \(1/m\) for variants in Hardy-Weinberg equilibrium and is further motivated in the paper cited above. (The r",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:140971,Performance,load,loadings,140971," will also introduce noise for rare variants; common practice is to filter out variants with minor allele frequency below some cutoff.) The factor \(1/m\) gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply \(MM^T\).; PCA then computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2, \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\). Typically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:141084,Performance,load,loadings,141084,"ves each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply \(MM^T\).; PCA then computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2, \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\). Typically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading s",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:141389,Performance,load,loadings,141389,"vectors (orthonormal in \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2, \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\). Typically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:141405,Performance,load,loadings,141405,"vectors (orthonormal in \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2, \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\). Typically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:142223,Performance,load,loadings,142223,"data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous va",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:143241,Performance,load,loadings,143241,"loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:143324,Performance,load,loadings,143324,"ctors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the curre",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:143337,Performance,load,loadings,143337,"ing the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memo",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:143512,Performance,load,loadings,143512,"e of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documen",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:143556,Performance,load,loadings,143556,"e of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documen",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:143715,Performance,load,loadings,143715,"spect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linre",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:143778,Performance,load,loadings,143778,"spect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linre",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:144280,Performance,cache,cache,144280,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:144407,Performance,perform,performance,144407,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:144439,Performance,cache,cache,144439,") – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogram; >>> gq_hist = vds.query_genotypes('gs.map(g => g.gq).hist(0, 100, 100)'). Compute call rate; >>> call_rate = vds.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:152192,Performance,perform,performance,152192,"t_table('data/sample_mapping.txt'); >>> mapping_dict = {row.old_id: row.new_id for row in mapping_table.collect()}; >>> vds_result = vds.rename_samples(mapping_dict). Parameters:mapping (dict) – Mapping from old to new sample IDs. Returns:Dataset with remapped sample IDs. Return type:VariantDataset. repartition(num_partitions, shuffle=True)[source]¶; Increase or decrease the number of variant dataset partitions.; Examples; Repartition the variant dataset to have 500 partitions:; >>> vds_result = vds.repartition(500). Notes; Check the current number of partitions with num_partitions().; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with \(M\) variants is first imported, each of the \(k\) partition will contain about \(M/k\) of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it’s recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores.; Partitions are a core concept of distributed computation in Spark, see here for details. With shuffle=True, Hail does a full shuffle of the data and creates equal sized partitions. With shuffle=False, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the repartition and coalesce commands in Spark, respectively. In particular, when shuffle=False, num_partitions cannot exceed current number of partitions. Parameters:; num_partitions (int) – Desired number of partitions, must be less than the current number if shuffle=False; shuffle (bool) – If true, use full shuffle to repartition. Returns:Variant dataset with the number of partitions equal to at most num_partitions. Return type:VariantDataset. rowkey_schema¶; Returns the signatu",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:171567,Performance,perform,performed,171567,"ds_chromY); >>> vds_union2 = all_vds[0].union(*all_vds[1:]); >>> vds_union3 = VariantDataset.union(*all_vds). Notes. In order to combine two datasets, these requirements must be met:. the samples must match; the variant annotation schemas must match (field order within structs matters).; the cell (genotype) schemas must match (field order within structs matters). The column annotations in the resulting dataset are simply the column annotations; from the first dataset; the column annotation schemas do not need to match.; This method can trigger a shuffle, if partitions from two datasets overlap. Parameters:vds_type (tuple of VariantDataset) – Datasets to combine. Returns:Dataset with variants from all datasets. Return type:VariantDataset. unpersist()[source]¶; Unpersists this VDS from memory/disk.; Notes; This function will have no effect on a VDS that was not previously persisted.; There’s nothing stopping you from continuing to use a VDS that has been unpersisted, but doing so will result in; all previous steps taken to compute the VDS being performed again since the VDS must be recomputed. Only unpersist; a VDS when you are done with it. variant_qc(root='va.qc')[source]¶; Compute common variant statistics (quality control metrics). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds_result = vds.variant_qc(). Annotations; variant_qc() computes 18 variant statistics from the ; genotype data and stores the results as variant annotations that can be accessed ; with va.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of samples with called genotypes. AF; Double; Calculated alternate allele frequency (q). AC; Int; Count of alternate alleles. rHeterozygosity; Double; Proportion of heterozygotes. rHetHomVar; Double; Ratio of heterozygotes to homozygous alternates. rExpectedHetFrequency; Double; Expected rHeterozygosity based on HWE. pHWE; Do",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:175075,Performance,cache,cache,175075,"ns. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /pa",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:176356,Performance,cache,cache,176356,"efault: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_allele: String,; exac_afr_maf: Double,; exac_amr_allele: String,",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109434,Safety,predict,predicting,109434," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:144361,Safety,avoid,avoid,144361,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:144367,Safety,redund,redundant,144367,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:152622,Safety,avoid,avoid,152622,"variant dataset to have 500 partitions:; >>> vds_result = vds.repartition(500). Notes; Check the current number of partitions with num_partitions().; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with \(M\) variants is first imported, each of the \(k\) partition will contain about \(M/k\) of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it’s recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores.; Partitions are a core concept of distributed computation in Spark, see here for details. With shuffle=True, Hail does a full shuffle of the data and creates equal sized partitions. With shuffle=False, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the repartition and coalesce commands in Spark, respectively. In particular, when shuffle=False, num_partitions cannot exceed current number of partitions. Parameters:; num_partitions (int) – Desired number of partitions, must be less than the current number if shuffle=False; shuffle (bool) – If true, use full shuffle to repartition. Returns:Variant dataset with the number of partitions equal to at most num_partitions. Return type:VariantDataset. rowkey_schema¶; Returns the signature of the row key (variant) contained in this VDS.; Examples; >>> print(vds.rowkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.rowkey_schema). Return type:Type. rrm(force_block=False, force_gramian=False)[source]¶; Computes the Realized Relationship Matrix (RRM).; Examples; >>> kinship_matrix = vds.rrm(). Notes; The Realized Relationship Matrix i",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:12127,Security,access,accessed,12127,"e resulting genotype schema is not TGenotype,; subsequent function calls on the annotated variant dataset may not work such as; pca() and linreg().; Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to TGenotype.; Genotypes are immutable. For example, if g is initially of type Genotype, the expression; g.gt = g.gt + 1 will return a Struct with one field gt of type Int and NOT a Genotype; with the gt incremented by 1. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global(path, annotation, annotation_type)[source]¶; Add global annotations from Python objects.; Examples; Add populations as a global annotation:; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotation to add to global; annotation_type (Type) – Hail type of annotation. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global_expr(expr)[source]¶; Annotate global with expression.; Example; Annotate global with an array of populations:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. global: global annotations. Parameters:expr (str or list of str) – Annotation expression. Returns:Annota",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:19691,Security,access,accessible,19691,"annotations = select(table, toKeep1, toKeep2, toKeep3)'. The above is equivalent to; expr='''sa.annotations.toKeep1 = table.toKeep1,; sa.annotations.toKeep2 = table.toKeep2,; sa.annotations.toKeep3 = table.toKeep3'''. Finally, for more information about importing key tables from text, ; see the documentation for HailContext.import_table(). Parameters:; table (KeyTable) – Key table.; root (str or None) – Sample annotation path to store text table. (This or expr required).; expr (str or None) – Annotation expression. (This or root required).; vds_key (str, list of str, or None.) – Join key for the dataset, if not sample ID.; product (bool) – Join with all matching keys (see note). Returns:Annotated variant dataset. Return type:VariantDataset. annotate_variants_db(annotations, gene_key=None)[source]¶; Annotate variants using the Hail annotation database. Warning; Experimental. Supported only while running Hail on the Google Cloud Platform. Documentation describing the annotations that are accessible through this method can be found here.; Examples; Annotate variants with CADD raw and PHRED scores:; >>> vds = vds.annotate_variants_db(['va.cadd.RawScore', 'va.cadd.PHRED']) . Annotate variants with gene-level PLI score, using the VEP-generated gene symbol to map variants to genes:; >>> pli_vds = vds.annotate_variants_db('va.gene.constraint.pli') . Again annotate variants with gene-level PLI score, this time using the existing va.gene_symbol annotation ; to map variants to genes:; >>> vds = vds.annotate_variants_db('va.gene.constraint.pli', gene_key='va.gene_symbol') . Notes; Annotations in the database are bi-allelic, so splitting multi-allelic variants in the VDS before using this ; method is recommended to capture all appropriate annotations from the database. To do this, run split_multi() ; prior to annotating variants with this method:; >>> vds = vds.split_multi().annotate_variants_db(['va.cadd.RawScore', 'va.cadd.PHRED']) . To add VEP annotations, or to add gene-leve",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:38783,Security,access,accessible,38783,"otype-level information to delimited text file.; Examples; Export genotype information with identifiers that form the header:; >>> vds.export_genotypes('output/genotypes.tsv', 'SAMPLE=s, VARIANT=v, GQ=g.gq, DP=g.dp, ANNO1=va.anno1, ANNO2=va.anno2'). Export the same information without identifiers, resulting in a file with no header:; >>> vds.export_genotypes('output/genotypes.tsv', 's, v, g.gq, g.dp, va.anno1, va.anno2'). Notes; export_genotypes() outputs one line per cell (genotype) in the data set, though HomRef and missing genotypes are not output by default if the genotype schema is equal to TGenotype. Use the export_ref and export_missing parameters to force export of HomRef and missing genotypes, respectively.; The expr argument is a comma-separated list of fields or expressions, all of which must be of the form IDENTIFIER = <expression>, or else of the form <expression>. If some fields have identifiers and some do not, Hail will throw an exception. The accessible namespace includes g, s, sa, v, va, and global. Warning; If the genotype schema does not have the type TGenotype, all genotypes will be exported unless the value of g is missing.; Use filter_genotypes() to filter out genotypes based on an expression before exporting. Parameters:; output (str) – Output path.; expr (str) – Export expression for values to export.; types (bool) – Write types of exported columns to a file at (output + “.types”); export_ref (bool) – If true, export reference genotypes. Only applicable if the genotype schema is TGenotype.; export_missing (bool) – If true, export missing genotypes.; parallel (bool) – If true, writes a set of files (one per partition) rather than serially concatenating these files. export_plink(output, fam_expr='id = s', parallel=False)[source]¶; Export variant dataset as PLINK2 BED, BIM and FAM. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Import data from a VCF file, split multi-allelic variants, and expor",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:43797,Security,access,accessible,43797,"the; struct.* syntax. This syntax produces one column per field in the; struct, and names them according to the struct field name.; For example, the following invocation (assuming va.qc was generated; by variant_qc()):; >>> vds.export_variants('output/file.tsv', 'variant = v, va.qc.*'). will produce the following set of columns:; variant callRate AC AF nCalled ... Note that using the .* syntax always results in named arguments, so it; is not possible to export header-less files in this manner. However,; naming the “splatted” struct will apply the name in front of each column; like so:; >>> vds.export_variants('output/file.tsv', 'variant = v, QC = va.qc.*'). which produces these columns:; variant QC.callRate QC.AC QC.AF QC.nCalled ... Notes; This module takes a comma-delimited list of fields or expressions to; print. These fields will be printed in the order they appear in the; expression in the header and on each line.; One line per variant in the VDS will be printed. The accessible namespace includes:. v (Variant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. Designating output with an expression; Much like the filtering methods, this method uses the Hail expression language.; While the filtering methods expect an; expression that evaluates to true or false, this method expects a; comma-separated list of fields to print. These fields take the; form IDENTIFIER = <expression>. Parameters:; output (str) – Output file.; expr (str) – Export expression for values to export.; types (bool) – Write types of exported columns to a file at (output + “.types”); parallel (bool) – If true, writes a set of files (one per partition) rather than serially concatenating these files. export_vcf(output, append_to_header=None, export_pp=False, parallel=False)[source]¶; Export variant dataset as a .vcf or .vcf.bgz file.; Examples; Export to VCF as a block-compressed file:; >>> vds.export_vcf('output/example.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:72812,Security,access,accessed,72812," less than the threshold given by maf-threshold are removed; Variants in the pseudoautosomal region (X:60001-2699520) || (X:154931044-155260560) are included if the include_par optional parameter is set to true.; The minor allele frequency (maf) per variant is calculated.; For each variant and sample with a non-missing genotype call, \(E\), the expected number of homozygotes (from population MAF), is computed as \(1.0 - (2.0*maf*(1.0-maf))\).; For each variant and sample with a non-missing genotype call, \(O\), the observed number of homozygotes, is computed as 0 = heterozygote; 1 = homozygote; For each variant and sample with a non-missing genotype call, \(N\) is incremented by 1; For each sample, \(E\), \(O\), and \(N\) are combined across variants; \(F\) is calculated by \((O - E) / (N - E)\); A sex is assigned to each sample with the following criteria: F < 0.2 => Female; F > 0.8 => Male. Use female-threshold and male-threshold to change this behavior. Annotations; The below annotations can be accessed with sa.imputesex. isFemale (Boolean) – True if the imputed sex is female, false if male, missing if undetermined; Fstat (Double) – Inbreeding coefficient; nTotal (Long) – Total number of variants considered; nCalled (Long) – Number of variants with a genotype call; expectedHoms (Double) – Expected number of homozygotes; observedHoms (Long) – Observed number of homozygotes. Parameters:; maf_threshold (float) – Minimum minor allele frequency threshold.; include_par (bool) – Include pseudoautosomal regions.; female_threshold (float) – Samples are called females if F < femaleThreshold; male_threshold (float) – Samples are called males if F > maleThreshold; pop_freq (str) – Variant annotation for estimate of MAF.; If None, MAF will be computed. Returns:Annotated dataset. Return type:VariantDataset. join(right)[source]¶; Join two variant datasets.; Notes; This method performs an inner join on variants,; concatenates samples, and takes variant and; global annotations fr",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:86995,Security,access,accessible,86995,"notation of type Array, then that; variant will be counted twice in that key’s group. With single_key=True, variant_keys expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. Notes; This method modifies linreg() by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the linear regression model using the supplied phenotype and covariates.; The model is that of linreg() with sample genotype gt replaced by the score in the sample; key table. For each key, missing scores are mean-imputed across all samples.; The resulting linear regression key table has the following columns:. value of key_name (String) – descriptor of variant group key (key column); beta (Double) – fit coefficient, \(\hat\beta_1\); se (Double) – estimated standard error, \(\widehat{\mathrm{se}}\); tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); pval (Double) – \(p\)-value. linreg_burden() returns both the linear regression key table and the sample key table.; Extended example; Let’s walk through these steps in the max() toy example above.; There are six samples with th",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:120481,Security,access,accessible,120481,"g the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a si",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:156225,Security,access,accessed,156225,"ariant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of genotypes called. nHomRef; Int; Number of homozygous reference genotypes. nHet; Int; Number of heterozygous genotypes. nHomVar; Int; Number of homozygous alternate genotypes. nCalled; Int; Sum of nHomRef + nHet + nHomVar. nNotCalled; Int; Number of uncalled genotypes. nSNP; Int; Number of SNP alternate alleles. nInsertion; Int; Number of insertion alternate alleles. nDeletion; Int; Number of deletion alternate alleles. nSingleton; Int; Number of private alleles. nTransition; Int; Number of transition (A-G, C-T) alternate alleles. nTransversion; Int; Number of transversion alternate alleles. nNonRef; Int; Sum of nHet and nHomVar. rTiTv; Double; Transition/Transversion ratio. rHetHomVar; Double; Het/HomVar genotype ratio. rInsertionDeletion; Double; Insertion/Deletion ratio. dpMean;",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:172035,Security,access,accessed,172035,"taset; the column annotation schemas do not need to match.; This method can trigger a shuffle, if partitions from two datasets overlap. Parameters:vds_type (tuple of VariantDataset) – Datasets to combine. Returns:Dataset with variants from all datasets. Return type:VariantDataset. unpersist()[source]¶; Unpersists this VDS from memory/disk.; Notes; This function will have no effect on a VDS that was not previously persisted.; There’s nothing stopping you from continuing to use a VDS that has been unpersisted, but doing so will result in; all previous steps taken to compute the VDS being performed again since the VDS must be recomputed. Only unpersist; a VDS when you are done with it. variant_qc(root='va.qc')[source]¶; Compute common variant statistics (quality control metrics). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds_result = vds.variant_qc(). Annotations; variant_qc() computes 18 variant statistics from the ; genotype data and stores the results as variant annotations that can be accessed ; with va.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of samples with called genotypes. AF; Double; Calculated alternate allele frequency (q). AC; Int; Count of alternate alleles. rHeterozygosity; Double; Proportion of heterozygotes. rHetHomVar; Double; Ratio of heterozygotes to homozygous alternates. rExpectedHetFrequency; Double; Expected rHeterozygosity based on HWE. pHWE; Double; p-value from Hardy Weinberg Equilibrium null model. nHomRef; Int; Number of homozygous reference samples. nHet; Int; Number of heterozygous samples. nHomVar; Int; Number of homozygous alternate samples. nCalled; Int; Sum of nHomRef, nHet, and nHomVar. nNotCalled; Int; Number of uncalled samples. nNonRef; Int; Sum of nHet and nHomVar. rHetHomVar; Double; Het/HomVar ratio across all samples. dpMean; Double; Depth mean across all samples. dpStDev; Double; De",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:180864,Security,access,accessible,180864,"g,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. Parameters:; config (str) – Path to VEP configuration file.; block_size (int) – Number of variants to annotate per VEP invocation.; root (str) – Variant annotation path to store VEP output.; csq (bool) – If True, annotates VCF CSQ field as a String.; If False, annotates with the full nested struct schema. Returns:An annotated with variant annotations from VEP. Return type:VariantDataset. was_split()[source]¶; True if multiallelic variants have been split into multiple biallelic variants.; Result is True if split_multi() or filter_multi() has been called on this variant dataset,; or if the variant dataset was imported with import_plink(), import_gen(),; or import_bgen(), or if the variant dataset was simulated with balding_nichols_model(). Return type:bool. write(output, overwrite=False, parquet_genotypes=False)[source]¶; Write variant dataset as VDS file.; Examples; Import data from a VCF file and then write the data to a VDS file:; >>> vds.write(""output/sample.vds""). Parameters:; output (str) – Path of VDS file to write.; overwrite (bool) – If true, overwrite any existing VDS file. Cannot be used to read from and write to the same path.; parquet_genotypes (bool) – If true, store genotypes as Parquet rather than Hail’s serialization. The resulting VDS will be larger and slower in Hail but the genotypes will be accessible from other tools that support Parquet. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:5292,Testability,test,test,5292,"rix (GRM). hardcalls; Drop all genotype fields except the GT field. ibd; Compute matrix of identity-by-descent estimations. ibd_prune; Prune samples from the VariantDataset based on ibd() PI_HAT measures of relatedness. impute_sex; Impute sex of samples by calculating inbreeding coefficient on the X chromosome. join; Join two variant datasets. ld_matrix; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. ld_prune; Prune variants in linkage disequilibrium (LD). linreg; Test each variant for association using linear regression. linreg3; Test each variant for association with multiple phenotypes using linear regression. linreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the linear regression model. linreg_multi_pheno; Test each variant for association with multiple phenotypes using linear regression. lmmreg; Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. logreg; Test each variant for association using logistic regression. logreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the logistic regression model. make_table; Produce a key with one row per variant and one or more columns per sample. mendel_errors; Find Mendel errors; count per variant, individual and nuclear family. min_rep; Gives minimal, left-aligned representation of alleles. naive_coalesce; Naively descrease the number of partitions. num_partitions; Number of partitions. pc_relate; Compute relatedness estimates between individuals using a variant of the PC-Relate method. pca; Run Principal Component Analysis (PCA) on the matrix of genotypes. persist; Persist this variant dataset to memory and/or disk. query_genotypes; Performs aggregation queries over genotypes, and returns Python object(s). query_genotypes_typed; Performs",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:5327,Testability,log,logreg,5327,"bd_prune; Prune samples from the VariantDataset based on ibd() PI_HAT measures of relatedness. impute_sex; Impute sex of samples by calculating inbreeding coefficient on the X chromosome. join; Join two variant datasets. ld_matrix; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. ld_prune; Prune variants in linkage disequilibrium (LD). linreg; Test each variant for association using linear regression. linreg3; Test each variant for association with multiple phenotypes using linear regression. linreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the linear regression model. linreg_multi_pheno; Test each variant for association with multiple phenotypes using linear regression. lmmreg; Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. logreg; Test each variant for association using logistic regression. logreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the logistic regression model. make_table; Produce a key with one row per variant and one or more columns per sample. mendel_errors; Find Mendel errors; count per variant, individual and nuclear family. min_rep; Gives minimal, left-aligned representation of alleles. naive_coalesce; Naively descrease the number of partitions. num_partitions; Number of partitions. pc_relate; Compute relatedness estimates between individuals using a variant of the PC-Relate method. pca; Run Principal Component Analysis (PCA) on the matrix of genotypes. persist; Persist this variant dataset to memory and/or disk. query_genotypes; Performs aggregation queries over genotypes, and returns Python object(s). query_genotypes_typed; Performs aggregation queries over genotypes, and returns Python object(s) and type(s). query_samples; Performs aggregation queries ove",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:5375,Testability,log,logistic,5375,"bd_prune; Prune samples from the VariantDataset based on ibd() PI_HAT measures of relatedness. impute_sex; Impute sex of samples by calculating inbreeding coefficient on the X chromosome. join; Join two variant datasets. ld_matrix; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. ld_prune; Prune variants in linkage disequilibrium (LD). linreg; Test each variant for association using linear regression. linreg3; Test each variant for association with multiple phenotypes using linear regression. linreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the linear regression model. linreg_multi_pheno; Test each variant for association with multiple phenotypes using linear regression. lmmreg; Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. logreg; Test each variant for association using logistic regression. logreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the logistic regression model. make_table; Produce a key with one row per variant and one or more columns per sample. mendel_errors; Find Mendel errors; count per variant, individual and nuclear family. min_rep; Gives minimal, left-aligned representation of alleles. naive_coalesce; Naively descrease the number of partitions. num_partitions; Number of partitions. pc_relate; Compute relatedness estimates between individuals using a variant of the PC-Relate method. pca; Run Principal Component Analysis (PCA) on the matrix of genotypes. persist; Persist this variant dataset to memory and/or disk. query_genotypes; Performs aggregation queries over genotypes, and returns Python object(s). query_genotypes_typed; Performs aggregation queries over genotypes, and returns Python object(s) and type(s). query_samples; Performs aggregation queries ove",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:5516,Testability,log,logistic,5516,"mpute sex of samples by calculating inbreeding coefficient on the X chromosome. join; Join two variant datasets. ld_matrix; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. ld_prune; Prune variants in linkage disequilibrium (LD). linreg; Test each variant for association using linear regression. linreg3; Test each variant for association with multiple phenotypes using linear regression. linreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the linear regression model. linreg_multi_pheno; Test each variant for association with multiple phenotypes using linear regression. lmmreg; Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. logreg; Test each variant for association using logistic regression. logreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the logistic regression model. make_table; Produce a key with one row per variant and one or more columns per sample. mendel_errors; Find Mendel errors; count per variant, individual and nuclear family. min_rep; Gives minimal, left-aligned representation of alleles. naive_coalesce; Naively descrease the number of partitions. num_partitions; Number of partitions. pc_relate; Compute relatedness estimates between individuals using a variant of the PC-Relate method. pca; Run Principal Component Analysis (PCA) on the matrix of genotypes. persist; Persist this variant dataset to memory and/or disk. query_genotypes; Performs aggregation queries over genotypes, and returns Python object(s). query_genotypes_typed; Performs aggregation queries over genotypes, and returns Python object(s) and type(s). query_samples; Performs aggregation queries over samples and sample annotations, and returns Python object(s). query_samples_typed; Performs aggregation qu",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:8451,Testability,test,test,8451,"arize; Returns a summary of useful information about the dataset. tdt; Find transmitted and untransmitted variants; count per variant and nuclear family. union; Take the union of datasets vertically (include all variants). unpersist; Unpersists this VDS from memory/disk. variant_qc; Compute common variant statistics (quality control metrics). variants_table; Convert variants and variant annotations to a KeyTable. vep; Annotate variants with VEP. was_split; True if multiallelic variants have been split into multiple biallelic variants. write; Write variant dataset as VDS file. aggregate_by_key(key_exprs, agg_exprs)[source]¶; Aggregate by user-defined key and aggregation expressions to produce a KeyTable.; Equivalent to a group-by operation in SQL.; Examples; Compute the number of LOF heterozygote calls per gene per sample:; >>> kt_result = (vds; ... .aggregate_by_key(['Sample = s', 'Gene = va.gene'],; ... 'nHet = g.filter(g => g.isHet() && va.consequence == ""LOF"").count()'); ... .export(""test.tsv"")). This will produce a KeyTable with 3 columns (Sample, Gene, nHet). Parameters:; key_exprs (str or list of str) – Named expression(s) for which fields are keys.; agg_exprs (str or list of str) – Named aggregation expression(s). Return type:KeyTable. annotate_alleles_expr(expr, propagate_gq=False)[source]¶; Annotate alleles with expression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To create a variant annotation va.nNonRefSamples: Array[Int] where the ith entry of; the array is the number of samples carrying the ith alternate allele:; >>> vds_result = vds.annotate_alleles_expr('va.nNonRefSamples = gs.filter(g => g.isCalledNonRef()).count()'). Notes; This method is similar to annotate_variants_expr(). annotate_alleles_expr() dynamically splits multi-allelic sites,; evaluates each expression on each split allele separately, and for each expression annotates with an array with one element per alternate allele. In the split",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:50085,Testability,log,log,50085,"litatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 25,20.; DP: No change.; PL: The filtered alleles’ columns are eliminated and the remaining columns shifted so the minimum value is 0.; GQ: The second-lowest PL (after shifting). Downcode algorithm; The downcode algorithm (subset=False) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to split_multi().; The downcoding algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Expression Variables; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; aIndex (Int): the index of the allele being tested. The following s",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:51041,Testability,test,tested,51041,"g filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to split_multi().; The downcoding algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Expression Variables; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; aIndex (Int): the index of the allele being tested. The following symbols are in scope for annotation:. v (Variant): Variant; va: variant annotations; aIndices (Array[Int]): the array of old indices (such that aIndices[newIndex] = oldIndex and aIndices[0] = 0). Parameters:; expr (str) – Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index); annotation (str) – Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices); subset (bool) – If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs.; keep (bool) – If true, keep variants matching expr; filter_altered_genotypes (bool) – If true, genotypes that contain filtered-out alleles are set to missing.; max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that m",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:76873,Testability,test,tests,76873," && va.qc.AF <= 0.95""); ... .ld_prune(); ... .export_variants(""output/ldpruned.variants"", ""v"")). Notes; Variants are pruned in each contig from smallest to largest start position. The LD pruning algorithm is as follows:; pruned_set = []; for v1 in contig:; keep = True; for v2 in pruned_set:; if ((v1.position - v2.position) <= window and correlation(v1, v2) >= r2):; keep = False; if keep:; pruned_set.append(v1). The parameter window defines the maximum distance in base pairs between two variants to check whether; the variants are independent (\(R^2\) < r2) where r2 is the maximum \(R^2\) allowed.; \(R^2\) is defined as the square of Pearson’s correlation coefficient; \({\rho}_{x,y}\) between the two genotype vectors \({\mathbf{x}}\) and \({\mathbf{y}}\). \[{\rho}_{x,y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}\]; ld_prune() with default arguments is equivalent to plink --indep-pairwise 1000kb 1 0.2.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK.; Be sure to provide enough disk space per worker because ld_prune() persists up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters r2 and window. The number of bytes stored in memory per variant is about nSamples / 4 + 50. Warning; The variants in the pruned set are not guaranteed to be identical each time ld_prune() is run. We recommend running ld_prune() once and exporting the list of LD pruned variants using; export_variants() for future use. Parameters:; r2 (float) – Maximum \(R^2\) threshold between two variants in the pruned set within a given window.; window (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The n",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:78591,Testability,test,test,78591," (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The number of cores available. Equivalent to the total number of workers times the number of cores per worker. Returns:Variant dataset filtered to those variants which remain after LD pruning. Return type:VariantDataset. linreg(y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association using linear regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run linear regression per variant using a phenotype and two covariates stored in sample annotations:; >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The linreg() method computes, for each variant, statistics of; the \(t\)-test for the genotype coefficient of the linear function; of best fit from sample genotype and covariates to quantitative; phenotype or case-control status. Hail only includes samples for which; phenotype and all covariates are defined. For each variant, missing genotypes; as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Assuming there are sample annotations sa.pheno.height,; sa.pheno.age, sa.pheno.isFemale, and sa.cov.PC1, the code:; >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale', 'sa.cov.PC1']). considers a model of the form. \[\mathrm{height} = \beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{a",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:84404,Testability,test,test,84404,"ay[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; variant_block_size (int) – Number of variant regressions to perform simultaneously. Larger block size requires more memmory. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg_burden(key_name, variant_keys, single_key, agg_expr, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using linear regression on the maximum genotype per gene. Here va.genes is a variant; annotation of type Set[String] giving the set of genes containing the variant (see Extended example below; for a deep dive):; >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using linear regression on the weighted sum of genotypes per gene. Here va.gene is; a variant annotation of type String giving a single gene per variant (or no gene if missing), and va.weight; is a numeric variant annotation:; >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burde",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:84915,Testability,test,test,84915,"ock size requires more memmory. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg_burden(key_name, variant_keys, single_key, agg_expr, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using linear regression on the maximum genotype per gene. Here va.genes is a variant; annotation of type Set[String] giving the set of genes containing the variant (see Extended example below; for a deep dive):; >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using linear regression on the weighted sum of genotypes per gene. Here va.gene is; a variant annotation of type String giving a single gene per variant (or no gene if missing), and va.weight; is a numeric variant annotation:; >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()' where va.qc.AF; is the allele frequency over those samples that have no missing phenotype or covariates. Caution; With single_key=False, variant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlik",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:91722,Testability,test,test,91722,"ay[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. lmmreg(kinshipMatrix, y, covariates=[], global_root='global.lmmreg', va_root='va.lmmreg', run_assoc=True, use_ml=False, delta=None, sparsity_threshold=1.0, use_dosages=False, n_eigs=None, dropped_variance_fraction=None)[source]¶; Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Suppose the variant dataset saved at data/example_lmmreg.vds has a Boolean variant annotation va.useInKinship and numeric or Boolean sample annotations sa.pheno, sa.cov1, sa.cov2. Then the lmmreg() function in; >>> assoc_vds = hc.read(""data/example_lmmreg.vds""); >>> kinship_matrix = assoc_vds.filter_variants_expr('va.useInKinship').rrm(); >>> lmm_vds = assoc_vds.lmmreg(kinship_matrix, 'sa.pheno', ['sa.cov1', 'sa.cov2']). will execute the following four steps in order:. filter to samples in given kinship matrix to those for which sa.pheno, sa.cov, and sa.cov2 are all defined; compute the eigendecomposition \(K = USU^T\) of the kinship matrix; fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likel",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:92704,Testability,test,test,92704,"h variant for association. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Suppose the variant dataset saved at data/example_lmmreg.vds has a Boolean variant annotation va.useInKinship and numeric or Boolean sample annotations sa.pheno, sa.cov1, sa.cov2. Then the lmmreg() function in; >>> assoc_vds = hc.read(""data/example_lmmreg.vds""); >>> kinship_matrix = assoc_vds.filter_variants_expr('va.useInKinship').rrm(); >>> lmm_vds = assoc_vds.lmmreg(kinship_matrix, 'sa.pheno', ['sa.cov1', 'sa.cov2']). will execute the following four steps in order:. filter to samples in given kinship matrix to those for which sa.pheno, sa.cov, and sa.cov2 are all defined; compute the eigendecomposition \(K = USU^T\) of the kinship matrix; fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likelihood (REML), storing results in global annotations under global.lmmreg; test each variant for association, storing results under va.lmmreg in variant annotations. This plan can be modified as follows:. Set run_assoc=False to not test any variants for association, i.e. skip Step 5.; Set use_ml=True to use maximum likelihood instead of REML in Steps 4 and 5.; Set the delta argument to manually set the value of \(\delta\) rather that fitting \(\delta\) in Step 4.; Set the global_root argument to change the global annotation root in Step 4.; Set the va_root argument to change the variant annotation root in Step 5. lmmreg() adds 9 or 13 global annotations in Step 4, depending on whether \(\delta\) is set or fit. Annotation; Type; Value. global.lmmreg.useML; Boolean; true if fit by ML, false if fit by REML. global.lmmreg.beta; Dict[String, Double]; map from intercept and the given covariates expressions to the corresponding fit \(\beta\) coefficients. global.lmmreg.sigmaG2; Double; fit coefficient of genetic variance, \(\hat{\sigma}_g^2\). global.lmmreg.sigmaE2; Double; fit coefficient o",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:92861,Testability,test,test,92861,"e variant dataset saved at data/example_lmmreg.vds has a Boolean variant annotation va.useInKinship and numeric or Boolean sample annotations sa.pheno, sa.cov1, sa.cov2. Then the lmmreg() function in; >>> assoc_vds = hc.read(""data/example_lmmreg.vds""); >>> kinship_matrix = assoc_vds.filter_variants_expr('va.useInKinship').rrm(); >>> lmm_vds = assoc_vds.lmmreg(kinship_matrix, 'sa.pheno', ['sa.cov1', 'sa.cov2']). will execute the following four steps in order:. filter to samples in given kinship matrix to those for which sa.pheno, sa.cov, and sa.cov2 are all defined; compute the eigendecomposition \(K = USU^T\) of the kinship matrix; fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likelihood (REML), storing results in global annotations under global.lmmreg; test each variant for association, storing results under va.lmmreg in variant annotations. This plan can be modified as follows:. Set run_assoc=False to not test any variants for association, i.e. skip Step 5.; Set use_ml=True to use maximum likelihood instead of REML in Steps 4 and 5.; Set the delta argument to manually set the value of \(\delta\) rather that fitting \(\delta\) in Step 4.; Set the global_root argument to change the global annotation root in Step 4.; Set the va_root argument to change the variant annotation root in Step 5. lmmreg() adds 9 or 13 global annotations in Step 4, depending on whether \(\delta\) is set or fit. Annotation; Type; Value. global.lmmreg.useML; Boolean; true if fit by ML, false if fit by REML. global.lmmreg.beta; Dict[String, Double]; map from intercept and the given covariates expressions to the corresponding fit \(\beta\) coefficients. global.lmmreg.sigmaG2; Double; fit coefficient of genetic variance, \(\hat{\sigma}_g^2\). global.lmmreg.sigmaE2; Double; fit coefficient of environmental variance \(\hat{\sigma}_e^2\). global.lmmreg.delta; Double; fit ratio of variance component coefficients, \(\hat{\delta}",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:94555,Testability,log,log,94555,"ding fit \(\beta\) coefficients. global.lmmreg.sigmaG2; Double; fit coefficient of genetic variance, \(\hat{\sigma}_g^2\). global.lmmreg.sigmaE2; Double; fit coefficient of environmental variance \(\hat{\sigma}_e^2\). global.lmmreg.delta; Double; fit ratio of variance component coefficients, \(\hat{\delta}\). global.lmmreg.h2; Double; fit narrow-sense heritability, \(\hat{h}^2\). global.lmmreg.nEigs; Int; number of eigenvectors of kinship matrix used to fit model. global.lmmreg.dropped_variance_fraction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are Hom",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:94623,Testability,log,logDeltaGrid,94623,"{\sigma}_g^2\). global.lmmreg.sigmaE2; Double; fit coefficient of environmental variance \(\hat{\sigma}_e^2\). global.lmmreg.delta; Double; fit ratio of variance component coefficients, \(\hat{\delta}\). global.lmmreg.h2; Double; fit narrow-sense heritability, \(\hat{h}^2\). global.lmmreg.nEigs; Int; number of eigenvectors of kinship matrix used to fit model. global.lmmreg.dropped_variance_fraction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.e",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:94729,Testability,log,logLkhdVals,94729,"fit ratio of variance component coefficients, \(\hat{\delta}\). global.lmmreg.h2; Double; fit narrow-sense heritability, \(\hat{h}^2\). global.lmmreg.nEigs; Int; number of eigenvectors of kinship matrix used to fit model. global.lmmreg.dropped_variance_fraction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:94770,Testability,log,log,94770,"fit ratio of variance component coefficients, \(\hat{\delta}\). global.lmmreg.h2; Double; fit narrow-sense heritability, \(\hat{h}^2\). global.lmmreg.nEigs; Int; number of eigenvectors of kinship matrix used to fit model. global.lmmreg.dropped_variance_fraction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:94933,Testability,log,log,94933,"nt; number of eigenvectors of kinship matrix used to fit model. global.lmmreg.dropped_variance_fraction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:95038,Testability,log,log,95038,"raction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:95428,Testability,test,test,95428,"alized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:96539,Testability,test,test,96539,"l genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplicati",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:97362,Testability,test,testing,97362," common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X =",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:98195,Testability,test,tests,98195,"mance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X = n \times c\) matrix of sample covariates and intercept column of ones; \(K = n \times n\) kinship matrix; \(I = n \times n\) identity matrix; \(\beta = c \times 1\) vector of covariate coefficients; \(\sigma_g^2 =\) coefficient of genetic variance component \(K\); \(\sigma_e^2 =\) coefficient of environmental variance component \(I\); \(\delta = \frac{\sigma_e^2}{\sigma_g^2} =\) ratio of environmental and genetic variance component coefficients; \(h^2 = \frac{\sigma_g^2}{\sigma_g^2 + \sigma_e^2} = \frac{1}{1 + \delta} =\) genetic proportion of residual phenotypic variance. Under a linear mixed model, \(y\) is ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:98257,Testability,test,test,98257,"mance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X = n \times c\) matrix of sample covariates and intercept column of ones; \(K = n \times n\) kinship matrix; \(I = n \times n\) identity matrix; \(\beta = c \times 1\) vector of covariate coefficients; \(\sigma_g^2 =\) coefficient of genetic variance component \(K\); \(\sigma_e^2 =\) coefficient of environmental variance component \(I\); \(\delta = \frac{\sigma_e^2}{\sigma_g^2} =\) ratio of environmental and genetic variance component coefficients; \(h^2 = \frac{\sigma_g^2}{\sigma_g^2 + \sigma_e^2} = \frac{1}{1 + \delta} =\) genetic proportion of residual phenotypic variance. Under a linear mixed model, \(y\) is ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:101537,Testability,log,log,101537,"gebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. \[U^Ty \sim \mathrm{N}\left(U^TX\beta, \sigma_g^2 (S + \delta I)\right)\]; for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (\(y\)) and covariate vectors (columns of \(X\)) in \(\mathbb{R}^n\) by \(U^T\) transforms the model to one with independent residuals. For any particular value of \(\delta\), the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in \(n\). In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over \(\delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:101596,Testability,log,log,101596,"gebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. \[U^Ty \sim \mathrm{N}\left(U^TX\beta, \sigma_g^2 (S + \delta I)\right)\]; for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (\(y\)) and covariate vectors (columns of \(X\)) in \(\mathbb{R}^n\) by \(U^T\) transforms the model to one with independent residuals. For any particular value of \(\delta\), the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in \(n\). In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over \(\delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:102045,Testability,log,log,102045,"for the latter model can be solved exactly in time complexity that is linear rather than cubic in \(n\). In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over \(\delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using chan",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:102551,Testability,log,logged,102551,"hat is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distribut",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:103902,Testability,log,log,103902,"ct a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. \[\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\frac{1}{2 \sigma^2}\]; The standard error \(\hat{\sigma}\) is then estimated by solving for \(\sigma\).; Note that the mean and standard deviation of the (discretized or continuous) distribution held in global.lmmreg.fit.normLkhdH2 will not coincide with \(\hat{h}^2\) and \(\hat{\sigma}\), since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on \(h^2\).; Testing each variant for association; Fixing a single v",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:104081,Testability,log,log,104081,"g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. \[\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\frac{1}{2 \sigma^2}\]; The standard error \(\hat{\sigma}\) is then estimated by solving for \(\sigma\).; Note that the mean and standard deviation of the (discretized or continuous) distribution held in global.lmmreg.fit.normLkhdH2 will not coincide with \(\hat{h}^2\) and \(\hat{\sigma}\), since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on \(h^2\).; Testing each variant for association; Fixing a single variant, we define:. \(v = n \times 1\) vector of genotypes, with missing genotypes imputed as the mean of called genotypes; \(X_v = \left[v | X \right] ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:105500,Testability,test,test,105500,"ormal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on \(h^2\).; Testing each variant for association; Fixing a single variant, we define:. \(v = n \times 1\) vector of genotypes, with missing genotypes imputed as the mean of called genotypes; \(X_v = \left[v | X \right] = n \times (1 + c)\) matrix concatenating \(v\) and \(X\); \(\beta_v = (\beta^0_v, \beta^1_v, \ldots, \beta^c_v) = (1 + c) \times 1\) vector of covariate coefficients. Fixing \(\delta\) at the global REML estimate \(\hat{\delta}\), we find the REML estimate \((\hat{\beta}_v, \hat{\sigma}_{g,v}^2)\) via rotation of the model. \[y \sim \mathrm{N}\left(X_v\beta_v, \sigma_{g,v}^2 (K + \hat{\delta} I)\right)\]; Note that the only new rotation to compute here is \(U^T v\).; To test the null hypothesis that the genotype coefficient \(\beta^0_v\) is zero, we consider the restricted model with parameters \(((0, \beta^1_v, \ldots, \beta^c_v), \sigma_{g,v}^2)\) within the full model with parameters \((\beta^0_v, \beta^1_v, \ldots, \beta^c_v), \sigma_{g_v}^2)\), with \(\delta\) fixed at \(\hat\delta\) in both. The latter fit is simply that of the global model, \(((0, \hat{\beta}^1, \ldots, \hat{\beta}^c), \hat{\sigma}_g^2)\). The likelihood ratio test statistic is given by. \[\chi^2 = n \, \mathrm{ln}\left(\frac{\hat{\sigma}^2_g}{\hat{\sigma}_{g,v}^2}\right)\]; and follows a chi-squared distribution with one degree of freedom. Here the ratio \(\hat{\sigma}^2_g / \hat{\sigma}_{g,v}^2\) captures the degree to which adding the variant \(v\) to the global model reduces the residual phenotypic variance.; Kinship Matrix; FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with rrm(). However, any instance of KinshipMatrix may be used, so long as sample_list contains the complete samples of the caller variant dataset in the same order.; Low-rank approximation of kinship for improved performance; lm",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:105973,Testability,test,test,105973,"of called genotypes; \(X_v = \left[v | X \right] = n \times (1 + c)\) matrix concatenating \(v\) and \(X\); \(\beta_v = (\beta^0_v, \beta^1_v, \ldots, \beta^c_v) = (1 + c) \times 1\) vector of covariate coefficients. Fixing \(\delta\) at the global REML estimate \(\hat{\delta}\), we find the REML estimate \((\hat{\beta}_v, \hat{\sigma}_{g,v}^2)\) via rotation of the model. \[y \sim \mathrm{N}\left(X_v\beta_v, \sigma_{g,v}^2 (K + \hat{\delta} I)\right)\]; Note that the only new rotation to compute here is \(U^T v\).; To test the null hypothesis that the genotype coefficient \(\beta^0_v\) is zero, we consider the restricted model with parameters \(((0, \beta^1_v, \ldots, \beta^c_v), \sigma_{g,v}^2)\) within the full model with parameters \((\beta^0_v, \beta^1_v, \ldots, \beta^c_v), \sigma_{g_v}^2)\), with \(\delta\) fixed at \(\hat\delta\) in both. The latter fit is simply that of the global model, \(((0, \hat{\beta}^1, \ldots, \hat{\beta}^c), \hat{\sigma}_g^2)\). The likelihood ratio test statistic is given by. \[\chi^2 = n \, \mathrm{ln}\left(\frac{\hat{\sigma}^2_g}{\hat{\sigma}_{g,v}^2}\right)\]; and follows a chi-squared distribution with one degree of freedom. Here the ratio \(\hat{\sigma}^2_g / \hat{\sigma}_{g,v}^2\) captures the degree to which adding the variant \(v\) to the global model reduces the residual phenotypic variance.; Kinship Matrix; FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with rrm(). However, any instance of KinshipMatrix may be used, so long as sample_list contains the complete samples of the caller variant dataset in the same order.; Low-rank approximation of kinship for improved performance; lmmreg() can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter n_eigs to use only ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:108156,Testability,test,testing,108156,"e (also known as the trace, or the sum of the eigenvalues). For example, dropped_variance_fraction=0.01 will use the minimal number of eigenvectors to account for 99% of the sample variance. Specifying both parameters will apply the more stringent (fewest eigenvectors) of the two.; Further background; For the history and mathematics of linear mixed models in genetics, including FastLMM, see Christoph Lippert’s PhD thesis. For an investigation of various approaches to defining kinship, see Comparison of Methods to Account for Relatedness in Genome-Wide Association Studies with Family-Based Data. Parameters:; kinshipMatrix (KinshipMatrix) – Kinship matrix to be used.; y (str) – Response sample annotation.; covariates (list of str) – List of covariate sample annotations.; global_root (str) – Global annotation root, a period-delimited path starting with global.; va_root (str) – Variant annotation root, a period-delimited path starting with va.; run_assoc (bool) – If true, run association testing in addition to fitting the global model.; use_ml (bool) – Use ML instead of REML throughout.; delta (float or None) – Fixed delta value to use in the global model, overrides fitting delta.; sparsity_threshold (float) – Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant us",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:108868,Testability,log,logreg,108868," of str) – List of covariate sample annotations.; global_root (str) – Global annotation root, a period-delimited path starting with global.; va_root (str) – Variant annotation root, a period-delimited path starting with va.; run_assoc (bool) – If true, run association testing in addition to fitting the global model.; use_ml (bool) – Use ML instead of REML throughout.; delta (float or None) – Fixed delta value to use in the global model, overrides fitting delta.; sparsity_threshold (float) – Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:108875,Testability,test,test,108875," of str) – List of covariate sample annotations.; global_root (str) – Global annotation root, a period-delimited path starting with global.; va_root (str) – Variant annotation root, a period-delimited path starting with va.; run_assoc (bool) – If true, run association testing in addition to fitting the global model.; use_ml (bool) – Use ML instead of REML throughout.; delta (float or None) – Fixed delta value to use in the global model, overrides fitting delta.; sparsity_threshold (float) – Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:108908,Testability,log,logreg,108908,"Global annotation root, a period-delimited path starting with global.; va_root (str) – Variant annotation root, a period-delimited path starting with va.; run_assoc (bool) – If true, run association testing in addition to fitting the global model.; use_ml (bool) – Use ML instead of REML throughout.; delta (float or None) – Fixed delta value to use in the global model, overrides fitting delta.; sparsity_threshold (float) – Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genoty",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:108986,Testability,log,logistic,108986,"Global annotation root, a period-delimited path starting with global.; va_root (str) – Variant annotation root, a period-delimited path starting with va.; run_assoc (bool) – If true, run association testing in addition to fitting the global model.; use_ml (bool) – Use ML instead of REML throughout.; delta (float or None) – Fixed delta value to use in the global model, overrides fitting delta.; sparsity_threshold (float) – Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genoty",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109113,Testability,log,logistic,109113,"on to fitting the global model.; use_ml (bool) – Use ML instead of REML throughout.; delta (float or None) – Fixed delta value to use in the global model, overrides fitting delta.; sparsity_threshold (float) – Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109138,Testability,test,test,109138,"on to fitting the global model.; use_ml (bool) – Use ML instead of REML throughout.; delta (float or None) – Fixed delta value to use in the global model, overrides fitting delta.; sparsity_threshold (float) – Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109253,Testability,log,logreg,109253,"a (float or None) – Fixed delta value to use in the global model, overrides fitting delta.; sparsity_threshold (float) – Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) a",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109349,Testability,log,logreg,109349," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109409,Testability,test,test,109409," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109493,Testability,log,logistic,109493," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109697,Testability,test,test,109697,"ing eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; He",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109729,Testability,test,test,109729,"ing eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; He",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109753,Testability,test,test,109753,"ing eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; He",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:109780,Testability,test,test,109780,"ing eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; He",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:110947,Testability,test,test,110947,"called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton i",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111033,Testability,log,logreg,111033," use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations whi",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111106,Testability,log,logreg,111106,"hrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Valu",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111187,Testability,log,logreg,111187,"lues,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until converg",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111290,Testability,log,logreg,111290,"the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111324,Testability,test,testing,111324," from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111364,Testability,log,logreg,111364,"ple above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.log",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111443,Testability,log,logreg,111443,"thrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111499,Testability,log,logreg,111499," \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) chang",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111540,Testability,test,testing,111540,"thrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111575,Testability,log,logreg,111575,"silon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attem",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111623,Testability,log,logreg,111623,"athrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearl",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111658,Testability,test,testing,111658,"d; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111717,Testability,test,tests,111717,"rm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111738,Testability,log,logistic,111738,"rm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:111908,Testability,test,test,111908,"tations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:112134,Testability,log,logreg,112134,"andard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:112284,Testability,log,logreg,112284,"va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very fl",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:112365,Testability,log,logreg,112365,"eg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:112588,Testability,test,testing,112588," Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:113053,Testability,test,testing,113053,"itting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf p",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:113167,Testability,log,logistic,113167,"eaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:113314,Testability,test,testing,113314,"ded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is d",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:113609,Testability,log,logistic,113609,"ations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant asso",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:113664,Testability,test,tests,113664,"ations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant asso",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:113927,Testability,log,logistic,113927," of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:113943,Testability,log,logistic,113943," of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:114037,Testability,log,logistf,114037," of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:114057,Testability,log,logistf,114057," of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:114164,Testability,log,logfit,114164," of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:114217,Testability,log,logistf,114217," of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:114635,Testability,test,test,114635,"f quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper fu",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:114791,Testability,test,test,114791,". Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samp",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:115061,Testability,test,testing,115061,"(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.a",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:115332,Testability,test,testing,115332,"ly. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regressio",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:115404,Testability,log,logistic,115404,"ly. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regressio",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:115441,Testability,test,tests,115441,"ly. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regressio",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:115517,Testability,test,tests,115517,"om R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subse",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:115781,Testability,log,logistic,115781,"imation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘fir",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:116331,Testability,log,logistic,116331," low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:116351,Testability,test,tests,116351," low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:116402,Testability,test,tests,116402,"istic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:116674,Testability,test,test,116674,"rther analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burd",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:116699,Testability,test,test,116699,"rther analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burd",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:116957,Testability,log,logistic,116957,"ve missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_k",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:117081,Testability,log,logistic,117081,"ge else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test usi",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:117210,Testability,test,test,117210,"ile with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missi",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:117350,Testability,log,logistic,117350,"ile with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missi",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:117493,Testability,test,test,117493,"CTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and va.weight is a numeric variant annotation:; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:117508,Testability,log,logistic,117508,"CTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and va.weight is a numeric variant annotation:; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:117522,Testability,test,test,117522,"CTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and va.weight is a numeric variant annotation:; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:117965,Testability,test,test,117965,"sion.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and va.weight is a numeric variant annotation:; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()' where va.qc.AF; is the allele frequency over those samples that have no missing phenotype or covariates. Caution; With single_key=False, variant_keys expects a variant annotati",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:118078,Testability,test,test,118078,"t annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and va.weight is a numeric variant annotation:; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()' where va.qc.AF; is the allele frequency over those samples that have no missing phenotype or covariates. Caution; With single_key=False, variant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in m",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:118093,Testability,log,logistic,118093,"t annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and va.weight is a numeric variant annotation:; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()' where va.qc.AF; is the allele frequency over those samples that have no missing phenotype or covariates. Caution; With single_key=False, variant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in m",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:118108,Testability,test,test,118108,"t annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and va.weight is a numeric variant annotation:; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()' where va.qc.AF; is the allele frequency over those samples that have no missing phenotype or covariates. Caution; With single_key=False, variant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in m",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:118525,Testability,test,test,118525,"e maximum genotype per gene. Here va.genes is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see Extended example in linreg_burden() for a deeper dive in the context of linear regression):; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here va.gene is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and va.weight is a numeric variant annotation:; >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()' where va.qc.AF; is the allele frequency over those samples that have no missing phenotype or covariates. Caution; With single_key=False, variant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key’s group. With single_key=True, variant_keys expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. Notes; This method modifies logreg() by replacing the genotype covariate per variant and sam",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:119469,Testability,log,logreg,119469,"ght * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()' where va.qc.AF; is the allele frequency over those samples that have no missing phenotype or covariates. Caution; With single_key=False, variant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key’s group. With single_key=True, variant_keys expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. Notes; This method modifies logreg() by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through;",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:119887,Testability,test,test,119887,"ariant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key’s group. With single_key=True, variant_keys expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. Notes; This method modifies logreg() by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:119919,Testability,test,test,119919,"ariant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key’s group. With single_key=True, variant_keys expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. Notes; This method modifies logreg() by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:119943,Testability,test,test,119943,"ariant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key’s group. With single_key=True, variant_keys expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. Notes; This method modifies logreg() by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:119970,Testability,test,test,119970,"ariant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key’s group. With single_key=True, variant_keys expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. Notes; This method modifies logreg() by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:119992,Testability,test,test,119992,"ariant_keys expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key’s group. With single_key=True, variant_keys expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. Notes; This method modifies logreg() by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:120677,Testability,log,logistic,120677," key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (st",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:120749,Testability,test,test,120749," key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (st",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:120770,Testability,test,tests,120770," true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logist",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:120789,Testability,log,logreg,120789," true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logist",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:120951,Testability,log,logistic,120951,", the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:121105,Testability,log,logreg,121105," defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and one or more columns per sample.; Examples; Consider a VariantDataset vds with 2 variants and 3 samples:; V",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:121129,Testability,test,test,121129," defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and one or more columns per sample.; Examples; Consider a VariantDataset vds with 2 variants and 3 samples:; V",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:121138,Testability,log,logreg,121138," defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and one or more columns per sample.; Examples; Consider a VariantDataset vds with 2 variants and 3 samples:; V",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:121181,Testability,log,logistic,121181,"iants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and one or more columns per sample.; Examples; Consider a VariantDataset vds with 2 variants and 3 samples:; Variant FORMAT A B C; 1:1:A:T GT:GQ 0/1:99 ./. 0/0:99; 1:2:G:C GT:G",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:121623,Testability,test,test,121623,"each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and one or more columns per sample.; Examples; Consider a VariantDataset vds with 2 variants and 3 samples:; Variant FORMAT A B C; 1:1:A:T GT:GQ 0/1:99 ./. 0/0:99; 1:2:G:C GT:GQ 0/1:89 0/1:99 1/1:93. Then; >>> kt = vds.make_table('v = v', ['gt = g.gt', 'gq = g.gq']). returns a KeyTable with schema; v: Variant; A.gt: Int; A.gq: Int; B.gt: Int; B.gq: Int; C.gt: Int; C.gq: Int. and values; v A.gt A.gq B.gt B.gq C.gt C.gq; 1:1:A:T 1 99 NA NA 0 99; 1:2:G:C 1 89 1 99 2 93. The above table can be generated and exported as a TSV using KeyTable export().; Notes; Per sample field names in the result are formed by; concatenating the sample ID wit",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:121648,Testability,test,test,121648,"each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and one or more columns per sample.; Examples; Consider a VariantDataset vds with 2 variants and 3 samples:; Variant FORMAT A B C; 1:1:A:T GT:GQ 0/1:99 ./. 0/0:99; 1:2:G:C GT:GQ 0/1:89 0/1:99 1/1:93. Then; >>> kt = vds.make_table('v = v', ['gt = g.gt', 'gq = g.gq']). returns a KeyTable with schema; v: Variant; A.gt: Int; A.gq: Int; B.gt: Int; B.gq: Int; C.gt: Int; C.gq: Int. and values; v A.gt A.gq B.gt B.gq C.gt C.gq; 1:1:A:T 1 99 NA NA 0 99; 1:2:G:C 1 89 1 99 2 93. The above table can be generated and exported as a TSV using KeyTable export().; Notes; Per sample field names in the result are formed by; concatenating the sample ID wit",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:121806,Testability,log,logistic,121806,"ed by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and one or more columns per sample.; Examples; Consider a VariantDataset vds with 2 variants and 3 samples:; Variant FORMAT A B C; 1:1:A:T GT:GQ 0/1:99 ./. 0/0:99; 1:2:G:C GT:GQ 0/1:89 0/1:99 1/1:93. Then; >>> kt = vds.make_table('v = v', ['gt = g.gt', 'gq = g.gq']). returns a KeyTable with schema; v: Variant; A.gt: Int; A.gq: Int; B.gt: Int; B.gq: Int; C.gt: Int; C.gq: Int. and values; v A.gt A.gq B.gt B.gq C.gt C.gq; 1:1:A:T 1 99 NA NA 0 99; 1:2:G:C 1 89 1 99 2 93. The above table can be generated and exported as a TSV using KeyTable export().; Notes; Per sample field names in the result are formed by; concatenating the sample ID with the genotype_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:161655,Testability,test,test,161655,"fo.FS < 200.0 || va.info.ReadPosRankSum < 20.0)) va.filters.add(""HardFilter"") else va.filters']). If we now export this VDS as VCF, it would produce the following header (for these new fields):; ##INFO=<ID=AC_HC,Number=.,Type=String,Description="""". This header doesn’t contain all information that should be present in an optimal VCF header:; 1) There is no FILTER entry for HardFilter; 2) Since AC_HC has one entry per non-reference allele, its Number should be A; 3) AC_HC should have a Description; We can fix this by setting the attributes of these fields:; >>> annotated_vds = (annotated_vds; ... .set_va_attributes(; ... 'va.info.AC_HC',; ... {'Description': 'Allele count for high quality genotypes (DP >= 10, GQ >= 20)',; ... 'Number': 'A'}); ... .set_va_attributes(; ... 'va.filters',; ... {'HardFilter': 'This site fails GATK suggested hard filters.'})). Exporting the VDS with the attributes now prints the following header lines:; ##INFO=<ID=test,Number=A,Type=String,Description=""Allele count for high quality genotypes (DP >= 10, GQ >= 20)""; ##FILTER=<ID=HardFilter,Description=""This site fails GATK suggested hard filters."">. Parameters:; ann_path (str) – Path to variant annotation beginning with va.; attributes (dict) – A str-str dict containing the attributes to set. Returns:Annotated dataset with the attribute added to the variant annotation. Return type:VariantDataset. split_multi(propagate_gq=False, keep_star_alleles=False, max_shift=100)[source]¶; Split multiallelic variants. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds.split_multi().write('output/split.vds'). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:167975,Testability,test,test,167975,"ic variants.; snps (int) - Number of SNP alternate alleles.; mnps (int) - Number of MNP alternate alleles.; insertions (int) - Number of insertion alternate alleles.; deletions (int) - Number of deletions alternate alleles.; complex (int) - Number of complex alternate alleles.; star (int) - Number of star (upstream deletion) alternate alleles.; max_alleles (int) - The highest number of alleles at any variant. Returns:Object containing summary information. Return type:Summary. tdt(pedigree, root='va.tdt')[source]¶; Find transmitted and untransmitted variants; count per variant and; nuclear family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute TDT association results:; >>> pedigree = Pedigree.read('data/trios.fam'); >>> (vds.tdt(pedigree); ... .export_variants(""output/tdt_results.tsv"", ""Variant = v, va.tdt.*"")). Notes; The transmission disequilibrium test tracks the number of times the alternate allele is transmitted (t) or not transmitted (u) from a heterozgyous parent to an affected child under the null that the rate of such transmissions is 0.5. For variants where transmission is guaranteed (i.e., the Y chromosome, mitochondria, and paternal chromosome X variants outside of the PAR), the test cannot be used.; The TDT statistic is given by. \[(t-u)^2 \over (t+u)\]; and follows a 1 degree of freedom chi-squared distribution under the null hypothesis.; The number of transmissions and untransmissions for each possible set of genotypes is determined from the table below. The copy state of a locus with respect to a trio is defined as follows, where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X and child is male; Auto – otherwise (in autosome or PAR, or child is female). Kid; Dad; Mom; Copy State; T; U. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar;",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:168322,Testability,test,test,168322,"x (int) - Number of complex alternate alleles.; star (int) - Number of star (upstream deletion) alternate alleles.; max_alleles (int) - The highest number of alleles at any variant. Returns:Object containing summary information. Return type:Summary. tdt(pedigree, root='va.tdt')[source]¶; Find transmitted and untransmitted variants; count per variant and; nuclear family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute TDT association results:; >>> pedigree = Pedigree.read('data/trios.fam'); >>> (vds.tdt(pedigree); ... .export_variants(""output/tdt_results.tsv"", ""Variant = v, va.tdt.*"")). Notes; The transmission disequilibrium test tracks the number of times the alternate allele is transmitted (t) or not transmitted (u) from a heterozgyous parent to an affected child under the null that the rate of such transmissions is 0.5. For variants where transmission is guaranteed (i.e., the Y chromosome, mitochondria, and paternal chromosome X variants outside of the PAR), the test cannot be used.; The TDT statistic is given by. \[(t-u)^2 \over (t+u)\]; and follows a 1 degree of freedom chi-squared distribution under the null hypothesis.; The number of transmissions and untransmissions for each possible set of genotypes is determined from the table below. The copy state of a locus with respect to a trio is defined as follows, where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X and child is male; Auto – otherwise (in autosome or PAR, or child is female). Kid; Dad; Mom; Copy State; T; U. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; He",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:95600,Usability,simpl,simplest,95600,"aGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.;",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:98375,Usability,simpl,simply,98375,"er. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X = n \times c\) matrix of sample covariates and intercept column of ones; \(K = n \times n\) kinship matrix; \(I = n \times n\) identity matrix; \(\beta = c \times 1\) vector of covariate coefficients; \(\sigma_g^2 =\) coefficient of genetic variance component \(K\); \(\sigma_e^2 =\) coefficient of environmental variance component \(I\); \(\delta = \frac{\sigma_e^2}{\sigma_g^2} =\) ratio of environmental and genetic variance component coefficients; \(h^2 = \frac{\sigma_g^2}{\sigma_g^2 + \sigma_e^2} = \frac{1}{1 + \delta} =\) genetic proportion of residual phenotypic variance. Under a linear mixed model, \(y\) is sampled from the \(n\)-dimensional multivariate normal distribution with mean \(X \beta\) and variance components that are scalar multiples of \(K\) and \(I\):. \[y \sim \mathrm{N}\left(X\beta, \sigma_g^2 K + \s",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:105852,Usability,simpl,simply,105852,"\(v = n \times 1\) vector of genotypes, with missing genotypes imputed as the mean of called genotypes; \(X_v = \left[v | X \right] = n \times (1 + c)\) matrix concatenating \(v\) and \(X\); \(\beta_v = (\beta^0_v, \beta^1_v, \ldots, \beta^c_v) = (1 + c) \times 1\) vector of covariate coefficients. Fixing \(\delta\) at the global REML estimate \(\hat{\delta}\), we find the REML estimate \((\hat{\beta}_v, \hat{\sigma}_{g,v}^2)\) via rotation of the model. \[y \sim \mathrm{N}\left(X_v\beta_v, \sigma_{g,v}^2 (K + \hat{\delta} I)\right)\]; Note that the only new rotation to compute here is \(U^T v\).; To test the null hypothesis that the genotype coefficient \(\beta^0_v\) is zero, we consider the restricted model with parameters \(((0, \beta^1_v, \ldots, \beta^c_v), \sigma_{g,v}^2)\) within the full model with parameters \((\beta^0_v, \beta^1_v, \ldots, \beta^c_v), \sigma_{g_v}^2)\), with \(\delta\) fixed at \(\hat\delta\) in both. The latter fit is simply that of the global model, \(((0, \hat{\beta}^1, \ldots, \hat{\beta}^c), \hat{\sigma}_g^2)\). The likelihood ratio test statistic is given by. \[\chi^2 = n \, \mathrm{ln}\left(\frac{\hat{\sigma}^2_g}{\hat{\sigma}_{g,v}^2}\right)\]; and follows a chi-squared distribution with one degree of freedom. Here the ratio \(\hat{\sigma}^2_g / \hat{\sigma}_{g,v}^2\) captures the degree to which adding the variant \(v\) to the global model reduces the residual phenotypic variance.; Kinship Matrix; FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with rrm(). However, any instance of KinshipMatrix may be used, so long as sample_list contains the complete samples of the caller variant dataset in the same order.; Low-rank approximation of kinship for improved performance; lmmreg() can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used.",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:128432,Usability,simpl,simply,128432,"d with respect to reference; GRCh37:. X: 60001 - 2699520, 154931044 - 155260560; Y: 10001 - 2649520, 59034050 - 59363566. Parameters:pedigree (Pedigree) – Sample pedigree. Returns:Four tables with Mendel error statistics. Return type:(KeyTable, KeyTable, KeyTable, KeyTable). min_rep(max_shift=100)[source]¶; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; Examples; 1. Simple trimming of a multi-allelic site, no change in variant position; 1:10000:TAA:TAA,AA => 1:10000:TA:T,A; 2. Trimming of a bi-allelic site leading to a change in position; 1:10000:AATAA,AAGAA => 1:10002:T:G. Parameters:max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. Return type:VariantDataset. naive_coalesce(max_partitions)[source]¶; Naively descrease the number of partitions. Warning; naive_coalesce() simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike repartition(), so it can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions. Parameters:max_partitions (int) – Desired number of partitions. If the current number of partitions is less than max_partitions, do nothing. Returns:Variant dataset with the number of partitions equal to at most max_partitions. Return type:VariantDataset. num_partitions()[source]¶; Number of partitions.; Notes; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statisti",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:131231,Usability,simpl,simply,131231,"his is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of their first k principal; component coordinates. As such, the efficacy of this method rests on two; assumptions:. an individual’s first k principal component coordinates fully; describe their allele-frequency-relevant ancestry, and; the relationship between ancestry (as described by principal; component coordinates) and population allele frequency is linear. The estimators for kinship, and identity-by-descent zero, one, and two; follow. Let:. \(S_{ij}\",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:140294,Usability,simpl,simply,140294,"umns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is computed as half the mean of the non-missing entries of column \(j\). Entries of \(M\) are then mean-centered and variance-normalized as. \[M_{ij} = \frac{C_{ij}-2p_j}{\sqrt{2p_j(1-p_j)m}},\]; with \(M_{ij} = 0\) for \(C_{ij}\) missing (i.e. mean genotype imputation). This scaling normalizes genotype variances to a common value \(1/m\) for variants in Hardy-Weinberg equilibrium and is further motivated in the paper cited above. (The resulting amplification of signal from the low end of the allele frequency spectrum will also introduce noise for rare variants; common practice is to filter out variants with minor allele frequency below some cutoff.) The factor \(1/m\) gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply \(MM^T\).; PCA then computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2, \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\). Typically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:141604,Usability,simpl,simply,141604,"ically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the varia",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:154547,Usability,simpl,simply,154547,". rrm(force_block=False, force_gramian=False)[source]¶; Computes the Realized Relationship Matrix (RRM).; Examples; >>> kinship_matrix = vds.rrm(). Notes; The Realized Relationship Matrix is defined as follows. Consider the \(n \times m\) matrix \(C\) of raw genotypes, with rows indexed by \(n\) samples and; columns indexed by the \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which; can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is computed as half the mean of the non-missing entries of column; \(j\). Entries of \(M\) are then mean-centered and variance-normalized as. \[M_{ij} = \frac{C_{ij}-2p_j}{\sqrt{\frac{m}{n} \sum_{k=1}^n (C_{ij}-2p_j)^2}},\]; with \(M_{ij} = 0\) for \(C_{ij}\) missing (i.e. mean genotype imputation). This scaling normalizes each variant column to have empirical variance \(1/m\), which gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the \(n \times n\) sample correlation or realized relationship matrix (RRM) \(K\) as simply. \[K = MM^T\]; Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:159950,Usability,simpl,simplicity,159950,"es for a variant annotation.; Attributes are key/value pairs that can be attached to a variant annotation field.; The following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. INFO fields attributes (attached to (va.info.*)):; ‘Number’: The arity of the field. Can take values; 0 (Boolean flag),; 1 (single value),; R (one value per allele, including the reference),; A (one value per non-reference allele),; G (one value per genotype), and; . (any number of values); When importing: The value in read from the VCF INFO field definition; When exporting: The default value is 0 for Boolean, . for Arrays and 1 for all other types. ‘Description’ (default is ‘’). FILTER entries in the VCF header are generated based on the attributes; of va.filters. Each key/value pair in the attributes will generate; a FILTER entry in the VCF with ID = key and Description = value. Examples; Consider the following command which adds a filter and an annotation to the VDS (we’re assuming a split VDS for simplicity):. an INFO field AC_HC, which stores the allele count of high; confidence genotypes (DP >= 10, GQ >= 20) for each non-reference allele,; a filter HardFilter that filters all sites with the GATK suggested hard filters:; For SNVs: QD < 2.0 || FS < 60 || MQ < 40 || MQRankSum < -12.5 || ReadPosRankSum < -8.0; For Indels (and other complex): QD < 2.0 || FS < 200.0 || ReadPosRankSum < 20.0. >>> annotated_vds = vds.annotate_variants_expr([; ... 'va.info.AC_HC = gs.filter(g => g.dp >= 10 && g.gq >= 20).callStats(g => v).AC[1:]',; ... 'va.filters = if((v.altAllele.isSNP && (va.info.QD < 2.0 || va.info.FS < 60 || va.info.MQ < 40 || ' +; ... 'va.info.MQRankSum < -12.5 || va.info.ReadPosRankSum < -8.0)) || ' +; ... '(va.info.QD < 2.0 || va.info.FS < 200.0 || va.info.ReadPosRankSum < 20.0)) va.filters.add(""HardFilter"") else va.filters']). If we now export this VDS as VCF, it would produce the following header (for these new fields):;",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:163847,Usability,simpl,simply,163847,"to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype.; By default, GQ is recomputed from PL. If propagate_gq=True; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged.; Here is a second example for a het non-ref; A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as; A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. VCF Info Fields; Hail does not split annotations in the info field. This means; that if a multiallelic site with info.AC value [10, 2] is; split, each split site will contain the same array [10,; 2]. The provided allele index annotation va.aIndex can be used; to select the value corresponding to the split allele’s; position:; >>> vds_result = (vds.split_multi(); ... .filter_variants_expr('va.info.AC[va.aIndex - 1] < 10', keep = False)). VCFs split by Hail and exported to new VCFs may be; incompatible with other tools, if action is not taken; first. Since the “Number” of the arrays in split multiallelic; sites no longer matches the structure on import (“A” for 1 per; allele, for example), Hail will export these fields with; number “.”.; If the desired output is one value",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/hail.VariantDataset.html:170926,Usability,simpl,simply,170926,"digree (Pedigree) – Sample pedigree.; root – Variant annotation root to store TDT result. Returns:Variant dataset with TDT association results added to variant annotations. Return type:VariantDataset. union()[source]¶; Take the union of datasets vertically (include all variants).; Examples; Union two datasets:; >>> vds_union = vds_autosomal.union(vds_chromX). Given a list of datasets, union them all:; >>> all_vds = [vds_autosomal, vds_chromX, vds_chromY]. The following three syntaxes are equivalent:; >>> vds_union1 = vds_autosomal.union(vds_chromX, vds_chromY); >>> vds_union2 = all_vds[0].union(*all_vds[1:]); >>> vds_union3 = VariantDataset.union(*all_vds). Notes. In order to combine two datasets, these requirements must be met:. the samples must match; the variant annotation schemas must match (field order within structs matters).; the cell (genotype) schemas must match (field order within structs matters). The column annotations in the resulting dataset are simply the column annotations; from the first dataset; the column annotation schemas do not need to match.; This method can trigger a shuffle, if partitions from two datasets overlap. Parameters:vds_type (tuple of VariantDataset) – Datasets to combine. Returns:Dataset with variants from all datasets. Return type:VariantDataset. unpersist()[source]¶; Unpersists this VDS from memory/disk.; Notes; This function will have no effect on a VDS that was not previously persisted.; There’s nothing stopping you from continuing to use a VDS that has been unpersisted, but doing so will result in; all previous steps taken to compute the VDS being performed again since the VDS must be recomputed. Only unpersist; a VDS when you are done with it. variant_qc(root='va.qc')[source]¶; Compute common variant statistics (quality control metrics). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds_result = vds.variant_qc(). Annotations; variant_qc() computes 18 variant statistics f",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
https://hail.is/docs/0.1/index.html:386,Testability,test,tests,386,"﻿. . Contents — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Contents. View page source. Contents¶. Getting Started; Running Hail locally; Building Hail from source; BLAS and LAPACK; Running the tests. Overview; Variant Dataset (VDS); Expressions. Tutorials; Hail Overview; Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Language Constructs; Operators; Types; Functions. Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Database Query; Documentation; Important Notes; Suggest additions or edits. Other Resources; Hadoop Glob Patterns; SQL. Indices and tables¶. Index; Search Page. Next . © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/index.html
https://hail.is/docs/0.1/language_constructs.html:571,Integrability,depend,depending,571,"﻿. . Language Constructs — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Language Constructs. View page source. Language Constructs¶. va.foo = 5 + va.bar. Annotation expression. Bind variable va.foo to the result of evaluating 5 + va.bar. if (p) a else b. The value of the conditional is the value of a or b depending on p. If p is missing, the value of the conditional is missing.; if (5 % 2 == 0) 5 else 7; 7. if (5 > NA: Int) 5 else 7; NA: Int. let v1 = e1 and v2 = e2 and … and vn = en in b. Bind variables v1 through vn to result of evaluating the ei. The value of the let is the value of b. v1 is visible in e2 through en, etc.; let v1 = 5 and v2 = 7 and v3 = 2 in v1 * v2 * v3; 70. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/language_constructs.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/language_constructs.html
https://hail.is/docs/0.1/language_constructs.html:446,Modifiability,variab,variable,446,"﻿. . Language Constructs — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Language Constructs. View page source. Language Constructs¶. va.foo = 5 + va.bar. Annotation expression. Bind variable va.foo to the result of evaluating 5 + va.bar. if (p) a else b. The value of the conditional is the value of a or b depending on p. If p is missing, the value of the conditional is missing.; if (5 % 2 == 0) 5 else 7; 7. if (5 > NA: Int) 5 else 7; NA: Int. let v1 = e1 and v2 = e2 and … and vn = en in b. Bind variables v1 through vn to result of evaluating the ei. The value of the let is the value of b. v1 is visible in e2 through en, etc.; let v1 = 5 and v2 = 7 and v3 = 2 in v1 * v2 * v3; 70. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/language_constructs.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/language_constructs.html
https://hail.is/docs/0.1/language_constructs.html:764,Modifiability,variab,variables,764,"﻿. . Language Constructs — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Language Constructs. View page source. Language Constructs¶. va.foo = 5 + va.bar. Annotation expression. Bind variable va.foo to the result of evaluating 5 + va.bar. if (p) a else b. The value of the conditional is the value of a or b depending on p. If p is missing, the value of the conditional is missing.; if (5 % 2 == 0) 5 else 7; 7. if (5 > NA: Int) 5 else 7; NA: Int. let v1 = e1 and v2 = e2 and … and vn = en in b. Bind variables v1 through vn to result of evaluating the ei. The value of the let is the value of b. v1 is visible in e2 through en, etc.; let v1 = 5 and v2 = 7 and v3 = 2 in v1 * v2 * v3; 70. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/language_constructs.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/language_constructs.html
https://hail.is/docs/0.1/operators.html:2434,Modifiability,variab,variable,2434," operand by the right (modulus). 7 % 2; 1. 10 % 4; 2. // – Floor division - division that results into whole number adjusted to the left in the number line. 7 // 2; 3. -7 // 2; -4. Array[Numeric]¶; If one of the two operands is a scalar, the operation will be applied to each element of the Array. If both operands are Arrays, the operation will be applied positionally. This will fail if the array dimensions do not match. + – Add two operands. [1, 2, 3] + [1, 1, 1]; [2, 3, 4]. [2, 0, 1] + 5; [7, 5, 6]. - – Subtract right operand from the left. [1, 2, 3] - [1, 1, 1]; [0, 1, 2]. [2, 0, 1] - 5; [-3, -5, -4]. 3 - [2, 4, 5]; [1, -1, -2]. * – Multiply two operands. [1, 2, 3] * [1, 1, 1]; [1, 2, 3]. [2, 0, 1] * 5; [10, 0, 5]. / – Divide left operand by the right one. Always results in a Double. [1, 2, 3] / [1, 4, 9]; [1.0, 0.5, 0.333]. [2, 0, 1] / 5; [0.4, 0.0, 0.2]. 5 / [2, 4, 1]; [2.5, 1.25, 5.0]. Comparison¶. == – True if the left operand is equal to the right operand. [1, 2, 3] == [1, 2, 3]; true. != – True if the left operand is not equal to the right operand. [1, 2, 3] != [4, 5, 6]; true. < – True if the left operand is less than the right operand. 5 < 3; False. <= – True if the left operand is less than or equal to the right operand. 3 <= 5; True. > – True if the left operand is greater than the right operand. 7 > 2; True. >= – True if the left operand is greater than or equal to the right operand. 3 >= 9; False. ~ – True if a regular expression pattern matches the target string. ""1KG"" ~ ""Cohort_1KG_NA12878""; True. Logical¶. && – True if both the left and right operands are true. (5 >= 3) && (2 < 10); True. || – True if at least one operand is true. (5 <= 3) || (2 < 10); True. ! – Negates a boolean variable. Returns false if the variable is true and true if the variable is false. !(5 >= 3); False. String¶. + – Concatenate two strings together. ""a"" + ""b""; ""ab"". Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/operators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/operators.html
https://hail.is/docs/0.1/operators.html:2465,Modifiability,variab,variable,2465," operand by the right (modulus). 7 % 2; 1. 10 % 4; 2. // – Floor division - division that results into whole number adjusted to the left in the number line. 7 // 2; 3. -7 // 2; -4. Array[Numeric]¶; If one of the two operands is a scalar, the operation will be applied to each element of the Array. If both operands are Arrays, the operation will be applied positionally. This will fail if the array dimensions do not match. + – Add two operands. [1, 2, 3] + [1, 1, 1]; [2, 3, 4]. [2, 0, 1] + 5; [7, 5, 6]. - – Subtract right operand from the left. [1, 2, 3] - [1, 1, 1]; [0, 1, 2]. [2, 0, 1] - 5; [-3, -5, -4]. 3 - [2, 4, 5]; [1, -1, -2]. * – Multiply two operands. [1, 2, 3] * [1, 1, 1]; [1, 2, 3]. [2, 0, 1] * 5; [10, 0, 5]. / – Divide left operand by the right one. Always results in a Double. [1, 2, 3] / [1, 4, 9]; [1.0, 0.5, 0.333]. [2, 0, 1] / 5; [0.4, 0.0, 0.2]. 5 / [2, 4, 1]; [2.5, 1.25, 5.0]. Comparison¶. == – True if the left operand is equal to the right operand. [1, 2, 3] == [1, 2, 3]; true. != – True if the left operand is not equal to the right operand. [1, 2, 3] != [4, 5, 6]; true. < – True if the left operand is less than the right operand. 5 < 3; False. <= – True if the left operand is less than or equal to the right operand. 3 <= 5; True. > – True if the left operand is greater than the right operand. 7 > 2; True. >= – True if the left operand is greater than or equal to the right operand. 3 >= 9; False. ~ – True if a regular expression pattern matches the target string. ""1KG"" ~ ""Cohort_1KG_NA12878""; True. Logical¶. && – True if both the left and right operands are true. (5 >= 3) && (2 < 10); True. || – True if at least one operand is true. (5 <= 3) || (2 < 10); True. ! – Negates a boolean variable. Returns false if the variable is true and true if the variable is false. !(5 >= 3); False. String¶. + – Concatenate two strings together. ""a"" + ""b""; ""ab"". Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/operators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/operators.html
https://hail.is/docs/0.1/operators.html:2498,Modifiability,variab,variable,2498," operand by the right (modulus). 7 % 2; 1. 10 % 4; 2. // – Floor division - division that results into whole number adjusted to the left in the number line. 7 // 2; 3. -7 // 2; -4. Array[Numeric]¶; If one of the two operands is a scalar, the operation will be applied to each element of the Array. If both operands are Arrays, the operation will be applied positionally. This will fail if the array dimensions do not match. + – Add two operands. [1, 2, 3] + [1, 1, 1]; [2, 3, 4]. [2, 0, 1] + 5; [7, 5, 6]. - – Subtract right operand from the left. [1, 2, 3] - [1, 1, 1]; [0, 1, 2]. [2, 0, 1] - 5; [-3, -5, -4]. 3 - [2, 4, 5]; [1, -1, -2]. * – Multiply two operands. [1, 2, 3] * [1, 1, 1]; [1, 2, 3]. [2, 0, 1] * 5; [10, 0, 5]. / – Divide left operand by the right one. Always results in a Double. [1, 2, 3] / [1, 4, 9]; [1.0, 0.5, 0.333]. [2, 0, 1] / 5; [0.4, 0.0, 0.2]. 5 / [2, 4, 1]; [2.5, 1.25, 5.0]. Comparison¶. == – True if the left operand is equal to the right operand. [1, 2, 3] == [1, 2, 3]; true. != – True if the left operand is not equal to the right operand. [1, 2, 3] != [4, 5, 6]; true. < – True if the left operand is less than the right operand. 5 < 3; False. <= – True if the left operand is less than or equal to the right operand. 3 <= 5; True. > – True if the left operand is greater than the right operand. 7 > 2; True. >= – True if the left operand is greater than or equal to the right operand. 3 >= 9; False. ~ – True if a regular expression pattern matches the target string. ""1KG"" ~ ""Cohort_1KG_NA12878""; True. Logical¶. && – True if both the left and right operands are true. (5 >= 3) && (2 < 10); True. || – True if at least one operand is true. (5 <= 3) || (2 < 10); True. ! – Negates a boolean variable. Returns false if the variable is true and true if the variable is false. !(5 >= 3); False. String¶. + – Concatenate two strings together. ""a"" + ""b""; ""ab"". Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/operators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/operators.html
https://hail.is/docs/0.1/overview.html:1751,Energy Efficiency,efficient,efficient,1751," importing genotype data from a standard file format such as VCF, PLINK Binary files, GEN, or BGEN files into Hail’s Variant Dataset format.; Next, samples and variants are annotated with additional meta-information such as phenotype for samples and functional consequence for variants.; Samples, variants, and genotypes are filtered from the dataset based on expressions constructed using Hail’s Domain-Specific Language.; Once the dataset has been cleaned, various analytic methods such as PCA and logistic regression are used to find genetic associations.; Lastly, data is exported to a variety of file formats. Variant Dataset (VDS)¶. Hail represents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of co",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:3302,Integrability,depend,dependent,3302,",; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by va",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:2378,Modifiability,variab,variable,2378,"sents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to exp",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:2456,Modifiability,variab,variable,2456,"; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines t",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:2566,Modifiability,variab,variables,2566,"cts each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:4449,Modifiability,variab,variables,4449,"le, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s a",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:4482,Modifiability,variab,variable,4482,"le, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s a",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:4555,Modifiability,variab,variable,4555,"e PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the ",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:5041,Modifiability,variab,variable,5041," calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean samp",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:5149,Modifiability,variab,variable,5149,"e header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample ann",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:5226,Modifiability,variab,variable,5226,"pe = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample annotation based on the length of the sample ID. sa.idTooLong = s.length > 10. Add a new variant",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:5842,Modifiability,variab,variables,5842," VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample annotation based on the length of the sample ID. sa.idTooLong = s.length > 10. Add a new variant annotation that is a String representing the chromosome and start position. va.altName = v.contig + "":"" + v.start. Add a new variant annotation that splits a comma-separated string with gene names and keeps the first element of the resulting array. va.geneName = va.geneNames.split("","")[0]. Add a new variant annotation that is the log of an existing annotation. va.logIntensity = log(va.intensity). Add a new global annotation computed from existing global annotations. global.callRate = global.nCalled / global.nGenotypes. Variant Annotation Comput",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:6045,Modifiability,variab,variable,6045,"ample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample annotation based on the length of the sample ID. sa.idTooLong = s.length > 10. Add a new variant annotation that is a String representing the chromosome and start position. va.altName = v.contig + "":"" + v.start. Add a new variant annotation that splits a comma-separated string with gene names and keeps the first element of the resulting array. va.geneName = va.geneNames.split("","")[0]. Add a new variant annotation that is the log of an existing annotation. va.logIntensity = log(va.intensity). Add a new global annotation computed from existing global annotations. global.callRate = global.nCalled / global.nGenotypes. Variant Annotation Computed from a Genotype Aggregable (gs)¶; In the context of creating new variant annotations, a genotype aggregable (gs) represents a row of genotypes in the variant-sample matrix.; The result of evaluating the gen",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:8176,Modifiability,variab,variable,8176,"oolean lambda expression as input (g => Boolean Expression). Transform the genotype aggregable to an aggregable of GQ scores using the map function and then calculate summary statistics on the GQ scores with the stats function. va.gqStats = gs.map(g => g.gq).stats(). Filter the genotype aggregable based on case status (sa.pheno.isCase) and genotype call (g.isHet and g.isHomVar) and then count the number of elements remaining. va.caseMAC = gs.filter(g => sa.pheno.isCase && g.isHet).count() +; 2 * gs.filter(g => sa.pheno.isCase && g.isHomVar).count(). Define a filtered genotype aggregable from cases (sa.pheno.isCase) using the let..in syntax and then use the case-only genotype aggregable to calculate the fraction of genotypes called. va.caseCallRate = let caseGS = gs.filter(g => sa.pheno.isCase) in caseGS.fraction(g => g.isCalled). Count the number of genotypes remaining after filtering the genotype aggregable to genotypes with a variant allele (g.isCalledNonRef) and then create a boolean variable by comparing the result to 1. va.isSingleton = gs.filter(g => g.isCalledNonRef).count() == 1. Sample Annotation Computed from a Genotype Aggregable (gs)¶; In the context of creating new sample annotations, a genotype aggregable (gs) represents a column of genotypes in the variant-sample matrix.; The result of evaluating the genotype aggregable expression per column is added to the corresponding sample annotation.; The map function takes a lambda expression as input (g => ...). The filter function takes a boolean lambda expression as input (g => Boolean Expression). Filter the genotype aggregable to only genotypes that have a heterozygote call (g.isHet) and count the number of elements remaining. sa.numHet = gs.filter(g => g.isHet).count(). Count the number of elements remaining after filtering the genotype aggregable to only genotypes where the corresponding variant annotation is True for isSingleton and the genotype call has a variant allele (g.isCalledNonRef). sa.nSingleto",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:1600,Security,access,access,1600,"hon API; Annotation Database; Other Resources. Hail. Docs »; Overview. View page source. Overview¶; A typical workflow in Hail begins with importing genotype data from a standard file format such as VCF, PLINK Binary files, GEN, or BGEN files into Hail’s Variant Dataset format.; Next, samples and variants are annotated with additional meta-information such as phenotype for samples and functional consequence for variants.; Samples, variants, and genotypes are filtered from the dataset based on expressions constructed using Hail’s Domain-Specific Language.; Once the dataset has been cleaned, various analytic methods such as PCA and logistic regression are used to find genetic associations.; Lastly, data is exported to a variety of file formats. Variant Dataset (VDS)¶. Hail represents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside ",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:2394,Security,access,accessed,2394,"sents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to exp",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:2472,Security,access,accessed,2472,"; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines t",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:2520,Security,access,access,2520,"cts each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:3271,Security,access,accessible,3271,",; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by va",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:1238,Testability,log,logistic,1238,"riables to Methods; Filtering; Add New Annotations; Computed From Existing Annotations; Variant Annotation Computed from a Genotype Aggregable (gs); Sample Annotation Computed from a Genotype Aggregable (gs); Global Annotation Computed from a Sample Aggregable (samples); Global Annotation Computed from a Variant Aggregable (variants). Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Overview. View page source. Overview¶; A typical workflow in Hail begins with importing genotype data from a standard file format such as VCF, PLINK Binary files, GEN, or BGEN files into Hail’s Variant Dataset format.; Next, samples and variants are annotated with additional meta-information such as phenotype for samples and functional consequence for variants.; Samples, variants, and genotypes are filtered from the dataset based on expressions constructed using Hail’s Domain-Specific Language.; Once the dataset has been cleaned, various analytic methods such as PCA and logistic regression are used to find genetic associations.; Lastly, data is exported to a variety of file formats. Variant Dataset (VDS)¶. Hail represents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:4371,Testability,log,logistic,4371,"le, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s a",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:6567,Testability,log,log,6567,"path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample annotation based on the length of the sample ID. sa.idTooLong = s.length > 10. Add a new variant annotation that is a String representing the chromosome and start position. va.altName = v.contig + "":"" + v.start. Add a new variant annotation that splits a comma-separated string with gene names and keeps the first element of the resulting array. va.geneName = va.geneNames.split("","")[0]. Add a new variant annotation that is the log of an existing annotation. va.logIntensity = log(va.intensity). Add a new global annotation computed from existing global annotations. global.callRate = global.nCalled / global.nGenotypes. Variant Annotation Computed from a Genotype Aggregable (gs)¶; In the context of creating new variant annotations, a genotype aggregable (gs) represents a row of genotypes in the variant-sample matrix.; The result of evaluating the genotype aggregable expression per row is added to the corresponding variant annotation.; The map function takes a lambda expression as input (g => ...). The filter function takes a boolean lambda expression as input (g => Boolean Expression). Transform the genotype aggregable to an aggregable of GQ scores using the map function and then calculate summary statistics on the GQ scores with the stats function. va.gqStats = gs.map(g => g.gq).stats(). Filter the genotype aggregable based on case status (sa.pheno.isCase) and genotype call (g.isHet and g.isHomVar) and th",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:6601,Testability,log,logIntensity,6601,"nd side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample annotation based on the length of the sample ID. sa.idTooLong = s.length > 10. Add a new variant annotation that is a String representing the chromosome and start position. va.altName = v.contig + "":"" + v.start. Add a new variant annotation that splits a comma-separated string with gene names and keeps the first element of the resulting array. va.geneName = va.geneNames.split("","")[0]. Add a new variant annotation that is the log of an existing annotation. va.logIntensity = log(va.intensity). Add a new global annotation computed from existing global annotations. global.callRate = global.nCalled / global.nGenotypes. Variant Annotation Computed from a Genotype Aggregable (gs)¶; In the context of creating new variant annotations, a genotype aggregable (gs) represents a row of genotypes in the variant-sample matrix.; The result of evaluating the genotype aggregable expression per row is added to the corresponding variant annotation.; The map function takes a lambda expression as input (g => ...). The filter function takes a boolean lambda expression as input (g => Boolean Expression). Transform the genotype aggregable to an aggregable of GQ scores using the map function and then calculate summary statistics on the GQ scores with the stats function. va.gqStats = gs.map(g => g.gq).stats(). Filter the genotype aggregable based on case status (sa.pheno.isCase) and genotype call (g.isHet and g.isHomVar) and then count the number of elements remaining. va.caseM",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/overview.html:6616,Testability,log,log,6616,"nd side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample annotation based on the length of the sample ID. sa.idTooLong = s.length > 10. Add a new variant annotation that is a String representing the chromosome and start position. va.altName = v.contig + "":"" + v.start. Add a new variant annotation that splits a comma-separated string with gene names and keeps the first element of the resulting array. va.geneName = va.geneNames.split("","")[0]. Add a new variant annotation that is the log of an existing annotation. va.logIntensity = log(va.intensity). Add a new global annotation computed from existing global annotations. global.callRate = global.nCalled / global.nGenotypes. Variant Annotation Computed from a Genotype Aggregable (gs)¶; In the context of creating new variant annotations, a genotype aggregable (gs) represents a row of genotypes in the variant-sample matrix.; The result of evaluating the genotype aggregable expression per row is added to the corresponding variant annotation.; The map function takes a lambda expression as input (g => ...). The filter function takes a boolean lambda expression as input (g => Boolean Expression). Transform the genotype aggregable to an aggregable of GQ scores using the map function and then calculate summary statistics on the GQ scores with the stats function. va.gqStats = gs.map(g => g.gq).stats(). Filter the genotype aggregable based on case status (sa.pheno.isCase) and genotype call (g.isHet and g.isHomVar) and then count the number of elements remaining. va.caseM",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
https://hail.is/docs/0.1/sql.html:1986,Energy Efficiency,efficient,efficiently,1986,"nal” part means that the data is managed by an entity outside Hive (and; Impala). The table schema is read from one of the Parquet files in the VDS file; hierarchy.; To generate a Hive file:. Copy a VCF file into HDFS. $ hadoop fs -put src/test/resources/sample.vcf.bgz sample.vcf.bgz. Convert the VCF file into a VDS using Hail:; >>> hc.import_vcf(""sample.vcf.bgz"").write(""sample.vds"", parquet_genotypes=True). Note the use of parquet_genotypes=True, which writes the genotype; information using Parquet structures, rather than an opaque binary; representation that cannot be queried using SQL. Register the VDS as a Hive table. $ PARQUET_DATA_FILE=$(hadoop fs -stat '%n' hdfs:///user/$USER/sample.vds/rdd.parquet/*.parquet | head -1); $ impala-shell -q ""CREATE EXTERNAL TABLE variants LIKE PARQUET 'hdfs:///user/$USER/sample.vds/rdd.parquet/$PARQUET_DATA_FILE' STORED AS PARQUET LOCATION 'hdfs:///user/$USER/sample.vds/rdd.parquet'"". It is good practice to run Impala’s COMPUTE STATS command on the newly-created table, so that subsequent queries run efficiently.; $ impala-shell -q ""COMPUTE STATS variants"". Before running any queries it’s worth understanding the table schema, which is easily; done by calling DESCRIBE on the table:; $ impala-shell -q ""DESCRIBE variants"". +-------------+----------------------------------+-----------------------------+; | name | type | comment |; +-------------+----------------------------------+-----------------------------+; | variant | struct< | Inferred from Parquet file. |; | | contig:string, | |; | | start:int, | |; | | ref:string, | |; | | altalleles:array<struct< | |; | | ref:string, | |; | | alt:string | |; | | >> | |; | | > | |; | annotations | struct< | Inferred from Parquet file. |; | | rsid:string, | |; | | qual:double, | |; | | filters:array<string>, | |; | | pass:boolean, | |; | | info:struct< | |; | | negative_train_site:boolean, | |; | | hwp:double, | |; | | ac:array<int>, | |; ...; | | > | |; | | > | |; | gs | array<struct< | Infer",MatchSource.WIKI,docs/0.1/sql.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/sql.html
https://hail.is/docs/0.1/sql.html:518,Security,access,access,518,"﻿. . Querying using SQL — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources; Hadoop Glob Patterns; SQL; Impala. Hail. Docs »; Other Resources »; Querying using SQL. View page source. Querying using SQL¶; Since Hail uses the Parquet file format for data storage, a Hail VDS can be queried using; Hadoop SQL tools, like Hive or Impala. This mode of access may be convenient for users; who have ad hoc queries that they are able to express in SQL.; Note that SQL access is read-only: it is not possible to write Hail datasets using; SQL at the current time. Impala¶; Each VDS should be registered in the Hive metastore to allow Impala to query it (Impala uses Hive’s metastore to store table metadata). This is done by creating an external table in Hive, the “external” part means that the data is managed by an entity outside Hive (and; Impala). The table schema is read from one of the Parquet files in the VDS file; hierarchy.; To generate a Hive file:. Copy a VCF file into HDFS. $ hadoop fs -put src/test/resources/sample.vcf.bgz sample.vcf.bgz. Convert the VCF file into a VDS using Hail:; >>> hc.import_vcf(""sample.vcf.bgz"").write(""sample.vds"", parquet_genotypes=True). Note the use of parquet_genotypes=True, which writes the genotype; information using Parquet structures, rather than an opaque binary; representation that cannot be queried using SQL. Register the VDS as a Hive table. $ PARQUET_DATA_FILE=$(hadoop fs -stat '%n' hdfs:///user/$USER/sample.vds/rdd.parquet/*.parquet | head -1); $ impala-shell -q ""CREATE EXTERNAL TABLE variants LIKE PARQUET 'hdfs:///user/$USER/sample.vds/rdd.parquet/$PARQUET_DATA_FILE' STORED AS PARQUET LOCATION 'hdfs:///user/$USER/sample.vds/rdd.parquet'"". It is good practice to run Impala’s COMPUTE STATS command on the newly-created table, so that subsequent queries run efficiently.; $",MatchSource.WIKI,docs/0.1/sql.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/sql.html
https://hail.is/docs/0.1/sql.html:631,Security,access,access,631,"﻿. . Querying using SQL — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources; Hadoop Glob Patterns; SQL; Impala. Hail. Docs »; Other Resources »; Querying using SQL. View page source. Querying using SQL¶; Since Hail uses the Parquet file format for data storage, a Hail VDS can be queried using; Hadoop SQL tools, like Hive or Impala. This mode of access may be convenient for users; who have ad hoc queries that they are able to express in SQL.; Note that SQL access is read-only: it is not possible to write Hail datasets using; SQL at the current time. Impala¶; Each VDS should be registered in the Hive metastore to allow Impala to query it (Impala uses Hive’s metastore to store table metadata). This is done by creating an external table in Hive, the “external” part means that the data is managed by an entity outside Hive (and; Impala). The table schema is read from one of the Parquet files in the VDS file; hierarchy.; To generate a Hive file:. Copy a VCF file into HDFS. $ hadoop fs -put src/test/resources/sample.vcf.bgz sample.vcf.bgz. Convert the VCF file into a VDS using Hail:; >>> hc.import_vcf(""sample.vcf.bgz"").write(""sample.vds"", parquet_genotypes=True). Note the use of parquet_genotypes=True, which writes the genotype; information using Parquet structures, rather than an opaque binary; representation that cannot be queried using SQL. Register the VDS as a Hive table. $ PARQUET_DATA_FILE=$(hadoop fs -stat '%n' hdfs:///user/$USER/sample.vds/rdd.parquet/*.parquet | head -1); $ impala-shell -q ""CREATE EXTERNAL TABLE variants LIKE PARQUET 'hdfs:///user/$USER/sample.vds/rdd.parquet/$PARQUET_DATA_FILE' STORED AS PARQUET LOCATION 'hdfs:///user/$USER/sample.vds/rdd.parquet'"". It is good practice to run Impala’s COMPUTE STATS command on the newly-created table, so that subsequent queries run efficiently.; $",MatchSource.WIKI,docs/0.1/sql.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/sql.html
https://hail.is/docs/0.1/sql.html:6623,Security,access,access,6623,"LECT variant.contig, variant.start, variant.ref, gs.pos AS genotype_pos, gs.item.gt AS gt FROM variants, variants.gs WHERE variant.start = 13090728 AND gs.pos >= 20 AND gs.pos < 25;"". +----------------+---------------+-------------+--------------+----+; | variant.contig | variant.start | variant.ref | genotype_pos | gt |; +----------------+---------------+-------------+--------------+----+; | 20 | 13090728 | A | 20 | 1 |; | 20 | 13090728 | A | 21 | 0 |; | 20 | 13090728 | A | 22 | 0 |; | 20 | 13090728 | A | 23 | 0 |; | 20 | 13090728 | A | 24 | 0 |; +----------------+---------------+-------------+--------------+----+. We can also retrieve the values from the AD (Allelic Depths) array by doing a nested; query that returns one row per genotype and per AD value. The ad_pos column is; the index of the value in the AD array.; $ impala-shell -q ""SELECT variant.contig, variant.start, variant.ref, gs.pos AS genotype_pos, gs.item.gt AS gt, ad.pos AS ad_pos, ad.item AS ad FROM variants, variants.gs, gs.ad WHERE variant.start = 13090728 LIMIT 6;"". +----------------+---------------+-------------+--------------+----+--------+----+; | variant.contig | variant.start | variant.ref | genotype_pos | gt | ad_pos | ad |; +----------------+---------------+-------------+--------------+----+--------+----+; | 20 | 13090728 | A | 0 | 0 | 0 | 28 |; | 20 | 13090728 | A | 0 | 0 | 1 | 0 |; | 20 | 13090728 | A | 1 | 0 | 0 | 20 |; | 20 | 13090728 | A | 1 | 0 | 1 | 0 |; | 20 | 13090728 | A | 2 | 0 | 0 | 11 |; | 20 | 13090728 | A | 2 | 0 | 1 | 0 |; +----------------+---------------+-------------+--------------+----+--------+----+. If you no longer need to use SQL you can delete the table definition. Since the table; was registered as an external table the underlying data is not affected, so you can; still access the VDS from Hail.; $ impala-shell -q ""DROP TABLE variants""; $ hadoop fs -ls sample.vds. Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/sql.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/sql.html
https://hail.is/docs/0.1/sql.html:1173,Testability,test,test,1173,"ls; Expression Language; Python API; Annotation Database; Other Resources; Hadoop Glob Patterns; SQL; Impala. Hail. Docs »; Other Resources »; Querying using SQL. View page source. Querying using SQL¶; Since Hail uses the Parquet file format for data storage, a Hail VDS can be queried using; Hadoop SQL tools, like Hive or Impala. This mode of access may be convenient for users; who have ad hoc queries that they are able to express in SQL.; Note that SQL access is read-only: it is not possible to write Hail datasets using; SQL at the current time. Impala¶; Each VDS should be registered in the Hive metastore to allow Impala to query it (Impala uses Hive’s metastore to store table metadata). This is done by creating an external table in Hive, the “external” part means that the data is managed by an entity outside Hive (and; Impala). The table schema is read from one of the Parquet files in the VDS file; hierarchy.; To generate a Hive file:. Copy a VCF file into HDFS. $ hadoop fs -put src/test/resources/sample.vcf.bgz sample.vcf.bgz. Convert the VCF file into a VDS using Hail:; >>> hc.import_vcf(""sample.vcf.bgz"").write(""sample.vds"", parquet_genotypes=True). Note the use of parquet_genotypes=True, which writes the genotype; information using Parquet structures, rather than an opaque binary; representation that cannot be queried using SQL. Register the VDS as a Hive table. $ PARQUET_DATA_FILE=$(hadoop fs -stat '%n' hdfs:///user/$USER/sample.vds/rdd.parquet/*.parquet | head -1); $ impala-shell -q ""CREATE EXTERNAL TABLE variants LIKE PARQUET 'hdfs:///user/$USER/sample.vds/rdd.parquet/$PARQUET_DATA_FILE' STORED AS PARQUET LOCATION 'hdfs:///user/$USER/sample.vds/rdd.parquet'"". It is good practice to run Impala’s COMPUTE STATS command on the newly-created table, so that subsequent queries run efficiently.; $ impala-shell -q ""COMPUTE STATS variants"". Before running any queries it’s worth understanding the table schema, which is easily; done by calling DESCRIBE on the table:; $ ",MatchSource.WIKI,docs/0.1/sql.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/sql.html
https://hail.is/docs/0.1/tutorials-landing.html:220,Availability,down,download,220,"﻿. . Tutorials — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to c",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
https://hail.is/docs/0.1/tutorials-landing.html:919,Availability,down,download,919,"﻿. . Tutorials — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to c",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
https://hail.is/docs/0.1/tutorials-landing.html:2098,Availability,down,download,2098," Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by population stratification. Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language¶; This notebook starts with the basics of the Hail expression language, and builds up practical experience; with the type system, syntax, and functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice, dice, filter, and query genetic data. Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate¶; This notebook uses the Hail expression language to query, filter, and annotate the same thousand genomes; dataset from the overview. We also cover how to compute aggregate statis",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
https://hail.is/docs/0.1/tutorials-landing.html:3345,Availability,down,download,3345,"t the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by population stratification. Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language¶; This notebook starts with the basics of the Hail expression language, and builds up practical experience; with the type system, syntax, and functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice, dice, filter, and query genetic data. Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate¶; This notebook uses the Hail expression language to query, filter, and annotate the same thousand genomes; dataset from the overview. We also cover how to compute aggregate statistics from a dataset using the; expression language. Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
https://hail.is/docs/0.1/tutorials-landing.html:702,Modifiability,variab,variables,702,"﻿. . Tutorials — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to c",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
https://hail.is/docs/0.1/tutorials-landing.html:2897,Modifiability,variab,variables,2897,"t the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by population stratification. Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language¶; This notebook starts with the basics of the Hail expression language, and builds up practical experience; with the type system, syntax, and functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice, dice, filter, and query genetic data. Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate¶; This notebook uses the Hail expression language to query, filter, and annotate the same thousand genomes; dataset from the overview. We also cover how to compute aggregate statistics from a dataset using the; expression language. Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
https://hail.is/docs/0.1/tutorials-landing.html:1298,Testability,test,test,1298,"te sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by population stratification. Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Ra",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
https://hail.is/docs/0.1/tutorials-landing.html:1966,Testability,test,test,1966,"ltering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by population stratification. Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language¶; This notebook starts with the basics of the Hail expression language, and builds up practical experience; with the type system, syntax, and functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice, dice, filter, and query genetic data. Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: quer",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
https://hail.is/docs/0.1/types.html:36421,Availability,down,downcoded,36421,"oat – Convert value to a Float.; toInt(): Int – Convert value to an Integer.; toLong(): Long – Convert value to a Long. Float¶. abs(): Float – Returns the absolute value of a number.; max(a: Float): Float – Returns the maximum value.; min(a: Float): Float – Returns the minimum value.; signum(): Int – Returns the sign of a number (1, 0, or -1).; toDouble(): Double – Convert value to a Double.; toFloat(): Float – Convert value to a Float.; toInt(): Int – Convert value to an Integer.; toLong(): Long – Convert value to a Long. Genotype¶; A Genotype is a Hail data type representing a genotype in the Variant Dataset. It is referred to as g in the expression language. ad: Array[Int] – allelic depth for each allele. call(): Call – the integer gt = k*(k+1)/2 + j for call j/k (0 = 0/0, 1 = 0/1, 2 = 1/1, 3 = 0/2, etc.). dosage: Double – the expected number of non-reference alleles based on genotype probabilities. dp: Int – the total number of informative reads. fakeRef: Boolean – True if this genotype was downcoded in split_multi(). This can happen if a 1/2 call is split to 0/1, 0/1. fractionReadsRef(): Double – the ratio of ref reads to the sum of all informative reads. gp: Array[Double] – the linear-scaled probabilities. gq: Int – the difference between the two smallest PL entries. gt: Int – the integer gt = k*(k+1)/2 + j for call j/k (0 = 0/0, 1 = 0/1, 2 = 1/1, 3 = 0/2, etc.). gtj(): Int – the index of allele j for call j/k (0 = ref, 1 = first alt allele, etc.). gtk(): Int – the index of allele k for call j/k (0 = ref, 1 = first alt allele, etc.). isCalled(): Boolean – True if the genotype is not ./.. isCalledNonRef(): Boolean – True if either g.isHet or g.isHomVar is true. isHet(): Boolean – True if this call is heterozygous. isHetNonRef(): Boolean – True if this call is j/k with j>0. isHetRef(): Boolean – True if this call is 0/k with k>0. isHomRef(): Boolean – True if this call is 0/0. isHomVar(): Boolean – True if this call is j/j with j>0. isLinearScale: Boolean – True ",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
https://hail.is/docs/0.1/types.html:1054,Integrability,depend,depending,1054," HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Aggregable; Aggregable[Array[Double]]; Aggregable[Array[Float]]; Aggregable[Array[Int]]; Aggregable[Array[Long]]; Aggregable[Double]; Aggregable[Float]; Aggregable[Genotype]; Aggregable[Int]; Aggregable[Long]; Aggregable[T]. AltAllele; Array; Array[Array[T]]; Array[Boolean]; Array[Double]; Array[Float]; Array[Int]; Array[Long]; Array[String]; Array[T]. Boolean; Call; Dict; Double; Float; Genotype; Int; Interval; Locus; Long; Set; Set[Double]; Set[Float]; Set[Int]; Set[Long]; Set[Set[T]]; Set[String]; Set[T]. String; Struct; Variant. Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Types. View page source. Types¶. Aggregable¶; An Aggregable is a Hail data type representing a distributed row or column of a matrix. Hail exposes a number of methods to compute on aggregables depending on the data type. Aggregable[Array[Double]]¶. sum(): Array[Double] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Float]]¶. sum(): Array[Float] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Int]]¶. sum(): Array[Int]. Compute the sum by index. All elements in the aggregable must have the same length.; Examples; Count the total number of occurrences of each allele across samples, per variant:; >>> vds_result = vds.annotate_variants_expr('va.AC = gs.map(g => g.oneHotAlleles(v)).sum()'). Aggregable[Array[Long]]¶. sum(): Array[Long] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Double]¶. hist(start: Double, end: Double, bins: Int): Struct{binEdges:Array[Double],binFrequencies:Array[Long],nLess:Long,nGreater:Long}. binEdges (Array[Double]) – Array of bin cutoffs; binFrequencies (Array[Long]) – Number of e",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
https://hail.is/docs/0.1/types.html:2634,Modifiability,variab,variable,2634," vds.annotate_variants_expr('va.AC = gs.map(g => g.oneHotAlleles(v)).sum()'). Aggregable[Array[Long]]¶. sum(): Array[Long] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Double]¶. hist(start: Double, end: Double, bins: Int): Struct{binEdges:Array[Double],binFrequencies:Array[Long],nLess:Long,nGreater:Long}. binEdges (Array[Double]) – Array of bin cutoffs; binFrequencies (Array[Long]) – Number of elements that fall in each bin.; nLess (Long) – Number of elements less than the minimum bin; nGreater (Long) – Number of elements greater than the maximum bin. Compute frequency distributions of numeric parameters.; Examples; Compute GQ-distributions per variant:; >>> vds_result = vds.annotate_variants_expr('va.gqHist = gs.map(g => g.gq).hist(0, 100, 20)'). Compute global GQ-distribution:; >>> gq_hist = vds.query_genotypes('gs.map(g => g.gq).hist(0, 100, 100)'). Notes. The start, end, and bins params are no-scope parameters, which means that while computations like 100 / 4 are acceptable, variable references like global.nBins are not.; Bin size is calculated from (end - start) / bins; (bins + 1) breakpoints are generated from the range (start to end by binsize); Each bin is left-inclusive, right-exclusive except the last bin, which includes the maximum value. This means that if there are N total bins, there will be N + 1 elements in binEdges. For the invocation hist(0, 3, 3), binEdges would be [0, 1, 2, 3] where the bins are [0, 1), [1, 2), [2, 3]. Arguments. start (Double) – Starting point of first bin; end (Double) – End point of last bin; bins (Int) – Number of bins to create. max(): Double – Compute the maximum of all non-missing elements. The empty max is missing. min(): Double – Compute the minimum of all non-missing elements. The empty min is missing. product(): Double – Compute the product of all non-missing elements. The empty product is one. stats(): Struct{mean:Double,stdev:Double,min:Double,max:Double,nNotMissing:",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
https://hail.is/docs/0.1/types.html:27483,Modifiability,extend,extend,27483," of the slice (not included in result). [i:]: Array[T]. Returns a slice of the array from the i*th* element (0-indexed) to the; end. Negative indices are interpreted as offsets from the end of the array.; let a = [0, 2, 4, 6, 8, 10] in a[3:]; result: [6, 8, 10]. let a = [0, 2, 4, 6, 8, 10] in a[-5:]; result: [2, 4, 6, 8, 10]. Arguments. i (Int) – Starting index of the slice. [i]: T. Returns the i*th* element (0-indexed) of the array, or throws an exception if i is an invalid index.; let a = [0, 2, 4, 6, 8, 10] in a[2]; result: 4. Arguments. i (Int) – Index of the element to return. append(a: T): Array[T] – Returns the result of adding the element a to the end of this Array. exists(expr: T => Boolean): Boolean. Returns a boolean which is true if any element in the array satisfies the condition given by expr. false otherwise.; let a = [1, 2, 3, 4, 5, 6] in a.exists(e => e > 4); result: true. Arguments. expr (T => Boolean) – Lambda expression. extend(a: Array[T]): Array[T] – Returns the concatenation of this Array followed by Array a. filter(expr: T => Boolean): Array[T]. Returns a new array subsetted to the elements where expr evaluates to true.; let a = [1, 4, 5, 6, 10] in a.filter(e => e % 2 == 0); result: [4, 6, 10]. Arguments. expr (T => Boolean) – Lambda expression. find(expr: T => Boolean): T. Returns the first non-missing element of the array for which expr is true. If no element satisfies the predicate, find returns NA.; let a = [""cat"", ""dog"", ""rabbit""] in a.find(e => 'bb' ~ e); result: ""rabbit"". Arguments. expr (T => Boolean) – Lambda expression. flatMap(expr: T => Array[U]): Array[U]. Returns a new array by applying a function to each subarray and concatenating the resulting arrays.; let a = [[1, 2, 3], [4, 5], [6]] in a.flatMap(e => e + 1); result: [2, 3, 4, 5, 6, 7]. Arguments. expr (T => Array[U]) – Lambda expression. forall(expr: T => Boolean): Boolean. Returns a boolean which is true if all elements in the array satisfies the condition given by expr and ",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
https://hail.is/docs/0.1/types.html:1000,Security,expose,exposes,1000," HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Aggregable; Aggregable[Array[Double]]; Aggregable[Array[Float]]; Aggregable[Array[Int]]; Aggregable[Array[Long]]; Aggregable[Double]; Aggregable[Float]; Aggregable[Genotype]; Aggregable[Int]; Aggregable[Long]; Aggregable[T]. AltAllele; Array; Array[Array[T]]; Array[Boolean]; Array[Double]; Array[Float]; Array[Int]; Array[Long]; Array[String]; Array[T]. Boolean; Call; Dict; Double; Float; Genotype; Int; Interval; Locus; Long; Set; Set[Double]; Set[Float]; Set[Int]; Set[Long]; Set[Set[T]]; Set[String]; Set[T]. String; Struct; Variant. Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Types. View page source. Types¶. Aggregable¶; An Aggregable is a Hail data type representing a distributed row or column of a matrix. Hail exposes a number of methods to compute on aggregables depending on the data type. Aggregable[Array[Double]]¶. sum(): Array[Double] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Float]]¶. sum(): Array[Float] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Int]]¶. sum(): Array[Int]. Compute the sum by index. All elements in the aggregable must have the same length.; Examples; Count the total number of occurrences of each allele across samples, per variant:; >>> vds_result = vds.annotate_variants_expr('va.AC = gs.map(g => g.oneHotAlleles(v)).sum()'). Aggregable[Array[Long]]¶. sum(): Array[Long] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Double]¶. hist(start: Double, end: Double, bins: Int): Struct{binEdges:Array[Double],binFrequencies:Array[Long],nLess:Long,nGreater:Long}. binEdges (Array[Double]) – Array of bin cutoffs; binFrequencies (Array[Long]) – Number of e",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
https://hail.is/docs/0.1/types.html:21782,Security,access,accessing,21782,"rtion, Deletion, Star, MNP, Complex; isComplex(): Boolean – True if not a SNP, MNP, star, insertion, or deletion.; isDeletion(): Boolean – True if v.ref begins with and is longer than v.alt.; isIndel(): Boolean – True if an insertion or a deletion.; isInsertion(): Boolean – True if v.alt begins with and is longer than v.ref.; isMNP(): Boolean – True if v.ref and v.alt are the same length and differ in more than one position.; isSNP(): Boolean – True if v.ref and v.alt are the same length and differ in one position.; isStar(): Boolean – True if v.alt is *.; isTransition(): Boolean – True if a purine-purine or pyrimidine-pyrimidine SNP.; isTransversion(): Boolean – True if a purine-pyrimidine SNP.; ref: String – Reference allele base sequence. Array¶; An Array is a collection of items that all have the same data type (ex: Int, String) and are indexed. Arrays can be constructed by specifying [item1, item2, ...] and they are 0-indexed.; An example of constructing an array and accessing an element is:; let a = [1, 10, 3, 7] in a[1]; result: 10. They can also be nested such as Array[Array[Int]]:; let a = [[1, 2, 3], [4, 5], [], [6, 7]] in a[1]; result: [4, 5]. Array[Array[T]]¶. flatten(): Array[T]. Flattens a nested array by concatenating all its rows into a single array.; let a = [[1, 3], [2, 4]] in a.flatten(); result: [1, 3, 2, 4]. Array[Boolean]¶. sort(ascending: Boolean): Array[Boolean]. Sort the collection with the ordering specified by ascending.; Arguments. ascending (Boolean) – If true, sort the collection in ascending order. Otherwise, sort in descending order. sort(): Array[Boolean] – Sort the collection in ascending order. Array[Double]¶. max(): Double – Largest element in the collection. mean(): Double – Mean value of the collection. median(): Double – Median value of the collection. min(): Double – Smallest element in the collection. product(): Double – Product of all elements in the collection (returns 1 if empty). sort(ascending: Boolean): Array[Double]. S",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
https://hail.is/docs/0.1/types.html:48459,Security,access,accessing,48459,"racters, escape them with double backslash (\).; let s = ""1kg-NA12878"" in s.split(""-""); result: [""1kg"", ""NA12878""]. Arguments. delim (String) – Regular expression delimiter.; n (Int) – Number of times the pattern is applied. See the Java documentation for more information. split(delim: String): Array[String]. Returns an array of strings, split on the given regular expression delimiter. See the documentation on regular expression syntax delimiter. If you need to split on special characters, escape them with double backslash (\).; let s = ""1kg-NA12878"" in s.split(""-""); result: [""1kg"", ""NA12878""]. Arguments. delim (String) – Regular expression delimiter. toDouble(): Double – Convert value to a Double. toFloat(): Float – Convert value to a Float. toInt(): Int – Convert value to an Integer. toLong(): Long – Convert value to a Long. Struct¶; A Struct is like a Python tuple where the fields are named and the set of fields is fixed.; An example of constructing and accessing the fields in a Struct is; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in s.gene; result: ""ACBD"". A field of the Struct can also be another Struct. For example, va.info.AC selects the struct info from the struct va, and then selects the array AC from the struct info. Variant¶; A Variant is a Hail data type representing a variant in the Variant Dataset. It is referred to as v in the expression language.; The pseudoautosomal region (PAR) is currently defined with respect to reference GRCh37:. X: 60001 - 2699520, 154931044 - 155260560; Y: 10001 - 2649520, 59034050 - 59363566. Most callers assign variants in PAR to X. alt(): String – Alternate allele sequence. Assumes biallelic.; altAllele(): AltAllele – The alternate allele. Assumes biallelic.; altAlleles: Array[AltAllele] – The alternate alleles.; contig: String – String representation of contig, exactly as imported. NB: Hail stores contigs as strings. Use double-quotes when checking contig equality.; inXNonPar(): Boolean – True if chromosome is X and",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
https://hail.is/docs/0.1/types.html:42627,Testability,test,test,42627,"ection.; median(): Long – Median value of the collection.; min(): Long – Smallest element in the collection.; product(): Long – Product of all elements in the collection (returns 1 if empty).; sum(): Long – Sum of all elements in the collection. Set[Set[T]]¶. flatten(): Set[T]. Flattens a nested set by concatenating all its elements into a single set.; let s = [[1, 2].toSet(), [3, 4].toSet()].toSet() in s.flatten(); result: Set(1, 2, 3, 4). Set[String]¶. mkString(delimiter: String): String. Concatenates all elements of this set into a single string where each element is separated by the delimiter.; let s = [1, 2, 3].toSet() in s.mkString("",""); result: ""1,2,3"". Arguments. delimiter (String) – String that separates each element. Set[T]¶. add(a: T): Set[T] – Returns the result of adding the element a to this Set. contains(x: T): Boolean. Returns true if the element x is contained in the set, otherwise false.; let s = [1, 2, 3].toSet() in s.contains(5); result: false. Arguments. x (T) – Value to test. difference(a: Set[T]): Set[T] – Returns the elements of this Set that are not in Set a. exists(expr: T => Boolean): Boolean. Returns a boolean which is true if any element in the set satisfies the condition given by expr and false otherwise.; let s = [0, 2, 4, 6, 8, 10].toSet() in s.exists(e => e % 2 == 1); result: false. Arguments. expr (T => Boolean) – Lambda expression. filter(expr: T => Boolean): Set[T]. Returns a new set subsetted to the elements where expr evaluates to true.; let s = [1, 4, 5, 6, 10].toSet() in s.filter(e => e >= 5); result: Set(5, 6, 10). Arguments. expr (T => Boolean) – Lambda expression. find(expr: T => Boolean): T. Returns the first non-missing element of the array for which expr is true. If no element satisfies the predicate, find returns NA.; let s = [1, 2, 3].toSet() in s.find(e => e % 3 == 0); result: 3. Arguments. expr (T => Boolean) – Lambda expression. flatMap(expr: T => Set[U]): Set[U]. Returns a new set by applying a function to each subs",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
https://hail.is/docs/0.2/aggregators.html:2733,Availability,down,downsample,2733,"ords of expr. fraction(predicate); Compute the fraction of records where predicate is True. hardy_weinberg_test(expr[, one_sided]); Performs test of Hardy-Weinberg equilibrium. explode(f, array_agg_expr); Explode an array or set expression to aggregate the elements of all records. filter(condition, aggregation); Filter records according to a predicate. inbreeding(expr, prior); Compute inbreeding statistics on calls. call_stats(call, alleles); Compute useful call statistics. info_score(gp); Compute the IMPUTE information score. hist(expr, start, end, bins); Compute binned counts of a numeric expression. linreg(y, x[, nested_dim, weight]); Compute multivariate linear regression statistics. corr(x, y); Computes the Pearson correlation coefficient between x and y. group_by(group, agg_expr); Compute aggregation statistics stratified by one or more groups. array_agg(f, array); Aggregate an array element-wise using a user-specified aggregation function. downsample(x, y[, label, n_divisions]); Downsample (x, y) coordinate datapoints. approx_cdf(expr[, k, _raw]); Produce a summary of the distribution of values. hail.expr.aggregators.collect(expr)[source]; Collect records into an array.; Examples; Collect the ID field where HT is greater than 68:; >>> table1.aggregate(hl.agg.filter(table1.HT > 68, hl.agg.collect(table1.ID))); [2, 3]. Notes; The element order of the resulting array is not guaranteed, and in some; cases is non-deterministic.; Use collect_as_set() to collect unique items. Warning; Collecting a large number of items can cause out-of-memory exceptions. Parameters:; expr (Expression) – Expression to collect. Returns:; ArrayExpression – Array of all expr records. hail.expr.aggregators.collect_as_set(expr)[source]; Collect records into a set.; Examples; Collect the unique ID field where HT is greater than 68:; >>> table1.aggregate(hl.agg.filter(table1.HT > 68, hl.agg.collect_as_set(table1.ID))); {2, 3}. Note that when collecting a set-typed field with collect_as_set",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:28561,Availability,error,error,28561,"squared=0.1527377521613834,; f_stat=1.2704081632653061,; multiple_p_value=0.5314327326007864,; n=4). Regress blood pressure against an intercept (1), genotype, age, and; the interaction of genotype and age:; >>> ds_ann = ds.annotate_rows(linreg =; ... hl.agg.linreg(ds.pheno.blood_pressure,; ... [1,; ... ds.GT.n_alt_alleles(),; ... ds.pheno.age,; ... ds.GT.n_alt_alleles() * ds.pheno.age])). Warning; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; In relation to; lm.summary; in R, linreg(y, x = [1, mt.x1, mt.x2]) computes; summary(lm(y ~ x1 + x2)) and; linreg(y, x = [mt.x1, mt.x2], nested_dim=0) computes; summary(lm(y ~ x1 + x2 - 1)).; More generally, nested_dim defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; beta (tarray of tfloat64):; Estimated regression coefficient for each covariate.; standard_error (tarray of tfloat64):; Estimated standard error for each covariate.; t_stat (tarray of tfloat64):; t-statistic for each covariate.; p_value (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance.",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:28791,Availability,error,error,28791,">> ds_ann = ds.annotate_rows(linreg =; ... hl.agg.linreg(ds.pheno.blood_pressure,; ... [1,; ... ds.GT.n_alt_alleles(),; ... ds.pheno.age,; ... ds.GT.n_alt_alleles() * ds.pheno.age])). Warning; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; In relation to; lm.summary; in R, linreg(y, x = [1, mt.x1, mt.x2]) computes; summary(lm(y ~ x1 + x2)) and; linreg(y, x = [mt.x1, mt.x2], nested_dim=0) computes; summary(lm(y ~ x1 + x2 - 1)).; More generally, nested_dim defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; beta (tarray of tfloat64):; Estimated regression coefficient for each covariate.; standard_error (tarray of tfloat64):; Estimated standard error for each covariate.; t_stat (tarray of tfloat64):; t-statistic for each covariate.; p_value (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:33133,Availability,down,downsample,33133,"tart with a range table with an array of random boolean values:; >>> ht = hl.utils.range_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction True per element:; >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) ; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; This function requires that all values of array have the same length. If; two values have different lengths, then an exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:33282,Availability,down,downsampled,33282,"e_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction True per element:; >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) ; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; This function requires that all values of array have the same length. If; two values have different lengths, then an exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:33335,Availability,down,downsampled,33335," 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction True per element:; >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) ; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; This function requires that all values of array have the same length. If; two values have different lengths, then an exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly les",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:33530,Availability,down,downsample,33530,"48]. Notes; This function requires that all values of array have the same length. If; two values have different lengths, then an exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:33657,Availability,down,downsampled,33657,"n exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:34947,Availability,down,downstream,34947,"alues to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value values(i) occupying indices ranks(i) (inclusive) to; ranks(i+1) (exclusive).; The returned struct also contains an array _compaction_counts, which is; used internally to support downstream error estimation. Warning; This is an approximate and nondeterministic method. Parameters:. expr (Expression) – Expression to collect.; k (int) – Parameter controlling the accuracy vs. memory usage tradeoff. Returns:; StructExpression – Struct containing values and ranks arrays. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:34958,Availability,error,error,34958,"alues to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value values(i) occupying indices ranks(i) (inclusive) to; ranks(i+1) (exclusive).; The returned struct also contains an array _compaction_counts, which is; used internally to support downstream error estimation. Warning; This is an approximate and nondeterministic method. Parameters:. expr (Expression) – Expression to collect.; k (int) – Parameter controlling the accuracy vs. memory usage tradeoff. Returns:; StructExpression – Struct containing values and ranks arrays. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:35295,Deployability,update,updated,35295,"alues to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value values(i) occupying indices ranks(i) (inclusive) to; ranks(i+1) (exclusive).; The returned struct also contains an array _compaction_counts, which is; used internally to support downstream error estimation. Warning; This is an approximate and nondeterministic method. Parameters:. expr (Expression) – Expression to collect.; k (int) – Parameter controlling the accuracy vs. memory usage tradeoff. Returns:; StructExpression – Struct containing values and ranks arrays. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:29388,Integrability,depend,dependent,29388,"he effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; beta (tarray of tfloat64):; Estimated regression coefficient for each covariate.; standard_error (tarray of tfloat64):; Estimated standard error for each covariate.; t_stat (tarray of tfloat64):; t-statistic for each covariate.; p_value (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be include",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:29666,Integrability,depend,dependent,29666," (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be included in the; calculation.; In the case that there are no non-missing pairs, the result will be missing. See also; linreg(). Parameters:. x (Expression of type tfloat64); y (Expression of type tfloat64). Returns:; Float64Expression. hail.expr.aggregators.group_by(group, agg_expr)[source]; Compute aggregation statistics stratified ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:29676,Modifiability,variab,variable,29676," (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be included in the; calculation.; In the case that there are no non-missing pairs, the result will be missing. See also; linreg(). Parameters:. x (Expression of type tfloat64); y (Expression of type tfloat64). Returns:; Float64Expression. hail.expr.aggregators.group_by(group, agg_expr)[source]; Compute aggregation statistics stratified ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:29765,Modifiability,variab,variables,29765,"ror (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be included in the; calculation.; In the case that there are no non-missing pairs, the result will be missing. See also; linreg(). Parameters:. x (Expression of type tfloat64); y (Expression of type tfloat64). Returns:; Float64Expression. hail.expr.aggregators.group_by(group, agg_expr)[source]; Compute aggregation statistics stratified by one or more groups.; Examples; Compute linear regression statistics s",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:15490,Performance,perform,performs,15490,":; Expression of type tint64 or tfloat64 – Product of records of expr. hail.expr.aggregators.fraction(predicate)[source]; Compute the fraction of records where predicate is True.; Examples; Compute the fraction of rows where SEX is “F” and HT > 65:; >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; Missing values for predicate are treated as False. Parameters:; predicate (BooleanExpression) – Boolean predicate. Returns:; Expression of type tfloat64 – Fraction of records where predicate is True. hail.expr.aggregators.hardy_weinberg_test(expr, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; Test each row of a dataset:; >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:16141,Performance,perform,perform,16141,"taset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters fi",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:16703,Performance,perform,perform,16703,"4) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters field (Set[String]):; >>> dataset.aggregate_rows(hl.agg.explode(lambda elt: hl.agg.collect_as_set(elt), dataset.filters)); set(). Notes; This method can be used with aggregator functions to aggregate the elements; of collection types (tarray and tset). Parameters:. f (Function from Expression to Expression) – Aggregation function to apply to each element of the exploded array.; array_agg_expr (CollectionExpression) – Expression of type tarray or tset. Returns:; Expression – Aggregation express",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:714,Security,expose,exposed,714,"﻿. Hail | ; Aggregators. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Aggregators. View page source. Aggregators; The aggregators module is exposed as hl.agg, e.g. hl.agg.sum. collect(expr); Collect records into an array. collect_as_set(expr); Collect records into a set. count(); Count the number of records. count_where(condition); Count the number of records where a predicate is True. counter(expr, *[, weight]); Count the occurrences of each unique record and return a dictionary. any(condition); Returns True if condition is True for any record. all(condition); Returns True if condition is True for every record. take(expr, n[, ordering]); Take n records of expr, optionally ordered by ordering. min(expr); Compute the minimum expr. max(expr); Compute the maximum expr. sum(expr); Compute the sum of all records of expr. array_sum(expr); Compute the coordinate-wise sum of all records of expr. mean(expr); Compute the mean value of records of expr. approx_quantiles(expr, qs[, k]); Compute an array of approximate quantiles. approx_median(expr[, k]); Compute the approximate median. stats(expr); Compute a number of useful statistics about expr. product(expr); Compute the product of all records of expr. fraction(predicate); Compute the fraction of records where predicate is True. hardy_weinberg_test(expr[, one_sided]); Performs test of Hardy-Weinberg equilibrium. explode(f, array_agg_expr); Explode an array or set ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:1913,Testability,test,test,1913,"); Count the number of records where a predicate is True. counter(expr, *[, weight]); Count the occurrences of each unique record and return a dictionary. any(condition); Returns True if condition is True for any record. all(condition); Returns True if condition is True for every record. take(expr, n[, ordering]); Take n records of expr, optionally ordered by ordering. min(expr); Compute the minimum expr. max(expr); Compute the maximum expr. sum(expr); Compute the sum of all records of expr. array_sum(expr); Compute the coordinate-wise sum of all records of expr. mean(expr); Compute the mean value of records of expr. approx_quantiles(expr, qs[, k]); Compute an array of approximate quantiles. approx_median(expr[, k]); Compute the approximate median. stats(expr); Compute a number of useful statistics about expr. product(expr); Compute the product of all records of expr. fraction(predicate); Compute the fraction of records where predicate is True. hardy_weinberg_test(expr[, one_sided]); Performs test of Hardy-Weinberg equilibrium. explode(f, array_agg_expr); Explode an array or set expression to aggregate the elements of all records. filter(condition, aggregation); Filter records according to a predicate. inbreeding(expr, prior); Compute inbreeding statistics on calls. call_stats(call, alleles); Compute useful call statistics. info_score(gp); Compute the IMPUTE information score. hist(expr, start, end, bins); Compute binned counts of a numeric expression. linreg(y, x[, nested_dim, weight]); Compute multivariate linear regression statistics. corr(x, y); Computes the Pearson correlation coefficient between x and y. group_by(group, agg_expr); Compute aggregation statistics stratified by one or more groups. array_agg(f, array); Aggregate an array element-wise using a user-specified aggregation function. downsample(x, y[, label, n_divisions]); Downsample (x, y) coordinate datapoints. approx_cdf(expr[, k, _raw]); Produce a summary of the distribution of values. hail.expr.aggr",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:7105,Testability,test,test,7105,"he result can be stored in memory on a single machine. Warning; Using counter() with a large number of unique items can cause; out-of-memory exceptions. Parameters:. expr (Expression) – Expression to count by key.; weight (NumericExpression, optional) – Expression by which to weight each occurence (when unspecified,; it is effectively 1). Returns:; DictExpression – Dictionary with the number of occurrences of each unique record. hail.expr.aggregators.any(condition)[source]; Returns True if condition is True for any record.; Examples; >>> (table1.group_by(table1.SEX); ... .aggregate(any_over_70 = hl.agg.any(table1.HT > 70)); ... .show()); +-----+-------------+; | SEX | any_over_70 |; +-----+-------------+; | str | bool |; +-----+-------------+; | ""F"" | False |; | ""M"" | True |; +-----+-------------+. Notes; If there are no records to aggregate, the result is False.; Missing records are not considered. If every record is missing,; the result is also False. Parameters:; condition (BooleanExpression) – Condition to test. Returns:; BooleanExpression. hail.expr.aggregators.all(condition)[source]; Returns True if condition is True for every record.; Examples; >>> (table1.group_by(table1.SEX); ... .aggregate(all_under_70 = hl.agg.all(table1.HT < 70)); ... .show()); +-----+--------------+; | SEX | all_under_70 |; +-----+--------------+; | str | bool |; +-----+--------------+; | ""F"" | False |; | ""M"" | False |; +-----+--------------+. Notes; If there are no records to aggregate, the result is True.; Missing records are not considered. If every record is missing,; the result is also True. Parameters:; condition (BooleanExpression) – Condition to test. Returns:; BooleanExpression. hail.expr.aggregators.take(expr, n, ordering=None)[source]; Take n records of expr, optionally ordered by ordering.; Examples; Take 3 elements of field X:; >>> table1.aggregate(hl.agg.take(table1.X, 3)); [5, 6, 7]. Take the ID and HT fields, ordered by HT (descending):; >>> table1.aggregate(hl.agg.ta",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:7741,Testability,test,test,7741," .show()); +-----+-------------+; | SEX | any_over_70 |; +-----+-------------+; | str | bool |; +-----+-------------+; | ""F"" | False |; | ""M"" | True |; +-----+-------------+. Notes; If there are no records to aggregate, the result is False.; Missing records are not considered. If every record is missing,; the result is also False. Parameters:; condition (BooleanExpression) – Condition to test. Returns:; BooleanExpression. hail.expr.aggregators.all(condition)[source]; Returns True if condition is True for every record.; Examples; >>> (table1.group_by(table1.SEX); ... .aggregate(all_under_70 = hl.agg.all(table1.HT < 70)); ... .show()); +-----+--------------+; | SEX | all_under_70 |; +-----+--------------+; | str | bool |; +-----+--------------+; | ""F"" | False |; | ""M"" | False |; +-----+--------------+. Notes; If there are no records to aggregate, the result is True.; Missing records are not considered. If every record is missing,; the result is also True. Parameters:; condition (BooleanExpression) – Condition to test. Returns:; BooleanExpression. hail.expr.aggregators.take(expr, n, ordering=None)[source]; Take n records of expr, optionally ordered by ordering.; Examples; Take 3 elements of field X:; >>> table1.aggregate(hl.agg.take(table1.X, 3)); [5, 6, 7]. Take the ID and HT fields, ordered by HT (descending):; >>> table1.aggregate(hl.agg.take(hl.struct(ID=table1.ID, HT=table1.HT),; ... 3,; ... ordering=-table1.HT)); [Struct(ID=2, HT=72), Struct(ID=3, HT=70), Struct(ID=1, HT=65)]. Notes; The resulting array can include fewer than n elements if there are fewer; than n total records.; The ordering argument may be an expression, a function, or None.; If ordering is an expression, this expression’s type should be one with; a natural ordering (e.g. numeric).; If ordering is a function, it will be evaluated on each record of expr; to compute the value used for ordering. In the above example,; ordering=-table1.HT and ordering=lambda x: -x.HT would be; equivalent.; If orde",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:15127,Testability,test,test,15127," ignored (treated as one).; If expr is an expression of type tint32, tint64 or; tbool, then the result is an expression of type; tint64. If expr is an expression of type tfloat32; or tfloat64, then the result is an expression of type; tfloat64. Warning; Boolean values are cast to integers before computing the product. Parameters:; expr (NumericExpression) – Numeric expression. Returns:; Expression of type tint64 or tfloat64 – Product of records of expr. hail.expr.aggregators.fraction(predicate)[source]; Compute the fraction of records where predicate is True.; Examples; Compute the fraction of rows where SEX is “F” and HT > 65:; >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; Missing values for predicate are treated as False. Parameters:; predicate (BooleanExpression) – Boolean predicate. Returns:; Expression of type tfloat64 – Fraction of records where predicate is True. hail.expr.aggregators.hardy_weinberg_test(expr, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; Test each row of a dataset:; >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distri",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:15503,Testability,test,test,15503,":; Expression of type tint64 or tfloat64 – Product of records of expr. hail.expr.aggregators.fraction(predicate)[source]; Compute the fraction of records where predicate is True.; Examples; Compute the fraction of rows where SEX is “F” and HT > 65:; >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; Missing values for predicate are treated as False. Parameters:; predicate (BooleanExpression) – Boolean predicate. Returns:; Expression of type tfloat64 – Fraction of records where predicate is True. hail.expr.aggregators.hardy_weinberg_test(expr, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; Test each row of a dataset:; >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:15837,Testability,test,test,15837,"otes; Missing values for predicate are treated as False. Parameters:; predicate (BooleanExpression) – Boolean predicate. Returns:; Expression of type tfloat64 – Fraction of records where predicate is True. hail.expr.aggregators.hardy_weinberg_test(expr, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; Test each row of a dataset:; >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value.",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:16165,Testability,test,test,16165,"taset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters fi",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:16311,Testability,test,test,16311,"taset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters fi",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:16447,Testability,test,test,16447," This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters field (Set[String]):; >>> dataset.aggregate_rows(hl.agg.explode(lambda elt: hl.agg.collect_as_set(elt), dataset.filters)); set(). Notes; This method can be used with aggregator functions to aggregate the elements; of collection types (tarray and tset). P",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:16617,Testability,test,test,16617,", and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters field (Set[String]):; >>> dataset.aggregate_rows(hl.agg.explode(lambda elt: hl.agg.collect_as_set(elt), dataset.filters)); set(). Notes; This method can be used with aggregator functions to aggregate the elements; of collection types (tarray and tset). Parameters:. f (Function from Expression to Expression) – Aggregation function to apply to each element of the exploded array.; array_agg_expr",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:16721,Testability,test,test,16721,"4) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters field (Set[String]):; >>> dataset.aggregate_rows(hl.agg.explode(lambda elt: hl.agg.collect_as_set(elt), dataset.filters)); set(). Notes; This method can be used with aggregator functions to aggregate the elements; of collection types (tarray and tset). Parameters:. f (Function from Expression to Expression) – Aggregation function to apply to each element of the exploded array.; array_agg_expr (CollectionExpression) – Expression of type tarray or tset. Returns:; Expression – Aggregation express",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:29083,Testability,test,test,29083," R, linreg(y, x = [1, mt.x1, mt.x2]) computes; summary(lm(y ~ x1 + x2)) and; linreg(y, x = [mt.x1, mt.x2], nested_dim=0) computes; summary(lm(y ~ x1 + x2 - 1)).; More generally, nested_dim defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; beta (tarray of tfloat64):; Estimated regression coefficient for each covariate.; standard_error (tarray of tfloat64):; Estimated standard error for each covariate.; t_stat (tarray of tfloat64):; t-statistic for each covariate.; p_value (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.co",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/aggregators.html:34547,Usability,intuit,intuition,34547,"alues to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value values(i) occupying indices ranks(i) (inclusive) to; ranks(i+1) (exclusive).; The returned struct also contains an array _compaction_counts, which is; used internally to support downstream error estimation. Warning; This is an approximate and nondeterministic method. Parameters:. expr (Expression) – Expression to collect.; k (int) – Parameter controlling the accuracy vs. memory usage tradeoff. Returns:; StructExpression – Struct containing values and ranks arrays. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
https://hail.is/docs/0.2/annotation_database_ui.html:1220,Availability,avail,available,1220,"ion Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:.",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
https://hail.is/docs/0.2/annotation_database_ui.html:1818,Availability,avail,available,1818,"erimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:. db = hl.experimental.DB(region='us-central1', cloud='gcp'); mt = db.annotate_rows_db(mt); . name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
https://hail.is/docs/0.2/annotation_database_ui.html:718,Deployability,pipeline,pipelines,718,"﻿. Hail | ; Annotation Database. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the pan",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
https://hail.is/docs/0.2/annotation_database_ui.html:789,Deployability,pipeline,pipeline,789,"﻿. Hail | ; Annotation Database. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the pan",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
https://hail.is/docs/0.2/annotation_database_ui.html:2520,Deployability,update,updated,2520,"erimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:. db = hl.experimental.DB(region='us-central1', cloud='gcp'); mt = db.annotate_rows_db(mt); . name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
https://hail.is/docs/0.2/annotation_database_ui.html:1291,Energy Efficiency,charge,charges,1291,"ion Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:.",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
https://hail.is/docs/0.2/annotation_database_ui.html:656,Security,access,accessible,656,"﻿. Hail | ; Annotation Database. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the pan",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
https://hail.is/docs/0.2/annotation_database_ui.html:1413,Security,access,access,1413,"described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:. db = hl.experimental.DB(region='us-central1', cloud='gcp'); mt = db.annotate_rows_db(mt); . name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
https://hail.is/docs/0.2/annotation_database_ui.html:1718,Security,access,accessed,1718,"erimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:. db = hl.experimental.DB(region='us-central1', cloud='gcp'); mt = db.annotate_rows_db(mt); . name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
https://hail.is/docs/0.2/api.html:4262,Availability,avail,available,4262,"o my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global rando",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:7242,Availability,error,error,7242,"nfiguration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Nam",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:2269,Deployability,configurat,configuration,2269,"mentation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:2582,Deployability,configurat,configuration,2582,"perimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/bu",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:2748,Deployability,configurat,configuration,2748,"one, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:3809,Deployability,pipeline,pipeline,3809,"rk backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, opt",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:4419,Deployability,configurat,configuration,4419,", ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only.",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:5329,Deployability,configurat,configuration,5329,"option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Stor",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:5425,Deployability,configurat,configuration,5425," log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:6802,Deployability,configurat,configuration,6802," is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argumen",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:8969,Deployability,install,installed,8969,"bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Name of a previously loaded reference genome or one of Hail’s built-in; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'. Returns:; ReferenceGenome. hail.set_global_seed(seed)[source]; Deprecated.; Has no effect. To ensure reproducible randomness, use the global_seed; argument to init() and reset_global_randomness().; See the random functions reference docs for more. Parameters:; seed (int) – Integer used to seed Hail’s random number generator. hail.reset_global_randomness()[source]; Restore global randomness to initial state for test reproducibility. hail.citation(*, bibtex=False)[source]; Generate a Hail citation. Parameters:; bibtex (bool) – Generate a citation in BibTeX form. Returns:; str. hail.version()[source]; Get the installed Hail version. Returns:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:9065,Deployability,update,updated,9065,"bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Name of a previously loaded reference genome or one of Hail’s built-in; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'. Returns:; ReferenceGenome. hail.set_global_seed(seed)[source]; Deprecated.; Has no effect. To ensure reproducible randomness, use the global_seed; argument to init() and reset_global_randomness().; See the random functions reference docs for more. Parameters:; seed (int) – Integer used to seed Hail’s random number generator. hail.reset_global_randomness()[source]; Restore global randomness to initial state for test reproducibility. hail.citation(*, bibtex=False)[source]; Generate a Hail citation. Parameters:; bibtex (bool) – Generate a citation in BibTeX form. Returns:; str. hail.version()[source]; Get the installed Hail version. Returns:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:1022,Integrability,interface,interface,1022,"﻿. Hail | ; Hail Query Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Table; GroupedTable; MatrixTable; GroupedMatrixTable. Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions; init(); asc(); desc(); stop(); spark_context(); tmp_dir(); default_reference(); get_reference(); set_global_seed(); reset_global_randomness(); citation(); version(). hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API. View page source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:4596,Integrability,message,messages,4596,"the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defa",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:2145,Modifiability,config,configure,2145," source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl;",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:2269,Modifiability,config,configuration,2269,"mentation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:2582,Modifiability,config,configuration,2582,"perimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/bu",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:2684,Modifiability,variab,variable,2684,"one, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:2716,Modifiability,config,config,2716,"0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not speci",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:2748,Modifiability,config,configuration,2748,"one, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:3484,Modifiability,config,config,3484,"il selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:3534,Modifiability,config,config,3534,"il selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:4419,Modifiability,config,configuration,4419,", ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only.",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:5329,Modifiability,config,configuration,5329,"option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Stor",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:5425,Modifiability,config,configuration,5425," log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:6307,Modifiability,config,configure,6307," – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:6435,Modifiability,config,configure,6435,"onal) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; p",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:6802,Modifiability,config,configuration,6802," is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argumen",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:6843,Modifiability,config,config,6843," highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Return",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:8231,Performance,load,loaded,8231,"bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Name of a previously loaded reference genome or one of Hail’s built-in; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'. Returns:; ReferenceGenome. hail.set_global_seed(seed)[source]; Deprecated.; Has no effect. To ensure reproducible randomness, use the global_seed; argument to init() and reset_global_randomness().; See the random functions reference docs for more. Parameters:; seed (int) – Integer used to seed Hail’s random number generator. hail.reset_global_randomness()[source]; Restore global randomness to initial state for test reproducibility. hail.citation(*, bibtex=False)[source]; Generate a Hail citation. Parameters:; bibtex (bool) – Generate a citation in BibTeX form. Returns:; str. hail.version()[source]; Get the installed Hail version. Returns:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:1059,Security,access,access,1059," Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Table; GroupedTable; MatrixTable; GroupedMatrixTable. Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions; init(); asc(); desc(); stop(); spark_context(); tmp_dir(); default_reference(); get_reference(); set_global_seed(); reset_global_randomness(); citation(); version(). hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API. View page source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:3080,Security,access,accessing,3080,"log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Bac",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:3260,Security,access,accessing,3260,"ou must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] doe",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:1665,Testability,log,log,1665," source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl;",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:4435,Testability,log,log,4435," ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and pyth",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:4467,Testability,log,log,4467," ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and pyth",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:4592,Testability,log,log,4592,"the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defa",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:4648,Testability,log,log,4648," sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:5417,Testability,log,logging,5417," log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:7197,Testability,log,log,7197,"nfiguration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Nam",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/api.html:8767,Testability,test,test,8767,"bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Name of a previously loaded reference genome or one of Hail’s built-in; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'. Returns:; ReferenceGenome. hail.set_global_seed(seed)[source]; Deprecated.; Has no effect. To ensure reproducible randomness, use the global_seed; argument to init() and reset_global_randomness().; See the random functions reference docs for more. Parameters:; seed (int) – Integer used to seed Hail’s random number generator. hail.reset_global_randomness()[source]; Restore global randomness to initial state for test reproducibility. hail.citation(*, bibtex=False)[source]; Generate a Hail citation. Parameters:; bibtex (bool) – Generate a citation in BibTeX form. Returns:; str. hail.version()[source]; Get the installed Hail version. Returns:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
https://hail.is/docs/0.2/batch_api.html:645,Availability,avail,available,645,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
https://hail.is/docs/0.2/batch_api.html:763,Availability,avail,available,763,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
https://hail.is/docs/0.2/batch_api.html:852,Availability,avail,available,852,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
https://hail.is/docs/0.2/batch_api.html:1277,Availability,avail,available,1277,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
https://hail.is/docs/0.2/batch_api.html:873,Deployability,deploy,deploy,873,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
https://hail.is/docs/0.2/batch_api.html:1350,Deployability,update,updated,1350,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
https://hail.is/docs/0.2/batch_api.html:954,Usability,learn,learn,954,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
https://hail.is/docs/0.2/batch_api.html:1149,Usability,learn,learn,1149,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
https://hail.is/docs/0.2/change_log.html:10548,Availability,error,error,10548,"ary version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some da",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:14687,Availability,error,error,14687,"gators. Users may now; import these functions and use them directly.; (#14405); VariantDataset.validate now checks that all ref blocks are no; longer than the ref_block_max_length field, if it exists. Bug Fixes. (#14420) Fixes a; serious, but likely rare, bug in the Table/MatrixTable reader, which; has been present since Sep 2020. It manifests as many (around half or; more) of the rows being dropped. This could only happen when 1); reading a (matrix)table whose partitioning metadata allows rows with; the same key to be split across neighboring partitions, and 2); reading it with a different partitioning than it was written. 1); would likely only happen by reading data keyed by locus and alleles,; and rekeying it to only locus before writing. 2) would likely only; happen by using the _intervals or _n_partitions arguments to; read_(matrix)_table, or possibly repartition. Please reach; out to us if you’re concerned you may have been affected by this.; (#14330) Fixes; erroneous error in export_vcf with unphased haploid Calls.; (#14303) Fix; missingness error when sampling entries from a MatrixTable.; (#14288) Contigs may; now be compared for inquality while filtering rows. Deprecations. (#14386); MatrixTable.make_table is deprecated. Use .localize_entries; instead. Version 0.2.128; Released 2024-02-16; In GCP, the Hail Annotation DB and Datasets API have moved from; multi-regional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotat",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:14763,Availability,error,error,14763,"05); VariantDataset.validate now checks that all ref blocks are no; longer than the ref_block_max_length field, if it exists. Bug Fixes. (#14420) Fixes a; serious, but likely rare, bug in the Table/MatrixTable reader, which; has been present since Sep 2020. It manifests as many (around half or; more) of the rows being dropped. This could only happen when 1); reading a (matrix)table whose partitioning metadata allows rows with; the same key to be split across neighboring partitions, and 2); reading it with a different partitioning than it was written. 1); would likely only happen by reading data keyed by locus and alleles,; and rekeying it to only locus before writing. 2) would likely only; happen by using the _intervals or _n_partitions arguments to; read_(matrix)_table, or possibly repartition. Please reach; out to us if you’re concerned you may have been affected by this.; (#14330) Fixes; erroneous error in export_vcf with unphased haploid Calls.; (#14303) Fix; missingness error when sampling entries from a MatrixTable.; (#14288) Contigs may; now be compared for inquality while filtering rows. Deprecations. (#14386); MatrixTable.make_table is deprecated. Use .localize_entries; instead. Version 0.2.128; Released 2024-02-16; In GCP, the Hail Annotation DB and Datasets API have moved from; multi-regional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:16133,Availability,robust,robust,16133,"ional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remo",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:19254,Availability,error,errors,19254,"ght start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_r",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:19439,Availability,fault,faulty,19439,"ost frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Release",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:20620,Availability,error,errors,20620,"; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change th",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:21022,Availability,reliab,reliably,21022,"Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:21406,Availability,error,error,21406,"ences usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Miss",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:22111,Availability,error,errors,22111,"; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. Wh",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:22243,Availability,error,errors,22243," partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:22817,Availability,error,error,22817,"7) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; c",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:24102,Availability,avail,available,24102,"Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) T",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:25007,Availability,error,errors,25007,") Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enab",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:27155,Availability,degraded,degraded,27155,"or Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:29340,Availability,error,error,29340,"rectly list all files in a directory, not; just the first 1000. This could manifest in an import_table or; import_vcf which used a glob expression. In such a case, only the; first 1000 files would have been included in the resulting Table or; MatrixTable.; (#13550); hl.utils.range_table(n) now supports all valid 32-bit signed; integer values of n.; (#13500) In; Query-on-Batch, the client-side Python code will not try to list; every job when a QoB batch fails. This could take hours for; long-running pipelines or pipelines with many partitions. Deprecations. (#13275) Hail no; longer officially supports Python 3.8.; (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:31677,Availability,error,errors,31677,", call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakine",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:31841,Availability,error,error,31841,"past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:33387,Availability,error,error,33387,"t; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referenc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:33482,Availability,error,error,33482,"t; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referenc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:33652,Availability,error,errors,33652,"nning pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referencing blobs in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entri",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:33809,Availability,error,errors,33809,"of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referencing blobs in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point.; (#12901); hl.Struct now has a correct and useful implementation of; pprint. Version 0.2.115; Rele",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:38605,Availability,error,errors,38605,". (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:39219,Availability,checkpoint,checkpoint,39219,"l pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:39787,Availability,error,error,39787,"g; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendere",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:40497,Availability,error,error,40497,"ow throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:40968,Availability,error,errors,40968,"n and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:41228,Availability,error,error,41228,"ble. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; numbe",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:42890,Availability,failure,failures,42890,"the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; p",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:42962,Availability,error,error,42962,"ngle hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progre",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:43558,Availability,error,errors,43558,"rom the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDatase",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:43649,Availability,error,error,43649,"er generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:43767,Availability,error,error,43767,"https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:44718,Availability,error,errors,44718,"ist_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integer or floating point type.; (#12277) Fixed bug; in reading tables/matrixtables with partition intervals that led to; error or segfault. Version 0.2.101; Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Dep",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:44878,Availability,error,error,44878,"l use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integer or floating point type.; (#12277) Fixed bug; in reading tables/matrixtables with partition intervals that led to; error or segfault. Version 0.2.101; Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:47667,Availability,error,error,47667,"2062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries much faster by caching compiler data; structures.; (#12038) Speed up; hl.import_matrix_table by caching header line computation. Bug fixes. (#12115) When using; use_new_shuffle=True, fix a bug when there are more than 2^31; rows; (#12074) Fix bug; where hl.init could silently overwrite the global random seed.; (#12079) Fix bug in; handling of missing (aka NA) fields in grouped aggregation and; distinct by key.; (#12056) Fix; hl.export_vcf to actually create tabix files when requested.; (#12020) Fix bug in; hl.experimental.densify which manifested as an AssertionError; about dtypes. Version 0.2.97; Released 2022-06-30. New Features. (#11756); hb.BatchPoolExecutor and Python jobs both now also support async; functions. Bug fixes. (#11962) Fix error; (logged as (#11891)); in VCF combiner when exactly 10 or 100 files are combined.; (#11969) Fix; import_table and import_lines to use multiple partitions when; force_bgz is used.; (#11964) Fix; erroneous “Bucket is a requester pays bucket but no user project; provided.” errors in Google Dataproc by updating to the latest; Dataproc image version. Version 0.2.96; Released 2022-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#118",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:47944,Availability,error,errors,47944," hl.import_matrix_table by caching header line computation. Bug fixes. (#12115) When using; use_new_shuffle=True, fix a bug when there are more than 2^31; rows; (#12074) Fix bug; where hl.init could silently overwrite the global random seed.; (#12079) Fix bug in; handling of missing (aka NA) fields in grouped aggregation and; distinct by key.; (#12056) Fix; hl.export_vcf to actually create tabix files when requested.; (#12020) Fix bug in; hl.experimental.densify which manifested as an AssertionError; about dtypes. Version 0.2.97; Released 2022-06-30. New Features. (#11756); hb.BatchPoolExecutor and Python jobs both now also support async; functions. Bug fixes. (#11962) Fix error; (logged as (#11891)); in VCF combiner when exactly 10 or 100 files are combined.; (#11969) Fix; import_table and import_lines to use multiple partitions when; force_bgz is used.; (#11964) Fix; erroneous “Bucket is a requester pays bucket but no user project; provided.” errors in Google Dataproc by updating to the latest; Dataproc image version. Version 0.2.96; Released 2022-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_re",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:48337,Availability,error,error,48337,"port_vcf to actually create tabix files when requested.; (#12020) Fix bug in; hl.experimental.densify which manifested as an AssertionError; about dtypes. Version 0.2.97; Released 2022-06-30. New Features. (#11756); hb.BatchPoolExecutor and Python jobs both now also support async; functions. Bug fixes. (#11962) Fix error; (logged as (#11891)); in VCF combiner when exactly 10 or 100 files are combined.; (#11969) Fix; import_table and import_lines to use multiple partitions when; force_bgz is used.; (#11964) Fix; erroneous “Bucket is a requester pays bucket but no user project; provided.” errors in Google Dataproc by updating to the latest; Dataproc image version. Version 0.2.96; Released 2022-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; h",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:48594,Availability,error,error,48594,"th now also support async; functions. Bug fixes. (#11962) Fix error; (logged as (#11891)); in VCF combiner when exactly 10 or 100 files are combined.; (#11969) Fix; import_table and import_lines to use multiple partitions when; force_bgz is used.; (#11964) Fix; erroneous “Bucket is a requester pays bucket but no user project; provided.” errors in Google Dataproc by updating to the latest; Dataproc image version. Version 0.2.96; Released 2022-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:49084,Availability,toler,tolerance,49084,"22-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:49440,Availability,error,error,49440,"notate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:49540,Availability,error,error,49540,"ption strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52230,Availability,error,error,52230,"also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix erro",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52606,Availability,error,error,52606,"nsecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52933,Availability,error,error,52933,"from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:53202,Availability,error,errors,53202,6) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:53261,Availability,error,error,53261, a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated;,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:53754,Availability,failure,failure,53754,"1401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:56098,Availability,error,error,56098,"be able to read tables or matrix tables written by this; version of Hail. Version 0.2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequen",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:56198,Availability,down,down,56198,".2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile i",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:56287,Availability,error,error,56287,"rabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:56853,Availability,error,errors,56853,"ed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the tabl",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:57309,Availability,error,errors,57309,"on’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bu",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:58335,Availability,error,errors,58335,"ssTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Re",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:58425,Availability,error,error,58425,"tarting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) I",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:60991,Availability,error,error,60991,"Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now;",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:61169,Availability,error,error,61169,"is; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the d",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:61521,Availability,error,error,61521,"een iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performa",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:62875,Availability,error,error,62875,"; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performance of hl.experimental.densify by approximately 35%. Version 0.2.61; Released 2020-12-03. New features. (#9749) Add or_error; method to SwitchBuilder (hl.switch). Bug fixes. (#9775) Fixed race; condition leading to invalid intermediate files in VCF combiner.; (#9751) Fix bug where; constructing an array of empty structs causes type error.; (#9731) Fix error and; incorrect behavior when using hl.import_matrix_table with int64; data types. Version 0.2.60; Released 2020-11-16. New features. (#9696); hl.experimental.export_elasticsearch will now support; Elasticsearch versions 6.8 - 7.x by default. Bug fixes. (#9641) Showing hail; ndarray data now always prints in correct order. hailctl dataproc. (#9610) Support; interval fields in hailctl dataproc describe. Version 0.2.59; Released 2020-10-22. Datasets / Annotation DB. (#9605) The Datasets; API and the Annotation Database now support AWS, and users are; required to specify what cloud platform they’re using. hailctl dataproc. (#9609) Fixed bug; where hailctl dataproc modify did not correctly print; corresponding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache siz",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:62895,Availability,error,error,62895," unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performance of hl.experimental.densify by approximately 35%. Version 0.2.61; Released 2020-12-03. New features. (#9749) Add or_error; method to SwitchBuilder (hl.switch). Bug fixes. (#9775) Fixed race; condition leading to invalid intermediate files in VCF combiner.; (#9751) Fix bug where; constructing an array of empty structs causes type error.; (#9731) Fix error and; incorrect behavior when using hl.import_matrix_table with int64; data types. Version 0.2.60; Released 2020-11-16. New features. (#9696); hl.experimental.export_elasticsearch will now support; Elasticsearch versions 6.8 - 7.x by default. Bug fixes. (#9641) Showing hail; ndarray data now always prints in correct order. hailctl dataproc. (#9610) Support; interval fields in hailctl dataproc describe. Version 0.2.59; Released 2020-10-22. Datasets / Annotation DB. (#9605) The Datasets; API and the Annotation Database now support AWS, and users are; required to specify what cloud platform they’re using. hailctl dataproc. (#9609) Fixed bug; where hailctl dataproc modify did not correctly print; corresponding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:64752,Availability,error,errors,64752,"park 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9474) Add; ArrayExpression.first and ArrayExpression.last.; (#9459) Add; StringExpression.join, an analogue to Python’s str.join.; (#9398) Hail will now; throw HailUserErrors if the or_error branch of a; CaseBuilder is hit. Bug fixes. (#9503) NDArrays can; now hold arbitrary data types, though only ndarrays of primitives can; be collected to Python.; (#9501) Remove memory; leak in BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9424); hl.experimental.writeBlockMatrices didn’t correctly support; overwrite flag. Performance improvements. (#9506); hl.agg.ndarray_sum will now do a tree aggregation. hailctl dataproc. (#9502) Fix hailctl; dataproc modify to install dependencies of the wheel file.; (#9420) Add; --debug-mode flag to hailctl dataproc start. This will enable; heap dumps on OOM errors.; (#9520) Add support; for requester pays buckets to hailctl dataproc describe. Deprecations. (#9482); ArrayExpression.head has been deprecated in favor of; ArrayExpression.first. Version 0.2.57; Released 2020-09-03. New features. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:65823,Availability,checkpoint,checkpoint,65823,"ns. (#9482); ArrayExpression.head has been deprecated in favor of; ArrayExpression.first. Version 0.2.57; Released 2020-09-03. New features. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed err",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:66775,Availability,error,error,66775,"-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarr",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:66846,Availability,error,error,66846,"ession scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndar",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:67263,Availability,error,error,67263," version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Released 2020-07-08. Bug fixes. (#9058) Fixed memory; leak affecting Table.aggregate, MatrixTable.annotate_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.ag",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:68825,Availability,error,error,68825,"} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Released 2020-07-08. Bug fixes. (#9058) Fixed memory; leak affecting Table.aggregate, MatrixTable.annotate_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.agg.linreg with no aggregated data records.; (#9028) Fixed memory; leak affecting Table.annotate with scans,; hl.experimental.densify, and Table.group_by / aggregate.; (#8978) Fixed; aggregation behavior of; MatrixTable.{group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:69139,Availability,error,error,69139,"e_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.agg.linreg with no aggregated data records.; (#9028) Fixed memory; leak affecting Table.annotate with scans,; hl.experimental.densify, and Table.group_by / aggregate.; (#8978) Fixed; aggregation behavior of; MatrixTable.{group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocur",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:69916,Availability,failure,failures,69916,"al; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured co",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71160,Availability,error,error,71160,"ling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which con",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71228,Availability,error,error,71228,"ClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Incre",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71575,Availability,error,errors,71575,"t GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentati",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71641,Availability,error,errors,71641,"nt of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify t",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:72465,Availability,reliab,reliability,72465,"8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compile",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:73206,Availability,fault,fault,73206,object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-parti,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:73212,Availability,toler,tolerant,73212,object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-parti,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:73458,Availability,error,error,73458,r improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:73551,Availability,error,error,73551,"tion to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsist",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:73695,Availability,error,error,73695," requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_ag",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:73942,Availability,error,error,73942," was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% over",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:74004,Availability,error,error,74004," was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% over",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:74579,Availability,error,error,74579,"ror for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message co",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:75537,Availability,error,error,75537,"onsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:75688,Availability,down,downsample,75688,") Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixe",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:76920,Availability,failure,failures,76920," be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused b",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:76949,Availability,error,error,76949," be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused b",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:77797,Availability,error,error,77797," Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compres",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:77882,Availability,error,error,77882,"where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few ent",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:78074,Availability,error,error,78074,"ed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance r",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:78459,Availability,error,errors,78459,"l.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:79306,Availability,error,error,79306,"020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in l",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:79362,Availability,error,error,79362,"n to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Gene",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:80197,Availability,error,errors,80197,"peared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to mis",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:80478,Availability,down,down,80478,"l and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailct",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:80861,Availability,error,error,80861,"Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:81992,Availability,failure,failure,81992,"1-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailctl dataproc. (#7460) The Spark; monitor widget now automatically collapses after a job completes. Version 0.2.26; Released 2019-10-24. New Features. (#7325) Add; string.reverse function.; (#7328) Add; string.translate function.; (#7344) Add; hl.reverse_complement function.; (#7306) Teach the VCF; combiner to handle allele specific (AS_*) fields.; (#7346) Add; hl.agg.approx_median function. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:83756,Availability,checkpoint,checkpoints,83756,"ve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:84683,Availability,error,error,84683,"#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:84745,Availability,down,downstream,84745,"ies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and somet",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:85158,Availability,error,error,85158,"3. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.;",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:86849,Availability,error,errors,86849,"seful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; pro",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:87073,Availability,checkpoint,checkpoint,87073,"d performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:88522,Availability,error,error,88522,"on 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:91843,Availability,failure,failures,91843,"9)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it pos",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:94637,Availability,down,downgrade,94637,"or point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Ad",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:95906,Availability,failure,failure,95906,"rk 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail now uses; the correct unicode string encoding which resolves a number of issues; when a Table or MatrixTable has a key field containing unicode; characters.; (#5692) When; keyed is True, ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:95974,Availability,error,error,95974,"rk 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail now uses; the correct unicode string encoding which resolves a number of issues; when a Table or MatrixTable has a key field containing unicode; characters.; (#5692) When; keyed is True, ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:98616,Availability,checkpoint,checkpoint,98616,"nctional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer joins on; MatrixTables. Version 0.2.11; Released 2019-03-06. New features. (#5374) Add default; arguments to hl.add_sequence for running on GCP.; (#5481) Added; sample_cols method to MatrixTable.; (#5501) Exposed; MatrixTable.unfilter_entries. See filter_entries; documentation for more information.; (#5480) Added; n_cols argument to MatrixTable.head.; (#5529) Added; Table.{semi_join, anti_join} and; MatrixTable.{semi_join_rows, semi_join_cols, anti_join_rows, anti_join_cols}.; (#5528) Added; {MatrixTable, Table}.checkpoint methods as wrappers around; write / read_{matrix_table, table}. Bug fixes. (#5416) Resolved; issue wherein VEP and certain regressions were recomputed on each; use, rather than once.; (#5419) Resolved; issue with import_vcf force_bgz and file size checks.; (#5427) Resolved; issue with Table.show and dictionary field types.; (#5468) Resolved; ordering problem with Expression.show on key fields that are not; the first key.; (#5492) Fixed; hl.agg.collect crashing when collecting float32 values.; (#5525) Fixed; hl.trio_matrix crashing when complete_trios is False. Version 0.2.10; Released 2019-02-15. New features. (#5272) Added a new; ‘delimiter’ option to Table.export.; (#5251) Add utility; aliases to hl.plot for output_notebook and show.; (#5249) Add; histogram2d function to hl.plot module.; (#5247) Expose; MatrixTable.localize_entries method for converting to a Table; with an entries array.; (#5300) Add new; filter and find_replace arguments to hl.import_table and; hl.import_vcf to apply regex and substitutio",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:101211,Availability,error,error,101211,"iers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockM",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:102024,Availability,error,error,102024,"ausing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:102907,Availability,error,error,102907,"eparator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); F",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103071,Availability,error,error,103071,"rtain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103104,Availability,toler,tolerance,103104,"rtain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103133,Availability,failure,failure,103133,"rtain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103162,Availability,error,error,103162,"New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103414,Availability,error,errors,103414,"ion_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103569,Availability,error,error,103569," (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating infor",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104061,Availability,error,errors,104061,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104154,Availability,error,error,104154,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104205,Availability,error,error,104205,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104376,Availability,error,error,104376,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104593,Availability,avail,available,104593,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:7642,Deployability,release,released,7642," Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:7759,Deployability,release,released,7759,"es. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:8476,Deployability,release,release,8476,"sion Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; librar",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:9186,Deployability,release,release,9186,"ts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:9485,Deployability,release,released,9485,"s every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather th",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:11095,Deployability,pipeline,pipelines,11095,"ritically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:11234,Deployability,pipeline,pipelines,11234,"h; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:11299,Deployability,configurat,configuration,11299,"submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:11344,Deployability,pipeline,pipelines,11344,"submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:11700,Deployability,install,installed,11700," gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:11745,Deployability,upgrade,upgraded,11745," gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12140,Deployability,update,update,12140,"ory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matr",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12172,Deployability,install,installing,12172,"ory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matr",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12212,Deployability,install,install-gcs-connector,12212,"n-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12260,Deployability,update,update,12260,"ssions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12274,Deployability,install,installing,12274,"ssions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12361,Deployability,configurat,configuration,12361,".131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12460,Deployability,upgrade,upgrade,12460,"ort stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may choose their own version; of protobuf.; (#14360) Exposed; previous",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:16277,Deployability,pipeline,pipeline,16277," your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""g",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:17814,Deployability,upgrade,upgrade,17814,"hElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#1",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:18150,Deployability,configurat,configuration,18150,"Table tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, you",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:18984,Deployability,pipeline,pipelines,18984,"ers using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:19145,Deployability,install,installation,19145,"e passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:19175,Deployability,install,installation,19175,"e passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:21314,Deployability,pipeline,pipelines,21314,".default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:21687,Deployability,pipeline,pipelines,21687,"rays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#1375",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:22166,Deployability,pipeline,pipelines,22166," partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:22519,Deployability,update,update,22519,"hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_req",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:23784,Deployability,install,installation,23784," Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bu",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:24634,Deployability,pipeline,pipelines,24634,"filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Rele",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:24809,Deployability,pipeline,pipelines,24809,"f the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields para",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:25631,Deployability,configurat,configuration,25631,"tch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:25901,Deployability,install,install-completion,25901,"; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This sho",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:25927,Deployability,install,install,25927,"; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This sho",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:26945,Deployability,pipeline,pipelines,26945," already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_fro",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:27530,Deployability,pipeline,pipelines,27530,"relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:28934,Deployability,pipeline,pipelines,28934,"ctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an import_table or; import_vcf which used a glob expression. In such a case, only the; first 1000 files would have been included in the resulting Table or; MatrixTable.; (#13550); hl.utils.range_table(n) now supports all valid 32-bit signed; integer values of n.; (#13500) In; Query-on-Batch, the client-side Python code will not try to list; every job when a QoB batch fails. This could take hours for; long-running pipelines or pipelines with many partitions. Deprecations. (#13275) Hail no; longer officially supports Python 3.8.; (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:28947,Deployability,pipeline,pipelines,28947,"ctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an import_table or; import_vcf which used a glob expression. In such a case, only the; first 1000 files would have been included in the resulting Table or; MatrixTable.; (#13550); hl.utils.range_table(n) now supports all valid 32-bit signed; integer values of n.; (#13500) In; Query-on-Batch, the client-side Python code will not try to list; every job when a QoB batch fails. This could take hours for; long-running pipelines or pipelines with many partitions. Deprecations. (#13275) Hail no; longer officially supports Python 3.8.; (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:31601,Deployability,pipeline,pipelines,31601,", call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakine",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:31856,Deployability,pipeline,pipelines,31856,"past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:32579,Deployability,pipeline,pipelines,32579,"#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blo",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:34496,Deployability,release,release,34496," diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referencing blobs in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point.; (#12901); hl.Struct now has a correct and useful implementation of; pprint. Version 0.2.115; Released 2023-04-25. New Features. (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This can be used by; import hailtop.fs as hfs but has also replaced the underlying; implementation of the hl.hadoop_* methods. This means that the; hl.hadoop_* methods now support these additional blob storage; providers.; (#12917) ABS blob; URIs in the form of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported when running in Azure. Deprecations. (#12917) The; hail-az scheme for referencing ABS blobs in Azure is deprecated; in fa",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:35500,Deployability,release,release,35500,"s in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point.; (#12901); hl.Struct now has a correct and useful implementation of; pprint. Version 0.2.115; Released 2023-04-25. New Features. (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This can be used by; import hailtop.fs as hfs but has also replaced the underlying; implementation of the hl.hadoop_* methods. This means that the; hl.hadoop_* methods now support these additional blob storage; providers.; (#12917) ABS blob; URIs in the form of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported when running in Azure. Deprecations. (#12917) The; hail-az scheme for referencing ABS blobs in Azure is deprecated; in favor of the https scheme and will be removed in a future; release. Bug Fixes. (#12919) An; interactive hail session is no longer unusable after hitting CTRL-C; during a batch execution in Query-on-Batch; (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarray",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:35961,Deployability,patch,patch,35961,"s can be used by; import hailtop.fs as hfs but has also replaced the underlying; implementation of the hl.hadoop_* methods. This means that the; hl.hadoop_* methods now support these additional blob storage; providers.; (#12917) ABS blob; URIs in the form of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported when running in Azure. Deprecations. (#12917) The; hail-az scheme for referencing ABS blobs in Azure is deprecated; in favor of the https scheme and will be removed in a future; release. Bug Fixes. (#12919) An; interactive hail session is no longer unusable after hitting CTRL-C; during a batch execution in Query-on-Batch; (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc j",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:36643,Deployability,pipeline,pipelines,36643," bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:37163,Deployability,release,release,37163,"04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also ad",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:37312,Deployability,pipeline,pipelines,37312,"ws.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.M",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:38816,Deployability,pipeline,pipeline,38816," now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; err",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:42272,Deployability,update,updated,42272,"g; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:43054,Deployability,configurat,configuration,43054,"ction.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Rele",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:45164,Deployability,pipeline,pipelines,45164,"tures. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integer or floating point type.; (#12277) Fixed bug; in reading tables/matrixtables with partition intervals that led to; error or segfault. Version 0.2.101; Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:45902,Deployability,install,install-on-cluster,45902,"Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries muc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:46011,Deployability,pipeline,pipelines,46011,"andas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries much faster by caching compiler data; structures.; (#12038) Speed up; hl.import_matrix_table by caching hea",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:49025,Deployability,configurat,configuration,49025,"22-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:49903,Deployability,toggle,toggle,49903,"er batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This versi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:50454,Deployability,release,release,50454," 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This version of Hail includes all the necessary client-side; infrastructure to execute Hail Query pipelines on a Hail Batch; cluster. This effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulip",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:51006,Deployability,pipeline,pipelines,51006,"er images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This version of Hail includes all the necessary client-side; infrastructure to execute Hail Query pipelines on a Hail Batch; cluster. This effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52499,Deployability,release,release,52499,"1614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52521,Deployability,deploy,deploy,52521,"1614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52549,Deployability,release,release,52549,"to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52619,Deployability,deploy,deploy,52619,"nsecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52659,Deployability,release,release,52659,"nsecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52702,Deployability,release,release,52702,"ix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix b",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52876,Deployability,pipeline,pipelines,52876,"from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:57155,Deployability,pipeline,pipelines,57155,"ey field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays w",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:57643,Deployability,pipeline,pipelines,57643,"ling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes.",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:60931,Deployability,pipeline,pipelines,60931,"Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now;",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:64617,Deployability,install,install,64617,"onding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9474) Add; ArrayExpression.first and ArrayExpression.last.; (#9459) Add; StringExpression.join, an analogue to Python’s str.join.; (#9398) Hail will now; throw HailUserErrors if the or_error branch of a; CaseBuilder is hit. Bug fixes. (#9503) NDArrays can; now hold arbitrary data types, though only ndarrays of primitives can; be collected to Python.; (#9501) Remove memory; leak in BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9424); hl.experimental.writeBlockMatrices didn’t correctly support; overwrite flag. Performance improvements. (#9506); hl.agg.ndarray_sum will now do a tree aggregation. hailctl dataproc. (#9502) Fix hailctl; dataproc modify to install dependencies of the wheel file.; (#9420) Add; --debug-mode flag to hailctl dataproc start. This will enable; heap dumps on OOM errors.; (#9520) Add support; for requester pays buckets to hailctl dataproc describe. Deprecations. (#9482); ArrayExpression.head has been deprecated in favor of; ArrayExpression.first. Version 0.2.57; Released 2020-09-03. New features. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to h",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:65980,Deployability,update,update,65980,"tures. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key beh",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:69928,Deployability,pipeline,pipelines,69928,"al; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured co",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:70838,Deployability,pipeline,pipelines,70838,"el text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matm",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71938,Deployability,configurat,configuration,71938,"zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bu",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:72527,Deployability,install,installation,72527,"le JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:72697,Deployability,install,installation,72697,"ting by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read fro",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:73667,Deployability,pipeline,pipeline,73667," requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_ag",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:74237,Deployability,pipeline,pipelines,74237,"ersion 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233);",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:74290,Deployability,update,update,74290,"; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as o",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:75900,Deployability,install,installation,75900,"odegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:77592,Deployability,pipeline,pipelines,77592,"s written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:78474,Deployability,pipeline,pipelines,78474,"l.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:78542,Deployability,configurat,configuration,78542,"stically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Releas",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:81108,Deployability,update,update,81108,"r read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:83904,Deployability,pipeline,pipelines,83904,"ables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where li",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:84199,Deployability,update,update,84199,"ons (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:84570,Deployability,install,installed,84570,"ld.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log stat",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:84607,Deployability,configurat,configuration,84607,"ld.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log stat",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:86170,Deployability,pipeline,pipelines,86170," mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:87483,Deployability,update,update-hail-version,87483,"7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:89345,Deployability,pipeline,pipelines,89345," Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic c",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:89419,Deployability,pipeline,pipeline,89419,"ow(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls t",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:89532,Deployability,update,update,89532,"rror caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generat",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:90385,Deployability,pipeline,pipelines,90385,"ls. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for no",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:92085,Deployability,release,releases,92085," the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:92129,Deployability,install,installations,92129,"twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to searc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:92287,Deployability,release,release,92287,"e full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly wor",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:93331,Deployability,pipeline,pipelines,93331,"32)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the en",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:94538,Deployability,patch,patch,94538," pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:94544,Deployability,update,update,94544," pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:94592,Deployability,install,installs,94592,"ers where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first inde",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:94668,Deployability,upgrade,upgrade,94668,"mples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first elemen",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:95046,Deployability,update,update,95046,"atten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:95258,Deployability,pipeline,pipelines,95258,"related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886);",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:100809,Deployability,pipeline,pipelines,100809,"xporting missing genotypes without trailing; fields. Bug fixes. (#5306) Fix; ReferenceGenome.add_sequence causing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; opti",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104097,Deployability,pipeline,pipelines,104097,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104660,Deployability,update,updated,104660,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:26929,Energy Efficiency,reduce,reduce,26929," already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_fro",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:27096,Energy Efficiency,reduce,reduce,27096,"or Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:29746,Energy Efficiency,schedul,schedule,29746,"ble(n) now supports all valid 32-bit signed; integer values of n.; (#13500) In; Query-on-Batch, the client-side Python code will not try to list; every job when a QoB batch fails. This could take hours for; long-running pipelines or pipelines with many partitions. Deprecations. (#13275) Hail no; longer officially supports Python 3.8.; (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symme",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:33600,Energy Efficiency,reduce,reduce,33600,"nning pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referencing blobs in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entri",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:66134,Energy Efficiency,monitor,monitoring,66134,"ed 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; t",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:75966,Energy Efficiency,charge,charges,75966,"ble spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:80381,Energy Efficiency,efficient,efficient,80381,"unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version(",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:82258,Energy Efficiency,monitor,monitor,82258,"t.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailctl dataproc. (#7460) The Spark; monitor widget now automatically collapses after a job completes. Version 0.2.26; Released 2019-10-24. New Features. (#7325) Add; string.reverse function.; (#7328) Add; string.translate function.; (#7344) Add; hl.reverse_complement function.; (#7306) Teach the VCF; combiner to handle allele specific (AS_*) fields.; (#7346) Add; hl.agg.approx_median function. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the uns",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:85603,Energy Efficiency,allocate,allocated,85603,"lled packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl datap",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:90114,Energy Efficiency,reduce,reduces,90114,"e UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantile",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:10117,Integrability,depend,depend,10117,"ped to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that constru",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:13356,Integrability,depend,depends,13356,"at prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may choose their own version; of protobuf.; (#14360) Exposed; previously internal _num_allele_type as numeric_allele_type; and deprecated it. Add new AlleleType enumeration for users to be; able to easily use the values returned by numeric_allele_type.; (#14297); vds.sample_gc now uses independent aggregators. Users may now; import these functions and use them directly.; (#14405); VariantDataset.validate now checks that all ref blocks are no; longer than the ref_block_max_length field, if it exists. Bug Fixes. (#14420) Fixes a; serious, but likely rare, bug in the Table/MatrixTable reader, which; has been present since Sep 2020. It manifests as many (around half or; more) of the rows being dropped. This could only happen when 1); reading a (matrix)table whose partitioning metadata allows rows with; the same key to be split across neighboring partitions, and 2); reading it with a different partitioning than it was written. 1); would likely only happen by reading data keyed by loc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:17718,Integrability,message,message,17718,"able.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable way",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:21412,Integrability,message,message,21412,"ences usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Miss",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:21499,Integrability,depend,dependent,21499,"ug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:22384,Integrability,message,message,22384,"; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:32728,Integrability,message,messages,32728,"for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:32832,Integrability,message,messages,32832,"for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:33393,Integrability,message,message,33393,"t; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referenc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:38723,Integrability,depend,dependency,38723,"Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop*",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:39793,Integrability,message,message,39793,"g; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendere",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:45936,Integrability,message,message,45936,"Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries muc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:51964,Integrability,depend,dependency,51964," effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) New",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52007,Integrability,depend,depends,52007," effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) New",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:54593,Integrability,interface,interface,54593," that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ndarrays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:55701,Integrability,protocol,protocols,55701,"rays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tabl",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:58431,Integrability,message,messages,58431,"tarting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) I",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:61175,Integrability,message,message,61175,"is; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the d",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:61527,Integrability,message,message,61527,"een iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performa",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:64625,Integrability,depend,dependencies,64625,"onding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9474) Add; ArrayExpression.first and ArrayExpression.last.; (#9459) Add; StringExpression.join, an analogue to Python’s str.join.; (#9398) Hail will now; throw HailUserErrors if the or_error branch of a; CaseBuilder is hit. Bug fixes. (#9503) NDArrays can; now hold arbitrary data types, though only ndarrays of primitives can; be collected to Python.; (#9501) Remove memory; leak in BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9424); hl.experimental.writeBlockMatrices didn’t correctly support; overwrite flag. Performance improvements. (#9506); hl.agg.ndarray_sum will now do a tree aggregation. hailctl dataproc. (#9502) Fix hailctl; dataproc modify to install dependencies of the wheel file.; (#9420) Add; --debug-mode flag to hailctl dataproc start. This will enable; heap dumps on OOM errors.; (#9520) Add support; for requester pays buckets to hailctl dataproc describe. Deprecations. (#9482); ArrayExpression.head has been deprecated in favor of; ArrayExpression.first. Version 0.2.57; Released 2020-09-03. New features. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to h",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:66558,Integrability,interface,interface,66558,"nd BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#916",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:66781,Integrability,message,messages,66781,"-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarr",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:67034,Integrability,depend,dependency,67034,"; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Rel",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:69515,Integrability,depend,dependencies,69515,"group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:74011,Integrability,message,message,74011," was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% over",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:75544,Integrability,message,message,75544,"onsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:76956,Integrability,message,message,76956," be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused b",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:80430,Integrability,wrap,wrapping,80430,"l and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailct",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:84164,Integrability,depend,dependencies,84164,"ons (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:91223,Integrability,message,message,91223,"agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:93489,Integrability,depend,dependencies,93489,"which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:95121,Integrability,depend,dependency,95121,"e; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes.",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:95981,Integrability,message,message,95981,"rk 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail now uses; the correct unicode string encoding which resolves a number of issues; when a Table or MatrixTable has a key field containing unicode; characters.; (#5692) When; keyed is True, ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:98638,Integrability,wrap,wrappers,98638,"nctional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer joins on; MatrixTables. Version 0.2.11; Released 2019-03-06. New features. (#5374) Add default; arguments to hl.add_sequence for running on GCP.; (#5481) Added; sample_cols method to MatrixTable.; (#5501) Exposed; MatrixTable.unfilter_entries. See filter_entries; documentation for more information.; (#5480) Added; n_cols argument to MatrixTable.head.; (#5529) Added; Table.{semi_join, anti_join} and; MatrixTable.{semi_join_rows, semi_join_cols, anti_join_rows, anti_join_cols}.; (#5528) Added; {MatrixTable, Table}.checkpoint methods as wrappers around; write / read_{matrix_table, table}. Bug fixes. (#5416) Resolved; issue wherein VEP and certain regressions were recomputed on each; use, rather than once.; (#5419) Resolved; issue with import_vcf force_bgz and file size checks.; (#5427) Resolved; issue with Table.show and dictionary field types.; (#5468) Resolved; ordering problem with Expression.show on key fields that are not; the first key.; (#5492) Fixed; hl.agg.collect crashing when collecting float32 values.; (#5525) Fixed; hl.trio_matrix crashing when complete_trios is False. Version 0.2.10; Released 2019-02-15. New features. (#5272) Added a new; ‘delimiter’ option to Table.export.; (#5251) Add utility; aliases to hl.plot for output_notebook and show.; (#5249) Add; histogram2d function to hl.plot module.; (#5247) Expose; MatrixTable.localize_entries method for converting to a Table; with an entries array.; (#5300) Add new; filter and find_replace arguments to hl.import_table and; hl.import_vcf to apply regex and substitutio",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:100696,Integrability,interface,interface,100696,"xporting missing genotypes without trailing; fields. Bug fixes. (#5306) Fix; ReferenceGenome.add_sequence causing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; opti",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103168,Integrability,message,message,103168,"New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103401,Integrability,message,messages,103401,"ion_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103575,Integrability,message,message,103575," (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating infor",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:11299,Modifiability,config,configuration,11299,"submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12361,Modifiability,config,configuration,12361,".131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:15926,Modifiability,config,config,15926,"ies; instead. Version 0.2.128; Released 2024-02-16; In GCP, the Hail Annotation DB and Datasets API have moved from; multi-regional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output pa",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:18150,Modifiability,config,configuration,18150,"Table tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, you",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:21031,Modifiability,rewrite,rewrite,21031,"Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:21509,Modifiability,variab,variable,21509,"ug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:23551,Modifiability,config,config,23551,"te to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.rende",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:23558,Modifiability,variab,variable,23558,"te to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.rende",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:25631,Modifiability,config,configuration,25631,"tch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:25824,Modifiability,config,config,25824,"ommit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:41366,Modifiability,config,config,41366,"ail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for detai",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:41913,Modifiability,variab,variable,41913,"y leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but de",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:43054,Modifiability,config,configuration,43054,"ction.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Rele",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:49025,Modifiability,config,configuration,49025,"22-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:70907,Modifiability,config,configured,70907,"ce_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configura",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71938,Modifiability,config,configuration,71938,"zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bu",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71985,Modifiability,config,configure,71985,"aproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:75879,Modifiability,config,configure,75879,"odegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:78542,Modifiability,config,configuration,78542,"stically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Releas",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:84607,Modifiability,config,configuration,84607,"ld.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log stat",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:93537,Modifiability,rewrite,rewrite,93537,"og; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broke",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:97946,Modifiability,extend,extend,97946,"_independent_set now does not; produce duplicates.; (#5725) Docs now; consistently refer to hl.agg not agg.; (#5730)(#5782); Taught import_bgen to optimize its variants argument. Experimental. (#5732) The; hl.agg.approx_quantiles aggregate computes an approximation of; the quantiles of an expression.; (#5693)(#5396); Table._multi_way_zip_join now correctly handles keys that have; been truncated. Version 0.2.12; Released 2019-03-28. New features. (#5614) Add support; for multiple missing values in hl.import_table.; (#5666) Produce HTML; table output for Table.show() when running in Jupyter notebook. Bug fixes. (#5603)(#5697); Fixed issue where min_partitions on hl.import_table was; non-functional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer joins on; MatrixTables. Version 0.2.11; Released 2019-03-06. New features. (#5374) Add default; arguments to hl.add_sequence for running on GCP.; (#5481) Added; sample_cols method to MatrixTable.; (#5501) Exposed; MatrixTable.unfilter_entries. See filter_entries; documentation for more information.; (#5480) Added; n_cols argument to MatrixTable.head.; (#5529) Added; Table.{semi_join, anti_join} and; MatrixTable.{semi_join_rows, semi_join_cols, anti_join_rows, anti_join_cols}.; (#5528) Added; {MatrixTable, Table}.checkpoint methods as wrappers around; write / read_{matrix_table, table}. Bug fixes. (#5416) Resolved; issue wherein VEP and certain regressions were recomputed on each; use, rather than once.; (#5419) Resolved; issue with import_vcf force_bgz and file size checks.; (#5427) Resolved; issue with Table.show and dictionary field types.; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:102890,Modifiability,variab,variable,102890,"eparator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); F",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:3693,Performance,perform,performance,3693,ckMatrix from_numpy correctness bug; Bug fixes; Versioning. Version 0.2.89; Version 0.2.88; Version 0.2.87; Bug fixes. Version 0.2.86; Bug fixes; Performance improvements. Version 0.2.85; Bug fixes; New features. Version 0.2.84; Bug fixes; New features. Version 0.2.83; Bug fixes; New features; hailctl dataproc. Version 0.2.82; Bug fixes; New features; Performance Improvements; Python and Java Support; File Format. Version 0.2.81; hailctl dataproc. Version 0.2.80; New features; hailctl dataproc. Version 0.2.79; Bug fixes; New features. Version 0.2.78; Bug fixes; New features; Performance Improvements. Version 0.2.77; Bug fixes. Version 0.2.76; Bug fixes. Version 0.2.75; Bug fixes; New features; Performance improvements. Version 0.2.74; Bug fixes. Version 0.2.73; Bug fixes. Version 0.2.72; New Features; Bug fixes. Version 0.2.71; New Features; Bug fixes; hailctl dataproc. Version 0.2.70; Version 0.2.69; New Features; Bug fixes; hailctl dataproc. Version 0.2.68; Version 0.2.67; Critical performance fix. Version 0.2.66; New features. Version 0.2.65; Default Spark Version Change; New features; Performance improvements; Bug fixes. Version 0.2.64; New features; Bug fixes. Version 0.2.63; Bug fixes; Performance Improvements. Version 0.2.62; New features; Bug fixes; Performance improvements. Version 0.2.61; New features; Bug fixes. Version 0.2.60; New features; Bug fixes; hailctl dataproc. Version 0.2.59; Datasets / Annotation DB; hailctl dataproc. Version 0.2.58; New features; Bug fixes; Performance improvements; hailctl dataproc; Deprecations. Version 0.2.57; New features. Version 0.2.56; New features; Performance; Bug fixes; hailctl dataproc. Version 0.2.55; Performance; Bug fixes; File Format. Version 0.2.54; VCF Combiner; New features; Bug fixes. Version 0.2.53; Bug fixes. Version 0.2.52; Bug fixes. Version 0.2.51; Bug fixes. Version 0.2.50; Bug fixes; New features. Version 0.2.49; Bug fixes. Version 0.2.48; Bug fixes. Version 0.2.47; Bug fixes. Version 0.2.46; Site; Bug,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:5505,Performance,perform,performance,5505,ug fixes. Version 0.2.50; Bug fixes; New features. Version 0.2.49; Bug fixes. Version 0.2.48; Bug fixes. Version 0.2.47; Bug fixes. Version 0.2.46; Site; Bug fixes. Version 0.2.45; Bug fixes; hailctl dataproc. Version 0.2.44; New Features; Bug fixes; Performance. Version 0.2.43; Bug fixes. Version 0.2.42; New Features; Bug fixes. Version 0.2.41; Bug fixes; hailctl dataproc. Version 0.2.40; VCF Combiner; Bug fixes. Version 0.2.39; Bug fixes; New features; Performance Improvements; Documentation. Version 0.2.38; Critical Linreg Aggregator Correctness Bug; Performance improvements. Version 0.2.37; Bug fixes; New features. Version 0.2.36; Critical Memory Management Bug Fix; Bug fixes. Version 0.2.35; Critical Memory Management Bug Fix; New features; Bug fixes; Performance Improvements; hailctl dataproc. Version 0.2.34; New features; Bug fixes; hailctl dataproc; File Format. Version 0.2.33; New features; Bug fixes; hailctl dataproc. Version 0.2.32; Critical performance regression fix; Performance; Bug fixes; New features; Cheat sheets. Version 0.2.31; New features; File size; Performance; Bug fixes. Version 0.2.30; Performance; New features; Miscellaneous. Version 0.2.29; Bug fixes; Performance improvements; New features; hailctl dataproc. Version 0.2.28; Critical correctness bug fix; Bug fixes; New Features; hailctl dataproc; Documentation. Version 0.2.27; New Features; Bug fixes; hailctl dataproc. Version 0.2.26; New Features; Bug Fixes; Performance Improvements; File Format. Version 0.2.25; New features; Bug fixes; Performance improvements; File Format. Version 0.2.24; hailctl dataproc; New features; Bug fixes. Version 0.2.23; hailctl dataproc; Bug fixes; New features; Performance. Version 0.2.22; New features; Performance; hailctl dataproc. Version 0.2.21; Bug fixes; New features; Performance; hailctl dataproc. Version 0.2.20; Critical memory management fix; Bug fixes; New features. Version 0.2.19; Critical performance bug fix; Bug fixes; Performance Improvements; ha,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:6478,Performance,perform,performance,6478,ormance regression fix; Performance; Bug fixes; New features; Cheat sheets. Version 0.2.31; New features; File size; Performance; Bug fixes. Version 0.2.30; Performance; New features; Miscellaneous. Version 0.2.29; Bug fixes; Performance improvements; New features; hailctl dataproc. Version 0.2.28; Critical correctness bug fix; Bug fixes; New Features; hailctl dataproc; Documentation. Version 0.2.27; New Features; Bug fixes; hailctl dataproc. Version 0.2.26; New Features; Bug Fixes; Performance Improvements; File Format. Version 0.2.25; New features; Bug fixes; Performance improvements; File Format. Version 0.2.24; hailctl dataproc; New features; Bug fixes. Version 0.2.23; hailctl dataproc; Bug fixes; New features; Performance. Version 0.2.22; New features; Performance; hailctl dataproc. Version 0.2.21; Bug fixes; New features; Performance; hailctl dataproc. Version 0.2.20; Critical memory management fix; Bug fixes; New features. Version 0.2.19; Critical performance bug fix; Bug fixes; Performance Improvements; hailctl dataproc. Version 0.2.18; Critical performance bug fix; Bug fixes. Version 0.2.17; New features; Bug fixes; Experimental; File Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail com,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:6579,Performance,perform,performance,6579,"sion 0.2.31; New features; File size; Performance; Bug fixes. Version 0.2.30; Performance; New features; Miscellaneous. Version 0.2.29; Bug fixes; Performance improvements; New features; hailctl dataproc. Version 0.2.28; Critical correctness bug fix; Bug fixes; New Features; hailctl dataproc; Documentation. Version 0.2.27; New Features; Bug fixes; hailctl dataproc. Version 0.2.26; New Features; Bug Fixes; Performance Improvements; File Format. Version 0.2.25; New features; Bug fixes; Performance improvements; File Format. Version 0.2.24; hailctl dataproc; New features; Bug fixes. Version 0.2.23; hailctl dataproc; Bug fixes; New features; Performance. Version 0.2.22; New features; Performance; hailctl dataproc. Version 0.2.21; Bug fixes; New features; Performance; hailctl dataproc. Version 0.2.20; Critical memory management fix; Bug fixes; New features. Version 0.2.19; Critical performance bug fix; Bug fixes; Performance Improvements; hailctl dataproc. Version 0.2.18; Critical performance bug fix; Bug fixes. Version 0.2.17; New features; Bug fixes; Experimental; File Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Ha",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:16497,Performance,cache,cache,16497," require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, i",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:21357,Performance,latency,latency,21357,".default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:24778,Performance,optimiz,optimization,24778,"f the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields para",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:27164,Performance,perform,performance,27164,"or Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:27476,Performance,optimiz,optimization,27476,"relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:30115,Performance,perform,performance,30115," of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem i",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:33234,Performance,perform,performs,33234,"containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail q",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:39300,Performance,perform,performance,39300,"l pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:43220,Performance,perform,performance,43220," Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; value",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:46359,Performance,perform,performance,46359,"_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries much faster by caching compiler data; structures.; (#12038) Speed up; hl.import_matrix_table by caching header line computation. Bug fixes. (#12115) When using; use_new_shuffle=True, fix a bug when there are more than 2^31; rows; (#12074) Fix bug; where hl.init could silently overwrite the global random seed.; (#12079) Fix bug in; handling of missing (aka NA) fields in grouped aggregation and; distinct by key.; (#12056) Fix; hl.export_vcf to actually create tabix f",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:54795,Performance,perform,performance,54795,"sible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ndarrays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in additi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:56341,Performance,optimiz,optimized,56341,"rabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:57721,Performance,perform,performance,57721,"titions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() n",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:59754,Performance,perform,performance,59754," ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /home/hail by default. Version 0.2.68; Released 2021-05-27. Version 0.2.67. Critical performance fix; Released 2021-05-06. (#10451) Fixed a; memory leak / performance bug triggered by; hl.literal(...).contains(...). Version 0.2.66; Released 2021-05-03. New features. (#10398) Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:59825,Performance,perform,performance,59825,"-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /home/hail by default. Version 0.2.68; Released 2021-05-27. Version 0.2.67. Critical performance fix; Released 2021-05-06. (#10451) Fixed a; memory leak / performance bug triggered by; hl.literal(...).contains(...). Version 0.2.66; Released 2021-05-03. New features. (#10398) Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:62530,Performance,perform,performance,62530,"ssage from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performance of hl.experimental.densify by approximately 35%. Version 0.2.61; Released 2020-12-03. New features. (#9749) Add or_error; method to SwitchBuilder (hl.switch). Bug fixes. (#9775) Fixed race; condition leading to invalid intermediate files in VCF combiner.; (#9751) Fix bug where; constructing an array of empty structs causes type error.; (#9731) Fix error and; incorrect behavior when using hl.import_matrix_table with int64; data types. Version 0.2.60; Released 2020-11-16. New features. (#9696); hl.experimental.export_elasticsearch will now support; Elasticsearch versions 6.8 - 7.x by default. Bug fixes. (#9641) Showing hail; ndarray data now always prints in correct order. hailctl dataproc. (#9610) Support; interval fields in hailctl dataproc describe. Version 0.2.59; Released 2020-10-22. Datasets / Annotation DB. (#9605) The Datasets; API and the Annotation Database now support AWS, and users are; required to specify what cloud platform they’re using. hailctl datapr",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:63831,Performance,cache,cache,63831,"ting an array of empty structs causes type error.; (#9731) Fix error and; incorrect behavior when using hl.import_matrix_table with int64; data types. Version 0.2.60; Released 2020-11-16. New features. (#9696); hl.experimental.export_elasticsearch will now support; Elasticsearch versions 6.8 - 7.x by default. Bug fixes. (#9641) Showing hail; ndarray data now always prints in correct order. hailctl dataproc. (#9610) Support; interval fields in hailctl dataproc describe. Version 0.2.59; Released 2020-10-22. Datasets / Annotation DB. (#9605) The Datasets; API and the Annotation Database now support AWS, and users are; required to specify what cloud platform they’re using. hailctl dataproc. (#9609) Fixed bug; where hailctl dataproc modify did not correctly print; corresponding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9474) Add; ArrayExpression.first and ArrayExpression.last.; (#9459) Add; StringExpression.join, an analogue to Python’s str.join.; (#9398) Hail will now; throw HailUserErrors if the or_error branch of a; CaseBuilder is hit. Bug fixes. (#9503) NDArrays can; now hold arbitrary data types, though only ndarrays of primitives can; be collected to Python.; (#9501) Remove memory; leak in BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9424); hl.experimental.writeBlockMatrices didn’t correctly support; overwrite flag. Performance improvements. (#9506); hl.agg.ndarray_sum will now do a tree aggregation. hailctl dataproc. (#9502) Fix hailctl; dataproc modify to install dependencies of the wheel file.; (#9420) Add; --debug-mode flag to hailctl dataproc start. This will enable; heap dumps on OOM errors.; (#9520) Add support; for requester pays buckets to hailctl dataproc desc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:66643,Performance,perform,performance,66643,"d support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:67976,Performance,perform,performance,67976," key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Released 2020-07-08. Bug fixes. (#9058) Fixed memory; leak affecting Table.aggregate, MatrixTable.annotate_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.agg.linreg with no aggregated data records.; (#9028) Fixed memory; leak affecting Table.annotate with scans,; hl.experimental.densify, and Table.group_by / aggregate.; (#8978) Fixed; aggregation behavior of; MatrixTable.{group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:70008,Performance,perform,performance,70008,"ggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:72481,Performance,perform,performance,72481,"8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compile",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:72572,Performance,perform,performing,72572,"le JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:77338,Performance,perform,performance,77338,"ur cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/t",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:77382,Performance,perform,performance,77382,"gions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:77518,Performance,perform,performance,77518,sions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:77577,Performance,perform,performance,77577,"s written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:77673,Performance,perform,performance,77673,"Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:77872,Performance,optimiz,optimizer,77872,"where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few ent",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:78956,Performance,perform,performance,78956,"8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:79075,Performance,perform,performance,79075,"ures. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:79208,Performance,perform,performance,79208," MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) F",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:79351,Performance,optimiz,optimizer,79351,"n to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Gene",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:79611,Performance,perform,performance,79611,"ime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:79680,Performance,perform,performance,79680,"888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:81229,Performance,optimiz,optimizer,81229,"doop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailct",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:82136,Performance,optimiz,optimizer,82136,"ug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailctl dataproc. (#7460) The Spark; monitor widget now automatically collapses after a job completes. Version 0.2.26; Released 2019-10-24. New Features. (#7325) Add; string.reverse function.; (#7328) Add; string.translate function.; (#7344) Add; hl.reverse_complement function.; (#7306) Teach the VCF; combiner to handle allele specific (AS_*) fields.; (#7346) Add; hl.agg.approx_median function. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expre",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:82728,Performance,perform,performance,82728,"imental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailctl dataproc. (#7460) The Spark; monitor widget now automatically collapses after a job completes. Version 0.2.26; Released 2019-10-24. New Features. (#7325) Add; string.reverse function.; (#7328) Add; string.translate function.; (#7344) Add; hl.reverse_complement function.; (#7306) Teach the VCF; combiner to handle allele specific (AS_*) fields.; (#7346) Add; hl.agg.approx_median function. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMat",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:83628,Performance,optimiz,optimizer,83628,"on. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configurati",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:83701,Performance,perform,performance,83701,"ve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:83805,Performance,perform,performance,83805," Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:85489,Performance,perform,perform,85489,"od on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:85962,Performance,perform,performance,85962,"dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value di",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:86009,Performance,perform,performance,86009,"ks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:86062,Performance,perform,performance,86062," disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint.",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:86149,Performance,perform,performance,86149," mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:86370,Performance,perform,performance,86370,"Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) A",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:86449,Performance,perform,performance,86449,"generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:87117,Performance,perform,performance,87117," (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
